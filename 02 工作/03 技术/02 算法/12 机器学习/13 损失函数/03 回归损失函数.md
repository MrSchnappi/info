# 回归损失函数

- 均方误差损失 MSE
- 平均绝对值误差 MAE
- 平滑的平均绝对误差 Huber 损失函数
- Log-Cosh 损失
- 分位数损失（没懂）
- 损失函数对比


# 均方误差损失 MSE

又叫做：

- 均方误差损失
- 平方损失损失
- L2 损失
- quadratic cost
- MSE

公式：



$$
L(Y, f(x)) = \sum_N{(Y-f(x))}^2
$$

场景：

- 是最常用的回归损失函数


说明：

- 它只考虑误差的平均大小，不考虑其方向。
- 平方损失函数是光滑函数，能够用梯度下降法进行优化。


缺点：

- 但由于经过平方，与真实值偏离较多的预测值会比偏离较少的预测值受到更为严重的惩罚。当预测值距离真实值越远时，平方损失函数的惩罚力度越大，因此它**对异常点比较敏感**。

# 平均绝对值误差 MAE

也称为：

- L1 损失

公式：

$$
L(Y, f(x)) = |Y-f(x)|​
$$

场景：

- 用于回归模型的损失函数。

说明：

- 平均绝对误差（MAE）其只衡量了预测值误差的平均模长，而不考虑方向，取值范围也是从 0 到正无穷。
- 绝对损失函数相当于是在做中值回归，相比做均值回归的平方损失函数，绝对损失函数对异常点更鲁棒一些。但是，绝对损失函数在 $f=y$ 处无法求导数。


如果考虑方向，则是残差/误差的总和——平均偏差 MBE。<span style="color:red;">这个是用在什么场景？</span>



# 平滑的平均绝对误差 Huber



$$
L_{\delta}(y, f(x))=\left\{\begin{array}{ll}{\frac{1}{2}(y-f(x))^{2}} & {\text { for }|y-f(x)| \leq \delta} \\ {\delta|y-f(x)|-\frac{1}{2} \delta^{2}} & {\text { otherwise }}\end{array}\right.
$$


介绍：

- 相当于是 MSE 与 MAE 的结合。当 Huber 损失在 $[0-\delta, 0+\delta]$ 之间时，等价为 MSE，而在 $[-\infty, \delta]$ 和 $[\delta,+\infty]$ 时为 MAE。
- 对数据中的异常点没有平方误差损失那么敏感。它在 0 也可微分。

来源：

- 对于平均绝对值误差MAE 来说，最大的一个问题就是不变的大梯度可能会导致在梯度下降快要结束时，错过了最小点。此时对于均方误差 MSE 来说，，梯度会随着损失的减小而减小，使结果更加精确。因此，在这种情况下，Huber损失就非常有用。它会由于梯度的减小而落在最小值附近。


注意：

- 超参数 $\delta$ 的选择非常重要，因为这决定了你对与异常点的定义。当残差大于 $\delta$，应当采用 L1（对较大的异常值不那么敏感）来最小化，而残差小于超参数，则用 L2 来最小化。<span style="color:red;">什么是残差？</span>


均方误差损失函数、绝对损失函数、Huber 损失函数图示如下：

<p align="center">
    <img width="80%" height="70%" src="http://images.iterate.site/blog/image/20190407/zNMm3RmSRr1y.png?imageslim">
</p>


优点：

- Huber损失结合了 MSE 和 MAE 的优点。
  - 比起 MSE，它对异常点更加鲁棒。
  - 比起平均绝对值误差，在梯度下降快要结束时，更容易落到最小点处

缺点：

- 可能需要不断调整超参数 delta。<span style="color:red;">怎么调整？</span>

# Log-Cosh损失

来源：

- 比 L2 更平滑的的损失函数。
- 比 Huber 更好。

计算：

- 预测误差的双曲余弦的对数。

$$
L\left(y, y^{p}\right)=\sum_{i=1}^{n} \log \left(\cosh \left(y_{i}^{p}-y_{i}\right)\right)
$$

<span style="color:red;">p 哪里来的？</span>

应用：

- 回归问题


<p align="center">
    <img width="80%" height="70%" src="http://images.iterate.site/blog/image/20190902/unS5PiUEo4Km.png?imageslim">
</p>



优点：

- 对于较小的 $x$，$log(cosh(x))$ 近似等于 $(x^2)/2$，对于较大的 $x$，近似等于 $abs(x)-log(2)$。这意味着 $log(cosh(x))$ 基本类似于均方误差，但不易受到异常点的影响。
- 它具有 Huber 损失所有的优点，但不同于 Huber 损失的是，Log-cosh二阶处处可微。
  - **为什么需要二阶处处可微？**
  - 许多机器学习模型如 XGBoost，就是采用牛顿法来寻找最优点。而牛顿法就需要求解二阶导数（Hessian）。因此对于诸如 XGBoost 这类机器学习框架，损失函数的二阶可微是很有必要的。


缺点：

- 误差很大的话，一阶梯度和 Hessian 会变成定值，这就导致 XGBoost 出现缺少分裂点的情况。**？**



# 分位数损失

<span style="color:red;">没懂？到底是什么？</span>

**？**

场景：

- 想了解区间预测的损失。因为，知道预测的范围而非仅是估计点，对许多商业问题的决策很有帮助。


使用最小二乘回归进行区间预测，基于的假设是残差 $(y-y_hat)$是独立变量，且方差保持不变。

一旦违背了这条假设，那么线性回归模型就不成立。<span style="color:red;">是的。</span>

但是我们也不能因此就认为使用非线性函数或基于树的模型更好，而放弃将线性回归模型作为基线方法。

这时，分位数损失和分位数回归就派上用场了，因为即便对于具有变化方差或非正态分布的残差，基于分位数损失的回归也能给出合理的预测区间。

下面让我们看一个实际的例子，以便更好地理解基于分位数损失的回归是如何对异方差数据起作用的。

## 分位数回归与最小二乘回归

<p align="center">
    <img width="80%" height="70%" src="http://images.iterate.site/blog/image/20190902/j4SnaE7CSRyh.png?imageslim">
</p>

- 左：$X_1$和 $Y$ 为线性关系。具有恒定的残差方差。
- 右：$X_2$和 $Y$ 为线性关系，但 $Y$ 的方差随着 $X_2$ 增加。（异方差）


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/82lCIDfQoubr.png?imageslim">
</p>


橙线表示两种情况下 OLS 的估值(普通最小二乘法（OLS, ordinary least squares)

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/5rfTapV0mJWi.png?imageslim">
</p>


分位数回归。虚线表示基于 0.05 和 0.95 分位数损失函数的回归

## 理解分位数损失函数

如何选取合适的分位值取决于我们对正误差和反误差的重视程度。损失函数通过分位值（$\gamma$）对高估和低估给予不同的惩罚。例如，当分位数损失函数 $\gamma =0.25$ 时，对高估的惩罚更大，使得预测值略低于中值。

$$
L_{\gamma}\left(y, y^{p}\right)=\sum_{i : y_{i}<y_{i}^{p}}(1-\gamma)\left|y_{i}-y_{i}^{p}\right|+\sum_{i : y_{i} \geq y_{i}^{p}} \gamma\left|y_{i}-y_{i}^{p}\right|
$$

$\gamma$ 是所需的分位数，其值介于 0 和 1 之间。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/l5b55IkcuykL.png?imageslim">
</p>


## 这个损失函数也可以在神经网络或基于树的模型中计算预测区间

这个损失函数也可以在神经网络或基于树的模型中计算预测区间。以下是用 Sklearn 实现梯度提升树回归模型的示例。

使用分位数损失（梯度提升回归器）预测区间：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/7HcbnQaPBOVP.png?imageslim">
</p>



上图表明：在 sklearn 库的梯度提升回归中使用分位数损失可以得到 90％的预测区间。其中上限为 $\gamma=0.95$，下限为 $\gamma=0.05$。

# 四种损失函数对比


首先，我们建立了一个从 $\operatorname{sinc}(x)$ 函数中采样得到的数据集，并引入了两项人为噪声：高斯噪声分量 $\varepsilon \sim N(0, \sigma 2)$ 和脉冲噪声分量 $\xi \sim \operatorname{Bern}(p)$。

加入脉冲噪声是为了说明模型的鲁棒效果。以下是使用不同损失函数拟合 GBM 回归器的结果。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/CKUUuuiJNNbs.png?imageslim">
</p>


连续损失函数：

- （A）MSE损失函数；
- （B）MAE损失函数；
- （C）Huber损失函数；
- （D）分位数损失函数。将一个平滑的 GBM 拟合成有噪声的 $\operatorname{sinc}(x)$ 数据的示例：
- （E）原始 $\operatorname{sinc}(x)$ 函数；
- （F）具有 MSE 和 MAE 损失的平滑 GBM；
- （G）具有 Huber 损失的平滑 GBM，且 $\delta=\{4,2,1\}$；
- （H）具有分位数损失的平滑的 GBM，且 $\alpha=\{0.5,0.1,0.9\}$。

仿真对比的一些观察结果：

- MAE损失模型的预测结果受脉冲噪声的影响较小，而 MSE 损失函数的预测结果受此影响略有偏移。
- Huber损失模型预测结果对所选超参数不敏感。
- 分位数损失模型在合适的置信水平下能给出很好的估计。

最后，让我们将所有损失函数都放进一张图：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/eP3boMEGDO4C.png?imageslim">
</p>

