# 采样

# 随机采样

两种类型：

- 随机欠采样
  - 从多数类 $S_{maj}$ 中随机选择少量样本 E 再合并原有少数类样本作为新的训练数据集，新数据集为 S_min+E。
    - 有放回
    - 无放回
- 随机过采样
  - 通过多次有放回随机采样从少数类 S_min 中抽取数据集 E，采样的数量要大于原有少数类的数量，最终的训练集为 S_maj+E。


目的：

- 随机采样通过修改样本分布，从而让样本分布较为均衡。

问题：

- 对于随机欠采样：由于采样后的样本要少于原样本集合，因此会造成一些信息缺失，未被采样的样本往往带有很重要的信息。
- 对于随机过采样：由于需要对少数类样本进行复制因此扩大了数据集，造成模型训练复杂度加大，另一方面也容易造成模型的过拟合问题。

问题的解决：

- 针对这些问题提出了几种其它的采样算法。



# SMOTE 算法

合成少数类过采样技术 SMOTE 

Synthetic Minority Oversampling Technique


由来：

- 基于随机过采样算法的一种改进方案，摒弃了随机过采样复制样本的做法。
- 想解决，在随机过采样时，由于只是简单复制样本来增加少数类样本而产生的过拟合的问题。


基本思想：

- 对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中。

算法流程：


0. 根据样本不平衡比例设置一个采样比例以确定采样倍率 $N$ <span style="color:red;">N 在哪里使用的？</span>
1. 对于少数类中每一个样本 $x$，计算它到少数类样本集 $S_{min}$ 中所有样本的欧式距离，得到其 $k$ 近邻。
2. 对于每一个少数类样本 $x$，从其 $k$ 近邻中随机选择若干个样本，假设选择的近邻为 $\hat{x}$。
3. 对于每一个随机选出的近邻 $\hat{x}$，分别与原样本按照如下的公式构建新的样本。


$$
x_{n e w}=x+\operatorname{rand}(0,1) \times(\hat{x}-x)
$$


示意图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190828/YNvpR1JsASo0.png?imageslim">
</p>


优点：

- 可以防止随机过采样易过拟合的问题，实践证明此方法可以提高分类器的性能。

问题：

- 由于对每个少数类样本都生成新样本，因此容易发生生成样本重叠(Overlapping)的问题

## Borderline-SMOTE 算法

由来：

- 为了解决 由于对每个少数类样本都生成新样本，因此发生的生成样本重叠(Overlapping)问题

算法流程：

- 在 Borderline-SMOTE中，若少数类样本的每个样本 $x_i$ 求 $k$ 近邻，记作 $S_i-knn$，且 $S_i-knn$ 属于整个样本集合 $S$ 而不再是少数类样本
- 若满足 $\frac{\mathrm{k}}{2}<\left|S_{\mathrm{i}-\mathrm{knn}} \cap \mathrm{S}_{\mathrm{maj}}\right|<\mathrm{k}$，则将样本 $x_i$ 加入 DANGER 集合，可以看出 DANGER 集合代表了接近分类边界的样本。
- 将 DANGER 当作 SMOTE 种子样本的输入生成新样本。
  - 说明，当上述条件取 $=k$ 时，k 近邻中全部样本都是多数类，此样本不会被选择为种样本生成新样本，此情况下的样本为噪音。

示意图：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190828/4Del4IABf7af.png?imageslim">
</p>

可见：

- 噪音点与完全正确划分的点不参与新样本的生成。


# Informed undersampling 采样

（补充中文名翻译）


由来：

- 与 SMOTE 类似，来解决随机过采样容易发生的模型过拟合问题，也即欠采样造成的数据信息丢失问题。

Informed undersampling 采样的两种方法：

- EasyEnsemble 算法
- BalanceCascade 算法


EasyEnsemble 算法：（类似于随机森林的 Bagging 方法）

- 把数据划分为两部分，分别是多数类样本和少数类样本，对于多数类样本 S_maj，通过 n 次有放回抽样生成 n 份子集。
- 少数类样本分别和这 n 份样本合并训练一个模型，这样可以得到 n 个模型。
- 最终的模型是这 n 个模型预测结果的平均值。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190828/xTuLpiCdoXnS.png?imageslim">
</p>


BalanceCascade 算法：（是一种级联算法）

- 从多数类 S_maj中有效地选择 N 且满足 $\mid N\mid=\mid S_{min}\mid$，将 N 和 $S_{min}$ 合并为新的数据集进行训练，新训练集对每个多数类样本 $x_i$ 进行预测若预测对则 $S_maj=S_maj-x_i$。
- 依次迭代直到满足某一停止条件，最终的模型是多次迭代模型的组合。
