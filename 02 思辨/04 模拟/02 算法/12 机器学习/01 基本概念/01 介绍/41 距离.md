# 距离

- 各种距离的使用场景，以及他们的根本是什么？
  - 比如说是适用在迁移学习还是适用在 NLP 还是什么

有：

- 闵可夫斯基举例
  - 曼哈顿距离
  - 欧式距离
  - 契比雪夫距离
- 余弦相似度
- 马氏距离 马哈拉诺比斯距离
- 互信息
- Pearson 相关系数
- Jaccard 相关系数
- KL 散度与 JS 距离
- 最大均值差异 MMD
- Principal Angle
- A-distance
- 希尔伯特-施密特独立性系数 HSIC Hilbert-Schmidt Independence Criterion
- Wasserstein Distance
- 距离对比

# 闵可夫斯基距离

Minkowski distance

两个向量的 $p$ 阶距离：

$$
(|\mathbf{x}-\mathbf{y}|^p)^{1/p}
$$

亦：

$$
\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{p}\right)^{1 / p}
$$

当：

- $p=1$ 时就是曼哈顿距离 Manhattan distance
- $p=2$ 时就是欧氏距离 Euclidean distance
- 当 $p$ 趋近于无穷大时，闵可夫斯基距离转化成切比雪夫距离 Chebyshev distance

欧式：

$$
\sqrt{(\mathbf{x}-\mathbf{y})^\top (\mathbf{x}-\mathbf{y})}
$$

切比雪夫距离：


$$
\lim _{p \rightarrow \infty}\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^ p\right)^{\frac{1}{p}}=\max _{i=1}^{n}\left|x_{i}-y_{i}\right|
$$

# 余弦相似度

余弦相似度

计算：

- 通过计算两个向量的夹角余弦值来评估它们的相似度。


$$
\cos (\theta)=\frac{A \cdot B}{\|A\|\|B\|}=\frac{\sum_{i=1}^{n} A_{i} \times B_{i}}{\sqrt{\sum_{i=1}^{n}\left(A_{i}\right)^{2} \times \sqrt{\sum_{i=1}^{n}\left(B_{i}\right)^{2}}}}
$$

说明：

- 余弦值的范围在 $[-1,1]$ 之间。
  - 0度角的余弦值是 1
  - 而其他任何角度的余弦值都不大于 1
  - 最小值是-1。
- 值越趋近于 $1$，代表两个向量的方向越趋近于 $0$，它们的方向更加一致。相应的相似度也越高。
- 如果希望得到类似于距离的表示，将 1 减去余弦相似度即为余弦距离。因此，余弦距离的取值范围为 `[0，2]`，相同的两个向量余弦距离为 0。


注意：

- 结果是与向量的长度无关的，仅仅与向量的指向方向相关。余弦相似度通常用于正空间，因此给出的值为 0 到 1 之间。

应用：

- 余弦相似性最常用于高维正空间。
  - 例如在信息检索中，每个词项被赋予不同的维度，而一个文档由一个向量表示，其各个维度上的值对应于该词项在文档中出现的频率。余弦相似度因此可以给出两篇文档在其主题方面的相似度。 在信息检索的情况下，由于一个词的频率（TF-IDF权）不能为负数，所以这两个文档的余弦相似性范围从 0 到 1。并且，两个词的频率向量之间的角度不能大于 90°。
  - 另外，它通常用于文本挖掘中的文件比较。此外，在数据挖掘领域中，会用到它来度量集群内部的凝聚力。



## 余弦相似与欧氏距离的区别和联系

**区别**

假设 2人对三部电影的评分分别是 `A = [3, 3, 3]` 和 `B = [5, 5, 5]`

那么 2 人的：

- 欧式距离是 根号 12 = 3.46
- 余弦相似度是 1（方向完全一致）。

说明：

- 欧式距离和余弦相似度都能度量 2 个向量之间的相似度，但是
  - 欧式距离从 2 点之间的距离去考量
  - 余弦相似从 2 个向量之间的夹角去考量。

从上例可以发出，2人对三部电影的评价趋势是一致的，但是欧式距离并不能反映出这一点，余弦相似则能够很好地反应。余弦相似可以很好地规避指标刻度的差异，最常见的应用是计算 **文本的相似度** 。

**联系**

<p align="center">
    <img width="80%" height="70%" src="http://images.iterate.site/blog/image/20190915/V8fhcG2Xirl5.png?imageslim">
</p>


可见，归一化后计算的欧式距离是关于余弦相似的单调函数，可以认为归一化后，余弦相似与欧式距离效果是一致的（欧式距离越小等价于余弦相似度越大）。

因此可以将 **求余弦相似转为求欧式距离** ，余弦相似的计算复杂度过高，转为求欧式距离后，可以借助`KDTree`（KNN算法用到）或者`BallTree`（对高维向量友好）来降低复杂度。**？**


补充：（暂时没有合并进来）

对于两个向量 A 和 B，其余弦相似度定义为 $cos(A,B)=\frac{A\cdot B}{||A||_2||B||_2}$，即两个向量夹角的余弦，关注的是向量之间的角度关系，并不关心它们的绝对大小，其取值范围是 `[−1，1]`。

当一对文本相似度的长度差距很大、但内容相近时，如果使用词频或词向量作为特征，它们在特征空间中的的欧氏距离通常很大；而如果使用余弦相似度的话，它们之间的夹角可能很小，因而相似度高。<span style="color:red;">为什么在特征空间中，欧式距离会很大，但是余弦相似度没有那么大？这个地方没有特别理解，到底区别在哪里？</span>

此外，在文本、图像、视频等领域，研究的对象的特征维度往往很高，余弦相似度在高维情况下依然保持 “相同时为 1，正交时为 0，相反时为−1” 的性质，而欧氏距离的数值则受维度的影响，范围不固定，并且含义也比较模糊。<span style="color:red;">是的，这个倒是</span>


在一些场景，例如 Word2Vec 中，其向量的模长是经过归一化的，此时欧氏距离与余弦距离有着单调的关系，即：

$$||A-B||_2=\sqrt{2(1-cos(A,B))}$$

其中 $||A−B||_2$ 表示欧氏距离，$cos(A,B)$ 表示余弦相似度，$(1−cos(A,B))$ 表示余弦距离。在此场景下，如果选择距离最小（相似度最大）的近邻，那么使用余弦相似度和欧氏距离的结果是相同的。<span style="color:red;">有点不是特别清楚。要弄清楚。</span>


总体来说，欧氏距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异。

例如，统计两部剧的用户观看行为，用户 A 的观看向量为（0，1），用户 B 为（1，0）；此时二者的余弦距离很大，而欧氏距离很小；我们分析两个用户对于不同视频的偏好，更关注相对差异，显然应当使用余弦距离。<span style="color:red;">分析两个用户对于不同视频的偏好？没有很明白为什么这个地方适应余弦距离。</span>

而当我们分析用户活跃度，以登陆次数（单位：次）和平均观看时长（单位：分钟）作为特征时，余弦距离会认为（1，10）、（10，100）两个用户距离很近；但显然这两个用户活跃度是有着极大差异的，此时我们更关注数值绝对差异，应当使用欧氏距离。<span style="color:red;">嗯。</span>

特定的度量方法适用于什么样的问题，需要在学习和研究中多总结和思考，这样不仅仅对面试有帮助，在遇到新的问题时也可以活学活用。




## 余弦距离并不是一个严格定义的距离

距离的定义：

- 在一个集合中，如果每一对元素均可唯一确定一个实数，使得三条距离公理（正定性，对称性，三角不等式）成立，则该实数可称为这对元素之间的距离。

结论：

- 余弦距离满足正定性和对称性，但是不满足三角不等式，因此它并不是严格定义的距离。

证明：

**正定性**


根据余弦距离的定义，有：

$$dist(A,B)=1-cos\theta =\frac{||A||_2||B||_2-AB}{||A||_2||B||_2}$$

考虑到 $||A||_2||B||_2-AB\geq 0$，因此有 $dist(A,B)$ 恒成立。特别地，有：

$$dist(A,B)=0\Leftrightarrow ||A||_2||B||_2=AB\Leftrightarrow A=B$$

因此余弦距离满足正定性。

**对称性**

根据余弦距离的定义，有


$$dist(A,B)=\frac{||A||_2||B||_2-AB}{||A||_2||B||_2}=\frac{||B||_2||A||_2-AB}{||B||_2||A||_2} =dist(B,A)$$

因此余弦距离满足对称性。

**三角不等式**


该性质并不成立，下面给出一个反例。给定 A=（1，0），B=（1，1），C=（0，1），则有：

$$dist(A,B)=1-\frac{\sqrt{2}}{2}$$
$$dist(B,C)=1-\frac{\sqrt{2}}{2}$$
$$dist(A,C)=1$$

因此有

$$ dist(A,B)+dist(B,C)=2-\sqrt{2}< 1=dist(A,C)$$

假如面试时候紧张，一时想不到反例，该怎么办呢？此时可以思考余弦距离和欧氏距离的关系。从问题 1 中，我们知道单位圆上欧氏距离和余弦距离满足：

$$||A-B||=\sqrt{2(1-cos(A,B))}=\sqrt{2dist(A,B)}$$

即有如下关系：

$$dist(A,B)=\frac{1}{2}||A-B||^2$$

显然在单位圆上，余弦距离和欧氏距离的范围都是`[0，2]`。我们已知欧氏距离是一个合法的距离，而余弦距离与欧氏距离有二次关系，自然不满足三角不等式。

具体来说，可以假设 A 与 B、B与 C 非常近，其欧氏距离为极小量 u；此时 A、B、C虽然在圆弧上，但近似在一条直线上，所以 A 与 C 的欧氏距离接近于 2u。因此，A与 B、B与 C 的余弦距离为 $u^2/2$ ；A与 C 的余弦距离接近于 $2u^2$，大于 A 与 B、B与 C 的余弦距离之和。<span style="color:red;">嗯，是的。</span>




# 马氏距离

马哈拉诺比斯距离

Mahalanobis distance


意义：

- 表示数据的协方差距离。

**一种定义方式**

它是一种有效地计算两个未知样本集的相似度的方法。与欧氏距离不同的是它考虑各种特性之间的联系，并且是尺度无关的（Scale-invariant），即独立于测量尺度。

例如：

一条关于身高的信息会带来一条关于体重的信息，因为两者是有关联的。


对于一个均值为 $\mu=\left(\mu_{1}, \mu_{2}, \mu_{3}, \ldots, \mu_{p}\right)^{T}$ ，协方差矩阵为 $\Sigma$ 的多变量向量 $x=\left(x_{1}, x_{2}, x_{3}, \ldots, x_{p}\right)^{T}$，其马氏距离为：

$$
D_{M}(x)=\sqrt{(x-\mu)^{T} \Sigma^{-1}(x-\mu)}
$$


当 $\Sigma=\mathbf{I}$ 时，马氏距离退化为欧氏距离。


**另一种定义方式**

马氏距离也可以定义为两个服从同一分布并且其协方差矩阵为 $\Sigma$ 的随机变量 ${\vec  {x}}$ 与 ${\vec  {y}}$ 的差异程度：

$$
d(\vec{x}, \vec{y})=\sqrt{(\vec{x}-\vec{y})^{T} \Sigma^{-1}(\vec{x}-\vec{y})}
$$


如果协方差矩阵为单位矩阵，马哈拉诺比斯距离就简化为欧氏距离；如果协方差矩阵为对角阵，其也可称为正规化的欧氏距离。

$$
d(\vec{x}, \vec{y})=\sqrt{\sum_{i=1}^{p} \frac{\left(x_{i}-y_{i}\right)^{2}}{\sigma_{i}^{2}}}
$$

<span style="color:red;">上面这个式子没明白。</span>


其中 $\sigma_i$ 是 $x_{i}$ 的标准差。



# 互信息

互信息在什么时候使用的？

互信息

Mutual Information MI


概念：

- 两个随机变量的互信息是变量间相互依赖性的量度。

注意：

- 不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 $p(X,Y)$ 和分解的边缘分布的乘积 $p(X)p(Y)$ 的相似程度。**？**

互信息最常用的单位是 bit。


**定义**


两个离散随机变量 $X$ 和 $Y$ 的互信息：

$$
I(X ; Y)=\sum_{y \in Y} \sum_{x \in X} p(x, y) \log \left(\frac{p(x, y)}{p(x) p(y)}\right)
$$

说明：

- $p(x,y)$ 是 $X$ 和 $Y$ 的联合概率分布函数
- $p(x)$ 和 $p(y)$ 分别是 $X$ 和 $Y$ 的边缘概率分布函数。

在连续随机变量的情形下，求和被替换成了二重定积分：

$$
I(X ; Y)=\int_{Y} \int_{X} p(x, y) \log \left(\frac{p(x, y)}{p(x) p(y)}\right) d x d y
$$

说明：

- $p(x,y)$ 是 $X$ 和 $Y$ 的联合概率密度函数
- $p(x)$ 和 $ p(y)$ 分别是 $X$ 和 $Y$ 的边缘概率密度函数。

如果对数以 2 为基底，互信息的单位是 bit。


**互信息的理解** **？**

- 直观上，互信息度量 X 和 Y 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。例如，
  - 如果 X 和 Y 相互独立，则知道 X 不对 Y 提供任何信息，反之亦然，所以它们的互信息为零。
  - 在另一个极端，如果 X 是 Y 的一个确定性函数，且 Y 也是 X 的一个确定性函数，那么传递的所有信息被 X 和 Y 共享：知道 X 决定 Y 的值，反之亦然。因此，在此情形互信息与 Y（或 X）单独包含的不确定度相同，称作 Y（或 X）的熵。而且，这个互信息与 X 的熵和 Y 的熵相同。（这种情形的一个非常特殊的情况是当 X 和 Y 为相同随机变量时。）
- 互信息是 X 和 Y 的联合分布相对于假定 X 和 Y 独立情况下的联合分布之间的内在依赖性。 于是互信息以下面方式度量依赖性：$I(X; Y) = 0$ 当且仅当 X 和 Y 为独立随机变量。从一个方向很容易看出：当 X 和 Y 独立时，$p(x,y) = p(x) p(y)$，因此：$\log \left(\frac{p(x, y)}{p(x) p(y)}\right)=\log 1=0$
- 此外，互信息是非负的（即 $I(X;Y) ≥ 0$ ），而且是对称的（即 $I(X;Y) = I(Y;X)$）。

# 皮尔逊相关系数


皮尔逊积矩相关系数

Pearson product-moment correlation coefficient

PPMCC 或 PCCs

文章中常用 r 或 Pearson's r表示

说明：

- 用于度量两个变量 X 和 Y 之间的相关程度（**线性相关**）

值介于-1与 1 之间。

注意：

- Pearson 相关系数只能衡量线性相关性，但无法衡量非线性关系。如 y=x^2，x和 y 有很强的非线性关系。

**定义**


两个变量之间的皮尔逊相关系数定义为两个变量之间的协方差和标准差的商：

$$
\rho_{X, Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}=\frac{E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]}{\sigma_{X} \sigma_{Y}}
$$

上式定义了总体相关系数，常用希腊小写字母 ρ (rho) 作为代表符号。


估算样本的协方差和标准差，可得到样本相关系数(样本皮尔逊系数)，常用英文小写字母 r 代表：

$$
r=\frac{\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)\left(Y_{i}-\overline{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(Y_{i}-\overline{Y}\right)^{2}}}
$$


r 亦可由 $\left(X_{i}, Y_{i}\right)$ 样本点的标准分数均值估算，得到与上式等价的表达式：

$$
r=\frac{1}{n-1} \sum_{i=1}^{n}\left(\frac{X_{i}-\overline{X}}{\sigma_{X}}\right)\left(\frac{Y_{i}-\overline{Y}}{\sigma_{Y}}\right)
$$

其中 $\frac{X_{i}-\overline{X}}{\sigma_{X}}$、 $\overline{X}$ 及 $\sigma_{X}$ 分别是 $X_{i}$ 样本的标准分数、样本平均值和样本标准差。


范围： $[-1,1]$ ，绝对值越大表示（正/负）相关性越大。

# Jaccard 相关系数

雅卡尔指数

Jaccard index

Jaccard 相关系数是用于比较样本集的相似性与多样性的统计量。能够量度有限样本集合的相似度，其定义为两个集合交集大小与并集大小之间的比例：

$$
J(A, B)=\frac{|A \cap B|}{|A \cup B|}=\frac{|A \cap B|}{|A|+|B|-|A \cap B|}
$$

如果 A 与 B 完全重合，则定义 J(A,B) = 1。于是有

$$
0 \leq J(A, B) \leq 1
$$

雅卡尔距离（Jaccard distance）则用于量度样本集之间的不相似度，其定义为 1 减去雅卡尔系数，即

$$
d_{J}(A, B)=1-J(A, B)=\frac{|A \cup B|-|A \cap B|}{|A \cup B|}
$$


**应用例子**


**项目场景**：分析一个产品的竞品，譬如 app 的竞品、网站的竞品等等

**项目分析**：简单来说就是竞品分析，竞品分析有很多比较成熟的方法，竞品分析其实和推荐有着很大的相关性。譬如我要分析一个技术网站的竞品有哪些，通俗点说，就是看一个用户经常访问哪些网站、不同类的用户访问网站的偏好是什么、在同类技术网站里与之定位想进，用户人群相似的网站有哪些等等。抽象来看，即可得出两个关键词：用户和物品（或者说物品和竞品）。这个关键词是不是很熟悉？在推荐里我们经常会遇到 item 和 user 之间的相似度，那么竞品分析其实也可以同类化于相似度的计算问题。

**具体做法**：Jaccard相似度。

简单说下公式：

给定两个集合 A 和 B，A和 B 的 Jaccard 相似度 = |A与 B 的交集元素个数| / |A与 B 的并集元素个数|

那么这样一个公式是来应用到竞品分析中的呢？我们假设一个场景：

喜欢博客园的用户也喜欢浏览知乎、CSDN、Github等，喜欢知乎的用户也喜欢浏览 Github、InfoQ、V2EX、SegmentDefault、博客园等，假设我们根据浏览次数来进行排序，得到两个集合，那么我们可以简化为博客园和知乎的竞品分别为：

```
博客园=[知乎、CSDN、Github]
知乎=[Github、InfoQ、V2EX、SegmentDefault、博客园]
```

此时，第一版计算结果：博客园与知乎的 Jaccard 相似度为= 1 / 7=0.14

这是最简单的 Jaccard 相似度计算，然而我们发现，逛博客园的经常逛知乎，且知乎权重很高，但是他们俩的相似度却很低，只有 0.14，看起来好像并不符合常理，于是，我做了点修改，将需要计算的竞品本身也加入集合，即：

```
博客园=[博客园、知乎、CSDN、Github]
知乎=[知乎、Github、InfoQ、V2EX、SegmentDefault、博客园]
```

这样我们再来计算，得到第二版计算结果：博客园与知乎的 Jaccard 相似度 = 3 / 7 = 0.42

为什么我们要将竞品本身考虑进去呢？其实很简单，以博客园为例，我们的目的是找到博客园的竞品，分析出经常浏览博客园的用户还会经常浏览哪些同类技术网站，那么博客园的用户肯定是经常浏览博客园的，这点显而易见，一个物品本身也是自身的竞品。将要分析的竞品本身加入集合后就可避免我们第一次计算时出现的不符合常识的结果。

但是，还得思考一个问题，博客园对知乎的 Jaccard 相似度与知乎对博客园的 Jaccard 相似度应该是一样的吗？

按照前两次计算，我们认为是一样的，因为只是考虑的交集的个数，并没有考虑集合中元素所处的位置因素。然而实际上，集合中的元素位置其实是有先后之分的，按降序排列，即竞品相关度是越来越低的。此时未考虑元素的位置因素似乎也有悖尝试。举个例子：一个经常看博客园的用户，也会经常看知乎，那么一个经常看知乎的用户是否也代表也会经常看博客园呢？这个结论与我们给出的条件是相悖的：一个经常看知乎的用户，相比于博客园，更偏好于 Github。所以我们得到结论：两个竞品 A 和 B，A对 B 的重要性不一定等于 B 对 A 的重要性。

　　所以，我们对此进行进一步改进

```
博客园=[博客园、知乎、CSDN、Github]　　　　　　 ====》博客园 = [1.0,0.6,0.3,0.1]
知乎=[知乎、Github、InfoQ、V2EX、SegmentDefault、博客园]　  ====》知乎 = [1.0,0.55,0.15,0.14,0.11,0.05]
```

(注：竞品本身加入集合我设定权重为 1，其他竞品元素总分为 1)

此时，计算得到第三版计算结果：

博客园对知乎的 Jaccard 相似度 = （ 两者交集的权重得分和/ 两者权重总和 ) * 知乎在博客园集合中所占的权重 = ( 1+0.6+0.1+1+0.55+0.05 / (2+2) ）* 0.6 = （ 3.3 /4 ）* 0.6 = 0.495

知乎对博客园的 Jaccard 相似度 =  （ 两者交集的权重得分和/ 两者权重总和 ) * 博客园在知乎集合中所占的权重 =（ 1+0.6+0.1+1+0.55+0.05 / (2+2) ）* 0.05 = ( 3.3 /4 ）*0.05 = 0.04

由此可得，博客园与知乎的竞品相似度是不相同的，也符合常理

# KL 散度


KL散度

Kullback–Leibler divergence，简称 KLD

KL散度在信息系统中称为相对熵（relative entropy），在连续时间序列中称为 randomness，在统计模型推断中称为信息增益（information gain）。也称信息散度（information divergence）。

注意：

- 被俗称为距离，却不满足三条距离公理，不满足对称性和三角不等式。

应用：

- 常用于计算两个分布之间的差异

**说明**

KL散度是两个概率分布 P 和 Q 差别的非对称性的度量。 KL散度是用来度量使用基于 Q 的分布来编码服从 P 的分布的样本所需的额外的平均比特数。典型情况下，P表示数据的真实分布，Q表示数据的理论分布、估计的模型分布、或 P 的近似分布。


$$
D_{K L}(P \| Q)=-\sum_{x \in X} P(x) \log \frac{1}{P(x)}+\sum_{x \in X} P(x) \log \frac{1}{Q(x)}=\sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}
$$

**定义**


对于离散随机变量，其概率分布 P 和 Q的 KL 散度可按下式定义为

$$
D_{\mathrm{KL}}(P \| Q)=-\sum_{i} P(i) \ln \frac{Q(i)}{P(i)}
$$

等价于

$$
D_{\mathrm{KL}}(P \| Q)=\sum_{i} P(i) \ln \frac{P(i)}{Q(i)}
$$

即按概率 $P$ 求得的 $P$ 和 $Q$ 的对数商的平均值。KL散度仅当概率 P 和 Q 各自总和均为 1，且对于任何 i 皆满足 $Q(i)>0$ 及 $P(i)>0$ 时，才有定义。式中出现 $0\ln 0$ 的情况，其值按 0 处理。

对于连续随机变量，其概率分布 P 和 Q 可按积分方式定义为：

$$
D_{\mathrm{KL}}(P \| Q)=\int_{-\infty}^{\infty} p(x) \ln \frac{p(x)}{q(x)} \mathrm{d} x
$$

其中 p 和 q 分别表示分布 P 和 Q 的密度。

更一般的，若 P 和 Q 为集合 X 的概率测度，且 P 关于 Q 绝对连续，则从 P 到 Q 的 KL 散度定义为

$$
D_{\mathrm{KL}}(P \| Q)=\int_{X} \ln \frac{\mathrm{d} P}{\mathrm{d} Q} \mathrm{d} P
$$

其中，假定右侧的表达形式存在，则 $\frac{\mathrm{d} Q}{\mathrm{d} P}$ 为 Q 关于 P 的 R–N导数。

相应的，若 P 关于 Q 绝对连续，则

$$
D_{\mathrm{KL}}(P \| Q)=\int_{X} \ln \frac{\mathrm{d} P}{\mathrm{d} Q} \mathrm{d} P=\int_{X} \frac{\mathrm{d} P}{\mathrm{d} Q} \ln \frac{\mathrm{d} P}{\mathrm{d} Q} \mathrm{d} Q
$$

即为 P 关于 Q 的相对熵。


**特性**

因为对数函数是凸函数，所以 KL 散度的值为非负数。

$$
D_{\mathrm{KL}}(P \| Q) \geq 0
$$

由吉布斯不等式可知，当且仅当 P = Q时 $D_{\mathrm{kL}}(A | Q)$ 为零。

**不是真正的距离**

有时会将 KL 散度称为 KL 距离，但它并不满足距离的性质。

因为 KL 散度不具有对称性：从分布 P 到 Q 的距离通常并不等于从 Q 到 P 的距离。

$$
D_{\mathrm{KL}}(P \| Q) \neq D_{\mathrm{KL}}(Q \| P)
$$


KL散度也不满足三角不等式。


# JS距离

Jensen–Shannon divergence，基于 KL 散度发展而来，是对称度量：


JS散度度量了两个概率分布的相似度，基于 KL 散度的变体，解决了 KL 散度非对称的问题。

一般地，JS散度是对称的，其取值是 0 到 1 之间。

定义如下：



$$
J S\left(P_{1} \| P_{2}\right)=\frac{1}{2} K L\left(P_{1} \| \frac{P_{1}+P_{2}}{2}\right)+\frac{1}{2} K L\left(P_{2} \| \frac{P_{1}+P_{2}}{2}\right)
$$

即：

$$
JS(P||Q)= \frac{1}{2} D_{KL}(P||M) + \frac{1}{2} D_{KL}(Q||M)
$$

其中 $M=\frac{1}{2}(P+Q)$ 。

**存在的问题**

KL散度和 JS 散度度量的时候有一个问题：

如果两个分配 P,Q离得很远，完全没有重叠的时候，那么 KL 散度值是没有意义的，而 JS 散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为 0。梯度消失了。

<span style="color:red;">没懂。</span>

# 最大均值差异 MMD

最大均值差异

Maximum mean discrepancy MMD

最大均值差异是迁移学习中使用频率最高的度量。它度量在再生希尔伯特空间中两个分布的距离，是一种核学习方法。两个随机变量的 MMD 平方距离为


最大平均差异最先提出的时候用于双样本的检测（two-sample test）问题，用于判断两个分布 p 和 q 是否相同。它的基本假设是：如果对于所有以分布生成的样本空间为输入的函数 f，如果两个分布生成的足够多的样本在 f 上的对应的像的均值都相等，那么那么可以认为这两个分布是同一个分布。现在一般用于度量两个分布之间的相似性。

下面从任意空间到 RKHS 上介绍了 MMD 的计算。

**任意函数空间（arbitary function space）的 MMD**

具体而言，基于 MMD（maximize mean discrepancy）的统计检验方法是指下面的方式：基于两个分布的样本，通过寻找在样本空间上的连续函数 f，求不同分布的样本在 f 上的函数值的均值，通过把两个均值作差可以得到两个分布对应于 f 的 mean discrepancy。寻找一个 f 使得这个 mean discrepancy有最大值，就得到了 MMD。

最后取 MMD 作为检验统计量（test statistic），从而判断两个分布是否相同。如果这个值足够小，就认为两个分布相同，否则就认为它们不相同。同时这个值也用来判断两个分布之间的相似程度。如果用 F 表示一个在样本空间上的连续函数集，那么 MMD 可以用下面的式子表示：

$$
\mathrm{MMD}[\mathcal{F}, p, q] :=\sup _{f \in \mathcal{F}}\left(\mathbf{E}_{x \sim p}[f(x)]-\mathbf{E}_{y \sim q}[f(y)]\right)
$$


假设 X 和 Y 分别是从分布 p 和 q 通过独立同分布(iid)采样得到的两个数据集，数据集的大小分别为 m 和 n。基于 X 和 Y 可以得到 MMD 的经验估计(empirical estimate)为：

$$
\operatorname{MMD}[\mathcal{F}, X, Y] :=\sup _{f \in \mathcal{F}}\left(\frac{1}{m} \sum_{i=1}^{m} f\left(x_{i}\right)-\frac{1}{n} \sum_{i=1}^{n} f\left(y_{i}\right)\right)
$$

在给定两个分布的观测集 X,Y的情况下，这个结果会严重依赖于给定的函数集 F。为了能表示 MMD 的性质：当且仅当 p 和 q 是相同分布的时候 MMD 为 0，那么要求 F 足够 rich；另一方面为了使检验具有足够的连续性（be consistent in power），从而使得 MMD 的经验估计可以随着观测集规模增大迅速收敛到它的期望，F必须足够 restrictive。文中证明了当 F 是 universal RKHS上的（unit ball）单位球时，可以满足上面两个性质。

**再生核希尔伯特空间的 MMD（The MMD In reproducing kernel Hilbert Spaces）：**


这部分讲述了在 RHKS 上单位球（unit ball）作为 F 的时，通过有限的观测来对 MMD 进行估计，并且设立一些 MMD 可以用来区分概率度量的条件。
在 RKHS 上，每个 f 对应一个 feature map。在 feature map的基础上，首先对于某个分布 p 定义一个 mean embedding of p，它满足如下的性质：

$$
\mu_{p} \in \mathcal{H} \text { such that } \mathbf{E}_{x} f=\left\langle f, \mu_{p}\right\rangle_{\mathcal{H}} \text { for all } f \in \mathcal{H}
$$


mean embedding存在是有约束条件的。在 p 和 q 的 mean embedding存在的条件下，MMD的平方可以表示如下：

$$
\begin{aligned} \operatorname{MMD}^{2}[\mathcal{F}, p, q] &=\left[\sup _{\|f\|_{\mathcal{Y} \in 1}}\left(\mathbf{E}_{x}[f(x)]-\mathbf{E}_{y}[f(y)]\right)\right]^{2} \\ &=\left[\sup _{\|f\|_{\mathcal{Y} \in 1}}\left\langle\mu_{p}-\mu_{q}, f\right\rangle_{\mathcal{H}}\right]^{2} \\ &=\left\|\mu_{p}-\mu_{q}\right\|_{\mathcal{H}}^{2} \end{aligned}
$$

下面是关于 MMD 作为一个 Borel probability measures时，对 F 的一个约束及其证明，要求 F：be a unit ball in a universal RKHS。比如 Gaussian 和 Laplace RKHSs。进一步在给定了 RKHS 对应核函数，这个 MMD 的平方可以表示：

$$
\operatorname{MMD}^{2}[\mathcal{F}, p, q]=\mathbf{E}_{x, x^{\prime}}\left[k\left(x, x^{\prime}\right)\right]-2 \mathbf{E}_{x, y}[k(x, y)]+\mathbf{E}_{y, y^{\prime}}\left[k\left(y, y^{\prime}\right)\right]
$$

x和 x’分别表示两个服从于 p 的随机变量，y和 y‘分别表示服从 q 的随机变量。对于上面的一个统计估计可以表示为：

$$
\operatorname{MMD}[\mathcal{F}, X, Y]=\left[\frac{1}{m^{2}} \sum_{i, j=1}^{m} k\left(x_{i}, x_{j}\right)-\frac{2}{m n} \sum_{i, j=1}^{m, n} k\left(x_{i}, y_{j}\right)+\frac{1}{n^{2}} \sum_{i, j=1}^{n} k\left(y_{i}, y_{j}\right)\right]^{\frac{1}{2}}
$$


对于一个 two-sample test, 给定的 null hypothesis: p和 q 是相同，以及 the alternative hypothesis: p和 q 不等。这个通过将 test statistic和一个给定的阈值相比较得到，如果 MMD 大于阈值，那么就 reject null hypothesis，也就是两个分布不同。如果 MMD 小于某个阈值，就接受 null hypothesis。由于 MMD 的计算时使用的是有限的样本数，这里会出现两种类型的错误：第一种错误出现在 null hypothesis被错误的拒绝了；也就是本来两个分布相同，但是却被判定为相同。反之，第二种错误出现在 null hypothesis被错误的接受了。文章[1]中提供了许多关于 hypothesis test的方法，这里不讨论。

在 domain adaptation中，经常用到 MMD 来在特征学习的时候构造正则项来约束学到的表示，使得两个域上的特征尽可能相同。从上面的定义看，我们在判断两个分布 p 和 q 的时候，需要将观测样本首先映射到 RKHS 空间上，然后再判断。但实际上很多文章直接将观测样本用于计算，省了映射的那个步骤。


# Principal Angle


也是将两个分布映射到高维空间(格拉斯曼流形)中，在流形中两堆数据就可以看成两个点。Principal angle是求这两堆数据的对应维度的夹角之和。

对于两个矩阵 $\mathbf{X},\mathbf{Y}$ ，计算方法：首先正交化(用 PCA)两个矩阵，然后：


$$
PA(\mathbf{X},\mathbf{Y})=\sum_{i=1}^{\min(m,n)} \sin \theta_i
$$

其中 $m,n$ 分别是两个矩阵的维度， $\theta_i$ 是两个矩阵第 $i$ 个维度的夹角， $\Theta=\{\theta_1,\theta_2,\cdots,\theta_t\}$ 是两个矩阵 SVD 后的角度：

$$
\mathbf{X}^\top\mathbf{Y}=\mathbf{U} (\cos \Theta) \mathbf{V}^\top
$$


# A-distance

$\mathcal{A}$ -distance 是一个很简单却很有用的度量。它可以用来估计不同分布之间的差异性。

$\mathcal{A}$ -distance被定义为建立一个线性分类器来区分两个数据领域的 hinge 损失(也就是进行二类分类的 hinge 损失)。

它的计算方式是，我们首先在源域和目标域上训练一个二分类器 $h$ ，使得这个分类器可以区分样本是来自于哪一个领域。我们用 $err(h)$ 来表示分类器的损失，则 $\mathcal{A}$ -distance定义为：

$$
\mathcal{A}(\mathcal{D}_s,\mathcal{D}_t) = 2(1 - 2 err(h))
$$

$\mathcal{A}$ -distance通常被用来计算两个领域数据的相似性程度，以便与实验结果进行验证对比。

# 希尔伯特-施密特独立性系数

HSIC

Hilbert-Schmidt Independence Criterion

用来检验两组数据的独立性：

$$
HSIC(X,Y) = trace(HXHY)
$$

其中 $X,Y$ 是两堆数据的 kernel 形式。



# Wasserstein Distance

- 学好实变和测度论对机器学习是很有帮助的。

可以安全的把概率测度(probability measure)理解为概率分布(probability distribution)，只要我们关心的空间是 $\mathbb{R}^{n}$。两个概率分布之间的距离有很多种描述方式，一个比较脍炙人口的是 KL divergence:

$$
K L(p \| q)=\int_{\mathbb{R}^{n}} p(x) \log \frac{p(x)}{q(x)} d x
$$

尽管它严格意义上不是一个距离(比如不满足对称性)。从定义可以看出，KL并不关心 $\mathbb{R}^{n}$ 的几何性质，因为 $p$ 和 $q$ 的比较都是在同一点进行的(换句话说，只要 $x_{1} \neq x_{2}$，KL并不 care $p\left(x_{1}\right) / q\left(x_{2}\right)$ 的大小)。举个例子，考虑如下两个一维高斯分布：$\mathrm{p}=\mathcal{N}\left(0, \epsilon^{3}\right)$ 和 $q=\mathcal{N}\left(\epsilon, \epsilon^{3}\right)$ ，借蛮力可算出 $K L(p \| q)=\frac{1}{2 \epsilon}$ 。q只是 p 的一个微小平移，但当平移量趋于 0 时，KL却 blow up了！

这就激励我们定义一种分布间的距离，使其能够把 $\mathbb{R}^{n}$ 的几何/度量性质也考虑进去。Wasserstein distance就做到了这一点，而且是高调的做到了这一点，因为 $d(x, y)$ 显式的出现在了定义中。具体的，对于定义在 $\mathbb{R}^{n}$ 上的概率分布 $\mu$ 和 $\nu$，

$$
W_{p}(\mu, \nu) :=\inf _{\gamma \in \Gamma(\mu, \nu)}\left(\int_{\mathbb{R}^{n} \times \mathbb{R}^{n}} d(x, y)^{p} d \gamma(x, y)\right)^{1 / p}=\left(\inf _{\xi} \mathbf{E}\left[d(x, y)^{p}\right]\right)^{1 / p}
$$

其中 $\xi$ 是一个 $\mathbb{R}^{n} \times \mathbb{R}^{n}$ 上的联合分布，必须同时满足 $\mu$ 和 $\nu$ 是其边缘分布。$d$ 可以是 $\mathbb{R}^{n}$ 上的任意距离，比如欧式距离，$L_{1}$ 距离等等。举个特例，当 $\mu=\delta_{x}$ 和 $\nu=\delta_{y}$ 时，唯一符合条件的 $\xi$ 只有 $\delta_{(x, y)}$，所以 $W_{p}(\mu, \nu)=d(x, y)$ ，两个 delta 分布间的距离正好等于它们中心间的距离，非常的符号直觉对不对！(因为建立了 $\mathbb{R}^{n}$ 和 delta 分布之间的 isometry)

刚才的例子也告诉我们，Wasserstein distance是可以定义两个 support 不重合，甚至一点交集都没有的分布之间的距离的，而 KL 在这种情况并不适用。

维基中也给出了两个正态分布的 Wasserstein distance ($p=2$ 时候) 的公式，大家可以去看一下，正好是两部分的和，一部分代表了中心间的几何距离，另一部分代表了两个分布形状上的差异。现在返回去看上面 KL 时候举的那个例子，它们之间的 Wasserstein distance正好是 $\epsilon$。

实际应用中 Wasserstein distance的计算大都依赖离散化，因为目前只对有限的几个分布存在解析解。对于任意分布 $\mu$ 我们可以用 delta 分布来逼近 $\mu \approx \frac{1}{n} \sum_{i=1}^{n} \delta_{x_{i}}$，这里并不要求 $x_i$ 是唯一的。对于 $\nu$ 做同样的近似 $\nu \approx \frac{1}{n} \sum_{i=1}^{n} \delta_{y_{i}}$。为什么 $\mu$ 和 $\nu$ 的近似能够取相同的 $n$？因为总是可以把当前的近似点拷贝几份然后 renormalize，所以取 $n$ 为两者原始近似点数量的最小公倍数即可。那么

$$
W_{p}(\mu, \nu) \approx W_{p}\left(\frac{1}{n} \sum_{i=1}^{n} \delta_{x_{i}}, \frac{1}{n} \sum_{i=1}^{n} \delta_{y_{i}}\right)=\min _{\sigma \in S_{n}}\left(\sum_{i=1}^{n} d\left(x_{\sigma_{i}}, y_{i}\right)^{p}\right)^{1 / p}
$$

这就变成了一个组合优化的问题：有 $n$ 个位于 $x_{i}$ 的石子，大自然借助水的力量将它们冲到的新的位置 $y_i$。就像光总是走最短路一样，大自然总是选取最短的路径来移动这些石子。这些石子总共移动的距离等于 Wasserstein distance。

## 定义

Wasserstein Distance是一套用来衡量两个概率分部之间距离的度量方法。该距离在一个度量空间 $(M,\rho)$ 上定义，其中 $\rho(x,y)$ 表示集合 $M$ 中两个实例 $x$ 和 $y$ 的距离函数，比如欧几里得距离。两个概率分布 $\mathbb{P}$ 和 $\mathbb{Q}$ 之间的 $p{\text{-th} }$ Wasserstein distance可以被定义为

$$
W_p(\mathbb{P}, \mathbb{Q}) = \Big(\inf_{\mu \in \Gamma(\mathbb{P}, \mathbb{Q}) } \int \rho(x,y)^p d\mu(x,y) \Big)^{1/p},
$$

其中 $\Gamma(\mathbb{P}, \mathbb{Q})$ 是在集合 $M\times M$ 内所有的以 $\mathbb{P}$ 和 $\mathbb{Q}$ 为边缘分布的联合分布。著名的 Kantorovich-Rubinstein定理表示当 $M$ 是可分离的时候，第一 Wasserstein distance可以等价地表示成一个积分概率度量(integral probability metric)的形式

$$
W_1(\mathbb{P},\mathbb{Q})= \sup_{\left \| f \right \|_L \leq 1} \mathbb{E}_{x \sim \mathbb{P} }[f(x)] - \mathbb{E}_{x \sim \mathbb{Q} }[f(x)],
$$

其中 $\left \| f \right \|_L = \sup{|f(x) - f(y)|} / \rho(x,y)$ 并且 $\left \| f \right \|_L \leq 1$ 称为 $1-$ 利普希茨条件。

# 距离对比

在数据标准化后，Pearson相关性系数、余弦相似(Cosine相似度)、欧式距离的平方可认为是等价的。这三种方法都是用来度量线性相关性的。
