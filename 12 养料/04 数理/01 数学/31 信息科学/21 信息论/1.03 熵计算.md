---
title: 1.03 熵计算
toc: true
date: 2019-09-15
---


# 例1

计算变量 $[0,0,1,1,1]$ 的信息熵。

- A $-(3/5 log(3/5) + 2/5 log(2/5))$
- B $3/5 log(3/5) + 2/5 log(2/5)$
- C $2/5 log(3/5) + 3/5 log(2/5)$
- D $3/5 log(2/5) – 2/5 log(3/5)$


**A**，公式：$H(X) = -sum(p(xi) * log(p(xi))) (i = 1, 2, 3, ... , n)$

香农的信息熵本质上是对“不确定现象”的数学化度量。

例如，如果我们用$1$代表下雨，0代表不下雨。$[0,0,1,1,1]$代表有 $3/5$ 的概率下雨，信息熵是 0.97；而 $[1,1,1,1,1]$ 代表$100%$的概率下雨，信息熵是 $-1 * log(5/5) = 0$。

完全确定的事件，信息熵为0；信息熵越大，代表不确定性越大。


## 投掷均匀正六面体骰子的熵


2.6比特


如何计算信息量的多少？在日常生活中，极少发生的事件一旦发生是容易引起人们关注的，而司空见惯的事不会引起注意，也就是说，极少见的事件所带来的信息量多。

如果用统计学的术语来描述，就是出现概率小的事件信息量多。因此，事件出现得概率越小，信息量愈大。即信息量的多少是与事件发生频繁(即概率大小)成反比。

1.如已知事件 $X_i$已发生，则表示$X_i$所含有或所提供的信息量


$$H(X_i) = − logP(X_i)$$

例题：若估计在一次国际象棋比赛中谢军获得冠军的可能性为0.1(记为事件A)，而在另一次国际象棋比赛中她得到冠军的可能性为0.9(记为事件B)。试分别计算当你得知她获得冠军时，从这两个事件中获得的信息量各为多少？

$$H(A)=-log2 P(0.1)≈3.32(比特)$$

$$H(B)=-log2 P(0.9)≈0.152(比特)$$


2.统计信息量的计算公式为：

$$H(X)=sum^{n}_{i=1}P(X_i)log P(X_i)$$

$X_i$ —— 表示第i个状态(总共有n种状态)；

$P(X_i)$——表示第i个状态出现的概率；

$H(X)$——表示用以消除这个事物的不确定性所需要的信息量。

例题：向空中投掷硬币，落地后有两种可能的状态，一个是正面朝上，另一个是反面朝上，每个状态出现的概率为1/2。如投掷均匀的正六面体的骰子，则可能会出现的状态有6个，每一个状态出现的概率均为1/6。试通过计算来比较状态的不肯定性与硬币状态的不肯定性的大小。

$$H(硬币)= -\sum^{n}_{i=1}P(X_i)log P(X_i)= -(2×1/2)×logP2(1/2)≈1(比特)$$

$$H(骰子)= -\sum^{n}_{i=1}P(X_i)log P(X_i)= -6×(1/6)×logP2(1/6)≈2.6(比特)$$

由以上计算可以得出两个推论：

- [推论1] 当且仅当某个$P(X_i)=1$，其余的都等于$0$时， $H(X)= 0$。

- [推论2]当且仅当某个$P(X_i)=1/n$，$i=1， 2，……， n$时，$H(X)$有极大值$log n$。
