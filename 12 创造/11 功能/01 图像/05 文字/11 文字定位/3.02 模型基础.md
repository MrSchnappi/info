---
title: 3.02 模型基础
toc: true
date: 2019-09-15
---

## 模型基础

图文识别任务中充当特征提取模块的基础网络，可以来源于通用场景的图像分类模型。例如，VGGNet，ResNet、InceptionNet、DenseNet、Inside-Outside Net、Se-Net等。

图文识别任务中的基础网络，也可以来源于特定场景的专用网络模型。例如，擅长提取图像细节特征的 FCN 网络，擅长做图形矫正的 STN 网络。

由于大家对通用网络模型已经很熟悉，所以本节只简单介绍上述专用网络模型。

**FCN网络**

全卷积网络（FCN,fully convolutional network）， 是去除了全连接(fc)层的基础网络，最初是用于实现语义分割任务。FC的优势在于利用反卷积（deconvolution）、上池化（unpooling）等上采样（upsampling）操作，将特征矩阵恢复到接近原图尺寸，然后对每一个位置上的像素做类别预测，从而能识别出更清晰的物体边界。基于 FCN 的检测网络，不再经过候选区域回归出物体边框, 而是根据高分辨率的特征图直接预测物体边框。因为不需要像 Faster-RCNN那样在训练前定义好候选框长宽比例，FCN在预测不规则物体边界时更加鲁棒。由于 FCN 网络最后一层特征图的像素分辨率较高，而图文识别任务中需要依赖清晰的文字笔画来区分不同字符（特别是汉字），所以 FCN 网络很适合用来提取文本特征。当 FCN 被用于图文识别任务时，最后一层特征图中每个像素将被分成文字行（前景）和非文字行（背景）两个类别。

![img](https://pic1.zhimg.com/80/v2-ecdde0317e149eedf2825fdd5e9907d8_hd.jpg)（选自 arXiv:1411.4038，’ Fully Convolutional Networks for Semantic Segmentation’）

**STN网络**

空间变换网络（STN，Spatial Transformer Networks）的作用是对输入特征图进行空间位置矫正得到输出特征图，这个矫正过程是可以进行梯度传导的，从而能够支持端到端的模型训练。

如下图所示，STN网络由定位网络（Localization Network） ，网格生成器（Grid generator），采样器（Sampler）共 3 个部分组成。定位网络根据原始特征图 U 计算出一套控制参数，网格生成器这套控制参数产生采样网格（sampling grid），采样器根据采样网格核函数将原始图 U 中像素对应采样到目标图 V 中。

空间变换的控制参数是根据原始特征图 U 动态生成的，生成空间变换控制参数的元参数则是在模型训练阶段学习到的、并且存放于定位网络的权重（weights）矩阵中。

![img](https://pic4.zhimg.com/80/v2-594664de3bd24e28ca90e29bb182b7c3_hd.jpg)（选自 arXiv: 1506.02025，’Spatial Transformer Networks’）
