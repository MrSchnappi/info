---
title: 2.01 特征值和特征向量
toc: true
date: 2019-09-03
---

### 1.3.1 特征值分解与特征向量

特征值分解可以得到特征值(eigenvalues)与特征向量(eigenvectors)；

特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。<span style="color:red;">嗯。</span>

如果说一个向量 $\vec{v}$ 是方阵 $A$ 的特征向量，将一定可以表示成下面的形式：

$$
A\nu = \lambda \nu
$$

$\lambda$ 为特征向量 $\vec{v}$ 对应的特征值。特征值分解是将一个矩阵分解为如下形式：

$$
A=Q\sum Q^{-1}
$$

其中，$Q$ 是这个矩阵 $A$ 的特征向量组成的矩阵，$\sum$ 是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。<span style="color:red;">厉害呀，一般为什么要进行这个分解？而且，所有的都可以进行特征分解吗？</span>也就是说矩阵 $A$ 的信息可以由其特征值和特征向量表示。







# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
