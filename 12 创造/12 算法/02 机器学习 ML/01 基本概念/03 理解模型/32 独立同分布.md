---
title: 32 独立同分布
toc: true
date: 2018-07-30 09:41:46
---




## 独立同分布

通常假设样本空间中全体样本服从一个未知 “分布” (distribution)  $\mathcal{D}$ ，我们获得的每个样本都是独立地从这个分布上采样获得的，即 “独立同分布” (independent and identically distributed，简称 i.i.d. )。

一般而言，训练样本越多，我们得到的关于  $\mathcal{D}$ 的信息越多，这样就越有可能通过学习获得具有强泛化能力的模型。 <span style="color:red;">为什么说，训练样本越多，我们得到的关于 $\mathcal{D}$ 的信息越多？</span>

<span style="color:red;">关于这个独立同分布，现实中真的有这样的基本假设存在吗？机器学习中的那些事基于这个独立同分布的？如果不满足独立同分布，是不是有些机器学习算法那就不能使用了？到底怎么判断一些东西是不是独立同分布？关于这个还是不是很清楚。</span>




### 训练集和测试集的数据收集

当我们只能观测到训练集时，如何才能影响测试集的性能呢？

统计学习理论(statistical learning theory)提供了一些答案。如果训练集和测试集的数据是任意收集的，那么我们能够做的确实很有限。如果可以对训练集和测试集数据的收集方式有些假设，那么我们能够对算法做些改进。<span style="color:red;">嗯。</span>

训练集和测试集数据通过数据集上被称为数据生成过程(data generating process)的概率分布生成。<span style="color:red;">嗯，通过数据集上被称为数据生成过程的概率分布生成。</span>通常，我们会做一系列被统称为独立同分布假设(i.i.d.assumption)的假设。<span style="color:red;">嗯，独立同分布假设。</span>该假设是说，每个数据集中的样本都是彼此相互独立的(independent)，并且训练集和测试集是同分布的(identically distributed)，采样自相同的分布。这个假设使我们能够在单个样本的概率分布描述数据生成过程。然后相同的分布可以用来生成每一个训练样本和每一个测试样本。我们将这个共享的潜在分布称为数据生成分布(data generating distribution)，记作 $p_{\mathrm{data}}$。<span style="color:red;">这个共享的潜在分布称为数据生成分布。嗯，这个之前知道是同分布的，但是不知道这个名字是数据生成分布。</span>这个概率框架和独立同分布假设允许我们从数学上研究训练误差和测试误差之间的关系。<span style="color:red;">嗯。</span>



我们能观察到训练误差和测试误差之间的直接联系是，随机模型训练误差的期望和该模型测试误差的期望是一样的。<span style="color:red;">嗯，对，两者的期望是一样的。</span>假设我们有概率分布 $p(\boldsymbol{x}, y)$，从中重复采样生成训练集和测试集。对于某个固定的 $\boldsymbol{w}$，训练集误差的期望恰好和测试集误差的期望一样，这是因为这两个期望的计算都使用了相同的数据集生成过程。这两种情况的唯一区别是数据集的名字不同。<span style="color:red;">嗯。</span>

当然，在使用机器学习算法时，我们不会提前固定参数，然后采样得到两个数据集。我们采样得到训练集，然后挑选参数去降低训练集误差，然后采样得到测试集。在这个过程中，测试误差期望会大于或等于训练误差期望。<span style="color:red;">嗯，在这个过程中，测试误差期望会大于或等于训练误差期望。</span>

# 相关

- 《机器学习》周志华
