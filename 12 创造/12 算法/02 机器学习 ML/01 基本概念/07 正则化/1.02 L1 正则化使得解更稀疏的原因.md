---
title: 1.02 L1 正则化使得解更稀疏的原因
toc: true
date: 2019-04-13
---
# 可以补充进来的

- 还是没懂。


# L1 正则化使得模型参数具有稀疏性的原理

## 角度 1：解空间形状



我们可以通过 KKT 条件给出一种解释。


事实上，“带正则项”和“带约束条件”是等价的。为了约束 $w$ 的可能取值空间从而防止过拟合，我们为该最优化问题加上一个约束，就是 $w$ 的 L2 范数的平方不能大于 $m$：<span style="color:red;">嗯。</span>


$$
\left\{\begin{array}{l}{\min \sum_{i=1}^{N}\left(y_{i}-w^{\mathrm{T}} x_{i}\right)^{2}} \\ {\text {s.t.}\|w\|_{2}^{2} \leqslant m}\end{array}\right. \tag{7.55}
$$


为了求解带约束条件的凸优化问题，写出拉格朗日函数：<span style="color:red;">嗯。</span>

$$
\sum_{i=1}^{N}\left(y_{i}-w^{\mathrm{T}} x_{i}\right)^{2}+\lambda\left(\|\boldsymbol{w}\|_{2}^{2}-m\right)\tag{7.56}
$$

若 $w^{*}$ 和 $\lambda^{*}$ 分别是原问题和对偶问题的最优解，则根据 KKT 条件，它们应满足；


$$
\left\{\begin{array}{l}{0=\nabla_{w}\left(\sum_{i=1}^{N}\left(y_{i}-w^{* \mathrm{T}} x_{i}\right)^{2}+\lambda^{*}\left(\left\|w^{*}\right\|_{2}^{2}-m\right)\right.} \\ {0 \leqslant \lambda^{*}}\end{array}\right.\tag{7.57}
$$

仔细一看，第一个式子不就是 $w^{*}$ 为带 L2 正则项的优化问题的最优解的条件嘛，而 $\lambda^{*}$ 就是 L2 正则项前面的正则参数。<span style="color:red;">嗯。好像有些明白。</span>

这时回头再看开头的问题就清晰了：L2正则化相当于为参数定义了一个圆形的解空间（因为必须保证 L2 范数不能大于 m），而 L1 正则化相当于为参数定义了一个棱形的解空间。L1 和 L2 的解空间是不同的。



<center>

![](http://images.iterate.site/blog/image/180629/943jCmBh5K.png?imageslim){ width=55% }

</center>

<center>

![](http://images.iterate.site/blog/image/20190413/xSb9wKReRwmR.png?imageslim){ width=55% }

</center>



在二维的情况下，黄色的部分是 L2 和 L1 正则项约束后的解空间，绿色的等高线是凸优化问题中目标函数的等高线，如图所示。

由图可知，L2正则项约束后的解空间是圆形，而 L1 正则项约束的解空间是多边形。显然，多边形的解空间更容易在尖角处与等高线碰撞出稀疏解。<span style="color:red;">为什么呢？</span>

采用 $\mathrm{L}_{1}$ 范数时平方误差项等值线与正则化项等值线的交点常出现在坐标轴上，即 $w_1$ 或 $w_2$ 为 0，<span style="color:red;">为什么呢？</span>而在采用 $\mathrm{L}_{2}$ 范数时，两者的交点常出现在某个象限中，即 $w_1$ 或 $w_2$ 均非 0；

换言之，采用范数 $\mathrm{L}_{1}$ 比 $\mathrm{L}_{2}$ 范数更易于得到稀疏解。

如果原问题目标函数的最优解不是恰好落在解空间内，那么约束条件下的最优解一定是在解空间的边界上，而 L1“棱角分明”的解空间显然更容易与目标函数等高线在角点碰撞，从而产生稀疏解。<span style="color:red;">原问题目标函数的最优解不是恰好落在解空间内，那么约束条件下的最优解一定是在解空间边界上。没有很懂。</span>

## 角度 2：函数叠加

第二个角度试图用更直观的图示来解释 L1 产生稀疏性这一现象。

仅考虑一维的情况，多维情况是类似的，如图下图所示。假设棕线是原始目标函数 $L(w)$ 的曲线图，显然最小值点在蓝点处，且对应的 $w^{\star}$ 值非 0。

<center>

![](http://images.iterate.site/blog/image/20190413/FsAL4pqvbiv4.png?imageslim){ width=55% }

</center>

- 首先，考虑加上 L2 正则化项，目标函数变成 $L(w)+C w^{2}$ ，其函数曲线为黄色。此时，最小值点在黄点处，对应的 $w^{\star}$ 的绝对值减小了，但仍然非 0。
- 然后，考虑加上 L1 正则化项，目标函数变成 $L(w)+C|w|$ ，其函数曲线为绿色。此时，最小值点在红点处，对应的 $w$ 是 0，产生了稀疏性。<span style="color:red;">为什么这个就产生了稀疏性？</span>

产生上述现象的原因也很直观：

- 加入 L1 正则项后，对带正则项的目标函数求导，正则项部分产生的导数在原点左边部分是 $−C$，在原点右边部分是 $C$，因此，只要原目标函数的导数绝对值小于 $C$，那么带正则项的目标函数在原点左边部分始终是递减的，在原点右边部分始终是递增的，最小值点自然在原点处。

- 相反，L2 正则项在原点处的导数是 0，只要原目标函数在原点处的导数不为 0，那么最小值点就不会在原点，所以 L2 只有减小 $w$ 绝对值的作用，对解空间的稀疏性没有贡献。<span style="color:red;">嗯。</span>

在一些在线梯度下降算法中，往往会采用截断梯度法来产生稀疏性，这同 L1 正则项产生稀疏性的原理是类似的。<span style="color:red;">什么意思？截断梯度法是什么方法？怎么使用？什么时候使用？为什么要使用？有类似的场景吗？</span>


## 角度 3：贝叶斯先验

从贝叶斯的角度来理解 L1 正则化和 L2 正则化，简单的解释是：

- L1正则化相当于对模型参数 $w$ 引入了拉普拉斯先验，
- L2正则化相当于引入了高斯先验
- 拉普拉斯先验使参数为 $0$ 的可能性更大。

<span style="color:red;">什么是拉普拉斯先验和高斯先验？</span>

高斯分布曲线图：

<center>

![](http://images.iterate.site/blog/image/20190413/51BFsCjfJORQ.png?imageslim){ width=55% }

</center>

由图可见，高斯分布在极值点（0点）处是平滑的，也就是高斯先验分布认为 $w$ 在极值点附近取不同值的可能性是接近的。这就是 L2 正则化只会让 $w$ 更接近 0 点，但不会等于 0 的原因。<span style="color:red;">多理解下。</span>

拉普拉斯分布曲线图：

<center>

![](http://images.iterate.site/blog/image/20190413/WPsOPJULiBK0.png?imageslim){ width=55% }

</center>

由图可见，拉普拉斯分布在极值点（0点）处是一个尖峰，所以拉普拉斯先验分布中参数 $w$ 取值为 $0$ 的可能性要更高。

在此我们不再给出 L1 和 L2 正则化分别对应拉普拉斯先验分布和高斯先验分布的详细证明。<span style="color:red;">还是要补充下的。感觉没有特别清楚。每一点都要弄明白。</span>



# 相关

- 《机器学习》周志华
- 《百面机器学习》
