---
title: 1.05 使用线性回归来讲解学习的过程
toc: true
date: 2019-08-27
---
# 可以补充进来的

- 这个没必要放在这里，拆分到线性模型里面。


# 使用线性回归来讲解学习的过程

我们将机器学习算法定义为：通过经验以提高计算机程序在某些任务上性能的算法。

这个定义有点抽象。

下面我们展示一个简单的机器学习示例：线性回归(linear regression)。


顾名思义，线性回归解决回归问题。换言之，我们的目标是建立一个系统，将向量 $\boldsymbol{x} \in \mathbb{R}^{n}$ 作为输入，预测标量 $y \in \mathbb{R}$ 作为输出。线性回归的输出是其输入的线性函数。令 $\hat{y}$ 表示模型预测 $y$ 应该取的值。我们定义输出为：


$$
\hat{y}=\boldsymbol{w}^{\top} \boldsymbol{x}\tag{5.3}
$$

其中 $\boldsymbol{w} \in \mathbb{R}^{n}$ 是参数(parameter)向量。

参数是控制系统行为的值。在这种情况下，$w_{i}$ 是系数，会和特征 $x_{i}$ 相乘之后全部相加起来。我们可以将 $\boldsymbol{w}$ 看作一组决定每个特征如何影响预测的权重(weight)。如果特征 $x_{i}$ 对应的权重 $w_{i}$ 是正的，那么特征的值增加，我们的预测值也会增加。如果特征 $x_{i}$ 对应的权重 $w_{i}$ 是负的，那么特征的值增加，我们的预测值会减少。如果特征权重的大小很大，那么它对预测有很大的影响；如果特征权重的大小是零，那么它对预测没有影响。<span style="color:red;">嗯。</span>


## 定义任务 T

我们定义任务 T：通过输出 $\hat{y}=\boldsymbol{w}^{\top} \boldsymbol{x}$ 从 $\mathcal{x}$ 预测 $y$ 。


## 定义性能度量 P

接下来我们需要定义性能度量 P。

假设我们有 $m$ 个输入样本组成的设计矩阵，不用它来训练模型，而是评估模型性能如何。我们也有每个样本对应的正确值 $y$ 组成的回归目标向量。因为这个数据集只是用来评估性能，我们称之为测试集(test set)。我们将输入的设计矩阵记作 $\boldsymbol{X}^{(\text { test })}$，回归目标向量记作 $\boldsymbol{y}^{(\text { test })}$。

度量模型性能的一种方法是计算模型在测试集上的均方误差(mean squared error)。如果 $\hat{\boldsymbol{y}}^{(\text { test })}$ 表示模型在测试集上的预测值，那么均方误差表示为：

$$
\mathrm{MSE}_{\mathrm{test}}=\frac{1}{m} \sum_{i}\left(\hat{\boldsymbol{y}}^{(\mathrm{test})}-\boldsymbol{y}^{(\mathrm{test})}\right)_{i}^{2}\tag{5.4}
$$

直观上，当 $\hat{\boldsymbol{y}}^{(\text { test })}=\boldsymbol{y}^{(\text { test })}$ 时，我们会发现误差降为 $0$。我们也可以看到

$$
\mathrm{MSE}_{\mathrm{test}}=\frac{1}{m}\left\|\hat{\boldsymbol{y}}^{(\mathrm{test})}-\boldsymbol{y}^{(\mathrm{test})}\right\|_{2}^{2}\tag{5.5}
$$

所以当预测值和目标值之间的欧几里得距离增加时，误差也会增加。<span style="color:red;">嗯。</span>



## 设计一个算发，减少 $\mathrm{MSE}_{\text { test }}$ 以改进权重 $\boldsymbol{w}$

为了构建一个机器学习算法，我们需要设计一个算法，通过观察训练集 $\left(\boldsymbol{X}^{(\text { train })}, \boldsymbol{y}^{(\text { train })}\right)$ 获得经验，减少 $\mathrm{MSE}_{\text { test }}$ 以改进权重 $\boldsymbol{w}$。一种直观方式是最小化训练集上的均方误差，即 $\mathrm{MSE}_{\text { train }}$。<span style="color:red;">是的。后续的第 5.5.1节说明其合法性。</span>

最小化 $\mathrm{MSE}_{\text { train }}$，我们可以简单地求解其导数为 $0$ 的情况：<span style="color:red;">重新整理下这个公式，去掉一些不必要的写法和标识，而且，对齐。</span>



$$
\nabla_{\boldsymbol{w}} \mathrm{MSE}_{\mathrm{train}}=0\tag{5.06}
$$

$$
\Rightarrow \nabla_{w} \frac{1}{m} \| \hat{\boldsymbol{y}}^{(\text { train })}-\boldsymbol{y}^{(\text { train }} ) \|_{2}^{2}=0\tag{5.07}
$$

$$
\Rightarrow \frac{1}{m} \nabla_{w}\left\|\boldsymbol{X}^{(\operatorname{train})} \boldsymbol{w}-\boldsymbol{y}^{(\operatorname{train})}\right\|_{2}^{2}=0\tag{5.08}
$$

$$
\Rightarrow \nabla_{w}\left(\boldsymbol{X}^{(\text { train })} \boldsymbol{w}-\boldsymbol{y}^{(\text { train })}\right)^{\top}\left(\boldsymbol{X}^{(\text { train })} \boldsymbol{w}-\boldsymbol{y}^{(\text { train })}\right)=0\tag{5.09}
$$

$$
\Rightarrow \nabla_{w}\left(\boldsymbol{w}^{\top} \boldsymbol{X}^{(\text { train })} \boldsymbol{T} \boldsymbol{X}^{(\text { train })} \boldsymbol{w}-2 \boldsymbol{w}^{\top} \boldsymbol{X}^{(\text { train }) \top} \boldsymbol{y}^{(\text { train })}+\boldsymbol{y}^{(\text { train }) \top} \boldsymbol{y}^{(\text { train })}\right)=0\tag{5.10}
$$

$$
\Rightarrow 2 \boldsymbol{X}^{(\text { train }) \top} \boldsymbol{X}^{(\text { train })} \boldsymbol{w}-2 \boldsymbol{X}^{(\text { train }) \top} \boldsymbol{y}^{(\text { train })}=0\tag{5.11}
$$

$$
\Rightarrow \boldsymbol{w}=\left(\boldsymbol{X}^{(\text { train }) \top} \boldsymbol{X}^{(\text { train })}\right)^{-1} \boldsymbol{X}^{(\text { train }) \top} \boldsymbol{y}^{(\text { train })}\tag{5.12}
$$

我们得到解的系统方程，称为正规方程(normal equation)。那么计算式 (5.12) 构成了一个简单的机器学习算法。

图 5.1展示了线性回归算法的使用示例。

<center>

![](http://images.iterate.site/blog/image/20190519/r4L9jpgJokaW.png?imageslim){ width=55% }

</center>

> 图 5.1　一个线性回归问题，其中训练集包括 10 个数据点，每个数据点包含一个特征。因为只有一个特征，权重向量 $\boldsymbol{w}$ 也只有一个要学习的参数 $w_{1}$。
>
> - (左)我们可以观察到线性回归学习 $w_{1}$，从而使得直线 $y=w_{1} x$ 能够尽量接近穿过所有的训练点。
> - (右)标注的点表示由正规方程学习到的 $w_{1}$ 的值，我们发现它可以最小化训练集上的均方误差。

值得注意的是，术语线性回归(linear regression)通常用来指稍微复杂一些，附加额外参数(截距项 $b$ )的模型。在这个模型中，

$$
\hat{y}=\boldsymbol{w}^{\top} \boldsymbol{x}+b\tag{5.13}
$$

因此从参数到预测的映射仍是一个线性函数，而从特征到预测的映射是一个仿射函数。<span style="color:red;">仿射函数是什么来着？补充下。</span>如此扩展到仿射函数意味着模型预测的曲线仍然看起来像是一条直线，只是这条直线没必要经过原点。除了通过添加偏置参数 $b$，我们还可以使用仅含权重的模型，但是 $\boldsymbol{x}$ 需要增加一项永远为 $1$ 的元素。对应于额外 $1$ 的权重起到了偏置参数的作用。当我们在本书中提到仿射函数时，会经常使用术语“线性”。

截距项 $b$ 通常被称为仿射变换的偏置(bias)参数。这个术语的命名源自该变换的输出在没有任何输入时会偏移 $b$。它和统计偏差中指代统计估计算法的某个量的期望估计偏离真实值的意思是不一样的。

线性回归当然是一个极其简单且有局限的学习算法，但是它提供了一个说明学习算法如何工作的例子。



# 相关

- 《深度学习》花书
