---
title: 3.15 逻辑回归与线性回归
toc: true
date: 2019-08-27
---

## 逻辑回归相比于线性回归，有何异同？

逻辑回归，乍一听名字似乎和数学中的线性回归问题异派同源，但其本质却是大相径庭。

首先，逻辑回归处理的是分类问题，线性回归处理的是回归问题，这是两者的最本质的区别。<span style="color:red;">是的。</span>

逻辑回归中，因变量取值是一个二元分布，模型学习得出的是 $y=\theta^Tx+\epsilon$ 的一个近似，其中 $\epsilon$ 代表误差项，我们使用这个近似项来处理回归问题。


分类和回归是如今机器学习中两个不同的任务，而属于分类算法的逻辑回归，其命名有一定的历史原因。这个方法最早由统计学家 David Cox在他 1958 年的论文《二元序列中的回归分析》（The regression analysis of binary sequences）中提出，当时人们对于回归与分类的定义与今天有一定区别，只是将“回归”这一名字沿用了。<span style="color:red;">嗯</span>

实际上，将逻辑回归的公式进行整理，我们可以得到 $\frac{p}{1-p}$，那么逻辑回归可以看作是对于“y=1|x”这一事件的对数几率的线性回归，于是“逻辑回归”这一称谓也就延续了下来。<span style="color:red;">？这个地方什么意思？</span>


在关于逻辑回归的讨论中，我们均认为 $y$ 是因变量，而非 $\frac{p}{1-p}$，这便引出逻辑回归与线性回归最大的区别，即逻辑回归中的因变量为离散的，而线性回归中的因变量是连续的。并且在自变量 $x$ 与超参数 $\theta$ 确定的情况下，逻辑回归可以看作广义线性模型（Generalized Linear Models）在因变量 $y$ 服从二元分布时的一个特殊情况；而使用最小二乘法求解线性回归时，我们认为因变量 $y$ 服从正态分布。<span style="color:red;">嗯嗯，这个地方乍一看还有点奇怪，不过一想又的确是这样。很好。</span>


当然逻辑回归和线性回归也不乏相同之处：<span style="color:red;">这个地方再补充细一点。</span>

- 首先，我们可以认为二者都使用了极大似然估计来对训练样本进行建模：
  - 线性回归使用最小二乘法，实际上就是在自变量 $x$ 与超参数 $θ$ 确定，因变量 $y$ 服从正态分布的假设下，使用极大似然估计的一个化简；
  - 而逻辑回归中通过对似然函数 $L(\theta)=\prod_{i=1}^{N}P(y_i|x_i;\theta)=\prod_{i=1}^{N}(\pi(x_i))^{y_i}(1-\pi(x_i))^{1-y_i}$ 的学习，得到最佳参数 $θ$。
- 另外，二者在求解超参数的过程中，都可以使用梯度下降的方法，这也是监督学习中一个常见的相似之处。



# 相关

- 《百面机器学习》
