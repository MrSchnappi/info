---
title: 3.13 Attention 在机器翻译中的应用
toc: true
date: 2019-04-19
---
# 可以补充进来的

# Attention 在机器翻译中的应用

Seq2Seq 模型引入注意力机制是为了解决什么问题？为什么选用了双向的循环神经网络模型？

在实际任务（例如机器翻译）中，使用 Seq2Seq 模型，通常：

- 会先使用一个循环神经网络作为编码器，将输入序列（源语言句子的词向量序列）编码成为一个向量表示；
- 然后再使用一个循环神经网络模型作为解码器，从编码器得到的向量表示里解码得到输出序列（目标语言句子的词序列）。


序列到序列模型结构示意图：

<center>

![](http://images.iterate.site/blog/image/20190419/JwGCzh6CdVGU.png?imageslim){ width=55% }

</center>

在 Seq2Seq 模型中（见图 10.6），当前隐状态以及上一个输出词决定了当前输出词，即：

$$
s_{i}=f\left(y_{i-1}, s_{i-1}\right)\tag{10.20}
$$


$$
p\left(y_{i} | y_{1}, y_{2} \ldots y_{i-1}\right)=g\left(y_{i-1}, s_{i}\right)\tag{10.21}
$$


其中 $f$ 和 $g$ 是非线性变换，通常是多层神经网络；$y_i$ 是输出序列中的一个词，$s_i$ 是对应的隐状态。

在实际使用中，会发现随着输入序列的增长，模型的性能发生了显著下降。这是因为编码时输入序列的全部信息压缩到了一个向量表示中。随着序列增长，句子越前面的词的信息丢失就越严重。<span style="color:red;">是的。</span>

试想翻译一个有 100 个词的句子，需要将整个句子全部词的语义信息编码在一个向量中。而在解码时，目标语言的第一个词大概率是和源语言的第一个词相对应的，这就意味着第一步的解码就需要考虑 100 步之前的信息。

建模时的一个小技巧是将源语言句子逆序输入，或者重复输入两遍来训练模型，以得到一定的性能提升。<span style="color:red;">嗯。好吧。</span>

使用长短期记忆模型能够在一定程度上缓解这个问题，但在实践中对于过长的序列仍然难以有很好的表现。

同时，Seq2Seq 模型的输出序列中，常常会损失部分输入序列的信息，这是因为在解码时，当前词及对应的源语言词的上下文信息和位置信息在编解码过程中丢失了。<span style="color:red;">这是因为在解码时，当前词及对应的源语言词的上下文信息和位置信息在编解码过程中丢失了。</span>

Seq2Seq 模型中引入注意力机制就是为了解决上述的问题。在注意力机制中，仍然可以用普通的循环神经网络对输入序列进行编码，得到隐状态 $h_{1}, h_{2} \ldots h_{T}$。但是在解码时，每一个输出词都依赖于前一个隐状态以及输入序列每一个对应的隐状态：


$$
s_{i}=f\left(s_{i-1}, y_{i-1}, c_{i}\right)\tag{10.22}
$$

$$
p\left(y_{i} | y_{1}, y_{2}, \ldots, y_{i-1}\right)=g\left(y_{i-1}, s_{i}, c_{i}\right)\tag{10.23}
$$

其中语境向量 $c_i$ 是输入序列全部隐状态 $h_{1}, h_{2} \ldots h_{T}$ 的一个加权和：


$$
c_{i}=\sum_{j=1}^{T} \alpha_{i j} h_{j}\tag{10.24}
$$


其中注意力权重参数 $\alpha_{i j}$ 并不是一个固定权重，而是由另一个神经网络计算得到：


$$
\alpha_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{T} \exp \left(e_{i k}\right)}\tag{10.25}
$$

$$
e_{i j}=a\left(s_{i-1}, h_{j}\right)\tag{10.26}
$$

神经网络 a 将上一个输出序列隐状态 $s_{i−1}$ 和输入序列隐状态 $h_j$ 作为输入，计算出一个 $x_j$，$y_i$ 对齐的值 $e_{ij}$，再归一化得到权重 $\alpha_{i j}$。<span style="color:red;">没懂</span>

我们可以对此给出一个直观的理解：在生成一个输出词时，会考虑每一个输入词和当前输出词的对齐关系，对齐越好的词，会有越大的权重，对生成当前输出词的影响也就越大。图 10.7展示了翻译时注意力机制的权重分布，在互为翻译的词对上会有最大的权重[29]。

<center>

![](http://images.iterate.site/blog/image/20190420/0EbDDKhhRuxv.png?imageslim){ width=55% }

</center>

在机器翻译这样一个典型的 Seq2Seq 模型里，生成一个输出词 $y_j$，会用到第 i 个输入词对应的隐状态 $h_i$ 以及对应的注意力权重 $\alpha_{ij}$ 。如果只使用一个方向的循环神经网络来计算隐状态，那么 $h_i$ 只包含了 $x_0$ 到 $x_i$ 的信息，相当于在 $\alpha_{ij}$ 这里丢失了 $x_i$ 后面的词的信息。而使用双向循环神经网络进行建模，第 $i$ 个输入词对应的隐状态包含了 $\overrightarrow{h}_i$ 和 $\overleftarrow{h}_i$ ，前者编码 $x_0$ 到 $x_i$ 的信息，后者编码 $x_i$ 及之后所有词的信息，防止了前后文信息的丢失，如图 10.8所示。<span style="color:red;">嗯，好像是这样。</span>

<center>

![](http://images.iterate.site/blog/image/20190420/z4BjOYXMVKOA.png?imageslim){ width=55% }

</center>




# 相关

- 《百面机器学习》
