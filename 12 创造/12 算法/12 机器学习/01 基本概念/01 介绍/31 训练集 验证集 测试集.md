
# 训练集 验证集 测试集

- 集合介绍
- 抽样方法
  - 留出法
  - 交叉验证
  - 自助法

## 集合介绍

**超参数的存在**

超参数存在原因：

- **有时一个选项被设为学习算法不用学习的超参数，是因为它太难优化了。**
- **更多的情况是，该选项必须是超参数，因为它不适合在训练集上学习。** 这适用于控制模型容量的所有超参数。如果在训练集上学习超参数，这些超参数总是趋向于最大可能的模型容量，导致过拟合。
  - 例如，相比低次多项式和正的权重衰减设定，更高次的多项式和权重衰减参数设定 $\lambda=0$ 总能在训练集上更好地拟合。

**验证集的存在**

要怎么解决这些不能优化的超参数的更新问题呢？

- 需要一个训练算法观测不到的验证集样本。


区分：

- 测试集。用来估计学习过程完成之后的学习器的泛化误差。
  - 其重点在于测试样本不能以任何形式参与到模型的选择中，包括设定超参数。基于这个原因，测试集中的样本不能用于验证集。
- 从训练数据中划分。
  - 训练集。用于学习参数。
  - 验证集。用于估计训练中或训练后的泛化误差，更新超参数。

<p align="center">
    <img width="90%" height="70%" src="http://images.iterate.site/blog/image/180728/dcI5B4J3Km.png?imageslim">
</p>

**模型交付之前需要注意**

（不确定）

最终交给用户模型的时候需要注意的

给定包含 $m$ 个样本的数据集 $D$ ，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型。因此，在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集 $D$ 重新训练模型，这个模型在训练过程中使用了所有 $m$ 个样本，这才是我们最终提交给用户的模型。


那么，有那些划分的方法呢？

# 抽样方法

在样本划分和模型验证的过程中，存在着不同的抽样方法和验证方法。

- 留出法 Holdout
- 交叉验证
- 自助法 Bootstrap


这些方法都对数据集有一些基本的假设：

- 数据集是随机抽取且独立同分布的
- 分布是平稳的，不会随时间发生变化
- 始终从同一个分布中抽取样本


# 留出法 Holdout

Holdout：

- 将原始的样本集合随机划分成训练集和验证集两部分。

比如：

- 对于一个点击率预测模型，我们把样本按照 70%~30% 的比例分成两部分，70% 的样本用于模型训练；30% 的样本用于模型验证，包括绘制 ROC 曲线、计算精确率和召回率等指标来评估模型性能。

注意：

- 留出法实际上是最常用的，但是，这中间有个问题，我们在划分数据集的时候，是需要尽可能的保持数据分布的一致性的，不然的话会引入额外的偏差，这个偏差会对最终结果产生影响。

## 如何在划分的时候保持数据分布的一致性

常用划分方式：

- **分层采样** (stratified sampling)。

比如：

- 我们想对 $D$ 进行 7/3 分 ，若 $D$ 包含 500 个正例、500 个反例，那么分层采样得到的 $S$ 应包含 350 个正例、350 个反例，而 $T$ 则包含 150 个正例和 150 个反例。

## 单次留出法也是不可靠的

因为：

- 即便在给定 7/3 分的比例之后，我们仍然是有很多种划分方式来对 $D$ 进行分割的。


比如：

- 上面的例子中，可以把 $D$ 中的样本排序，然后把前 350 个正例放到 $S$ 中，也可以把后 350 个正例放到 $S$ 中，等等，这些不同的划分还是会将导致不同的训练集/测试集，相应的，模型评估的结果也还是会有差别。


推荐：

- 使用留出法时，**最好采用若干次随机划分、然后重复进行实验评估后取平均值作为留出法的评估结果。**

比如：

- 例如我们可以进行 100 次随机划分，每次产生一个训练集/测试集用于实验评估，那么 100 次后就得到 100 个结果，而留出法返回的则是这 100 个结果的平均。（同时可得估计结果的标准差，<span style="color:red;">这个标准差有用吗？能衡量什么吗？</span>）

## Holdout 检验的缺点：

- 在验证集上计算出来的最后评估指标与原始分组有很大关系。<span style="color:red;">是的。好像现在平时用的就是这个。</span>
- 如果我令 $S$ 包含绝大多数样本，那么训练出的模型可以更接近用全部的 $D$ 训练出的模型，但是，这时候，由于 $T$ 比较小，因此评估结果可能不够稳定准确。
- 而如果我们令 $T$ 多包含一些样本，那么 $S$ 与 $D$ 的差别就更大了，也就是说，根据 $S$ 生成的模型与用 $D$ 训练出的模型是有可能有较大差别的，这就而降低了评估结果的保真性 (fidelity)。



# 交叉检验


## k-fold 交叉验证：

- 首先将全部样本划分成 K 个大小相等的样本子集；依次遍历这 K 个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后把 K 次评估指标的平均值作为最终的评估指标。

在实际实验中，K 经常取 10。

## 留一验证

- 每次留下 1 个样本作为验证集，其余所有样本作为测试集。样本总数为 n，依次对 n 个样本进行遍历，进行 n 次验证，再将评估指标求平均值得到最终的评估指标。

在样本总数较多的情况下，留一验证法的时间开销极大。<span style="color:red;">是呀。</span>事实上，留一验证是留 p 验证的特例。留 p 验证是每次留下 p 个样本作为验证集，而从 n 个元素中选择 p 个元素有种可能，因此它的时间开销更是远远高于留一验证，故而很少在实际工程中被应用。

<span style="color:red;">留一验证和留 p 验证一般什么时候使用？</span>


# 自助法 bootstrapping

不管是 Holdout 检验还是交叉检验，都是基于划分训练集和测试集的方法进行模型评估的。

然而，当样本规模比较小时，将样本集进行划分会让训练集进一步减小，这可能会影响模型训练效果。

有没有能维持训练集样本规模的验证方法呢？自助法可以比较好地解决这个问题。<span style="color:red;">即使我之前看到过，再次看到还是感觉有些惊奇。而且，还是想知道，这样不会有分布上的问题吗？分布不会发生变化吗？</span>


自助法是基于自助采样法 bootstrap sampling 的检验方法。对于总数为 $n$ 的样本集合，进行 $n$ 次有放回的随机抽样，得到大小为 $n$ 的训练集。$n$ 次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证，这就是自助法的验证过程。<span style="color:red;">嗯，这个自助法，这个名字感觉与这个定义不是特别搭，为什么起这么个名字呢？是自助餐的意思吗？</span>


## 在自助法的采样过程中，对 $n$ 个样本进行 $n$ 次自助抽样，当 $n$ 趋于无穷大时，最终有多少数据从未被选择过？


一个样本在一次抽样过程中未被抽中的概率为 $(1-\frac{1}{n})$，$n$ 次抽样均未抽中的概率为 $(1-\frac{1}{n})^n$ 。当 $n$ 趋于无穷大时，概率为 $\lim_{n\rightarrow \infty} (1-\frac{1}{n})^n$。

根据重要极限 $\lim_{n\rightarrow \infty} (1-\frac{1}{n})^n=e$，所以有：

$$
\begin{aligned}
\lim_{n\rightarrow \infty}(1-\frac{1}{n})^n &= \lim_{n\rightarrow\infty}\frac{1}{(1+\frac{1}{n-1})^n}\\
&= \frac{1}{lim_{n\rightarrow \infty}(1+\frac{1}{n-1})^{n-1}}\cdot \frac{1}{lim_{n\rightarrow \infty}(1+\frac{1}{n-1})}\\
&=\frac{1}{e}\approx 0.368
\end{aligned}
$$

因此，当样本数很大时，大约有 $36.8\%$ 的样本从未被选择过，可作为验证集。<span style="color:red;">嗯，还是很好的。</span>

也即是：实际评估的模型与期望评估的模型都使用 $m$ 个训练样本，仍有数据总量约 $1/3$ 的、没在训练集中出现的样本可以用于测试。

## 自助法的使用场景

那么这么好的方法，我们在什么情况下使用呢？

自助法在数据集较小、难以有效划分训练集/测试集的时候还是很有用的。<span style="color:red;">那么大的数据集的时候不推荐使用吗？而且小的数据集的时候这种自助法不是会产生一些重复的数据吗？不会对模型有影响吗？</span>而且，**自助法由于能够从初始数据集中产生多个不同的训练集，这就对集成学习等方法有很大的好处。**<span style="color:red;">感觉很厉害的，不过还是要确认下 ensemble 与这种自助法的详细关系。</span>

然而，**自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差，因此，在初始数据量足够多的时候，还是留出法和交叉验证法更常用一些。**<span style="color:red;">估计偏差要怎么消除？难道小数据量的时候就可以不考虑估计偏差吗？</span>

<span style="color:red;">嗯，这些方法的选取在实践中再确认下，然后将经验补充到这里。</span>





