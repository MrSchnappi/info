---
title: 23 JS 距离
toc: true
date: 2019-08-28
---

# JS距离

Jensen–Shannon divergence，基于 KL 散度发展而来，是对称度量：


JS散度度量了两个概率分布的相似度，基于 KL 散度的变体，解决了 KL 散度非对称的问题。

一般地，JS散度是对称的，其取值是 0 到 1 之间。

定义如下：



$$
J S\left(P_{1} \| P_{2}\right)=\frac{1}{2} K L\left(P_{1} \| \frac{P_{1}+P_{2}}{2}\right)+\frac{1}{2} K L\left(P_{2} \| \frac{P_{1}+P_{2}}{2}\right)
$$

即：

$$
JS(P||Q)= \frac{1}{2} D_{KL}(P||M) + \frac{1}{2} D_{KL}(Q||M)
$$

其中 $M=\frac{1}{2}(P+Q)$ 。

## 存在的问题

KL散度和 JS 散度度量的时候有一个问题：

如果两个分配 P,Q离得很远，完全没有重叠的时候，那么 KL 散度值是没有意义的，而 JS 散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为 0。梯度消失了。

<span style="color:red;">没懂。</span>




# 相关

- [迁移学习简明手册](https://github.com/jindongwang/transferlearning-tutorial)  [王晋东](https://zhuanlan.zhihu.com/p/35352154)
- wiki
