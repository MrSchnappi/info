---
title: 3.02 分类损失函数对比
toc: true
date: 2019-09-03
---
# 分类损失函数对比


最后，我们将 0-1 Loss、Cross Entropy Loss、Hinge Loss、Exponential Loss、Modified Huber Loss 画在一张图中：


<center>

![mark](http://images.iterate.site/blog/image/20190902/3iumpccC1aMb.png?imageslim)

</center>



上图 $ys$ 的取值范围是 $[-2,+2]$，若我们把 $ys$ 的坐标范围取得更大一些，上面 5 种 Loss 的差别会更大一些，见下图：

<center>

![mark](http://images.iterate.site/blog/image/20190902/7NcqsvAw48va.png?imageslim)

</center>

显然，这时候 Exponential Loss 会远远大于其它 Loss。从训练的角度来看，模型会更加偏向于惩罚较大的点，赋予其更大的权重。如果样本中存在离群点，Exponential Loss 会给离群点赋予更高的权重，但却可能是以牺牲其他正常数据点的预测效果为代价，可能会降低模型的整体性能，使得模型不够健壮（robust）。

相比 Exponential Loss，其它四个 Loss，包括 Softmax Loss，都对离群点有较好的“容忍性”，受异常点的干扰较小，模型较为健壮。



# 相关

- [确定不收藏？机器学习必备的分类损失函数速查手册](https://redstonewill.com/1584/)
