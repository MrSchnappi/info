---
title: 5.11 二分类问题的损失函数
toc: true
date: 2019-08-27
---

# 二分类问题的损失函数

对二分类问题，$Y=\{1,-1\}$ ，我们希望 $\operatorname{sign} f\left(x_{i}, \theta\right)=y_{i}$，最自然的损失函数是 0-1 损失，即：

$$
L_{0-1}(f, y)=1_{f y \leqslant 0}\tag{7.1}
$$

<span style="color:red;">为什么是 $f y \leqslant 0$？ </span><span style="color:blue;">哦，看到下面这句解释就明白了。</span>

其中 $1_{p}$ 是指示函数（Indicator Function），当且仅当 $P$ 为真时取值为 $1$，否则取值为 $0$。该损失函数能够直观地刻画分类的错误率，但是由于其非凸、非光滑的特点，使得算法很难直接对该函数进行优化。<span style="color:red;">嗯。是呦</span>

0-1损失的一个代理损失函数是 Hinge 损失函数：

$$
L_{\text { hinge }}(f, y)=\max \{0,1-f y\}\tag{7.2}
$$

Hinge 损失函数是 0-1 损失函数相对紧的凸上界，且当 $f y \geq 1$ 时，该函数不对其做任何惩罚。Hinge损失在 $f y=1$ 处不可导，因此不能用梯度下降法进行优化，而是用次梯度下降法（Subgradient Descent Method）。<span style="color:red;">哇塞，什么是次梯度下降法？</span>

0-1损失的另一个代理损失函数是 Logistic 损失函数：

$$
L_{\text { logistic }}(f, y)=\log _{2}(1+\exp (-f y))\tag{7.3}
$$

Logistic 损失函数也是 0-1 损失函数的凸上界，且该函数处处光滑，因此可以用梯度下降法进行优化。但是，该损失函数对所有的样本点都有所惩罚，因此对异常值相对更敏感一些。当预测值 $f \in[-1,1]$ 时，另一个常用的代理损失函数是交叉熵（Cross Entropy）损失函数：

$$
L_{\text { cross entropy }}(f, y)=-\log _{2}\left(\frac{1+f y}{2}\right)\tag{7.4}
$$

交叉熵损失函数也是 0-1 损失函数的光滑凸上界。

这四种损失函数的曲线如图 7.1 所示，二分类问题的损失函数：

![](http://images.iterate.site/blog/image/20190407/0erAbOTRGYPI.png?imageslim){ width=55% }

<span style="color:red;">平时我们用什么比较多？</span>




# 相关

- 《百面机器学习》
