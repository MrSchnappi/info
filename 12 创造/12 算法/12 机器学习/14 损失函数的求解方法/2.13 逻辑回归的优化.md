# 逻辑回归的优化

分类：

- 凸优化问题。

说明：

- 对于二分类问题，$Y=\{1,-1\}$，假设模型参数为 $\theta$ ，则逻辑回归的优化问题为：

$$
\min _{\theta} L(\theta)=\sum_{i=1}^{n} \log \left(1+\exp \left(-y_{i} \theta^{\mathrm{T}} x_{i}\right)\right)
$$

验证凸性：

- 可以通过计算目标函数的二阶 Hessian 矩阵来验证凸性。<span style="color:red;">二阶 Hessian 矩阵</span>
- 令：

$$
L_{i}(\theta)=\log \left(1+\exp \left(-y_{i} \theta^{\mathrm{T}} x_{i}\right)\right)
$$

- 对该函数求一阶导，得到：<span style="color:red;">为什么 $\exp \left(-y_{i} \theta^{\mathrm{T}} x_{i}\right)$ 被去掉了？</span>

$$
\begin{aligned} \nabla L_{i}(\theta) &=\frac{1}{1+\exp \left(-y_{i} \theta^{\mathrm{T}} x_{i}\right)} \exp \left(-y_{i} \theta^{\mathrm{T}} x_{i}\right) \cdot\left(-y_{i} x_{i}\right) \\ &=\frac{-y_{i} x_{i}}{1+\exp \left(y_{i} \theta^{\mathrm{T}} x_{i}\right)} \end{aligned}
$$

- 继续求导，得到函数的 Hessian 矩阵：<span style="color:red;">f'g 为什么直接消去了？</span>

$$
\begin{aligned}
\nabla^{2} L_{i}(\theta)&=\frac{y_{i} x_{i} \cdot \exp \left(y_{i} \theta^{\mathrm{T}} x_{i}\right) \cdot y_{i} x_{i}^{T}}{\left(1+\exp \left(y_{i} \theta^{\mathrm{T}} x_{i}\right)\right)^{2}}\\
&=\frac{\exp \left(y_{i} \theta^{\mathrm{T}} x_{i}\right)}{\left(1+\exp \left(y_{i} \theta^{\mathrm{T}} x_{i}\right)\right)^{2}} x_{i} x_{i}^{\mathrm{T}}\end{aligned}
$$

- 该矩阵满足半正定的性质 $\nabla^{2} L_{i}(\theta) \succeq 0$，因此 $\nabla^{2} L(\theta)=\sum_{i=1}^{n} \nabla^{2} L_{i}(\theta) \succeq 0$。
- 因此函数 $L(\cdot)$ 为凸函数。


求解：

- 对于凸优化问题，所有的局部极小值都是全局极小值，因此这类问题一般认为是比较容易求解的问题。
