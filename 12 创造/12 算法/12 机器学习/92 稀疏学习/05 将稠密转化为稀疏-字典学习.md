---
title: 05 将稠密转化为稀疏-字典学习
toc: true
date: 2018-06-29 19:06:09
---



# 将稠密转化为稀疏-字典学习



那么，若给定数据集 $D$ 是稠密的，即普通非稀疏数据，能否将其转化为 “稀疏表示” (sparse representation) 形式，从而享有稀疏性所带来的好处呢？ 需注意的是，我们所希望的稀疏表示是“恰当稀疏”，而不是“过度稀疏”。仍以汉语文档为例，基于《现代汉语常用字表》得到的可能是恰当稀疏，即其稀疏性足以让学习任务变得简单可行；而基于《康熙字典》则可能是过度稀疏，与前者相比，也许并未给学习任务带来更多的好处。


显然，在一般的学习任务中(例如图像分类)并没有《现代汉语常用字表》 可用，我们需学习出这样一个“字典”。

为普通稠密表达的样本找到合适的字典，将样本转化为合适的稀疏表示形式，从而使学习任务得以简化，模型复杂度得以降低，通常称为 “字典学习”(dictionary learning)，亦称“稀疏编码” (sparse coding)。

这两个称谓稍有差别：

- “字典学习”更侧重于学得字典的过程
- “稀疏编码”则更侧重于对样本进行稀疏表达的过程

由于两者通常是在同一个优化求解过程中完成的，因此下面我们不做进一步区分，笼统地称为字典学习。

## 字典学习的表达式

给定数据集 $\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \dots, \boldsymbol{x}_{m}\right\}$ 字典学习最简单的形式为

$$
\min _{\mathbf{B}, \boldsymbol{\alpha}_{i}} \sum_{i=1}^{m}\left\|\boldsymbol{x}_{i}-\mathbf{B} \boldsymbol{\alpha}_{i}\right\|_{2}^{2}+\lambda \sum_{i=1}^{m}\left\|\boldsymbol{\alpha}_{i}\right\|_{1}\tag{11.15}
$$

其中  $\mathbf{B} \in \mathbb{R}^{d \times k}$ 为字典矩阵，$k$ 称为字典的词汇量，通常由用户指定，$\boldsymbol{\alpha}_{i} \in \mathbb{R}^{k}$ 则是样本 $\boldsymbol{x}_{i} \in \mathbb{R}^{d}$ 的稀疏表示。显然，式(11.15)的第一项是希望 $\boldsymbol{\alpha}_{i}$ 能很好地重构 $\boldsymbol{x}_{i}$ ，第二项则是希望 $\boldsymbol{\alpha}_{i}$ 尽量稀疏。


与 LASSO相比，式(11.15)显然麻烦得多，因为除了类似于式(11.7)中 $\boldsymbol{w}$ 的 $\boldsymbol{\alpha}_{i}$ ，还需学习字典矩阵 $\mathbf{B}$。不过，受 LASSO 的启发，我们可采用变量交替优化的策略来求解式(11.15)。<span style="color:red;">为什么与 LASSO 比？</span>

## 计算

首先在第一步，我们固定住字典 $\mathbf{B}$，若将式(11.15)按分量展开，可看出其中 不涉及 $\alpha_{i}^{u} \alpha_{i}^{v}(u \neq v)$ 这样的交叉项，于是可参照 LASSO 的解法求解下式，从而为每个样本 $\boldsymbol{x}_{i}$ 找到相应的 $\boldsymbol{\alpha}_{i}$ ：

$$
\min _{\boldsymbol{\alpha}_{i}}\left\|\boldsymbol{x}_{i}-\mathbf{B} \boldsymbol{\alpha}_{i}\right\|_{2}^{2}+\lambda\left\|\boldsymbol{\alpha}_{i}\right\|_{1}\tag{11.16}
$$

在第二步，我们固定住 $\boldsymbol{\alpha}_{i}$ 来更新字典 $\mathbf{B}$，此时可将式(11.15) 写为

$$
\min _{\mathbf{B}}\|\mathbf{X}-\mathbf{B} \mathbf{A}\|_{F}^{2}\tag{11.17}
$$

其中 $\mathbf{X}=\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right) \in \mathbb{R}^{d \times m}$ ，$\mathbf{A}=\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \ldots, \boldsymbol{\alpha}_{m}\right) \in \mathbb{R}^{k \times m}$，$\|\cdot\|_{F}$ 是矩阵的 Erobenius 范数。式(11.17)有多种求解方法，常用的有基于逐列更新策略的 KSVD 。令 $\boldsymbol{b}_{i}$ 表示字典矩阵 $\mathbf{B}$ 的第 $i$ 列，$\boldsymbol{\alpha}^{i}$ 表示稀疏矩阵 $\mathbf{A}$ 的第 $i$ 行，式(11.17)可重写为：

$$
\begin{aligned} \min _{\mathbf{B}}\|\mathbf{X}-\mathbf{B A}\|_{F}^{2} &=\min _{\boldsymbol{b}_{i}}\left\|\mathbf{X}-\sum_{j=1}^{k} \boldsymbol{b}_{j} \boldsymbol{\alpha}^{j}\right\|_{F}^{2} \\ &=\min _{\boldsymbol{b}_{i}}\left\|\left(\mathbf{X}-\sum_{j \neq i} \boldsymbol{b}_{j} \boldsymbol{\alpha}^{j}\right)-\boldsymbol{b}_{i} \boldsymbol{\alpha}^{i}\right\|_{F}^{2} \\ &=\min _{\boldsymbol{b}_{i}}\left\|\mathbf{E}_{i}-\boldsymbol{b}_{i} \boldsymbol{\alpha}^{i}\right\|_{F}^{2} \end{aligned}\tag{11.18}
$$

> $$\begin{aligned}
> \underset{\boldsymbol B}{min}\left \|\boldsymbol  X-\boldsymbol B\boldsymbol A \right \|_{F}^{2}
> & =\underset{b_{i}}{min}\left \| \boldsymbol X-\sum_{j=1}^{k}b_{j}\alpha ^{j} \right \|_{F}^{2}\\
> & =\underset{b_{i}}{min}\left \| \left (\boldsymbol X-\sum_{j\neq i}b_{j}\alpha ^{j} \right )- b_{i}\alpha ^{i}\right \|_{F}^{2} \\
> & =\underset{b_{i}}{min}\left \|\boldsymbol  E_{\boldsymbol i}-b_{i}\alpha ^{i} \right \|_{F}^{2} &
> \end{aligned}
> $$
> [推导]：此处只推导一下 $BA=\sum_{j=1}^{k}\boldsymbol b_{\boldsymbol j}\boldsymbol \alpha ^{\boldsymbol j}$，其中 $\boldsymbol b_{\boldsymbol j}$ 表示**B**的第 j 列，$\boldsymbol \alpha ^{\boldsymbol j}$ 表示**A**的第 j 行。
> 然后，用 $b_{j}^{i}$，$\alpha _{j}^{i}$ 分别表示**B**和**A**的第 i 行第 j 列的元素，首先计算**BA**:
> $$
> \begin{aligned}
> \boldsymbol B\boldsymbol A
> & =\begin{bmatrix}
> b_{1}^{1} &b_{2}^{1}  & \cdot  & \cdot  & \cdot  & b_{k}^{1}\\
> b_{1}^{2} &b_{2}^{2}  & \cdot  & \cdot  & \cdot  & b_{k}^{2}\\
> \cdot  & \cdot  & \cdot  &  &  & \cdot \\
> \cdot  &  \cdot &  & \cdot  &  &\cdot  \\
>  \cdot & \cdot  &  &  & \cdot  & \cdot \\
>  b_{1}^{d}& b_{2}^{d}  & \cdot  & \cdot  &\cdot   &  b_{k}^{d}
> \end{bmatrix}_{d\times k}\cdot
> \begin{bmatrix}
> \alpha_{1}^{1} &\alpha_{2}^{1}  & \cdot  & \cdot  & \cdot  & \alpha_{m}^{1}\\
> \alpha_{1}^{2} &\alpha_{2}^{2}  & \cdot  & \cdot  & \cdot  & \alpha_{m}^{2}\\
> \cdot  & \cdot  & \cdot  &  &  & \cdot \\
> \cdot  &  \cdot &  & \cdot  &  &\cdot  \\
>  \cdot & \cdot  &  &  & \cdot  & \cdot \\
>  \alpha_{1}^{k}& \alpha_{2}^{k}  & \cdot  & \cdot  &\cdot   &  \alpha_{m}^{k}
> \end{bmatrix}_{k\times m} \\
> & =\begin{bmatrix}
> \sum_{j=1}^{k}b_{j}^{1}\alpha _{1}^{j} &\sum_{j=1}^{k}b_{j}^{1}\alpha _{2}^{j} & \cdot  & \cdot  & \cdot  & \sum_{j=1}^{k}b_{j}^{1}\alpha _{m}^{j}\\
> \sum_{j=1}^{k}b_{j}^{2}\alpha _{1}^{j} &\sum_{j=1}^{k}b_{j}^{2}\alpha _{2}^{j}  & \cdot  & \cdot  & \cdot  & \sum_{j=1}^{k}b_{j}^{2}\alpha _{m}^{j}\\
> \cdot  & \cdot  & \cdot  &  &  & \cdot \\
> \cdot  &  \cdot &  & \cdot  &  &\cdot  \\
>  \cdot & \cdot  &  &  & \cdot  & \cdot \\
> \sum_{j=1}^{k}b_{j}^{d}\alpha _{1}^{j}& \sum_{j=1}^{k}b_{j}^{d}\alpha _{2}^{j}  & \cdot  & \cdot  &\cdot   &  \sum_{j=1}^{k}b_{j}^{d}\alpha _{m}^{j}
> \end{bmatrix}_{d\times m} &
> \end{aligned}
> $$
> 然后计算 $\boldsymbol b_{\boldsymbol j}\boldsymbol \alpha ^{\boldsymbol j}$:
> $$
> \begin{aligned}
> \boldsymbol b_{\boldsymbol j}\boldsymbol \alpha ^{\boldsymbol j}
> & =\begin{bmatrix}
> b_{1}^{j}\\ b_{w}^{j}
> \\ \cdot
> \\ \cdot
> \\ \cdot
> \\ b_{d}^{j}
> \end{bmatrix}\cdot
> \begin{bmatrix}
>  \alpha _{1}^{j}& \alpha _{2}^{j} & \cdot  & \cdot  & \cdot  & \alpha _{m}^{j}
> \end{bmatrix}\\
> & =\begin{bmatrix}
> b_{j}^{1}\alpha _{1}^{j} &b_{j}^{1}\alpha _{2}^{j} & \cdot  & \cdot  & \cdot  & b_{j}^{1}\alpha _{m}^{j}\\
> b_{j}^{2}\alpha _{1}^{j} &b_{j}^{2}\alpha _{2}^{j}  & \cdot  & \cdot  & \cdot  & b_{j}^{2}\alpha _{m}^{j}\\
> \cdot  & \cdot  & \cdot  &  &  & \cdot \\
> \cdot  &  \cdot &  & \cdot  &  &\cdot  \\
>  \cdot & \cdot  &  &  & \cdot  & \cdot \\
> b_{j}^{d}\alpha _{1}^{j}& b_{j}^{d}\alpha _{2}^{j}  & \cdot  & \cdot  &\cdot   &  b_{j}^{d}\alpha _{m}^{j}
> \end{bmatrix}_{d\times m} &
> \end{aligned}
> $$
> 求和可得：
> $$
> \begin{aligned}
> \sum_{j=1}^{k}\boldsymbol b_{\boldsymbol j}\boldsymbol \alpha ^{\boldsymbol j}
> & = \sum_{j=1}^{k}\left (\begin{bmatrix}
> b_{1}^{j}\\ b_{w}^{j}
> \\ \cdot
> \\ \cdot
> \\ \cdot
> \\ b_{d}^{j}
> \end{bmatrix}\cdot
> \begin{bmatrix}
>  \alpha _{1}^{j}& \alpha _{2}^{j} & \cdot  & \cdot  & \cdot  & \alpha _{m}^{j}
> \end{bmatrix} \right )\\
> & =\begin{bmatrix}
> \sum_{j=1}^{k}b_{j}^{1}\alpha _{1}^{j} &\sum_{j=1}^{k}b_{j}^{1}\alpha _{2}^{j} & \cdot  & \cdot  & \cdot  & \sum_{j=1}^{k}b_{j}^{1}\alpha _{m}^{j}\\
> \sum_{j=1}^{k}b_{j}^{2}\alpha _{1}^{j} &\sum_{j=1}^{k}b_{j}^{2}\alpha _{2}^{j}  & \cdot  & \cdot  & \cdot  & \sum_{j=1}^{k}b_{j}^{2}\alpha _{m}^{j}\\
> \cdot  & \cdot  & \cdot  &  &  & \cdot \\
> \cdot  &  \cdot &  & \cdot  &  &\cdot  \\
>  \cdot & \cdot  &  &  & \cdot  & \cdot \\
> \sum_{j=1}^{k}b_{j}^{d}\alpha _{1}^{j}& \sum_{j=1}^{k}b_{j}^{d}\alpha _{2}^{j}  & \cdot  & \cdot  &\cdot   &  \sum_{j=1}^{k}b_{j}^{d}\alpha _{m}^{j}
> \end{bmatrix}_{d\times m} &
> \end{aligned}
> $$

在更新字典的第 $i$ 列时，其他各列都是固定的，因此 $\mathbf{E}_{i}=\sum_{j \neq i} \boldsymbol{b}_{j} \boldsymbol{\alpha}^{j}$ 是固定的，于是最小化式(11.18)原则上只需对 $\mathbf{E}_{i}$ 进行奇异值分解以取得最大奇异值所对应的正交向量。然而，直接对 $\mathbf{E}_{i}$ 进行奇异值分解会同时修改 $\boldsymbol{b}_{i}$ 和 $\boldsymbol{\alpha}^{i}$ ，从而可能破坏 $\mathbf{A}$ 的稀疏性。为避免发生这种情况，KSVD 对 $\mathbf{E}_{i}$  和 $\boldsymbol{\alpha}^{i}$ 进行专门处理: $\boldsymbol{\alpha}^{i}$ 仅保留非零元素， $\mathbf{E}_{i}$ 则仅保留 $\boldsymbol{b}_{i}$ 与 $\boldsymbol{\alpha}^{i}$ 的非零元素的乘积项，然后再进行奇异值分解，这样就保持了第一步所得到的稀疏性。

初始化字典矩阵 $\mathbf{B}$ 之后反复迭代上述两步，最终即可求得字典 $\mathbf{B}$ 和样本 $\boldsymbol{x}_{i}$ 的稀疏表示 $\boldsymbol{\alpha}_{i}$ ，在上述字典学习过程中，用户能通过设置词汇量 $k$ 的大小来控制字典的规模，从而影响到稀疏程度。





# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
