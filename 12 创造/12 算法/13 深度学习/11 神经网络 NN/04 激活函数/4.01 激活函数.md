---
title: 4.01 激活函数
toc: true
date: 2019-08-31
---
# 激活函数


激活函数使用非线性变换对数据的分布进行重新映射。以满足真实情况中的线性不可分问题。

激活函数的设计是一个非常活跃的研究领域，并且还没有许多明确的指导性理论原则。


对于深度神经网络，我们在每一层线性变换后叠加一个非线性激活函数，以避免多层网络等效于单层线性函数，从而获得更强大的学习与拟合能力。<span style="color:red;">嗯，是的。</span>



整流线性单元是激活函数极好的默认选择。

许多其他类型的激活函数也是可用的。决定何时使用哪种类型的激活函数是困难的事（尽管整流线性单元通常是一个可接受的选择）。我们这里描述对于每种激活函数的一些基本直觉。这些直觉可以用来建议我们何时来尝试一些单元。通常不可能预先预测出哪种激活函数工作得最好。设计过程充满了试验和错误，先直觉认为某种激活函数可能表现良好，然后用它组成神经网络进行训练，最后用验证集来评估它的性能。<span style="color:red;">好吧，不可能预先预测出那种激活函数工作的最好。</span>

一些激活函数可能并不是在所有的输入点上都是可微的。例如，整流线性单元 $g(z)=\max\{0, z\}$ 在 $z=0$ 处不可微。

除非另有说明，大多数的激活函数都可以描述为接受输入向量 $\boldsymbol x$，计算仿射变换 $\boldsymbol z=\boldsymbol W^\top \boldsymbol x+\boldsymbol b$，然后使用一个逐元素的非线性函数 $g(\boldsymbol z)$。大多数激活函数的区别仅仅在于激活函数 $g(\boldsymbol z)$ 的形式。<span style="color:red;">有特殊的吗？比如是别的结构的？或者并不使用仿射变换？</span>



- Sigmoid
- Tanh(双曲正切)
- ReLU  修正线性单元   用的最多的
- Leaky ReLU 对 ReLU 做一些修正
- ELU  对 Leaky ReLU 做一些修正
- Maxout



# 相关

- 《百面机器学习》
- 《深度学习》花书
