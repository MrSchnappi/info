---
title: 26 无监督学习-领域嵌入
toc: true
date: 2019-08-18
---
![mark](http://images.iterate.site/blog/image/20190818/SaxVYhI2nDKQ.png?imageslim)


![mark](http://images.iterate.site/blog/image/20190818/H68GVAVsQiLV.png?imageslim)



TSNE的 NE 就是 Neighbor embedding的缩写。我们现在要做的事情就是我们之前讲过的降维，只是我们要做的是非线 性的降维。

我们知道 data point可能是在高维空间里面的一个 manifold，也就是说：data point的分布其实是在低维的一个空间里，只是被扭曲被塞到高维空间里面。讲到 manifold ，常常举的例子是地球，地球的表面就是一个 manifold(一个二维的平面，被塞到一个三维的空间里面)。在 manifold 里面只有很近距离的点，Euclidean distance才会成立，如果距离很远的时候，欧式几何不一定成立。也就是说：我们在这个 S 型的空间里面取一个点，我们用 Euclidean distance比较跟其他两个点的距离。这个点跟离的近的点比较像，离的远的点比较不像(相邻的两个点)。如果是距离比较远的时候，如果你要比较这个点跟离的非常远的点的相似程度，你在高维空间中，你直接算 Euclidean distance，你就变得不 made sence。

所以 manifold learning要做的事情是把 S 型的这块东西展开，把塞到高维空间的低维空间摊平。摊平的好处就是：把这个塞到高维空间里的 manifold 摊平以后，那我们就可以在这个 manifold 上面用 Euclidean distance(来算点和点之间的距离)，这会对接下来你要做 supervised learning都是会有帮助的

## 局部线性嵌入



![mark](http://images.iterate.site/blog/image/20190818/Ae3Nu1CXKW8L.png?imageslim)


有个方法叫做 Locally Linear Embedding，这个方法意思是说：在原来的空间里面，你的点分别长这个样子。有某一个点叫做 $x^i$，我们先选出 $x^i$ 的 neighbor 叫做 $x^j$。接下来我们找 $x^i$ 跟 $x^j$ 之间的关系，它们之间的关系我们写作 $w_{ij}$。

这个 $w_{ij}$ 是怎么找出来的呢？我们假设说：每一个 $x^i$ 都是可以用它的 neighbor 做 linear combination以后组合而成，这个 $w_{ij}$ 是拿 $x^j$ 组成 $x^i$ 的时候，linear combination的 weight。那找这一组 $x_{ij}$ 要咋样做呢，你就说：我们现在找一组 $x_{ij}$，$x^i$ 减掉 summation over$x_{ij}$ 乘以 $x^j$ 的 L2-Norm是越接近越好，然后 summation over所以的 data point。接下来我们要做 dimension reduction，把原来的 $x^i,x^j$ 转成 $z^i,z^j$。但是现在的原则就是：从 $x^i,x^j 转成 z^i,z^j$ 它们中间的关系 $w_{ij}$ 是不变的



![mark](http://images.iterate.site/blog/image/20190818/GxQrS0Nc7qkJ.png?imageslim)


这个东西就是白居易《长恨歌里面》讲的“在天愿作比翼鸟，在地愿为连理枝”。所谓的“在天”就是 $x^i,x^j$ 在原来的 space 上面，“比翼鸟”就是“$w_{ij}$”，“在地”就是把 $x^i,x^j$transform到另外一个 sapce 就是 $z^i,z^j$，“连理枝”等于“比翼鸟”就是 $w_{ij}$



![mark](http://images.iterate.site/blog/image/20190818/RyB9OiHMR99C.png?imageslim)


所以 LLE 做的事情，首先 $x^i,x^j$ 在原来的 space 上面找完以后就 fix 住它，接下来为每一个 $x^i,x^j$ 找另外一个 vector$z^i,z^j$(因为我们在做 dimension reduction，新找的 dimension 要比原来的要小)。我们找的 $z^i,z^j$ 可以 minimize 这个 function。也就是说原来的 $x^i$ 它可以做 linear combination产生 x，原来的 $x^j$ 做 linear combination 产生 $x^i$，这些 $z^j$ 它也可以用同样的 linear combination产生 $z^i$，我们就是要找这一组 z 可以满足这个 $w_{ij}$ 给我们的 constraint，所以在这个式子里面 $w_{ij}$ 变成是已知的，但是我们要找一组 z，让 $z^j$ 透过 $w_{ij}$ 跟 $ 在 z^i$ 越接近越好，$z^j$ 用这组 weight 做 linear combination以后跟 $z^i$ 越接近越好，然后 summation over所以的 data point。


这个 LLE 你要注意一下，其实它并没有一个明确的 function 告诉你说我们咋样来做 dimension reduction，不像我们在做 auto encoding的时候，你 learn 出一个 encoding 的 network，你 input 一个新的 data point，然后你就得到结果。今天在 LLE 里面，你并没有找一个明确的 function 告诉我们，怎么样从一个 x 变到 z，z完全就是另外凭空找出来的。

![mark](http://images.iterate.site/blog/image/20190818/DtapTsakMwBJ.png?imageslim)


其实 LLE 你要好好的调以下你的 neighbor，neighbor的数目要刚刚好，你才会得到好的结果。这个是从原始的 paper 里面截得图，它调了不同的 k。如果 k 太小，得出来的结果会不太好，k太大，得出的结果的也不太好。为什么 k 太大，得出的结果也不好呢？因为我们之前的假设是 Euclidean distance只是在很近的距离里面可以这样想，当 k 很大的时候，你会考虑很远的点，所以你不应该把它考虑进来，你的 k 要选一个适当的值。

## 拉普拉斯特征映射

![mark](http://images.iterate.site/blog/image/20190818/iUqunEOlWnVU.png?imageslim)


另外一个方法是 Laplaciain Eigenmaps，它的想法是这样子的。我们之前在讲 smei-supervised learning的时候，我们有讲过 smothness assumption。如果你要比较这两个红点之间的距离，算他们的 Euclidean diastance是不足够的。你要看的是在他们 high distant range之间的 distance，如果两个点之间有 high density collection，那它们才是真正的很接近。

那你可以用 graph 来描述这件事，你把你的 data point construct成一个 graph(把比较近的点连起来，变成一个 graph)，你把点变成 graph 以后，你考虑 smoothness 的距离，就可以被 graph 上面的 collection 来 approximate

![mark](http://images.iterate.site/blog/image/20190818/5NhrSGGFNLOw.png?imageslim)


我们之间在讲 semi-supervised learning的时候，如果 $x^1,x^2$ 在 high density region上面它们是相近的，那它们的 laebl$y^1,y^2$ 很有可能是一样的。所以做 supervised learning的时候，我们可以这么做：我们考虑有 label data的项，另一个跟 laebl data没有关系的(可以利用 unlabel data)，这一项的作用是：要考虑我们现在得到的 label 是不是 smooth 的，它的作用很像 regularization term。S这一项等于 $y^i$ 跟 $y^j$ 之间的距离 乘以 $w_{i,j}$($w_{i,j}$ 的意思是：两个 data point它们在图上是相连的，$w_{i,j}$ 是它们的相似程度；在图上没有相连就是 0)。如果 $x^i$ 跟 $x^j$ 很接近的话，那 $w_{i,j}$ 就是一个很大的值。这个 S 就是 evaluate 你现在得到的 label 有多么的 smooth，这个 S 还可以写成 $S=y^TLy$(L是 graph laplacian，L=D-W)

![mark](http://images.iterate.site/blog/image/20190818/6mopnC7JbzIR.png?imageslim)


也可以在 Unsupervised learning上面，如果说 $x^1,x^2$ 在 high desity region 是 close 的，那我们就希望 $z^1,z^2$ 也是相近的。我们刚才 smooth 式子写出来，如果 $x^i,x^j$ 两个 data point很像，那 $z^i,z^j$ 做完 dimension reduction以后距离就很近，反之 $w_{i,j}$ 很小，距离要怎样都可以。

找一个 $z^i,z^j$minimize S的值，这样做的话是有问题的。你不需要告诉我 $w_{i,j}$ 是什么，要 minimize S的时候，我选 $z^i=z^j=0$，S=0，这样的话就结。所以光做这样的式子是不够的，你可能会说在 supervised learning的时候，你咋不讲这句话呢？之前在 semi-supervised learning的时候，我们还有 supervised learning 给我们的那一项。如果你把所有的 label 都设成一样的，那你在 supervised 那一项，你得到的 loss 就会很大。那我们要同时 supervised 跟 semi-supervised那一项，所以你不选择让所有的 y 通通一样的。这边少了 supervised 的东西，所以选择所以的 z 都是一样，反而是一个最好的 solution。

所以这件事是行不通的，你要给你的 z 一些 constraints：如果 z 降维以后是 M 维空间，你不会希望说：你的 z 它还分布在比 M 还要小的 dimension 里面。我们现在要做的是希望把高维空间中塞进去的低维空间展开，我们不希望展开的结果是在更低维的空间里面。今天假设你的 z 的 dimension 是 M，你会希望你找出来的那些点(假设现在总共有 N 个点，$z^1$ 到 $z^N$)它们做 span 以后会等于 $R^M$

如果你要解这个式子的话，你会发现解出来 z 跟我们前面看到的 graph Laplacian L是有关系的，它其实 graph Laplacian eigenvector。

## T-分布随机领域

![mark](http://images.iterate.site/blog/image/20190818/R3MOdcjwVmMN.png?imageslim)




前面的那些问题是：它只假设相近的点应该要是接近的，但它没有假设说不相近的点没有要接近(不相近的点要分开)。比如说 LLE 在 MNIST 上你会遇到这样的情形：它确实会把同一个 class 聚集在一起，但它没有防止不同 class 的点不要叠成一团。

![mark](http://images.iterate.site/blog/image/20190818/XGKUApT8bc5P.png?imageslim)


t-SNE也是在做降维，把原来的 data point x变成 low dimension vector z。在原来的 space 上面，我们会计算所有点的 pair，$x^i,x^j$ 之间的 similarity(S($x^i,x^j$))。我们会计算一个 $x^j$given $ x^i$，分子是 $x^i,x^j$ 的 similarity，分母是 summation over除了 $x^i$ 以外所有其他的点之间的距离。假设我们今天找出了一个 low dimension z，我们也可以计算 $x^i,x^j$ 之间的 similarity，$z^j$given $z^i$，分子是 $z^i,z^j$ 的 similarity，分母是 summation over除了 $z^i$ 以外所有其他的点之间的距离。有做这个 normlization 是很有必要的，不知道他们的 scale 是否一样，那你有做这个 normlization 就可以把他们变为几率，值会介于 0-1之间。

我们现在还不知道 $z^i,z^j$ 他么的值是多少(它是被我们找出来的)，我们希望找一组 $z^i,z^j$，它可以让这两个 distribution 越接近越好。

那怎么衡量两个 distribution 之间的相似度呢？就是 KL 距离。所以我们要做的事情就是找一组 z，它可以做到，$x^i$ 对其他 point 的 distribution 跟 $z^i$ 对其他 point 的 distribution，这样的 distribution 之间的 KL 距离越小越好，然后 summation over 所以的 data point，你要使得这这个值(L)越小越好。




你在 t-SNE的时候，它会计算所有 data point的 similarity，所以它的运算有点大。在 data point比较多的时候，t-SNE比较麻烦。第一个常见的做法是：你会先做降维，比如说：你原来的 dimension 很大，你不会直接从很高的 dimension 直接做 t-SNE，因为你这样计算 similarity 时间会很长，你通常会先用 PCA 做将降维，降到 50 维，再用 t-SNE降到 2 维，这个是比较常见的做法。

如果你给 t-SNE一个新的 data point，它是没办法做的。它只能够你先给它一大群 x，它帮你把每个 x 的 z 先找出来，但你找完这些 z 以后，你再给它一个新的 x，你要重新跑一遍这一整套演算法，所以就很麻烦。你有一大堆的 x 是 high dimension，你想要它在二维空间的分布是什么样子，你用 t-SNE，t-SNE会给你往往不错的结果。

![mark](http://images.iterate.site/blog/image/20190818/shackQIunKet.png?imageslim)


t-SNE这个 similarity 的选择是非常神妙的，我们在原来的 data point space上面，evaluate similarity 是计算 $x^i,x^j$ 之间的 Euclidean distance，取一个负号，再取 exponent，这种方法比较好，因为它可以确保说只有非常相近的点才有值，exponential掉的比较快，只要距离一拉开，similarity就会变得很小。 在 t-SNE之前，有一个方法叫做 SNE：dimension reduction以后的 space，它选择 measure 跟原来的 space 是一样的。t-SNE神妙的地方在：dimension reduction以后的 space，它选择 measure 跟原来的 space 是不一样的，它在 dimension reduction之后选的 space 是 t distribution的其中一种($S^{'}(z^i,z^j)=\frac{1}{1+||z^i-z^j||_2}$)



![mark](http://images.iterate.site/blog/image/20190818/4Cgn4bPjXzxp.png?imageslim)
为什么要这样做呢，我可以提供一个很直觉的理由，假设横轴代表了在原来 space 上面的 Euclidean distance或者是做 dimension reduction以后的 Euclidean distance。其中红色这条线是：$exp(-||x^i-x^j||_2)$，蓝色这条线是：$\frac{1}{1+||z^i-z^j||_2}$。 图中的两个点做 dimension reduction以后，要怎样才能维持它原来人的 space 呢？变成图中这个样子。原来在高维空间里面，如果距离很近，那做完 transform 以后，它还是很近。原来就有一段距离，做完 transform 以后，会被拉的很远。



![mark](http://images.iterate.site/blog/image/20190818/lAvr6nsce8u7.png?imageslim)


所以，t-SNE画出来的图往往长的这样，它会把你的 data point 聚集成一群一群的，只要你的 data point离的比较远，那做完 t-SNE之后，就会强化，变得更远了。

![mark](http://images.iterate.site/blog/image/20190818/2EqELbBRAhMC.png?imageslim)


![mark](http://images.iterate.site/blog/image/20190818/DBYMdm3IDl3e.png?imageslim)


![mark](http://images.iterate.site/blog/image/20190818/aWiBY3lJWeJ7.png?imageslim)


如图为 t-SNE的动画。因为这是利用 gradient descent 来 train 的，所以你会看到随着 iteration process点会被分的越来越开。





# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)
