---
title: 02 以编程方式部署应用程序
toc: true
date: 2019-07-02
---
11.2 以编程方式部署应用程序
不像 Jupyer 笔记本，当你使用 spark-submit命令时，你需要自己准备 SparkSession 并且将它配置好以便你的应用程序能够正常运行。
在本节中，我们将学习如何创建和配置 SparkSession，以及如何对 Spark 使用外部模块。如果你没有创建 Databricks 或者 Microsoft 的免费账户（或者其他 Spark 的供应商），别担心，我们仍将使用你本地的机器，因为这样做更容易开始。不过，如果你决定将应用程序部署到云上，那么在提交作业时，只需要更改——master参数即可。
11.2.1 配置你的 SparkSession
以编程方式使用 Jupyter 和提交作业的主要区别是，你必须创造你的 Spark 上下文背景（Spark context）（和 Hive，如果你计划使用 HiveQL），而当使用 Jupter 运行 Spark，上下文背景会自动开始。
在本节中，我们将开发一个简单的应用程序，使用来自 Uber 的公开数据，包含 2016 年 6 月纽约地区的行程信息；从 https://s3.amazonaws.com/nyc-tlc/trip＋data/yellow_tripdata_2016-06.Csv下载数据集（当心，这是一个接近 3GB 的文件）。原始数据集包含 1100 万次行程，但是对于我们的示例，我们只检索了 330 万个，并且只选择所有可用列的子集。转换后的数据集可以从 http://www.tomdrabas.com/data/LearningPySpark/uber_data_nyc_2016-06_3m_partitioned.csv.zip下载。下载文件并将其从 GitHub 解压到第 13 章的文件夹。这个文件可能看起来很奇怪，因为它实际上是一个目录，里面包含了 4 个文件，用 Spark 读取时会形成一个数据集。
那么，让我们开始吧！
11.2.2 创建 SparkSession
到创建 SparkSession 时，Spark 2.0已经稍微变得比之前的版本简单了。事实上，目前 Spark 使用 SparkSession 来公开其更高级的功能，而不是明确地创建 SparkContext。以下是你该如何做：
以上代码就是你所需要的全部！如果你想使用 RDD 的 API，你还是可以继续使用。然而，当 SparkSession 在后台启动时，你不需要再创建一个 SparkContext。为了获得访问权，你可以简单地调用（借鉴前面的例子）sc＝spark.SparkContext。
在这个例子中，我们首先创建了 SparkSession 对象，并且调用了它的.builder内部类。.appName（……）方法允许我们给应用程序一个名字，而.getOrCreate（）方法则创建或获取了一个已经创建的 SparkSession。给你的应用程序一个有意义的名字是一个极好的约定，因为它有助于：（1）在集群中找到你的应用程序；（2）为每个人带来更少的混乱。在后台，Spark会话创建了一个 SparkContext 对象。当你在 SparkSession 上调用.stop（）时，实际上是在 SparkSession 中终止 SparkContext。
11.2.3 模块化代码
以这样的一种方式构建代码，以便以后重用该代码也还是一件值得做的事。同样可以用 Spark——你可以模块化你的方法，稍后再重用这些方法。这也有助于提高代码的可读性和可维护性。
在这个例子中，我们将建立一个模块，并且在数据集上做一些计算：这将计算出上车和下车位置的直线距离（按英里计算）（使用半正矢（Haversine）公式），并且将计算的距离从英里转化成公里。更多的半正矢公式可以在此找到：http://www.movable-type.co.uk/scripts/latlong.html。
所以，首先我们将构建一个模块。模块结构
我们把无关方法的代码放进 additionalCode 文件夹。看看这本书的 GitHub 库，如果你还没有看的话：https://github.com/drabastomek/learningPySpark/tree/master/Chapter11。
文件夹的树状结构如下所示：



你可以看到，它有一个稍微正常的 python 包结构：在顶层有一个 setup.py文件，所以可以打包我们的模块，在这些模块里有我们的代码。
本例中的 setup.py文件如下所示：
我们不会深入到结构细节（它本身是相当不言自明的）：你可以在此读到更多关于如何定义其他项目的 setup.py文件（https://pythonhosted.org/an_example_pypi_project/setuptools.html）。
在实用工具文件夹中的__init__.py文件有如下的代码：
它有效地公开了 geoCalc.py和 converter（马上会介绍更多有关内容）。计算两点之间的距离
我们提到的第一种方法采用了半正矢（Haversine）公式计算地图上两点之间的直线距离（直角坐标系）。执行此操作的代码位于该模块的 geocalc.py文件中。
calculateDistance（……）是 geoCalc 类的静态方法。它需要两个地理位置，表示为一个元组或者一个具有两个元素的列表（按顺序排列的纬度和经度），并且使用半正矢（Haversine）公式计算距离。计算距离所需的地球半径用英里表示，所以计算的距离也将是英里。转变距离单位
我们构建了实用程序包，以便它可以更通用。作为包的一部分，我们公开了在不同度量单位之间转换的方法。此时，我们仅将其限制到距离，但是功能上可以进一步扩展到其他区域，如面积、体积或者温度。
为了便于使用，作为 converter 实现的任何类都应该公开类似的接口。这就是为什么建议是要来自 BaseConverter 的类（查看 base.py）：
这是一个不能实例化的纯抽象类：它的唯一目的是强制派生类实现 convert（……）方法。参看 distance.py文件有关实施的细节。对于精通 python 的人来说，这段代码应该可以自己理解，所以此处我们不会逐一介绍。创建一个 egg 文件
既然我们已经有了所有的代码，那么就可以打包它们了。PySpark文档指出，你可以用逗号分隔传递.py文件（使用——py-files转换）给 spark-submit脚本。然而，更方便的是把模块打包进一个.zip或者一个.egg。当 setup.py文件方便使用时，你应该做的就是调用 additionalCode 文件夹里的内容：
如果一切顺利的话，你应该看到 3 个文件夹：PySparkUtilities.egg-info、build和 dist，我们感兴趣的是 dist 文件夹里的文件



：PySparkUtilities-0.1.dev0-py3.5.egg。运行之前的命令后，你可能会发现.egg文件的名称略有不同，因为你可能用了不同的 python 版本。你仍然可以在你的 Spark 作业中使用它，但是你必须要调整 spark-submit命令来反映出你的.egg文件名称。Spark中的用户定义函数
为了对在 PySpark 中的 DataFrame 执行操作，你有两个选择：使用内置函数来处理数据（大多数情况下都足以达到你的需求，作为更高性能的代码而被推荐）或者创建你自己用户定义的函数。
为了定义一个 UDF，你必须把 python 函数封装在.udf（……）方法中，并且定义它的返回值类型。以下便是我们如何在我们的脚本中来实现的（检查 calculatingGeoDistance.py文件）：
接下来我们可以使用这些函数来计算距离并换算成英里：
使用 withColumn（……）方法为我们感兴趣的值创建附加列。必须在此说一句谨慎的话。如果你使用 PySpark 内置函数，即使你称其为 python 对象，但在下一层中，该调用会被作为 Scala 代码翻译和执行。不过如果你用 python 写了自己的方法，这个方法是不会被译成 Scala 的，并且还必须要在驱动程序上执行。这将导致显著的性能障碍。查看堆栈溢出的详细信息：http://stackoverflow.com/questions/32464122/spark-performance-for-scala-vs-python。
现在我们把所有的谜题都放在一起，最终提交我们的作业。
11.2.4 提交作业
在你的命令行键入以下命令（假设你保持了和 GitHub 上一样结构不变的文件夹）：
我们还需要给你展示一些 launch_spark_submit.sh shell脚本的解释。在额外的章节中，我们配置了 Spark 的实例，运行 Jupyter（通过对 jupyter 设置 PYSPARK_DRIVER_PYTHON系统变量）。如果你用这个方法，在配置的机器上简单使用了 spark-submit，那么很可能会得到以下错误的其他版本：
因此，在运行 spark-submit命令之前，我们必须先还原该变量并且运行这段代码。这可能会突然变得令人厌倦，所以我们使用 launch_spark_submit.sh脚本让其自动化执行：





正如你所见，这只不过是一个 spark-submit命令的封套。
如果一切顺利，你会在命令行界面中看到下面的一长串输出：
阅读这些输出可以得到很多有用的信息：
·Spark当前的版本：2.1.0
·Spark UI（有助于跟踪你的作业进度）在 http://localhost：4040上成功启动
·我们的.egg文件已成功添加到执行中
·uber_data_nyc_2016-06_3m_partitioned.csv已经成功读取
·每个作业和任务的开始和结束都已经列出
一旦作业完成，你将看到类似于以下的内容：
从前面的截图来看，我们可以看到其正确地报告了距离。你还可以看到 Spark UI进程现在已经停止，并且所有清理作业都已执行。
11.2.5 监控执行



当你使用 spark-submit命令时，spark会启动一个允许你跟踪作业执行情况的本地服务器。以下是窗口显示内容：
在窗口的最顶部，你可以切换作业（Jobs）或者阶段（Stages）视图；作业视图允许你跟踪每一个执行完完整脚本的作业，而阶段视图则允许你跟踪所有执行的阶段。
你还可以在每个阶段执行配置文件中到达顶峰，并通过单击阶段的链接来跟踪每个任务的执行情况。在下面的截图中，你可以看到第 3 个阶段的执行配置文件，其中有 4 个任务正在执行：在一个集群设置中，而不是驱动程序或者本地服务器（driver/localhost）中，你会看到驱动程序的数量和主机的 IP 地址。
在一个作业或者阶段内，你可以点击 DAG 可视化来查看你的作业或者阶段是如何执行的（以下图表左边展示了作业视图，右边则展示了阶段视图）：1
