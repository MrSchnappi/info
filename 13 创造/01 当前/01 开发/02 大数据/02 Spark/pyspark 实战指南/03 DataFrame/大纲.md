---
title: 大纲
toc: true
date: 2019-07-02
---
DataFrame是一种不可变的分布式数据集，这种数据集被组织成指定的列，类似于关系数据库中的表。SchemaRDD作为 Apache Spark 1.0版本中的试验性功能，它在 Apache Spark 1.3版本中被命名为 DataFrame。对于熟悉 python pandas DataFrame或者 R DataFrame的读者，Spark DataFrame是一个近似的概念，即允许用户轻松地使用结构化数据（如数据表）。不过也存在一些差异，所以请一起期待吧。
通过在分布式数据集上施加结构，让 Spark 用户利用 Spark SQL来查询结构化的数据或使用 Spark 表达式方法（而不是 lambda）。在本章中，我们将给出两种方法的代码示例。通过构建数据，使得 Apache Spark引擎——具体来说就是 catalyst 优化器（Catalyst Optimizer）——显著提高了 Spark 的查询性能。Spark早期的 API 中（即 RDD），由于 Java JVM和 Py4J 之间的通信开销，使用 python 执行的查询会明显变慢。如果你熟悉 Spark 过去版本（Spark 1.x）中的 DataFrame，你会注意到 Spark 2.0中我们使用 SparkSession 来替代 SQLContext。各种 Spark 的上下文语境——HiveContext、SQLContext、StreamingContext和 SparkContext 都被整合到了 SparkSession。这样一来，只需要将此会话作为读取数据的入口点，和元数据、配置以及群集资源管理一起来使用。
更多的信息请参阅 How to use SparkSession in Apache Spark2.0（http://bit.ly/2br0Fr1）。
