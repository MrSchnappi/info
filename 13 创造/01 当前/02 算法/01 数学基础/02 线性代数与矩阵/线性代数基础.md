---
title: 线性代数基础
toc: true
date: 2019-02-18
---
# 线性代数

## 一、基本知识

1. 本书中所有的向量都是列向量的形式：

$$\mathbf{\vec x}=(x_1,x_2,\cdots,x_n)^T=\begin{bmatrix}x_1\\x_2\\  \vdots \\x_n\end{bmatrix}$$


本书中所有的矩阵 $\mathbf X\in \mathbb R^{m\times n}$ 都表示为：

$$\mathbf X = \begin{bmatrix}
x_{1,1}&x_{1,2}&\cdots&x_{1,n}\\
x_{2,1}&x_{2,2}&\cdots&x_{2,n}\\
\vdots&\vdots&\ddots&\vdots\\
x_{m,1}&x_{m,2}&\cdots&x_{m,n}\\
\end{bmatrix}$$

   简写为： $(x_{i,j})_ {m\times n}$ 或者 $[x_{i,j}]_ {m\times n}$ 。

2. 矩阵的`F`范数：设矩阵 $\mathbf A=(a_{i,j})_ {m\times n}$，则其`F` 范数为：$||\mathbf A||_ F=\sqrt{\sum_{i,j}a_{i,j}^{2}}$ 。<span style="color:red;">这个地方是平方吗？是 F 次方吧？确认下。</span>

   它是向量的 $L_2$ 范数的推广。

3. 矩阵的迹：设矩阵 $\mathbf A=(a_{i,j})_ {m\times n}$ ，则 $\mathbf A$ 的迹为：$tr(\mathbf A)=\sum_{i}a_{i,i}$ 。<span style="color:red;">这个地方是 i,i 吗？</span>

   迹的性质有：

   - $\mathbf A$ 的`F` 范数等于 $\mathbf A\mathbf A^T$ 的迹的平方根： $||\mathbf A||_ F=\sqrt{tr(\mathbf A \mathbf A^{T})}$ 。<span style="color:red;">为什么</span>
   - $\mathbf A$ 的迹等于 $\mathbf A^T$ 的迹：$tr(\mathbf A)=tr(\mathbf A^{T})$ 。<span style="color:red;">为什么</span>
   - 交换律：假设 $\mathbf A\in \mathbb R^{m\times n},\mathbf B\in \mathbb R^{n\times m}$ ，则有：$tr(\mathbf A\mathbf B)=tr(\mathbf B\mathbf A)$ 。
   - 结合律：$tr(\mathbf A\mathbf B\mathbf C)=tr(\mathbf C\mathbf A\mathbf B)=tr(\mathbf B\mathbf C\mathbf A)$ 。

## 二、向量操作

1. 一组向量 $\mathbf{\vec v}_1,\mathbf{\vec v}_2,\cdots,\mathbf{\vec v}_n$ 是线性相关的：指存在一组不全为零的实数 $a_1,a_2,\cdots,a_n$ ，使得：$\sum_{i=1}^{n}a_i\mathbf{\vec v}_i=\mathbf{\vec 0}$ 。

   一组向量 $\mathbf{\vec v}_1,\mathbf{\vec v}_2,\cdots,\mathbf{\vec v}_n$ 是线性无关的，当且仅当 $a_i=0,i=1,2,\cdots,n$ 时，才有：$\sum_{i=1}^{n}a_i\mathbf{\vec v}_i=\mathbf{\vec 0}$ 。<span style="color:red;">嗯。</span>

2. 一个向量空间所包含的最大线性无关向量的数目，称作该向量空间的维数。<span style="color:red;">最大线性无关向量的数目，是该向量空间的维数</span>

3. 三维向量的点积：$\mathbf{\vec u}\cdot\mathbf{\vec v} =u _ xv_x+u_yv_y+u_zv_z = |\mathbf{\vec u}| | \mathbf{\vec v}| \cos(\mathbf{\vec u},\mathbf{\vec v})$
 。

![](http://images.iterate.site/blog/image/20190218/VTy4fmgh3lBG.png?imageslim){ width=55% }

4. 三维向量的叉积：

$$\mathbf{\vec w}=\mathbf{\vec u}\times \mathbf{\vec v}=\begin{bmatrix}\mathbf{\vec i}& \mathbf{\vec j}&\mathbf{\vec k}
\\    u_x&u_y&u_z\\    v_x&v_y&v_z\\    \end{bmatrix}$$

   其中 $\mathbf{\vec i}, \mathbf{\vec j},\mathbf{\vec k}$ 分别为 $x,y,z$ 轴的单位向量。

$$\mathbf{\vec u}=u_x\mathbf{\vec i}+u_y\mathbf{\vec j}+u_z\mathbf{\vec k},\quad \mathbf{\vec v}=v_x\mathbf{\vec i}+v_y\mathbf{\vec j}+v_z\mathbf{\vec k}$$



   - $\mathbf{\vec u} $ 和 $\mathbf{\vec v}$ 的叉积垂直于 $\mathbf{\vec u},\mathbf{\vec v}$ 构成的平面，其方向符合右手规则。<span style="color:red;">右手规则补充下。</span>
   - 叉积的模等于 $\mathbf{\vec u},\mathbf{\vec v}$ 构成的平行四边形的面积
   - $\mathbf{\vec u}\times \mathbf{\vec v}=-\mathbf{\vec v}\times \mathbf{\vec u}$
   - $\mathbf{\vec u}\times( \mathbf{\vec v} \times \mathbf{\vec w})=(\mathbf{\vec u}\cdot \mathbf{\vec w})\mathbf{\vec v}-(\mathbf{\vec u}\cdot \mathbf{\vec v})\mathbf{\vec w} $ <span style="color:red;">这个是这样吗？</span>

   ![](http://images.iterate.site/blog/image/20190218/QhK8pFRSVeNv.png?imageslim){ width=55% }

5. 三维向量的混合积：

$$[\mathbf{\vec u} \;\mathbf{\vec v} \;\mathbf{\vec w}]=(\mathbf{\vec u}\times \mathbf{\vec v})\cdot \mathbf{\vec w}= \mathbf{\vec u}\cdot (\mathbf{\vec v} \times \mathbf{\vec w})\\    =\begin{vmatrix} u_x&u_y&u_z\\    v_x&v_y&v_z\\    w_x&w_y&w_z \end{vmatrix} =\begin{vmatrix} u_x&v_x&w_x\\    u_y&v_y&w_y\\    u_z&v_z&w_z\end{vmatrix} $$

   其物理意义为：以 $\mathbf{\vec u} ,\mathbf{\vec v} ,\mathbf{\vec w}$ 为三个棱边所围成的平行六面体的体积。 当 $\mathbf{\vec u} ,\mathbf{\vec v} ,\mathbf{\vec w}$ 构成右手系时，该平行六面体的体积为正号。

6. 两个向量的并矢：给定两个向量 $\mathbf {\vec x}=(x_1,x_2,\cdots,x_n)^{T},   \mathbf {\vec y}= (y_1,y_2,\cdots,y_m)^{T}$ ，则向量的并矢记作：

$$\mathbf {\vec x}\mathbf {\vec y}  =\begin{bmatrix}x_1y_1&x_1y_2&\cdots&x_1y_m\\    x_2y_1&x_2y_2&\cdots&x_2y_m\\    \vdots&\vdots&\ddots&\vdots\\    x_ny_1&x_ny_2&\cdots&x_ny_m\\    \end{bmatrix}$$

   也记作 $\mathbf {\vec x}\otimes\mathbf {\vec y}$ 或者 $\mathbf {\vec x} \mathbf {\vec y}^{T}$。<span style="color:red;">并矢的概念之前有接触过吗？这个是用在什么地方的？</span>

## 三、矩阵运算

1. 给定两个矩阵 $\mathbf A=(a_{i,j}) \in \mathbb R^{m\times n},\mathbf B=(b_{i,j})  \in \mathbb R^{m\times n}$ ，定义：

   - 阿达马积`Hadamard product`（又称作逐元素积）：
   $$\mathbf A \circ \mathbf B  =\begin{bmatrix} a_{1,1}b_{1,1}&a_{1,2}b_{1,2}&\cdots&a_{1,n}b_{1,n}\\    a_{2,1}b_{2,1}&a_{2,2}b_{2,2}&\cdots&a_{2,n}b_{2,n}\\    \vdots&\vdots&\ddots&\vdots\\    a_{m,1}b_{m,1}&a_{m,2}b_{m,2}&\cdots&a_{m,n}b_{m,n}\end{bmatrix}$$
   - 克罗内积`Kronnecker product`：
   $$\mathbf A \otimes \mathbf B =\begin{bmatrix}a_{1,1}\mathbf B&a_{1,2}\mathbf B&\cdots&a_{1,n}\mathbf B\\    a_{2,1}\mathbf B&a_{2,2}\mathbf B&\cdots&a_{2,n}\mathbf B\\    \vdots&\vdots&\ddots&\vdots\\    a_{m,1}\mathbf B&a_{m,2}\mathbf B&\cdots&a_{m,n}\mathbf B \end{bmatrix}$$

2. 设  为  阶向量，  为  阶方阵，则有：


1. 如果  是一元函数，则：

   - 其逐元向量函数为： 。

   - 其逐矩阵函数为：

   - 其逐元导数分别为：

2. 各种类型的偏导数：

   - 标量对标量的偏导数：  。

   - 标量对向量（ 维向量）的偏导数 ： 。

   - 标量对矩阵( 阶矩阵)的偏导数：

   - 向量（ 维向量）对标量的偏导数：  。

   - 向量（ 维向量）对向量 ( 维向量) 的偏导数（雅可比矩阵，行优先）



     如果为列优先，则为上面矩阵的转置。

   - 矩阵( 阶矩阵)对标量的偏导数

3. 对于矩阵的迹，有下列偏导数成立：

4. 假设  是关于  的矩阵值函数（），且  是关于  的实值函数（），则下面链式法则成立：


## 四、特殊函数

1. 这里给出机器学习中用到的一些特殊函数。

### 4.1 sigmoid 函数

1. `sigmoid`函数：



   - 该函数可以用于生成二项分布的  参数。
   - 当  很大或者很小时，该函数处于饱和状态。此时函数的曲线非常平坦，并且自变量的一个较大的变化只能带来函数值的一个微小的变化，即：导数很小。

   ![img](http://www.huaxiaozhuan.com/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/imgs/algebra/sigmoid.png)

### 4.2 softplus 函数

1. `softplus`函数： 。

   - 该函数可以生成正态分布的  参数。
   - 它之所以称作`softplus`，因为它是下面函数的一个光滑逼近： 。

   ![img](http://www.huaxiaozhuan.com/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/imgs/algebra/softplus.png)

2. 如果定义两个函数：



   则它们分布获取了  的正部分和负部分。

   根据定义有： 。而  逼近的是 ，  逼近的是 ，于是有：

3. `sigmoid`和`softplus`函数的性质：



   其中  为反函数。

    也称作`logit`函数。

   ![img](http://www.huaxiaozhuan.com/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/imgs/algebra/sigmoid_softplus.png)

### 4.3 伽马函数

1. 伽马函数定义为：



   ![img](http://www.huaxiaozhuan.com/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/imgs/algebra/gamma.jpg)

   性质为：

   - 对于正整数  有：  。

   -  ，因此伽马函数是阶乘在实数域上的扩展。

   - 与贝塔函数的关系：

   - 对于  有：



     则可以推导出重要公式：  。

   - 对于 ，伽马函数是严格凹函数。

2. 当  足够大时，可以用`Stirling` 公式来计算`Gamma`函数值： 。

### 4.4 贝塔函数

1. 对于任意实数  ，定义贝塔函数：



   其它形式的定义：

2. 性质：

   - 连续性：贝塔函数在定义域  内连续。

   - 对称性： 。

   - 递个公式：

   - 当  较大时，有近似公式：

   - 与伽马函数关系：

     - 对于任意正实数  ，有：

     -  。
