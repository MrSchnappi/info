# 分类损失函数

- 0-1 损失函数
- 对数损失函数
- 交叉熵代价函数 Cross Entropy Loss
- 合页损失 Hinge Loss
- 指数损失 Exponential Loss
- Modified Huber Loss
- Softmax Loss
- 分类损失函数对比



# 0-1 损失函数


场景：

- 二分类问题。

<p align="center">
    <img width="80%" height="70%" src="http://images.iterate.site/blog/image/20190902/JQb8uyG1g92S.png?imageslim">
</p>


概念：

$$
L(Y, f(x)) =
\begin{cases}
1,& Y\ne f(x)\\
0,& Y = f(x)
\end{cases}
$$


可适当放宽条件：

$$
L(Y, f(x)) =
\begin{cases}
1,& |Y-f(x)|\geqslant T\\
0,& |Y-f(x)|< T
\end{cases}
$$


缺点：

- 对每个错分类点都施以相同的惩罚（损失为 1），这样对犯错比较大的点（ys 远小于 0）无法进行较大的惩罚，所有犯错点都同等看待，这不符合常理，不太合适。
- 不连续、非凸、不可导，难以使用梯度优化算法。

实际使用：

- 实际应用中，0-1 损失很少使用。


# 对数损失函数

应用：

- 常见的逻辑回归使用的就是对数损失函数。
  - 注意：很多人认为逻辑回归的损失函数是平方损失，其实不然。逻辑回归它假设样本服从伯努利分布（0-1分布），进而求得满足该分布的似然函数，接着取对数求极值等。逻辑回归推导出的经验风险函数是最小化负的似然函数，从损失函数的角度看，就是对数损失函数。

概念：

$$
L(Y, P(Y|X)) = -\log{P(Y|X)}
$$


## 对数损失函数是如何度量损失的


举例：

例如，在高斯分布中，我们需要确定均值和标准差。

如何确定这两个参数？最大似然估计是比较常用的方法。最大似然的目标是找到一些参数值，这些参数值对应的分布可以最大化观测到数据的概率。

因为需要计算观测到所有数据的全概率，即所有观测到的数据点的联合概率。现考虑如下简化情况：

（1）假设观测到每个数据点的概率和其他数据点的概率是独立的。
（2）取自然对数。

假设观测到单个数据点 $x_i(i=1,2,...n)$ 的概率为：

$$
P(x_i;\mu,\sigma)=\frac{1}{\sigma \sqrt{2\pi}}\exp
\left( - \frac{(x_i-\mu)^2}{2\sigma^2} \right)
$$

（3）其联合概率为：

$$
P(x_1,x_2,...,x_n;\mu,\sigma)=\frac{1}{\sigma \sqrt{2\pi}}\exp
\left( - \frac{(x_1-\mu)^2}{2\sigma^2} \right) \\ \times
 \frac{1}{\sigma \sqrt{2\pi}}\exp
\left( - \frac{(x_2-\mu)^2}{2\sigma^2} \right) \times ... \times
\frac{1}{\sigma \sqrt{2\pi}}\exp
\left( - \frac{(x_n-\mu)^2}{2\sigma^2} \right)
$$

​对上式取自然对数，可得：

$$
\ln(P(x_1,x_2,...x_n;\mu,\sigma))=
\ln \left(\frac{1}{\sigma \sqrt{2\pi}} \right)- \frac{(x_1-\mu)^2}{2\sigma^2}  \\ +
\ln \left( \frac{1}{\sigma \sqrt{2\pi}} \right)- \frac{(x_2-\mu)^2}{2\sigma^2} +...+
\ln \left( \frac{1}{\sigma \sqrt{2\pi}} \right)- \frac{(x_n-\mu)^2}{2\sigma^2}
$$

根据对数定律，上式可以化简为：

$$
\ln(P(x_1,x_2,...x_n;\mu,\sigma))=-n\ln(\sigma)-\frac{n}{2} \ln(2\pi)\\
-\frac{1}{2\sigma^2}[(x_1-\mu)^2+(x_2-\mu)^2+...+(x_n-\mu)^2]
$$

然后求导为：


$$
\frac{\partial\ln(P(x_1,x_2,...,x_n;\mu,\sigma))}{\partial\mu}=
\frac{n}{\sigma^2}[\mu - (x_1+x_2+...+x_n)]
$$
​

上式左半部分为对数损失函数。<span style="color:red;">为什么左半部分是对数损失函数？这与 $log{P(Y|X)}$ 不同吧？</span> 损失函数越小越好，因此我们令等式左半的对数损失函数为 0，可得：

$$
\mu=\frac{x_1+x_2+...+x_n}{n}
$$

同理，可计算 $\sigma ​$。






# 交叉熵代价函数 Cross Entropy Loss

补充：

- 与平方误差损失函数的对比没写。
- 二次代价函数适合输出神经元是线性的情况，交叉熵代价函数适合输出神经元是 S 型函数的情况。 是这样吗？

场景：

- 二分类。


## 二分类问题的交叉熵损失

主要有两种形式：

- 输出的标签的表示方式为 $\{0,1\}$
- 输出的标签的表示方式为 $\{-1,+1\}$


**输出的标签的表示方式为 $\{0,1\}$**

交叉熵损失表达式为：

$$
L=-[y \log \hat{y}+(1-y) \log (1-\hat{y})]
$$


推导：

- 从极大似然性的角度出发，预测类别的概率可以写成：

$$
P(y | x)=\hat{y}^{y} \cdot(1-\hat{y})^{(1-y)}
$$

- 因此，当真实样本标签 $y = 1$ 时，$P(y | x)$ 为：

$$
P(y=1 | x)=\hat{y}
$$

- 当真实样本标签 $y = 0$ 时，$P(y | x)$ 为：

$$
P(y=0 | x)=1-\hat{y}
$$

- 我们希望的是概率 $P(y|x)$ 越大越好。
- 首先，我们对 $P(y|x)$ 引入 $\log$ 函数，因为 $\log$ 运算并不会影响函数本身的单调性。则有：

$$
\begin{array}{ll}
\log P(y | x)&= \log \left(\hat{y}^{y} \cdot(1-\hat{y})^{(1-y)}\right) \\&= y \log \hat{y}+(1-y) \log (1-\hat{y})
\end{array}
$$


- 我们希望 $\log P(y|x)$ 越大越好，反过来，只要 $\log P(y|x)$ 的负值 $-\log P(y|x)$ 越小就行了。那我们就可以引入损失函数，且令 $Loss = -\log P(y|x)$ 即可。则得到损失函数为：


$$
L=-[y \log \hat{y}+(1-y) \log (1-\hat{y})]
$$

- 我们来看，当 $y = 1$ 时：

$$
L=-\log \hat{y}
$$

- 因为，<span style="color:red;">为什么？ s 是什么？</span>

$$
\hat{y}=\frac{1}{1+e^{-s}}
$$

- 所以，代入到 $L$ 中，得：

$$
L=\log \left(1+e^{-s}\right)
$$

这时候，Loss 的曲线如下图所示：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/o0cFuzOCt6sX.png?imageslim">
</p>


从图中明显能够看出，$s$ 越大于零，$L$ 越小，函数的变化趋势也完全符合实际需要的情况。<span style="color:red;">为什么要看 s 与 L 的关系？</span>

- 当 $y = 0$ 时：

$$
L=-\log (1-\hat{y})
$$

- 对上式进行整理，同样能得到：

$$
L=\log \left(1+e^{s}\right)
$$

这时候，Loss 的曲线如下图所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/1vT7sXQ8LjzA.png?imageslim">
</p>


从图中明显能够看出，$s$ 越小于零，$L$ 越小，函数的变化趋势也完全符合实际需要的情况。

**输出的标签的表示方式为 $\{-1,+1\}$**

此时交叉熵损失为：

$$
L=\log \left(1+e^{-y s}\right)
$$

<span style="color:red;"> ys 是什么？</span>

推导：

- $ys$ 的符号反映了预测的准确性，而且，$ys$ 的数值大小也反映了预测的置信程度。所以，从概率角度来看，预测类别的概率可以写成：<span style="color:red;">为什么？</span>

$$
P(y | x)=g(y s)
$$

- 分别令 $y = +1$ 和 $y = -1$ 就能很容易理解上面的式子。

- 接下来，同样引入 $log$ 函数，要让概率最大，反过来，只要其负数最小即可。那么就可以定义相应的损失函数为：

$$
L=-\log g(y s)=-\log \frac{1}{1+e^{-y s}}=\log \left(1+e^{-y s}\right)
$$

这时候，我们以 $ys$ 为横坐标，可以绘制 Loss 的曲线如下图所示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/KN5eco4EL9Yk.png?imageslim">
</p>


其实上面介绍的两种形式的交叉熵损失是一样的，只不过由于标签的表示方式不同，公式稍有变化。

标签用 $\{-1,+1\}$ 表示的好处是可以把 $ys$ 整合在一起，作为横坐标，容易作图且具有实际的物理意义。


## 交叉熵损失的优点

交叉熵损失的优点是：

- 在整个实数域内，Loss 近似线性变化。
  - 尤其是当 $ys << 0$ 的时候，Loss 更近似线性。这样，模型受异常点的干扰就较小。 
- 交叉熵损失连续可导，便于求导计算，是使用最广泛的损失函数之一。



# 合页损失 Hinge Loss

应用：

- 多用于支持向量机（SVM）中，体现了 SVM 距离最大化的思想。

合页损失标准形式：

$$
L(y) = \max{(0, 1-ty)}
$$

统一的形式：

$$
L(Y, f(x)) = \max{(0, Yf(x))}
$$

其中：

- $y$ 是预测值，范围为$(-1,1)$
- $t$ 为目标值，其为 $-1$ 或 $1$。<span style="color:red;">什么是 t？</span>

图示：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/AFDbAjeFbVQS.png?imageslim">
</p>



可见：

- 只有当 $ys < 1$ 时，Loss 才大于零
- 对于 $ys > 1$ 的情况，Loss 始终为零。


举例：

在线性支持向量机中，最优化问题可等价于

$$
\underset{\min}{w,b}\sum_{i=1}^N (1-y_i(wx_i+b))+\lambda\Vert w\Vert ^2
$$

上式相似于下式：<span style="color:red;">没清楚，为什么相似？</span>


$$
\frac{1}{m}\sum_{i=1}^{N}l(wx_i+by_i) + \Vert w\Vert ^2
$$

其中 $l(wx_i+by_i)$ 是 Hinge 损失函数，$\Vert w\Vert ^2$ 可看做为正则化项。


**优缺点：**

- 多用于支持向量机（SVM）中，体现了 SVM 距离最大化的思想。当 Loss 大于零时，是线性函数，便于梯度下降算法求导。
- 使得 $ys > 0$ 的样本损失皆为 0，由此带来了稀疏解，使得 SVM 仅通过少量的支持向量就能确定最终超平面。


# 指数损失 Exponential Loss

应用：

- 一般多用于 AdaBoost 中。

指数损失：

$$
L(Y, f(x)) = \exp(-Yf(x))
$$

推导：

- 类似于第二种形式的交叉熵 Loss，去掉 log 和 log 中的常数 1，并不影响 Loss 的单调性。因此，推导得出了指数损失的表达式。<span style="color:red;">补充。</span>



曲线如下：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/gFEnps7cI9NE.png?imageslim">
</p>

优点：

- 指数损失与交叉熵损失类似，但它是指数下降的，因此梯度较其它 Loss 来说，更大一些。
- Exponential Loss 一般多用于 AdaBoost 中。因为使用 Exponential Loss 能比较方便地利用加法模型推导出 AdaBoost算法。<span style="color:red;">如何推导？</span>


# Modified Huber Loss


Huber Loss 集合了 MSE 和 MAE 的优点，也能应用于分类问题中，称为 Modified Huber Loss。

表达式：

$$
L(y, s)=\left\{\begin{array}{ll}{\max (0,1-y s)^{2},} & {y s \geq-1} \\ {-4 y s,} & {y s<-1}\end{array}\right.
$$

曲线：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/A0TrWV4I21wh.png?imageslim">
</p>

优点：

- Modified Huber Loss 结合了 Hinge Loss 和 交叉熵 Loss 的优点。
  - 一方面能在 $ys > 1$ 时产生稀疏解提高训练效率
  - 另一方面对于 $ys < −1$ 样本的惩罚以线性增加，这意味着受异常点的干扰较少。


举例：

- scikit-learn 中的 SGDClassifier 就使用了 Modified Huber Loss。<span style="color:red;">不知</span>




# Softmax Loss

应用：

- 多用于神经网络多分类问题中。

公式：

$$
L=-\log \frac{e^{s}}{\sum_{j=1}^{C} e^{s_{j}}}=-s+\log \sum_{j=1}^{C} e^{s_{j}}
$$

推导：

- 机器学习模型的 Softmax 层，正确类别对于的输出是：

$$
S=\frac{e^{s}}{\sum_{j=1}^{C} e^{s_{j}}}
$$

- 说明：
    - $C$ 为类别个数
    - 小写字母 $s$ 是正确类别对应的 Softmax 输入
    - 大写字母 $S$ 是正确类别对应的 Softmax 输出。

- 由于 $log$ 运算符不会影响函数的单调性，我们对 $S$ 进行 $log$ 操作。另外，我们希望 $log(S)$ 越大越好，即正确类别对应的相对概率越大越好，那么就可以对 $log(S)$ 前面加个负号，来表示损失函数：

$$
L=-\log \frac{e^{s}}{\sum_{j=1}^{C} e^{s_{j}}}=-s+\log \sum_{j=1}^{C} e^{s_{j}}
$$

曲线：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/dLOBpxhxzHmE.png?imageslim">
</p>


说明：

- 当 $s<<0$ 时，Softmax 近似线性
- 当 $s>>0$ 时，Softmax 趋向于零


优点：

- 受异常点的干扰较小




# 分类损失函数对比


最后，我们将 

- 0-1 Loss
- Cross Entropy Loss
- Hinge Loss
- Exponential Loss
- Modified Huber Loss

画在一张图中：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/3iumpccC1aMb.png?imageslim">
</p>




上图 $ys$ 的取值范围是 $[-2,+2]$，若我们把 $ys$ 的坐标范围取得更大一些，上面 5 种 Loss 的差别会更大一些，见下图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190902/7NcqsvAw48va.png?imageslim">
</p>


显然，这时候 Exponential Loss 会远远大于其它 Loss。从训练的角度来看，模型会更加偏向于惩罚较大的点，赋予其更大的权重。如果样本中存在离群点，Exponential Loss 会给离群点赋予更高的权重，但却可能是以牺牲其他正常数据点的预测效果为代价，可能会降低模型的整体性能，使得模型不够健壮（robust）。

相比 Exponential Loss，其它四个 Loss，包括 Softmax Loss，都对离群点有较好的“容忍性”，受异常点的干扰较小，模型较为健壮。

