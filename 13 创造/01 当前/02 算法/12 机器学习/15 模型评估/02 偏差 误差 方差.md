
# 偏差 误差 方差

Error = Bias + Variance + Noise


- 误差 Error
- 偏差 Bias
- 方差 Variance

说明：


OK，我们回顾一下上面几个偏差、方差、噪声的公式：

- 误差。
  - 一般把学习器的实际预测值与样本的真实标签之间的差异称为“误差”。反映的是整个模型的准确度。
  - Error = Bias + Variance + Noise
- 偏差
  - 衡量模型拟合训练数据的能力（注：训练数据不一定是整个训练集，而是指用于训练它的那一部分数据，例如：mini-batch）
  - 偏差反映的是模型在样本上的输出与真实值之间的误差，即模型本身的拟合能力。
  - 偏差越小，拟合能力越高，越可能产生过拟合。
- 方差
  - 方差描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。公式：$S_{N}^{2}=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}$
  - 方差越大，数据的分布越分散，模型的稳定程度越差。
  - 方差越小，模型的泛化的能力越高；反之，模型的泛化的能力越低。
  - 如果模型在训练集上拟合效果比较优秀，但是在测试集上拟合效果比较差劣，则方差较大，说明模型的稳定程度较差，出现这种现象可能是由于模型对训练集过拟合造成的。 如下图右列所示。
- 噪声。
  - 描述了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。


<p align="center">
    <img width="99%" height="70%" src="http://images.iterate.site/blog/image/20190826/46JXI1pijVxo.png?imageslim">
</p>




# 偏差-方差分解

“偏差-方差分解” 是解释学习算法泛化性能的一种重要工具。

以回归任务为例：

- 假设：
  - 测试样本 $\boldsymbol{x}$
  - $y$ 为 $\boldsymbol{x}$ 的真实的标记，
  - $y_{d}$ 为 $\boldsymbol{x}$ 在数据集中的标记。（说明：有可能出现噪声使得 $y_{d} \neq y$）
  - $f$ 为训练集 $d$ 上学得模型 $f$ 在 $\boldsymbol{x}$ 上的预测输出。

- 以回归任务为例，学习算法的期望预测为：

$$
\overline{f}=E_{d}[f]
$$

- 使用样本数相同的不同训练集产生的方差为：

$$
\operatorname{var}(\boldsymbol{x})=E_{d}\left[(f-\overline{f})^{2}\right]
$$

- 噪声为：

$$
\varepsilon^{2}=E_{d}\left[\left(y_{d}-y\right)^{2}\right]
$$

- 期望输出与真实标记的差别称为偏差 (bias) ，即

$$
\operatorname{bias}^{2}(\boldsymbol{x})=(\overline{f}-y)^{2}
$$

- 为便于讨论，假定噪声期望为零，即 $E_d[y_d-y]=0$ 。
- 通过简单的多项式展开合并，可对算法的期望泛化误差进行分解：

$$
\begin{aligned} E(f ; d) =&E_{d}\left[\left(f-y_{d}\right)^{2}\right] \\=& E_{d}\left[\left(f-\overline{f}+\overline{f}-y_{d}\right)^{2}\right] \\=& E_{d}\left[(f-\overline{f})^{2}\right]+E_{d}\left[\left(\overline{f}-y_{d}\right)^{2}\right] \\ &+E_{d}\left[2(f-\overline{f})\left(\overline{f}-y_{d}\right)\right] \\ =&E_{d}\left[(f-\overline{f})^{2}\right]+E_{d}\left[\left(\overline{f}-y_{d}\right)^{2}\right] \\=& E_{d}\left[(f-\overline{f})^{2}\right]+E_{d}\left[\left(\overline{f}-y+y-y_{d}\right)^{2}\right] \\=& E_{d}\left[(f-\overline{f})^{2}\right]+E_{d}\left[(\overline{f}-y)^{2}\right]+E_{d}\left[\left(y-y_{d}\right)^{2}\right] \\ &+2 E_{d}\left[(\overline{f}-y)\left(y-y_{d}\right)\right]\\=&E_{d}\left[(f-\overline{f})^{2}\right]+(\overline{f}-y)^{2}+E_{d}\left[\left(y_{d}-y\right)^{2}\right]\end{aligned}
$$

- 说明：
  - 中间的第三步到第四步，由期望预测的公式，最后一项为 0
  - 倒数第二步到倒数第一步，因为噪声期望为 0，因此最后一项为 0.
- 于是：

$$
E(f ; D)=\operatorname{bias}^{2}(\boldsymbol{x})+\operatorname{var}(\boldsymbol{x})+\varepsilon^{2}
$$

- 也就是说，泛化误差可分解为偏差、方差与噪声之和.



偏差-方差分解说明，泛化性能是由下面三个原因共同决定的：

- 学习算法的能力
- 数据的充分性
- 学习任务本身的难度


# 偏差-方差窘境

一般来说，偏差与方差是有冲突的，这称为偏差-方差窘境：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200305/UU8MhBPfTt5i.png?imageslim">
</p>


训练程度不同时：

- 训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导了泛化错误率；
- 训练程度的加深, 学习器的拟合能力逐渐増强，训练数据发生的扰动渐渐能被学习器学到，方差， 逐渐主导了泛化错误率；
- 在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合。