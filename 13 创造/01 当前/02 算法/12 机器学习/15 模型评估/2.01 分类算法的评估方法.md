

# 术语与混淆矩阵

假设我们的分类目标只有两类，计为正例（positive）和负例（negative）

术语：

- True positives(TP):  实际为正例且被分类器划分为正例。
- False positives(FP): 实际为负例但被分类器划分为正例。
- False negatives(FN):实际为正例但被分类器划分为负例。
- True negatives(TN): 实际为负例且被分类器划分为负例。　

混淆矩阵：

<p align="center">
	<img width="80%" height="70%" src="http://images.iterate.site/blog/image/20200305/PHtEVmD4aggc.png?imageslim">
</p>

说明：

- P，N 反映的是分类器的分类结果。
- T、F 反映的是分类器是否区分正确。
  -  TP 的实际类标 $=1*1=1$ 为正例
  -  FP 的实际类标 $=(-1)*1=-1$ 为负例
  -  FN 的实际类标 $=(-1)*(-1)=1$ 为正例
  -  TN 的实际类标 $=1*(-1)=-1$ 为负例



# 精度 错误率


- 错误率 error rate
- 精度 accuracy 即正确率

含义：

- 错误率：分类错误的样本数占样本总数的比例
- 精度：分类正确的样本数占样本总数的比例。 

计算：

- accuracy = (TP+TN)/(P+N)
- error rate = (FP+FN)/(P+N)

关系：

- 精度= 1-错误率：


对于分类任务：

$$
E(f ; D)=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right) \neq y_{i}\right)
$$

$$
\begin{aligned} \operatorname{acc}(f ; D) &=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right)=y_{i}\right) \\ &=1-E(f ; D) \end{aligned}
$$

对于预测任务：（概率密度函数 $p(\cdot )$ ）

$$
E(f ; \mathcal{D})=\int_{\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x}) \neq y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
$$

$$
\begin{aligned} \operatorname{acc}(f ; \mathcal{D}) &=\int_{\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x})=y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \\ &=1-E(f ; \mathcal{D}) \end{aligned}
$$





## 查准率 查全率


- 查准率 precision  
- 查全率 recall 即召回率 也即 sensitivity 灵敏度

说明：

- 查准率：是精确性的度量，分为正例的示例中实际为正例的比例。
- 查全率：是覆盖面的度量，度量有多个正例被正确的划分为正例

计算：

- precision=TP/(TP+FP)
- recall=TP/(TP+FN)=TP/P=sensitivity

应用：

- 查全率：
  - 例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。
  - 对于地震的预测，越全越好。
  - 在信息检索、Web搜索中，经常会关心 “检索出的信息中有多少比例是用户感兴趣的” ，“用户感兴趣的信息中有多少被检索出来了”。


关系：

查准率和查全率是一对矛盾的度量。一般来说：

- 查准率高时，查全率往往偏低；
- 查全率高时，查准率往往偏低。

举例：

- 如果我们希望将好瓜尽可能多地选出来， 那么可以通过增加选瓜的数量来实现，如果将所有西瓜都选上，那么所有的好瓜也必然都被选上了，但是这样的查准率就会较低。
- 如果我们希望选出的瓜中好瓜比例尽可能的高，那么我们就可以只挑选最有把握的瓜，但是这样就难免会漏掉不少好瓜，使得查全率较低。

通常只有在一些简单任务中，才可能使得查全率和查准率都很高。<span style="color:red;">嗯，好吧 看来还是有可能都高的。</span>


## 综合分类率 F1-score

**F1-score**

- 综合分类率 F1-score

说明：

- 综合考虑查准率与查全率。

计算：

- $$F 1=\frac{2 \times precision \times recall}{precision + recall}=\frac{2 \times T P}{样例总数+T P-T N}$$


**多个二分类混淆矩阵时的 F1**

<span style="color:red;">什么时候会使用？</span>

- 进行了多次训练/测试，每次得到一个混淆矩阵
- 或者是在多个数据集上进行了训练/测试，我们希望能够估计算法的 “全局” 性能；
- 或者是执行多分类任务，每两两类别的组合都对应一个混淆矩阵。<span style="color:red;">这也可以？</span>
- ……

我们希望在 $n$ 个二分类混淆矩阵上综合考察查准率和查全率。怎么办呢？

一种直接的做法是先在各混淆矩阵上分别计算出查准率和查全率， 记为 $(P_1,R_1),(P_2,R_2),\cdots ,(P_n,R_n)$ ，再计算平均值，这样就得到了宏查准率 （macro-P）、宏查全率 （macro-R），以及相应的宏 F1 （macro-F1）:

$$
P_{macro}=\frac{1}{n} \sum_{i=1}^{n} P_{i}
$$
$$
R_{macro}=\frac{1}{n} \sum_{i=1}^{n} R_{i}
$$
$$
F 1_{macro}=\frac{2 \times P_{macro} \times R_{macro}}{P_{macro}+R_{macro}}
$$

我们还可以将各混淆矩阵的对应元素进行平均，得到 $TP$、$FP$、$TN$、$FN$ 的平均值，分别记为 $\overline{TP}$ 、$\overline{FP}$ 、$\overline{TN}$ 、$\overline{FN}$ ，然后，我们再基于这些平均值计算出 “微查准率”（micro-P）、“微查全率”（micro-R）和 “微 F1” （micro-F1）:
$$
P_{micro}=\frac{\overline{T P}}{\overline{T P}+\overline{F P}}
$$
$$
R_{micro}=\frac{\overline{T P}}{\overline{T P}+\overline{F N}}
$$
$$
F 1_{micro}=\frac{2 \times P_{micro} \times R_{micro}}{P_{micro}+R_{micro}}
$$


对比：


- 宏平均 F1 与微平均 F1 是以两种不同的平均方式求的全局 F1 指标。
  - 宏平均 F1 的计算方法先对每个类别单独计算 F1 值，再取这些 F1 值的算术平均值作为全局指标。
  - 微平均 F1 的计算方法是先累加计算各个类别的 a、b、c、d的值，再由这些值求出 F1 值。
- 由两种平均 F1 的计算方式不难看出，宏平均 F1 平等对待每一个类别，所以它的值主要受到稀有类别的影响，而微平均 F1 平等考虑文档集中的每一个文档，所以它的值受到常见类别的影响比较大。


**根据对查准率与查全率的重视程度 $F_\beta$**

在一些应用中，对查准率和查全率的重视程度是有所不同的，比如：

- 在商品推荐系统中，为了尽可能的少打扰用户，我们更希望推荐内容是用户感兴趣的，这个时候查准率就更重要。
- 而在逃犯信息检索系统中，我们更希望尽可能的少漏掉逃犯，因此这个时候查全率更加重要。


F1 度量的一般形式 $F_\beta$ ，能够让我们表达出对查准率/查全率的不同偏好，它是基于查准率与查全率的加权调和平均：<span style="color:red;">之前从来不知道还有  $F_\beta$ ，有可能看到过，但是忘记了。</span>

$$
\frac{1}{F_{\beta}}=\frac{1}{1+\beta^{2}} \cdot\left(\frac{1}{P}+\frac{\beta^{2}}{R}\right)
$$

$$
F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}
$$

其中 $\beta >0$ 度量了查全率对查准率的相对重要性：

- $\beta =1$ 时退化为标准的 F1
- $\beta >1$ 时查全率有更大影响
- $\beta <1$ 时查准率有更大影响



## 特异性

- 特异性 specificity

说明：

- 表示的是所有负例中被分对的比例，衡量了分类器对负例的识别能力。

计算：

- specificity = TN/N




## ROC 曲线

ROC曲线（Receiver Operating Characteristic Curve，受试者工作特征曲线）


坐标轴：

- 以灵敏度（真阳性率）即 TP/P 为纵坐标，
- 以 1 -特异性（假阳性率）即 1-TN/N为横坐标

比较：

- 可以将不同模型对同一数据集的 ROC 曲线绘制在同一笛卡尔坐标系中，ROC 曲线越靠近左上角，说明其对应模型越可靠。

ROC曲线：

<p align="center">
  <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190722/9uJ6zDwOAU8A.png?imageslim">
</p>

AUC（Area Under Curve）：

- ROC 曲线下面的面积

AUC 评价：

- AUC 是衡量二分类模型优劣的一种评价指标，表示正例排在负例前面的概率。
- AUC越大，模型越可靠。
- AUC 比精确度、准确率、召回率更为常用。



一般在分类模型中，预测结果都是以概率的形式表现，如果要计算准确率，通常都会手动设置一个阈值来将对应的概率转化成类别，这个阈值也就很大程度上影响了模型准确率的计算。

举例：

现在假设有一个训练好的二分类器对 10 个正负样本（正例 5 个，负例 5 个）预测，得分按高到低排序得到的最好预测结果为 $[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]$，即 5 个正例均排在 5 个负例前面，正例排在负例前面的概率为 100%。然后绘制其 ROC 曲线，由于是 10 个样本，除去原点我们需要描 10 个点，如下：

<center>

![](http://images.iterate.site/blog/image/20190722/NzswJpnT2Eww.png?imageslim){ width=55% }

</center>


​描点方式按照样本预测结果的得分高低从左至右开始遍历。从原点开始，每遇到 1 便向 $y$ 轴正方向移动 $y$ 轴最小步长 1 个单位，这里是 1/5=0.2；每遇到 0 则向 $x$ 轴正方向移动 $x$ 轴最小步长 1 个单位，这里也是 0.2。不难看出，上图的 AUC 等于 1，印证了正例排在负例前面的概率的确为 100%。

​假设预测结果序列为 $[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]$。

<center>

![](http://images.iterate.site/blog/image/20190722/Yq5vwVM8rQom.png?imageslim){ width=55% }

</center>


​计算上图的 AUC 为 0.96 与计算正例与排在负例前面的概率 0.8 × 1 + 0.2 × 0.8 = 0.96相等，而左上角阴影部分的面积则是负例排在正例前面的概率 0.2 × 0.2 = 0.04。

​假设预测结果序列为 $[1, 1, 1, 0, 1, 0, 1, 0, 0, 0]$。

<center>

![](http://images.iterate.site/blog/image/20190722/qWGxky3R8qU6.png?imageslim){ width=55% }

</center>

计算上图的 AUC 为 0.88与计算正例与排在负例前面的概率 0.6 × 1 + 0.2 × 0.8 + 0.2 × 0.6 = 0.88相等，左上角阴影部分的面积是负例排在正例前面的概率 0.2 × 0.2 × 3 = 0.12。



AUC 使用：

- AUC 实际上是目标检测中常用的评价指标平均精度（Average Precision, AP）。AP越高，说明模型性能越好。<span style="color:red;">是目标检测中常用的吗？</span>


# P-R 曲线

我们可以根据预测结果的可能性的概率对样例进行排序：

- 排在前面的是学习器认为最可能是正例的样本，
- 排在最后的则是学习器认为最不可能是正例的样本。

OK，对于这个队列，我们逐个以某个样本为分界：

- 排在这个样本前面的认为是正例
- 排在这个样本后面的认为是反例

这样可以对于每个样本计算出以它为分界时候的查准率和查全率。

然后，我们以：

- 查准率为纵轴
- 查全率为横轴

作图，就得到了查准率-查全率曲线，简称 P-R曲线。

下图为三个模型对应的三条 P-R 曲线与平衡点示意图：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180713/JjBF4HAl99.png?imageslim">
</p>

说明：

- 如果一个学习器的 P-R 曲线被另一个学习器的曲线完全 “包住” ，则可断言后者的性能优于前者。因为查准率与查全率越大越好。例如上图中学习器 A 的性能优于学习器 C 。
- 如果两个学习器的 P-R曲线发生了交叉，例如上图中的 A 与 B，则难以一般性地断言两者孰优孰劣，可以：
  - 在具体的查准率或查全率条件下进行比较。
  - 比较 P-R 曲线下面积的大小，在一定程度上表征了学习器在查准率和查全率上取得相对 “双高” 的比例。不过面积不太容易估算。
  - 可以看下 “平衡点” (Break-Event Point，简称 BEP )，平衡点是 “查准率=查全率” 时的取值，例如上图中学习器 C 的 BEP 是 0.64，基于 BEP 的比较，我们可以认为学习器 A 优于 B。<span style="color:red;">为什么平衡点可以用来衡量？</span>

注意：

- 为绘图方便和美观，示意图显示出单调平滑曲线，但现实任务中的 P-R 曲线常是非单调、不平滑的， 在很多局部有上下波动。



