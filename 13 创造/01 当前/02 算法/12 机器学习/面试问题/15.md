---
title: 15
toc: true
date: 2018-07-30
---
# 机器学习和深度学习中一些值得思考的问题



**神经网络参数初始化**

神经网络一般用梯度下降法进行训练，参数初始值的设定是一个需要考虑的问题。和 logistic 回归，支持向量机等模型不同，神经网络的权重和偏置项不能用 0 或者 1 这样简单的固定值来初始化，而是用随机数进行初始化。

在编程实现时，最常用的两种随机数是正态分布和均匀分布。前者用某一区间如[0,1]内的均匀分布随机数初始化权重值。后者一般用 0 均值，单位方差的正态分布 N(0, 1)产生随机数来初始化权重。具体实现时，这些随机分布的参数如均值和方差可以用更复杂的策略来确定。这有什么理论依据吗？哪种初始化方法更好？我们先来看几个主要开源库的实现。



OpenCV中实现了多层感知器模型，即多层全连接神经网络。它的初始化采用了均匀分布的随机数，具体的，是 Nguyen-Widrow算法，使用的是均匀分布的随机数，根据本层和前一层的神经元数量对随机的权重值进行了缩放。具体做法如下：

1.生成[-1,1]内均匀分布的随机数

2.对这个随机数进行归一化，归一化系数由本层和前一层神经数，本层所有权重之和决定

3.将权重设置会归一化后的随机数



再来看 Caffe 中的实现。它提供了 7 种初始化策略：常量初始化（constant），高斯分布初始化（gaussian），positive_unitball初始化，均匀分布初始化（uniform），xavier初始化，MSRA初始化，双线性初始化（bilinear）。

常量初始化把权值或着偏置初始化为一个常数，这常数由用户自己定义。

均匀分布初始化用均匀分布的随机数来初始化网络参数，均匀分布的上限和下限值由用户自己定义。

高斯分布初始化用正态分布随机数初始化网络参数。正态分布的均值和方差由用户自己定义。

positive_unitball初始化也是用均匀分布的随机数初始化，但保证每个神经元与前一层所有神经元的连接的权重值之和为 1。

Xavier初始化采用了文献[1]中提出的方法，也是用均匀分布的随机数来初始化权重值。

MSRA初始化采用了文献[2]提出的方法，使用的是正态分布的随机数。

Bilinear使用的双线程插值公式来初始化权重值。



TensorFlow中也使用了正态分布，均匀分布这些随机数来初始化参数值，限于篇幅，我们不在这里详细介绍，感兴趣的读者可以阅读文档和源代码。



如果读者对神经网络权重初始化这一话题感兴趣，可以阅读上面的参考文献以及源代码。至于哪种初始化策略更好，这个至少目前小编还没有找到非常严格的理论分析。在讨论中，有群友指出：

均匀分布的随机数能够让更多的权重接近于 0

这和网络的结构，神经元的数量，激活函数的类型有关。到目前为止，小编至少没有找到有理论说服力的文章来证明一种初始化方法一定比另外一种好，能支撑结论的大多还是一些实验，而这些实验的结论是否具有普适性是有争议的。



**深度神经网络的泛化能力**

泛化能力是机器学习算法核心的指标之一。在复杂问题如机器视觉，语音识别上，深度学习模型的泛化能力比之前的经典算法要强。比如行人检测，用 HOG+SVM或者 HOG+AdaBoost的方案在训练集上能达到 95%以上的准确率，但在测试集上可能只有 70%多，在实际应用时更低。而卷积网络在测试集上也能达到 80%多的准确率。从一般情况下看，深度学习算法确实相对来说有更强的泛化能力。

但在实际使用时，同样会有过拟合的问题。典型的例子是人脸识别，在 LFW 上准确率可以接近 100%，实际使用时可能会非常糟糕。群友们对深度学习的泛化能力问题也展开了讨论，有群友指出了文章[3]，这篇文章分析了神经网络的泛化误差，同时这篇文章也充满了争议，感兴趣的读者可以阅读。



**分类问题为什么用交叉熵而不用欧氏距离做损失函数？**

在用神经网络做分类时，我们一般选择交叉熵作为损失函数，而不用欧氏距离，这是是为什么？有群友提出了这个问题。其他群友给出了下面一些答案：

如果用交叉熵，能保证神经网络训练时是一个凸优化问题。显然这种回答是错误的，凸函数的复合并不一定是凸函数。

用交叉熵，如果当前值与目标值相差很远，则梯度下降法迭代时收敛的更快一些。这个答案也站不住脚，仔细想想用欧氏距离和交叉熵作为损失函数时梯度下降法的迭代公式就知道了。

具体答案是什么？小编找到了之前看过的一篇文献[6]，这篇文章既有理论分析，也有实验结果比较，感兴趣的读者可以阅读。在这里我们不做过多的解释。



**Relu****和 dropout 的区别**

有群友问到了 Relu 和 dropout 的区别，Relu在输入变量小于 0 时导数值为 0，可以起到正则化的作用，而 dropout 也是一种正则化机制，二者有什么区别？

下面我们汇总了各位群友的回答：

Relu的主要作用是减轻梯度消失问题，因为输入变量大于 0 时到数值为 1，和 sigmoid，tanh相比，不容易产生饱和，如果想对 Relu 有跟深入的了解，可以阅读文献[8]。Dropout是一种正则化机制，在训练时通过随机让一些神经元不起作用，使得模型更稀疏，关于 Dropout 还有更深的解释，请阅读文献[7]。

某一群友的答案非常经典：

Relu是强制正则化，而 Dropout 是随机正则化。



**不同的激活函数在性能上到底有多大区别？**

目前可供选择的激活函数很多，究竟应该怎么选择，它们之间有没有比较实验？有群友提出了这个问题。对这个问题还没有一个明确的答案。

sigmoid和 tanh 在早期用的比较多，现在 ReLU 以及它的改进型用的更多，相比之下更不容易出现饱和问题。对于各种激活函数的对比实验，小编目前还没有找到，理论分析比较的文章也没有，如果有读者找到了，欢迎和我们联系！



# 相关


- [机器学习和深度学习中一些值得思考的问题](https://www.tensorinfinity.com/paper_58.html)
