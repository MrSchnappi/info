---
title: 是否可以将参数初始化为 0
toc: true
date: 2019-08-31
---

## 神经网络训练时是否可以将全部参数初始化为 0 ？

考虑全连接的深度神经网络，同一层中的任意神经元都是同构的，它们拥有相同的输入和输出，如果再将参数全部初始化为同样的值，那么无论前向传播还是反向传播的取值都是完全相同的。学习过程将永远无法打破这种对称性，最终同一网络层中的各个参数仍然是相同的。<span style="color:red;">是的呀。</span>


因此，我们需要随机地初始化神经网络参数的值，以打破这种对称性。简单来说，我们可以初始化参数为 $\left(-\frac{1}{\sqrt{d}}, \frac{1}{\sqrt{d}}\right)$ 取值范围的均匀分布，其中 $d$ 是一个神经元接受的输入维度。<span style="color:red;">为什么是这么一个范围？为什么这个范围与维度有关？</span>

对于偏置来说，可以被简单地设为 $0$，并不会导致参数对称的问题。<span style="color:red;">嗯。</span>



# 相关

- 《百面机器学习》
