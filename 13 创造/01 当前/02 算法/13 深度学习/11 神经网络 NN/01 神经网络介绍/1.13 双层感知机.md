---
title: 1.13 双层感知机
toc: true
date: 2018-06-26 16:01:33
---

# 双层感知机

要解决非线性可分问题，需考虑使用多层功能神经元。



我们来搭建一个可以学习 XOR 的网络。

## XOR 函数介绍

XOR 函数（"异或"逻辑）是两个二进制值 $x_1$ 和 $x_2$ 的运算。

- 当这些二进制值中恰好有一个为 1 时，XOR 函数返回值为 1。
- 其余情况下返回值为 0。

XOR 函数提供了我们想要学习的目标函数 $y = f^*(\boldsymbol x)$。我们的模型给出了一个函数 $y=f(\boldsymbol x; \boldsymbol \theta)$ 并且我们的学习算法会不断调整参数 $\boldsymbol \theta$ 来使得 $f$ 尽可能接近 $f^*$。

## 拟合训练集的挑战

在这个简单的例子中，我们不会关心统计泛化。我们希望网络在这四个点 $\mathbb X =\{[0, 0]^\top, [0, 1]^\top, [1, 0]^\top, [1, 1]^\top\}$ 上表现正确。我们会用全部这四个点来训练我们的网络，唯一的挑战是拟合训练集。

我们可以把这个问题当作是回归问题，并使用均方误差损失函数。我们选择这个损失函数是为了尽可能简化本例中用到的数学。


则评估整个训练集上表现的 MSE 损失函数为

$$
J(\boldsymbol{\theta})=\frac{1}{4} \sum_{\boldsymbol{x} \in \mathbb{X}}\left(f^{*}(\boldsymbol{x})-f(\boldsymbol{x} ; \boldsymbol{\theta})\right)^{2}
$$

我们现在必须要选择我们模型 $f(\boldsymbol x; \boldsymbol \theta)$ 的形式。假设我们选择一个线性模型，$\boldsymbol \theta$ 包含 $\boldsymbol w$ 和 $b$，那么我们的模型被定义成

$$
f(\boldsymbol x; \boldsymbol w, b) = \boldsymbol x^\top \boldsymbol w + b.
$$

我们可以使用正规方程关于 $\boldsymbol w$ 和 $b$ 最小化 $J(\boldsymbol \theta)$，来得到一个闭式解。

解正规方程以后，我们得到 $\boldsymbol w = 0$ 以及 $b = \frac{1}{2}$。线性模型仅仅是在任意一点都输出 0.5。为什么会发生这种事？

下图演示了线性模型为什么不能用来表示 XOR 函数。解决这个问题的其中一种方法是使用一个模型来学习一个不同的特征空间，在这个空间上线性模型能够表示这个解。


<center>

![](http://images.iterate.site/blog/image/20190710/Jd9pf6mzLzsq.png?imageslim){ width=55% }

</center>

> 6.1 通过学习一个表示来解决 XOR 问题。图上的粗体数字标明了学得的函数必须在每个点输出的值。
>
> - (左) 直接应用于原始输入的线性模型不能实现 XOR 函数。当 $x_1 = 0$ 时，模型的输出必须随着 $x_2$ 的增大而增大。当 $x_1 = 1$ 时，模型的输出必须随着 $x_2$ 的增大而减小。而线性模型必须对 $x_2$ 使用固定的系数 $w_2$。因此，线性模型不能使用 $x_1$ 的值来改变 $x_2$ 的系数，从而不能解决这个问题。<span style="color:red;">是的。</span>
> - (右) 在由神经网络提取的特征表示的变换空间中，线性模型现在可以解决这个问题了。在我们的示例解决方案中，输出必须为 1 的两个点折叠到了特征空间中的单个点。换句话说，非线性特征将 $\boldsymbol x = [1, 0]^\top$ 和 $\boldsymbol x = [0, 1]^\top$ 都映射到了特征空间中的单个点 $\boldsymbol h = [1, 0]^\top$ 。线性模型现在可以将函数描述为 $h_1$ 增大和 $h_2$ 减小。<span style="color:red;">这个地方没明白，怎么折叠到特征空间的单个点的？怎么描述为  $h_1$ 增大和 $h_2$ 减小的？ </span>在该示例中，学习特征空间的动机仅仅是使得模型的能力更大，使得它可以拟合训练集。在更现实的应用中，学习的表示也可以帮助模型泛化。

## 看做前馈网络

具体来说，我们这里引入一个非常简单的前馈神经网络，它有一层隐藏层并且隐藏层中包含两个单元。

这个前馈网络有一个通过函数 $f^{(1)}(\boldsymbol x;\boldsymbol W, \boldsymbol c)$ 计算得到的隐藏单元的向量 $\boldsymbol h$。这些隐藏单元的值随后被用作第二层的输入。第二层就是这个网络的输出层。输出层仍然只是一个线性回归模型，只不过现在它作用于 $\boldsymbol h$ 而不是 $\boldsymbol x$。网络现在包含链接在一起的两个函数：$\boldsymbol h=f^{(1)}(\boldsymbol x; \boldsymbol W, \boldsymbol c)$ 和 $y = f^{(2)}(\boldsymbol h; \boldsymbol w, b)$，完整的模型是 $f(\boldsymbol x; \boldsymbol W, \boldsymbol c, \boldsymbol w, b) = f^{(2)}(f^{(1)}(\boldsymbol x))$。

如下图：

<center>

![](http://images.iterate.site/blog/image/20190710/juwopylK47P1.png?imageslim){ width=55% }


</center>

> 6.2 使用两种不同样式绘制的前馈网络的示例。具体来说，这是我们用来解决 XOR 问题的前馈网络。它有单个隐藏层，包含两个单元。
>
> - (左) 在这种样式中，我们将每个单元绘制为图中的一个节点。这种风格是清楚而明确的，但对于比这个例子更大的网络，它可能会消耗太多的空间。
> - (右) 在这种样式中，我们将表示每一层激活的整个向量绘制为图中的一个节点。这种样式更加紧凑。有时，我们对图中的边使用参数名进行注释，这些参数是用来描述两层之间的关系的。这里，我们用矩阵 $\boldsymbol W$ 描述从 $\boldsymbol x$ 到 $\boldsymbol h$ 的映射，用向量 $\boldsymbol w$ 描述从 $\boldsymbol h$ 到 $y$ 的映射。当标记这种图时，我们通常省略与每个层相关联的截距参数。<span style="color:red;">嗯。</span>


## 非线性的加入

$f^{(1)}$ 应该是哪种函数？线性模型到目前为止都表现不错，让 $f^{(1)}$ 也是线性的似乎很有诱惑力。可惜的是，如果 $f^{(1)}$ 是线性的，那么前馈网络作为一个整体对于输入仍然是线性的。暂时忽略截距项，假设 $f^{(1)}(\boldsymbol x)= \boldsymbol W^\top \boldsymbol x$ 并且 $f^{(2)}(\boldsymbol h)=\boldsymbol h^\top \boldsymbol w$，那么 $f(\boldsymbol x) = \boldsymbol w^\top\boldsymbol W^\top \boldsymbol x$。我们可以将这个函数重新表示成 $f(\boldsymbol x) = \boldsymbol x^\top\boldsymbol w'$ 其中 $\boldsymbol w' = \boldsymbol W\boldsymbol w$。


显然，我们必须用非线性函数来描述这些特征。大多数神经网络通过仿射变换之后紧跟着一个被称为激活函数的固定非线性函数来实现这个目标，其中仿射变换由学得的参数控制。我们这里使用这种策略，定义 $\boldsymbol h=g(\boldsymbol W^\top \boldsymbol x+\boldsymbol c)$，其中 $\boldsymbol W$ 是线性变换的权重矩阵，$\boldsymbol c$ 是偏置。此前，为了描述线性回归模型，我们使用权重向量和一个标量的偏置参数来描述从输入向量到输出标量的仿射变换。现在，因为我们描述的是向量 $\boldsymbol x$ 到向量 $\boldsymbol h$ 的仿射变换，所以我们需要一整个向量的偏置参数。

激活函数 $g$ 通常选择对每个元素分别起作用的函数，有 $h_i =g(\boldsymbol x^\top \boldsymbol W_{:, i} + c_i)$。在现代神经网络中，默认的推荐是使用由激活函数 $g(z)=\boldsymbol ax\{0, z\}$ 定义的整流线性单元或者称为 ReLU，如图 6.3 所示。


<center>

![](http://images.iterate.site/blog/image/20190710/tsQaM8wTlmYM.png?imageslim){ width=55% }

</center>

> 6.3 整流线性激活函数。该激活函数是被推荐用于大多数前馈神经网络的默认激活函数。将此函数用于线性变换的输出将产生非线性变换。 然而，函数仍然非常接近线性，在这种意义上它是具有两个线性部分的分段线性函数。由于整流线性单元几乎是线性的，因此它们保留了许多使得线性模型易于使用基于梯度的方法进行优化的属性。它们还保留了许多使得线性模型能够泛化良好的属性。计算机科学的一个通用原则是，我们可以从最小的组件构建复杂的系统。就像图灵机的内存只需要能够存储 0 或 1 的状态，我们可以从整流线性函数构建一个万能函数近似器。<span style="color:red;">嗯。我们可以从最小的组件构建复杂的系统。</span>


我们现在可以指明我们的整个网络是

$$
f(\boldsymbol x; \boldsymbol W, \boldsymbol c, \boldsymbol w, b) = \boldsymbol w^\top \boldsymbol ax\{ 0, \boldsymbol W^\top \boldsymbol x + \boldsymbol c \} +b.
$$

<span style="color:red;">是的。</span>


## 给出一个解

我们现在可以给出 XOR 问题的一个解。令

$$\begin{aligned}
\boldsymbol W &= \begin{bmatrix}
1 & 1\\
1 & 1
\end{bmatrix},\\
\boldsymbol c &= \begin{bmatrix}
0\\
-1
\end{bmatrix},\\
\boldsymbol w &= \begin{bmatrix}
1\\
-2
\end{bmatrix},
\end{aligned}$$

以及 $b=0$。

我们现在可以了解这个模型如何处理一批输入。令 $\boldsymbol X$ 表示设计矩阵，它包含二进制输入空间中全部的四个点，每个样本占一行，那么矩阵表示为：

$$
\boldsymbol X = \begin{bmatrix}
0 & 0\\
0 & 1\\
1 & 0\\
1 & 1
\end{bmatrix}.
$$

神经网络的第一步是将输入矩阵乘以第一层的权重矩阵：

$$
\boldsymbol X\boldsymbol W = \begin{bmatrix}
0 & 0\\
1 & 1\\
1 & 1\\
2 & 2
\end{bmatrix}.
$$

然后，我们加上偏置向量 $\boldsymbol c$，得到

$$
\begin{bmatrix}
0 & -1\\
1 & 0\\
1 & 0\\
2 & 1
\end{bmatrix}.
$$

在这个空间中，所有的样本都处在一条斜率为 $1$ 的直线上。当我们沿着这条直线移动时，输出需要从 $0$ 升到 $1$，然后再降回 $0$。线性模型不能实现这样一种函数。为了用 $\boldsymbol h$ 对每个样本求值，我们使用整流线性变换：
$$
\begin{bmatrix}
0 & 0\\
1 & 0\\
1 & 0\\
2 & 1
\end{bmatrix}.
$$

这个变换改变了样本间的关系。它们不再处于同一条直线上了。<span style="color:red;">嗯，是的。</span>如图 6.1 所示，它们现在处在一个可以用线性模型解决的空间上。

我们最后乘以一个权重向量 $\boldsymbol w$:
$$
\begin{bmatrix}
0\\
1\\
1\\
0
\end{bmatrix}.
$$
神经网络对这一批次中的每个样本都给出了正确的结果。








<center>

![](http://images.iterate.site/blog/image/180626/11b2382672.png?imageslim){ width=55% }


</center>




在这个例子中，我们简单地指定了解决方案，然后说明它得到的误差为零。在实际情况中，可能会有数十亿的模型参数以及数十亿的训练样本，所以不能像我们这里做的那样进行简单地猜解。与之相对的，基于梯度的优化算法可以找到一些参数使得产生的误差非常小。我们这里给出的 XOR 问题的解处在损失函数的全局最小点，所以梯度下降算法可以收敛到这一点。梯度下降算法还可以找到 XOR 问题一些其他的等价解。梯度下降算法的收敛点取决于参数的初始值。在实践中，梯度下降通常不会找到像我们这里给出的那种干净的、容易理解的、整数值的解。<span style="color:red;">嗯，梯度下降算法的收敛点取决于参数的初始值。</span>


















# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
- 《深度学习》花书
