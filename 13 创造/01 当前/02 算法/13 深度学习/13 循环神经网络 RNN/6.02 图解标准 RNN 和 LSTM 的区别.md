---
title: 6.02 图解标准 RNN 和 LSTM 的区别
toc: true
date: 2019-09-03
---

### 6.11.2 图解标准 RNN 和 LSTM 的区别

​	所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层，如下图所示：

<center>

![](http://images.iterate.site/blog/image/20190722/YBxm7aruD4IX.png?imageslim){ width=75% }

</center>

<span style="color:red;">嗯，理解，不过，这个 A 是一个神经元，那么一个层要怎么表示？而且 h 到底是输出给谁？</span>

​	LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。

<center>

![](http://images.iterate.site/blog/image/20190722/TFMApF799vve.png?imageslim){ width=75% }

</center>

<span style="color:red;">嗯，以前看这个有点云里雾里，现在明白了，知道数据的流通是什么样的，但是，为什么这个结构是合理的？怎么构造出来的？</span>


注：上图图标具体含义如下所示：

<center>

![](http://images.iterate.site/blog/image/20190722/KVeAuD5zxml8.png?imageslim){ width=45% }

</center>

上图中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。
