---
title: 2.12 Unet 及各种变体
toc: true
date: 2019-09-01
---

**3.Unet及各种变体**

在生物医学图像处理中，得到图像中的每一个细胞的类别标签是非常关键的。生物医学中最大的挑战就是用于训练的图像是不容易获取的，数据量也不会很大。U-Net 是非常著名的解决方案，它在全连接卷积层上构建模型，对其做了修改使得它能够在少量的训练图像数据上运行，得到了更加精确的分割。

![img](https://pic1.zhimg.com/80/v2-1d916816f80af948ac0a211d9327e8ac_hd.jpg)





**5.Unet++**

**UNet ++**（嵌套 U-Net架构）用于更精确的分割。我们引入中间层来跳过 U-Net的连接，这自然形成了来自不同深度的多个新的上采样路径，集合了各种感受域的 U-Nets。

![img](https://pic3.zhimg.com/80/v2-221e781d752323b95dc2d681126ab976_hd.jpg)






## 9.3 U-Net

卷积网络被大规模应用在分类任务中，输出的结果是整个图像的类标签。然而，在许多视觉任务，尤其是生物医学图像处理领域，目标输出应该包括目标类别的位置，并且每个像素都应该有类标签。另外，在生物医学图像往往缺少训练图片。

所以，Ciresan 等人训练了一个卷积神经网络，用滑动窗口提供像素的周围区域（patch）作为输入来预测每个像素的类标签。这个网络有两个优点：<span style="color:red;">多少大的空间区域？</span>

1. 输出结果可以定位出目标类别的位置；<span style="color:red;">之前的 FCN 不行吗？</span>
2. 由于输入的训练数据是 patches，这样就相当于进行了数据增广，解决了生物医学图像数量少的问题。

但是，这个方法也有两个很明显缺点：

1. 它很慢，因为这个网络必须训练每个 patch，并且因为 patch 间的重叠有很多的冗余(冗余会造成什么影响呢？卷积核里面的 $W$，就是提取特征的权重，两个块如果重叠的部分太多，这个权重会被同一些特征训练两次，造成资源的浪费，减慢训练时间和效率，虽然说会有一些冗余，训练集大了，准确率不就高了吗？可是你这个是相同的图片啊，重叠的东西都是相同的，举个例子，我用一张相同的图片训练 $20$ 次，按照这个意思也是增大了训练集啊，可是会出现什么结果呢，很显然，会导致过拟合，也就是对你这个图片识别很准，别的图片就不一定了)。
2. 定位准确性和获取上下文信息不可兼得。大的 patches 需要更多的 max-pooling 层这样减小了定位准确性(为什么？因为你是对以这个像素为中心的点进行分类，如果 patch 太大，最后经过全连接层的前一层大小肯定是不变的，<span style="color:red;">不是没有全连接层了吗？</span>如果你 patch 大就需要更多的 pooling 达到这个大小，而 pooling 层越多，丢失信息的信息也越多)；小的 patches 只能看到很小的局部信息，包含的背景信息不够。<span style="color:red;">嗯。</span>

这篇论文建立了一个更好全卷积方法。我们定义和扩展了这个方法它使用更少的训练图片但产生更精确的分割。

<center>

![](http://images.iterate.site/blog/image/20190722/UnHnDCqLtqHk.png?imageslim){ width=85% }

</center>


1. 使用全卷积神经网络。(全卷积神经网络就是卷积取代了全连接层，全连接层必须固定图像大小而卷积不用，所以这个策略使得，你可以输入任意尺寸的图片，而且输出也是图片，所以这是一个端到端的网络。)
2. 左边的网络是收缩路径：使用卷积和 maxpooling。
3. 右边的网络是扩张路径：使用上采样产生的特征图与左侧收缩路径对应层产生的特征图进行 concatenate 操作。（pooling层会丢失图像信息和降低图像分辨率且是不可逆的操作，对图像分割任务有一些影响，对图像分类任务的影响不大，为什么要做上采样？因为上采样可以补足一些图片的信息，但是信息补充的肯定不完全，所以还需要与左边的分辨率比较高的图片相连接起来（直接复制过来再裁剪到与上采样图片一样大小）<span style="color:red;">怎么裁剪的？不是相同大小吗？而且，既然 pooling 会丢失图像信息，那么为什么还要进行 pooling ？</span>，这就相当于在高分辨率和更抽象特征当中做一个折衷，因为随着卷积次数增多，提取的特征也更加有效，更加抽象，上采样的图片是经历多次卷积后的图片，肯定是比较高效和抽象的图片，然后把它与左边不怎么抽象但更高分辨率的特征图片进行连接）。<span style="color:red;">大概理解了。不过还是有疑问。</span>
4. 最后再经过两次反卷积操作，生成特征图，再用两个 $1\times 1$ 的卷积做分类得到最后的两张 heatmap，例如第一张表示的是第一类的得分，第二张表示第二类的得分 heatmap，然后作为 softmax 函数的输入，算出概率比较大的 softmax 类，选择它作为输入给交叉熵进行反向传播训练。<span style="color:red;">什么是第一张和第二张？哪里来的两张？为什么第一张是第一类的得分？</span>

下面是 U-Net 模型的代码实现：

```py
def get_unet():
    inputs = Input((img_rows, img_cols, 1))
    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    # pool1 = Dropout(0.25)(pool1)
    # pool1 = BatchNormalization()(pool1)

    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)
    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    # pool2 = Dropout(0.5)(pool2)
    # pool2 = BatchNormalization()(pool2)

    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)
    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    # pool3 = Dropout(0.5)(pool3)
    # pool3 = BatchNormalization()(pool3)

    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)
    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)
    # pool4 = Dropout(0.5)(pool4)
    # pool4 = BatchNormalization()(pool4)

    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)
    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)

    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(
        2, 2), padding='same')(conv5), conv4], axis=3)
    # up6 = Dropout(0.5)(up6)
    # up6 = BatchNormalization()(up6)
    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)
    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)

    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(
        2, 2), padding='same')(conv6), conv3], axis=3)
    # up7 = Dropout(0.5)(up7)
    # up7 = BatchNormalization()(up7)
    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)
    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)

    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(
        2, 2), padding='same')(conv7), conv2], axis=3)
    # up8 = Dropout(0.5)(up8)
    # up8 = BatchNormalization()(up8)
    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)
    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)

    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(
        2, 2), padding='same')(conv8), conv1], axis=3)
    # up9 = Dropout(0.5)(up9)
    # up9 = BatchNormalization()(up9)
    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)
    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)

    # conv9 = Dropout(0.5)(conv9)

    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)

    model = Model(inputs=[inputs], outputs=[conv10])

    model.compile(optimizer=Adam(lr=1e-5),
                  loss=dice_coef_loss, metrics=[dice_coef])

    return model
```

<span style="color:red;">上面这个没有仔细看。</span>




# 相关

- [2019年最新基于深度学习的语义分割技术讲解](https://zhuanlan.zhihu.com/p/76418243)
