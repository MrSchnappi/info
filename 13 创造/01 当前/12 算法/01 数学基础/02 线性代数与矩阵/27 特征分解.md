---
title: 27 特征分解
toc: true
date: 2019-05-07
---
# 可以补充进来的

- 有些没有很明白，需要补充下，而且重新划分下。


# 特征分解



许多数学对象可以通过将它们分解成多个组成部分或者找到它们的一些属性来更好地理解。

这些属性是通用的，而不是由我们选择表示它们的方式所产生的。<span style="color:red;">嗯。</span>

例如，整数可以分解为质因数。我们可以用十进制或二进制等不同方式表示整数 $12$，但是 $12=2×2×3$ 永远是对的。从这个表示中我们可以获得一些有用的信息，比如 $12$ 不能被 $5$ 整除，或者 $12$ 的倍数可以被 $3$ 整除。<span style="color:red;">是的，这些属性不是由于我们选择表示他们的方式而产生的。</span>

正如我们可以通过分解质因数来发现整数的一些内在性质，我们也可以通过分解矩阵来发现矩阵表示成数组元素时不明显的函数性质。<span style="color:red;">哇塞，从这种类比进行切入真的有些厉害！</span>





特征分解(eigendecomposition)是使用最广的矩阵分解之一，即我们将矩阵分解成一组特征向量和特征值。<span style="color:red;">除了矩阵分解还有别的分解吗？</span><span style="color:blue;">嗯，还有奇异值分解，还有吗？</span>

方阵 $\boldsymbol{A}$ 的特征向量(eigenvector)是指与 $\boldsymbol{A}$ 相乘后相当于对该向量进行缩放的非零向量 $\boldsymbol{v}$：<span style="color:red;">对该向量进行缩放是什么意思？</span>

$$
\boldsymbol{A} \boldsymbol{v}=\lambda \boldsymbol{v}\tag{2.40}
$$


其中标量 $\lambda$ 称为这个特征向量对应的特征值(eigenvalue)。(类似地，我们也可以定义左特征向量(left eigenvector) $\boldsymbol{v}^{\top} \boldsymbol{A}=\lambda \boldsymbol{v}^{\top}$，但是通常我们更关注右特征向量(right eigenvector))。<span style="color:red;">哦，这个左特征向量和右特征向量以前没有注意过。嗯。</span>

如果 $\boldsymbol{v}$ 是 $\boldsymbol{A}$ 的特征向量，那么任何缩放后的向量 $s \boldsymbol{v}(s \in \mathbb{R}, \quad s \neq 0)$ 也是 $\boldsymbol{A}$ 的特征向量。<span style="color:red;">嗯。</span>此外，$s\boldsymbol{v}$ 和 $\boldsymbol{v}$ 有相同的特征值。基于这个原因，通常我们只考虑单位特征向量。<span style="color:red;">嗯。好的。</span>

假设矩阵 $\boldsymbol{A}$ 有 $n$ 个线性无关的特征向量 $\left\{\boldsymbol{v}^{(1)}, \ldots, \boldsymbol{v}^{(n)}\right\}$，对应着特征值 $\left\{\lambda_{1}, \dots, \lambda_{n}\right\}$。我们将特征向量连接成一个矩阵，使得每一列是一个特征向量：$V=\left[\boldsymbol{v}^{(1)}, \ldots, \boldsymbol{v}^{(n)}\right]$。类似地，我们也可以将特征值连接成一个向量 $\boldsymbol{\lambda}=\left[\lambda_{1}, \ldots, \lambda_{n}\right]^{\top}$。因此 A 的特征分解(eigendecomposition)可以记作：<span style="color:red;">嗯，有些厉害，讲的很清晰，不过还是要仔细琢磨下的。</span>

$$
\boldsymbol{A}=\boldsymbol{V} \operatorname{diag}(\boldsymbol{\lambda}) \boldsymbol{V}^{-1}\tag{2.41}
$$

我们已经看到了构建具有特定特征值和特征向量的矩阵，能够使我们在目标方向上延伸空间。<span style="color:red;">什么意思？在目标方向上延伸空间？没明白。</span>

然而，我们也常常希望将矩阵分解(decompose)成特征值和特征向量。这样可以帮助我们分析矩阵的特定性质，就像质因数分解有助于我们理解整数。<span style="color:red;">嗯，是的。</span>




不是每一个矩阵都可以分解成特征值和特征向量。在某些情况下，特征分解存在，但是会涉及复数而非实数。幸运的是，在本书中，我们通常只需要分解一类有简单分解的矩阵。具体来讲，每个实对称矩阵都可以分解成实特征向量和实特征值：<span style="color:red;">嗯。</span>


$$
\boldsymbol{A}=\boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top}
$$

其中 $\boldsymbol{Q}$ 是 $\boldsymbol{A}$ 的特征向量组成的正交矩阵，$\boldsymbol{\Lambda}$ 是对角矩阵。特征值 $\Lambda_{i, i}$ 对应的特征向量是矩阵 $\boldsymbol{Q}$ 的第 $i$ 列，记作 $\boldsymbol{Q}_{ :, i}$ 。因为  $\boldsymbol{Q}$ 是正交矩阵，我们可以将 $\boldsymbol{A}$ 看作沿方向 $\boldsymbol{v}^{(i)}$ 延展 $\lambda_{i}$ 倍的空间，如图 2.3所示。<span style="color:red;">嗯嗯，是的，这么一说还真的有点形象。</span>

<center>

![](http://images.iterate.site/blog/image/20190507/bRqJscGmGXDF.png?imageslim){ width=55% }

</center>


> 图 2.3　特征向量和特征值的作用效果。特征向量和特征值的作用效果的一个实例。在这里，矩阵 $\boldsymbol{A}$ 有两个标准正交的特征向量，对应特征值为 $\lambda_{1}$ 的 $\boldsymbol{v}^{(1)}$ 以及对应特征值为 $\lambda_{2}$ 的 $\boldsymbol{v}^{(2)}$。(左)我们画出了所有单位向量 $\boldsymbol{u} \in \mathbb{R}^{2}$ 的集合，构成一个单位圆。(右)我们画出了所有 $\boldsymbol{A} \boldsymbol{u}$ 点的集合。通过观察 $\boldsymbol{A}$ 拉伸单位圆的方式，我们可以看到它将 $\boldsymbol{v}^{(2)}$ 方向的空间拉伸了 $\lambda_{i}$ 倍。<span style="color:red;">是的是的，很清晰，很形象。</span>

虽然任意一个实对称矩阵 $\boldsymbol{A}$ 都有特征分解，但是特征分解可能并不唯一。如果两个或多个特征向量拥有相同的特征值，那么在由这些特征向量产生的生成子空间中，任意一组正交向量都是该特征值对应的特征向量。因此，我们可以等价地从这些特征向量中构成 $\boldsymbol{Q}$ 作为替代。按照惯例，我们通常按降序排列 $\boldsymbol{\Lambda}$ 的元素。在该约定下，特征分解唯一，当且仅当所有的特征值都是唯一的。<span style="color:red;">对于这个理解的没有很深。</span>

矩阵的特征分解给了我们很多关于矩阵的有用信息。矩阵是奇异的，当且仅当含有零特征值。实对称矩阵的特征分解也可以用于优化二次方程 $f(x)=x^{\top} A x$ ，其中限制 $\|x\|_{2}=1$ 。当 $x$ 等于 $\boldsymbol{A}$ 的某个特征向量时，$f$ 将返回对应的特征值。在限制条件下，函数 $f$ 的最大值是最大特征值，最小值是最小特征值。<span style="color:red;">有些厉害，但是感觉没有理解很深。</span>

所有特征值都是正数的矩阵称为正定(positive definite)；所有特征值都是非负数的矩阵称为半正定(positive semidefinite)。同样地，所有特征值都是负数的矩阵称为负定(negative definite)；所有特征值都是非正数的矩阵称为半负定(negative semidefinite)。半正定矩阵受到关注是因为它们保证 $\forall \boldsymbol{x}, \boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x} \geq 0$ 。此外，正定矩阵还保证 $\boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x}=0 \Rightarrow \boldsymbol{x}=\mathbf{0}$。<span style="color:red;">没明白。</span>




# 相关

- 《深度学习》花书
