---
title: 2.12 K-means 聚类
toc: true
date: 2019-08-28
---

# K 均值聚类


是划分方法中较经典的聚类算法之一。由于该算法的效率高，所以在对大规模数据进行聚类时被广泛应用。

特质如下：

- 一种简单的表示学习算法


支持向量机、逻辑回归、决策树等经典的机器学习算法主要用于分类问题，即根据一些已给定类别的样本，训练某种分类器，使得它能够对类别未知的样本进行分类。

与分类问题不同，聚类是在事先并不知道任何样本类别标签的情况下，通过数据之间的内在关系把样本划分为若干类别，使得同类别样本之间的相似度高，不同类别之间的样本相似度低。

图 5.1是一个二维空间中样本聚类的示意图，图 5.1（a）展示了所有样本在空间中的分布，图 5.1（b）展示了聚类的结果（不同颜色代表不同类别）。


![](http://images.iterate.site/blog/image/20190331/eyddEeSpbzkz.png?imageslim){ width=55% }


分类问题属于监督学习的范畴，而聚类则是非监督学习。

K均值聚类（K-Means Clustering）是最基础和最常用的聚类算法。

它的基本思想是，通过迭代方式寻找 K 个簇（Cluster）的一种划分方案，使得聚类结果对应的代价函数最小。特别地，代价函数可以定义为各个样本距离所属簇中心点的误差平方和：

$$
J(c, \mu)=\sum_{i=1}^{M}\left\|x_{i}-\mu_{c_{i}}\right\|^{2}\tag{5.1}
$$

其中 $x_i$ 代表第 $i$ 个样本，$c_i$ 是 $x_i$ 所属于的簇，$\mu_{ci}$ 代表簇对应的中心点，$M$ 是样本总数。

K均值聚类算法，ISODATA算法，EM算法（Expectation-Maximization Algorithm，最大期望算法）





k-均值聚类算法将训练集分成 k 个靠近彼此的不同样本聚类。因此我们可以认为该算法提供了 k-维的 one-hot编码向量 $\boldsymbol{h}$ 以表示输入 $\boldsymbol{x}$。当 $\boldsymbol{x}$ 属于聚类 $i$ 时，有 $h_i=1$， $\boldsymbol{h}$ 的其他项为零。



k-均值聚类提供的 one-hot编码也是一种稀疏表示，因为每个输入的表示中大部分元素为零。之后，我们会介绍能够学习更灵活的稀疏表示的一些其他算法(表示中每个输入 $\boldsymbol{x}$ 不只一个非零项)。one-hot编码是稀疏表示的一个极端示例，丢失了很多分布式表示的优点。one-hot编码仍然有一些统计优点(自然地传达了相同聚类中的样本彼此相似的观点)，也具有计算上的优势，因为整个表示可以用一个单独的整数表示。





k-均值聚类初始化 $k$ 个不同的中心点 $\left\{\boldsymbol{\mu}^{(1)}, \ldots, \boldsymbol{\mu}^{(k)}\right\}$，然后迭代交换两个不同的步骤直到收敛。步骤一，每个训练样本分配到最近的中心点 $\boldsymbol{\mu}^{(i)}$ 所代表的聚类 $i$。步骤二，每一个中心点 $\boldsymbol{\mu}^{(i)}$ 更新为聚类 $i$ 中所有训练样本 $\boldsymbol{x}^{(j)}$ 的均值。




# 原文及引用

- 《百面机器学习》
- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions) 原文
