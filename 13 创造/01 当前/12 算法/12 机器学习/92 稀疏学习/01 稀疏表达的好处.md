---
title: 01 稀疏表达的好处
toc: true
date: 2019-08-27
---
## 稀疏表达的好处


不妨把数据集 $D$ 考虑成一个矩阵，其每行对应于一个样本，每列对应于一个特征。

特征选择所考虑的问题是特征具有“稀疏性”，即矩阵中的许多列与当前学习任务无关，通过特征选择去除这些列，则学习器训练过程仅需在较小的矩阵上进行，学习任务的难度可能有所降低，涉及的计算和存储开销会减少，学得模型的可解释性也会提高。


现在我们来考虑另一种稀疏性： $D$ 所对应的矩阵中存在很多零元素，但这些零元素并不是以整列、整行形式存在的。在不少现实应用中我们会遇到这样的情形，例如在文档分类任务中，通常将每个文档看作一个样本，每个字(词)作为一个特征，字(词)在文档中出现的频率或次数作为特征的取值；

换言之，$D$ 所对应的矩阵的每行是一个文档，每列是一个字(词)，行、列交汇处就是某字(词在某文档中出现的频率或次数。那么，这个矩阵有多少列呢？以汉语为例，《康熙字典》中有 47035 个汉字，这意味着该矩阵可有 4 万多列，即便仅考 虑《现代汉语常用字表》中的汉字，该矩阵也有 3500 列。然而，给定一个文档， 相当多的字是不出现在这个文档中的，于是矩阵的每一行都有大量的零元素；对不同的文档，零元素出现的列往往很不相同。


当样本具有这样的稀疏表达形式时，对学习任务来说会有不少好处，例如线性支持向量机之所以能在文本数据上有很好的性能，恰是由于文本数据在使用上述的字频表示后具有高度的稀疏性，使大多数问题变得线性可分。同时，稀疏样本并不会造成存储上的巨大负担，因为稀疏矩阵已有很多高效的存储方法。


## 为什么希望模型参数具有稀疏性呢？

为什么希望模型参数具有稀疏性呢？

- 稀疏性，说白了就是模型的很多参数是 0。这相当于对模型进行了一次特征选择，只留下一些比较重要的特征，提高模型的泛化能力，降低过拟合的可能。<span style="color:red;">嗯，是的在理。</span>
- 在实际应用中，机器学习模型的输入动辄几百上千万维，稀疏性就显得更加重要，谁也不希望把这上千万维的特征全部搬到线上去。如果你真的要这样做的话，负责线上系统的同事可能会联合运维的同学一起拿着板砖来找你了。要在线上毫秒级的响应时间要求下完成千万维特征的提取以及模型的预测，还要在分布式环境下在内存中驻留那么大一个模型，估计他们只能高呼 “臣妾做不到啊”。<span style="color:red;">好吧，原来是这样，稀疏性可以保证降低过拟合，</span>




# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
- 《百面机器学习》
