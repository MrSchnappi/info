---
title: 11 从自编码器采样
toc: true
date: 2019-06-05
---
# 从自编码器采样

在第 14 章中，我们看到许多种学习数据分布的自编码器。得分匹配、去噪自编码器和收缩自编码器之间有着密切的联系。这些联系表明某些类型的自编码器以某些方式学习数据分布。我们还没有讨论如何从这样的模型中采样。

某些类型的自编码器，例如变分自编码器，明确地表示概率分布并且允许直接的原始采样。而大多数其他类型的自编码器则需要 MCMC 采样。

收缩自编码器被设计为恢复数据流形切面的估计。这意味着使用注入噪声的重复编码和解码将引起沿着流形表面的随机游走(Rifai et al.,2012;Mesnil et al.,2012)。这种流形扩散技术是马尔可夫链的一种。

更一般的马尔可夫链还可以从任何去噪自编码器中采样。

## 与任意去噪自编码器相关的马尔可夫链

上述讨论留下了一个开放问题 —— 注入什么噪声和从哪获得马尔可夫链(可以根据自编码器估计的分布生成样本)。Bengio et al.(2013c)展示了如何构建这种用于广义去噪自编码器(generalized denoising autoencoder)的马尔可夫链。广义去噪自编码器由去噪分布指定，给定损坏输入后，对干净输入的估计进行采样。

根据估计分布生成的马尔可夫链的每个步骤由以下子步骤组成，如图 20.11所示。

(1)从先前状态 x 开始，注入损坏噪声，从中采样。

(2)将编码为。
(3)解码 h 以获得的参数ω=g(h)。
(4)从采样下一状态 x。
Bengio et al.(2014)表明，如果自编码器 p(x|-x)形成对应真实条件分布的一致估计量，则上述马尔可夫链的平稳分布形成数据生成分布 x 的一致估计量(虽然是隐式的)。

图 20.11　马尔可夫链的每个步骤与训练好的去噪自编码器相关联，根据由去噪对数似然准则隐式训练的概率模型生成样本。每个步骤包括：(a)通过损坏过程 C 向状态 x 注入噪声产生；(b)用函数 f 对其编码，产生；(c)用函数 g 解码结果，产生用于重构分布的参数ω；(d)给定ω，从重构分布采样新状态。在典型的平方重构误差情况下，，并估计，损坏包括添加高斯噪声，并且从 p(x|ω)的采样包括第二次向重构-x添加高斯噪声。后者的噪声水平应对应于重构的均方误差，而注入的噪声是控制混合速度以及估计器平滑经验分布程度的超参数(Vincent,2011)。在所示的例子中，只有 C 和 p 条件是随机步骤(f和 g 是确定性计算)，我们也可以在自编码器内部注入噪声，如生成随机网络(Bengio et al.,2014)

## 夹合与条件采样

与玻尔兹曼机类似，去噪自编码器及其推广(例如下面描述的 GSN)可用于从条件分布 p(xf| xo)中采样，只需夹合观察单元 xf并在给定 xf和采好的潜变量(如果有的话)下仅重采样自由单元 xo。例如，MP-DBM 可以被解释为去噪自编码器的一种形式，并且能够采样丢失的输入。GSN 随后将 MP-DBM 中的一些想法推广以执行相同的操作(Bengio et al.,2014)。Alain et al.(2015)从 Bengio et al.(2014)的命题 1 中发现了一个缺失条件，即转移算子(由从链的一个状态到下一个状态的随机映射定义)应该满足细致平衡(detailed balance)的属性，表明无论转移算子正向或反向运行，马尔可夫链都将保持平衡。

在图 20.12中展示了夹合一半像素(图像的右部分)并在另一半上运行马尔可夫链的实验。



图 20.12　在每步仅重采样左半部分，夹合图像的右半部分并运行马尔可夫链的示意图。这些样本来自重构 MNIST 数字的 GSN(每个时间步使用回退过程)

## 回退训练过程

回退训练过程由 Bengio et al.(2013c)等人提出，作为一种加速去噪自编码器生成训练收敛的方法。不像执行一步编码-解码重建，该过程由交替的多个随机编码-解码步骤组成(如在生成马尔可夫链中)，以训练样本初始化(正如在第 18.2节中描述的对比散度算法)，并惩罚最后的概率重建(或沿途的所有重建)。

训练 k 个步骤与训练一个步骤是等价的(在实现相同稳态分布的意义上)，但是实际上可以更有效地去除来自数据的伪模式。




# 相关

- 《深度学习》花书
