---
title: 基于梯度的元学习算法
toc: true
date: 2019-11-17
---
# 学界 | DeepMind提出元梯度强化学习算法，显著提高大规模深度强化学习应用的性能



> 近日，来自 DeepMind 的研究者 David Silver 等人发布论文，提出一种基于梯度的元学习算法，可以在线调整元参数，使得回报既能适应具体问题，又能随着时间动态调整以适应不断变化的学习环境。



强化学习（RL）的核心目标是优化智能体的回报（累积奖励）。一般通过预测和控制相结合的方法来实现这一目标。预测的子任务是估计价值函数，即在任何给定状态下的预期回报。理想情况下，这可以通过朝着真值函数（true value function）的方向不断更新近似价值函数来实现。控制的子任务是优化智能体选择动作的策略，以最大化价值函数。理想情况下，策略只会在使真值函数增加的方向上更新。然而，真值函数是未知的，因此，对于预测和控制，我们需要将采样回报作为代理（proxy）。强化学习算法家族 [Sutton，1988；Rummery 和 Niranjan，1994；van Seijen 等，2009；Sutton 和 Barto，2018] 包括多种最先进的深度强化学习算法 [Mnih 等，2015；van Hasselt 等，2016；Harutyunyan 等，2016；Hessel 等，2018；Espeholt 等，2018]，它们的区别在于对回报的不同设定。



折扣因子 γ 决定了回报的时间尺度。接近 1 的折现因子更关注长期的累计回报，而接近 0 的折现因子优先考虑短期奖励，更关注短期目标。即使在明显需要关注长期回报的问题中，我们也经常观察到使用小于 1 的折扣因子可以获得更好的效果 [Prokhorov 和 Wunsch，1997]，这一现象在学习的早期体现得尤为明显。众所周知，许多算法在折扣因子较小时收敛速度较快 [Bertsekas 和 Tsitsiklis，1996]，但过小的折扣因子可能会导致过度短视的高度次优策略。在实践中，我们可以首先对短期目标进行优化，例如首先用 γ= 0 进行优化，然后在学习取得一定效果后再不断增加折扣 [Prokhorov and Wunsch，1997]。



我们同样可以在不同的时间段设定不同的回报。一个 n 步的回报需要考虑 n 步中奖励的累积，然后添加第 n 个时间步时的价值函数。λ-回报 [Sutton，1988；Sutton 和 Barto，2018] 是 n 步回报的几何加权组合。在任何一种情况下，元参数 n 或 λ 对算法的性能都很重要，因为他们影响到偏差和方差之间的权衡。许多研究人员对如何自动选择这些参数进行了探索 [Kearns 和 Singh，2000，Downey 和 Sanner，2010，Konidaris 等，2011，White and White，2016]。



还有很多其他的设计可以在回报中体现出来，包括离策略修正 [Espeholt 等，2018，Munos 等，2016]、目标网络 [Mnih 等，2015]、对特定状态的强调 [Sutton 等，2016]、奖励剪裁 [Mnih 等，2013]，甚至奖励本身 [Randløv 和 Alstrøm，1998；Singh 等，2005；Zheng 等，2018]。



本论文主要关注强化学习的一个基本问题：便于智能体最大化回报的最佳回报形式是什么？具体而言，本论文作者提出通过将回报函数当作包含可调整元参数 η 的参数函数来学习，例如折扣因子 γ 或 bootstrapping 参数 λ [Sutton，1988]。在智能体与环境的交互中，元参数 η 可以在线进行调整，使得回报既能适应具体问题，又能随着时间动态调整以适应不断变化的学习环境。研究者推导出一种实用的、基于梯度的元学习算法，实验表明它可以显著提高大规模深度强化学习应用的性能。



![img](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibicCBZFh6icBUicU6B6MmHibo8ViamdGryBjCP7j1VdFEXVudop7J8d08GgIMd1j9dgb2IdtFb5HE1uFQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*图 1：在各自的马尔可夫奖励过程（顶部）中，状态依赖可调整参数（a）bootstrapping 参数 λ 或（b）折扣因子 γ 的元梯度学习结果图示。在底部显示的每个子图中，第一幅图展示了元参数 γ 或 λ 在训练过程中的变化情况（10 个种子下的平均值 - 阴影区域覆盖了 20％-80％）。第二幅图显示了每种状态下 γ 或 λ 的最终值，分别指奇/偶状态的高/低值（小提琴图显示不同种子的分布情况）。*



![img](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibicCBZFh6icBUicU6B6MmHibo8Gy6GABxTrAwlmw25kCO4yMuqetKZSpxS2bdicycnsUeOMVIOwTQdaiaA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*表 1：与不使用元学习的基线 IMPALA 算法相比，元学习折扣参数 γ、时序差分学习参数 λ，或学习二者的结果。研究者使用的是 [Espeholt et al，2018] 最初报告的折扣因子 γ= 0.99 以及调整后的折扣因子 γ= 0.995（见附录 C）; 为了公平比较，研究者将元目标中的交叉验证折扣因子 γ’设置为相同的值。*



**论文：Meta-Gradient Reinforcement Learning（元梯度强化学习）**





![img](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibicCBZFh6icBUicU6B6MmHibo8FiblRl36mtrDb4htxkrVPKvezBgKuUE9M3sbR7jl5I3icg2SAAEwbVWQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



论文链接：https://arxiv.org/abs/1805.09801



摘要：强化学习算法的目标是估计和/或优化价值函数。然而与监督学习不同，强化学习中没有可以提供真值函数的教师或权威。相反，大多数强化学习算法估计和/或优化价值函数的代理。该代理通常基于对真值函数的采样和 bootstrapped 逼近，即回报。对回报的不同选择是决定算法本质的主要因素，包括未来奖励的折扣因子、何时以及如何设定奖励，甚至奖励本身的性质。众所周知，这些决策对强化学习算法的整体成功至关重要。我们讨论了一种基于梯度的元学习算法，它能够在线适应回报的本质，同时进行与环境的互动和学习。我们将该算法应用于超过 2 亿帧 Atari 2600 环境中的 57 场比赛，结果表明我们的算法取得了目前最好的性能。


# 相关

- [学界 | DeepMind提出元梯度强化学习算法，显著提高大规模深度强化学习应用的性能](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650743013&idx=5&sn=79a21fa9ea72d81b1f146a35a3d2b5ad&chksm=871ae49bb06d6d8d87ebcdb64e7a3e71b1c32382d96f05540b230cb7ab71271e66c7e8bd9394&mpshare=1&scene=1&srcid=0531O6kACENrOpFeBeWHHY0f#rd)
