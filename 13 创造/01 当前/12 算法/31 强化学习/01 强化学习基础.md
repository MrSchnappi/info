---
title: 01 强化学习基础
toc: true
date: 2019-04-20
---
# 可以补充进来的

- 再看一遍吧。


# 强化学习基础

假设我们有一个 3×3的棋盘，其中有一个单元格是马里奥，另一个单元格是宝藏，如图 11.1所示。

<center>

![](http://images.iterate.site/blog/image/20190420/tqR78MYumyXc.png?imageslim){ width=55% }

</center>

在游戏的每个回合，可以往上、下、左、右四个方向移动马里奥，直到马里奥找到宝藏，游戏结束。在这个场景中，强化学习需要定义一些基本概念来完成对问题的数学建模。


知识点

强化学习，马尔可夫决策过程，价值迭代（Value Iteration），策略迭代

## 强化学习中有哪些基本概念？在马里奥找宝藏问题中如何定义这些概念？

强化学习的基本场景可以用图 11.2来描述：


<center>

![](http://images.iterate.site/blog/image/20190420/goPEabo1JFm6.png?imageslim){ width=55% }

</center>

主要由：

- 环境（Environment）、
- 机器人（Agent）、
- 状态（State）、
- 动作（Action）、
- 奖励（Reward）
- 等基本概念构成。

一个机器人在环境中会做各种动作，环境会接收动作，并引起自身状态的变迁，同时给机器人以奖励。机器人的目标就是使用一些策略，做合适的动作，最大化自身的收益。

整个场景一般可以描述为一个马尔可夫决策过程（Markov Decision Process，MDP）。马尔可夫决策过程是马尔可夫过程与确定性的动态规划相结合的产物，指决策者周期地或连续地观察具有马尔可夫性的随机动态系统，序贯地做出决策的过程，以俄罗斯数学家安德雷·马尔可夫的名字命名。<span style="color:red;">什么叫确定性的动态规划？什么叫续惯的做出决策的过程？</span>

这个过程包括以下几个要素：


- 动作：所有可能做出的动作的集合，记作 A（可能是无限的）。对于本题，`A=马里奥在每个单元格可以行走的方向`，即 `{上、下、左、右}`。
- 状态：所有状态的集合，记作 S。对于本题，S 为棋盘中每个单元格的位置坐标 `{（x，y）; x=1，2，3; y=1，2，3}`，马里奥当前位于`（1，1）`，宝藏位于`（3，2）`。
- 奖励：机器人可能收到的奖励，一般是一个实数，记作 r。对于本题，如果马里奥每移动一步，定义 `r=−1`；如果得到宝藏，定义 `r=0`，游戏结束。
- 时间 `（t=1，2，3……）`：在每个时间点 $t$，机器人会发出一个动作 $a_t$，收到环境给出的收益 $r_t$，同时环境进入到一个新的状态 $s_t$。
●状态转移：$S \times A \rightarrow S$ 满足 $P_{a}\left(s_{t} | s_{t-1}, a_{t}\right)=P_{a}\left(s_{t} | s_{t-1}, a_{t}, s_{t-2}, a_{t-1} \ldots\right)$ ，也就是说，从当前状态到下一状态的转移，只与当前状态以及当前所采取的动作有关。这就是所谓的马尔可夫性。<span style="color:red;">嗯。</span>
- 累积收益：从当前时刻 0 开始累积收益的计算方法是 $\mathrm{R}=\mathrm{E}\left(\sum_{t=0}^{T} \gamma^{t} r_{t} | s_{0}=s\right)$ ，在很多时候，我们可以取 $T=\infty$。


强化学习的核心任务是，学习一个从状态空间 $S$ 到动作空间 $A$ 的映射，最大化累积受益。常用的强化学习算法有 Q-Learning、策略梯度，以及演员评判家算法（Actor-Critic）等。<span style="color:red;">只听说过 Q-Learning ，策略梯度、演员评判家算法都没有听说过，要总结下。</span>


## 根据图 11.1给定的马里奥的位置以及宝藏的位置，从价值迭代来考虑，如何找到一条最优路线？


上一问已经把强化学习问题形式化为马尔可夫决策过程。下面我们介绍如何利用价值迭代求解马尔可夫决策过程。

那么什么是价值呢？我们将当前状态 $s$ 的价值 $V(s)$ 定义为：从状态 $s=(x,y)$ 开始，能够获得的最大化奖励。结合图 11.3 可以非常直观地理解价值迭代。

<center>

![](http://images.iterate.site/blog/image/20190420/flCmSbNsjJi0.png?imageslim){ width=55% }

</center>

首先，初始化所有状态的价值 $V(s)=0$ 。然后，在每一轮迭代中，对每个状态 $s$ 依次执行以下步骤。

- 逐一尝试{上、下、左、右}四个动作 $a$，记录到达状态 $s′$ 和奖励 $r$。
- 计算每个动作的价值 $q(s, a)=r+V\left(s^{\prime}\right)$ 。
- 从四个动作中选择最优的动作 $\max _{a}\{q(s, a)\}$。
- 更新 s 状态价值￼。


在第一轮迭代中，由于初始状态 $V(s)$ 均为 0，因此对除宝藏所在位置外的状态 s 均有 $V(s)=r+V\left(s^{\prime}\right)=-1+0=-1$，即从当前位置出发走一步获得奖励 $r=-1$。

在第二轮迭代中，对于和宝藏位置相邻的状态，最优动作为一步到达 $V\left(s^{\prime}\right)=0$ 的状态，即宝藏所在的格子。因此，$V(s)$ 更新为 $r+V\left(s^{\prime}\right)=-1+0=-1$ ；其余只能一步到达 $V\left(s^{\prime}\right)=-1$ 的状态，$V(s)$ 更新为 $r+V\left(s^{\prime}\right)=-1+(-1)=-2$ 。

第三轮和第四轮迭代如法炮制。可以发现，在第四轮迭代中，所有 $V(s)$ 更新前后都没有任何变化，价值迭代已经找到了最优策略。最终，只需要从马里奥所在位置开始，每一步选择最优动作，即可最快地找到宝藏。

上面的迭代过程实际上运用了贝尔曼方程（Bellman Equation），来对每个位置的价值进行更新．


$$
V_{*}(s)=\max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma V_{*}\left(s^{\prime}\right)\right]\tag{11.1}
$$

<span style="color:red;">这个不知道，再补充下。</span>


贝尔曼方程中状态 $s$ 的价值 $V(s)$ 由两部分组成：

- 采取动作 $a$ 后带来的奖励 $r$
- 采取动作 $a$ 后到达的新状态的价值 $V\left(s^{\prime}\right)$。


## 根据图 11.1给定的马里奥的位置以及宝藏的位置，从策略迭代来考虑，如何找到一条最优路线？

本节介绍马尔可夫决策过程的另一种求解方法- 策略迭代。<span style="color:red;">价值迭代和策略迭代</span>

什么叫策略？策略就是根据当前状态决定该采取什么动作。以场景中的马里奥寻找宝箱为例，马里奥需要不断朝着宝藏的方向前进：

- 当前状态如果在宝藏左侧，策略应该是朝右走；
- 当前状态如果在宝藏上方，策略应该是朝下走。

如何衡量策略的好坏？这就需要介绍策略评估（Policy Evaluation）。<span style="color:red;">嗯，策略怎么进行评估呢？</span>

给定一个策略 $\pi$，我们可以计算出每个状态的期望价值 $V(s)$。策略迭代可以帮助我们找到更好的策略，即期望价值更高的策略，具体步骤如下。

1. 初始化：随机选择一个策略作为初始值。比如“不管什么状态，一律朝下走”，即 $P(A = 朝下走 | S_t=s)= 1$，$P(A = 其他 | S_t=s)= 0$。
2. 进行策略评估：根据当前的策略计算 $V_{\pi}(s)=E_{\pi}\left(r+\gamma V_{\pi}\left(s^{\prime}\right) | S_{t}=s\right)$。
3. 进行策略提升：计算当前状态的最优动作 $\max _{a}\left\{q_{\pi}(s, a)\right\}$ ，更新策略 $\pi(s)=\operatorname{argmax}_{a}\left\{q_{\pi}(s, a)\right\}=\operatorname{argmax}_{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma V_{\pi}\left(s^{\prime}\right)\right]$。
4. 不停地重复策略评估和策略提升，直到策略不再变化为止。


在马里奥寻找宝藏问题中，策略迭代过程如图 11.4所示。


<center>

![](http://images.iterate.site/blog/image/20190420/mWnntYv8pR1C.png?imageslim){ width=55% }

</center>


- 初始化策略为：不论马里奥处于哪个状态，一律朝下走。根据这一策略进行策略评估不难发现，只有宝藏正上方的状态可以到达宝藏，期望价值为到宝藏的距离（−2，−1和 0）；其余状态不能通过当前策略到达宝藏，期望价值为负无穷。
- 然后根据当前的期望价值进行策略提升：对于宝藏正上方的状态，策略已经最优，保持不变；对于不在宝藏正上方的状态，根据策略更新公式 $\pi(s)=\operatorname{argmax}_{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma V_{\pi}\left(s^{\prime}\right)\right]$ ，最优策略为横向移动一步。
- 通过上一轮的策略提升，这一轮的策略变为：对于宝藏正上方的状态，向下移动；对于不在宝藏正上方的状态，横向移动一步。根据当前策略进行策略评估，更新各状态期望价值：宝藏正上方的状态价值期望不变，仍等于到宝藏的距离；不在宝藏正上方的状态期望价值更新为 $r+\gamma V_{\pi}\left(s^{\prime}\right)$ ，即横向移动一步的奖励与目标状态的期望价值之和。
- 然后根据更新后的期望价值进行策略提升：不难发现，对于所有状态，当前策略已经最优，维持不变，中止策略提升过程。


最终，马里奥只需从初始状态（1，1）开始，按照当前位置的最优策略进行行动，即向右行动一步，然后一直向下行动，即可最快找到宝藏。




# 相关

- 《百面机器学习》

