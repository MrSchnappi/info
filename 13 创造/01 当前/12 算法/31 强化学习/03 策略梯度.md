---
title: 03 策略梯度
toc: true
date: 2019-04-20
---
# 可以补充进来的

- 感觉很厉害，但是没有特别明白。

# 策略梯度

Q-learning 因为涉及在状态空间上求 $Q$ 函数的最大值，所以只适用于处理离散的状态空间，对于连续的状态空间，最大化 $Q$ 函数将变得非常困难。所以对于机器人控制等需要复杂连续输出的领域，Q-learning就显得不太合适了。<span style="color:red;">是的呀。</span>

其次，包括深度 Q-learning 在内的大多数强化学习算法，都没有收敛性的保证，而策略梯度（Policy Gradient）则没有这些问题，它可以无差别地处理连续和离散状态空间，同时保证至少收敛到一个局部最优解。<span style="color:red;">有些厉害。</span>



强化学习，Q-learning

## 什么是策略梯度，它和传统 Q-learning 有什么不同，相对于 Q-learning 来说有什么优势?

在策略梯度中：

- 我们考虑前后两个状态之间的关系为 $s_{t+1} \sim p\left(s_{t+1} | s_{t}, a_{t}\right)$ ，其中 $s_{t}$、$s_{t+1}$ 是相继的两个状态， $a_t$ 是 $t$ 步时所采取的行动，$p$ 是环境所决定的下个时刻状态分布。

- 而动作 $a_t$ 的生成模型（策略）为 $a_{t} \sim \pi_{\theta}\left(a_{t} | s_{t}\right)$，其中 $\pi_{\theta}$ 是以 $\theta$ 为参变量的一个分布，$a_{t}$ 从这个分布进行采样。<span style="color:red;">以 $\theta$ 为参变量的一个分布 是什么样子的？怎么创建这个分布？</span>

这样，在同一个环境下，强化学习的总收益函数，$R(\theta)=E\left(\sum_{0}^{T} z_{t} r_{t}\right)$ ，完全由 $θ$ 所决定。<span style="color:red;">嗯，好像是这样， z_{t} 是什么？</span>

策略梯度的基本思想就是，直接用梯度方法来优化 $R(\theta)$ 。

- 可以看出，和 Q-learning 不同的是，策略梯度并不估算 $Q$ 函数本身，而是利用当前状态直接生成动作 $a_t$。这有效避免了在连续状态空间上最大化 $Q$ 函数的困难。<span style="color:red;">是的。</span>
- 同时，直接用梯度的方法优化 $R(\theta)$ 可以保证至少是局部收敛的。<span style="color:red;">嗯，总是会有一个值。</span>


要使用梯度法，首先要知道如何计算 $R(\theta)$ 的导数。设 $\tau$ 为某一次 $0$ 到 $T$ 时间所有状态及行动的集合（称作一条轨迹），则 $R(\theta)=E(r(\tau))$ ，其中函数 $r$ 计算了轨迹 $\tau$ 的得分。<span style="color:red;">嗯嗯，是的。</span>

我们有 $R(\theta)=E(r(\tau))=\int p_{\theta}(\tau) r(\tau) \mathrm{d} \tau$ ，所以




$$
\begin{aligned} \nabla_{\theta} R(\theta) &=\nabla_{\theta} \int p_{\theta}(\tau) r(\tau) \mathrm{d} \tau \\ &=\int p_{\theta}(\tau)\left(\nabla_{\theta} \log \left(p_{\theta}(\tau)\right) r(\tau) \mathrm{d} \tau\right.\\ &=E\left(\nabla_{\theta} \log \left(p_{\theta}(\tau)\right) r(\tau)\right) \\ &=E\left(\nabla_{\theta} \log \left(p\left(s_{0}\right) \cdot \pi_{\theta}\left(a_{0} | s_{0}\right) \cdot p\left(s_{1} | a_{0}, s_{0}\right) \cdot \right.\right.\\ & p\left(a_{1} | s_{1}\right) \cdot \ldots \cdot \pi_{\theta}\left(a_{T} | s_{T}\right) ) r(\tau) \\ &=E\left(\sum_{k=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{k} | s_{k}\right) \cdot r(\tau)\right) \end{aligned}\tag{11.2}
$$

<span style="color:red;">emmm，感觉是这样的。不过中间有几步变形有些不清楚了，微积分还是要重新总结下的。</span>


注意最后一步是因为 $p\left(s_{k+1} | a_{k}, s_{k}\right)$ 由环境决定从而与 $θ$ 无关，因此 $\nabla_{\theta} \log \left(p\left(s_{k+1} | a_{k}, s_{k}\right)\right)=0$ 。<span style="color:red;">是的。</span>每个轨迹 $\tau$ 所对应的梯度为：


$$
g(\tau)=\sum_{k=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{k} | s_{k}\right) \cdot r(\tau)\tag{11.3}
$$


其中 $s_k$，$a_k$ 为轨迹 $\tau$ 上每一步的状态和动作。这样，给定一个策略 $\pi_{\theta}$ ，我们可以通过模拟获得一些轨迹，对于每条轨迹，可以获得其收益 $r(\tau)$ 以及每一步的 `<状态、行动>` 对，从而可以通过式（11.2）和式（11.3）获得当前参数下对梯度的估计。一个简单的算法描述如图 11.7所示。

<center>

![](http://images.iterate.site/blog/image/20190420/w8iNYABqig9p.png?imageslim){ width=55% }

</center>

<span style="color:red;">是的，上面的步骤还是很清晰的。最后 5 的 $\nabla_{\theta} R(\theta)$ 把 4 的代入进去就行。</span>


OK，然后，我们注意到，$\nabla_{\theta} R(\theta)$ 实际上是一个随机变量 $g(\tau)$ 的期望。我们对 $g(\tau)$ 进行若干次独立采样，可以获得对其期望的一个估计。<span style="color:red;">是的。</span>

如果能在不改变期望的前提下减少 $g(\tau)$ 的方差，则能有效提高对其期望估计的效率。我们注意到 $\int p_{\theta}(\tau) \cdot \mathrm{d} \tau \equiv 1$ ，所以有 $E\left(\sum_{k=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{k} | s_{k}\right)\right)=\int \nabla p_{\theta}(\tau) \cdot \mathrm{d} \tau=\nabla \int p_{\theta}(\tau)=0$ 。<span style="color:red;">这个地方没有很明白。</span>

对于任一个常量 $b$，我们定义一个强化梯度 $g_{b}^{\prime}(\tau)=\left(\sum_{k=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{k} | s_{k}\right)\right)(r(\tau)-b)$ ，易知 $E(g(\tau))=E\left(\mathrm{g}_{b}^{\prime}(\tau)\right)$，选取合适的 $b$ ，可以获得一个方差更小的 $g_{b}^{\prime}(\tau)$ ，而维持期望不变。经过计算可以得到最优的 $b$ 为：<span style="color:red;">这一段没有明白。</span>

$$
b_{\text { optimal }}=\frac{E\left(\left(\sum_{k=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{k} | s_{k}\right)\right)^{2} r(\tau)\right)}{E\left(\left(\sum_{k=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{k} | s_{k}\right)\right)^{2}\right)}\tag{11.4}
$$

于是，得到一个改良的算法，如图 11.8所示：

<span style="color:red;">下面的没有怎么看了，没有明白。要重新消化下。</span>

<center>

![](http://images.iterate.site/blog/image/20190420/bEzi6f9BVJc7.png?imageslim){ width=55% }

</center>

在上述策略梯度算法中，通过估算一个新的强化梯度可以有效缩减原来梯度的方差，从而提高梯度估算的效率，那么如何推出最优的 $b$ 值呢？

我们回到策略梯度算法，$g_{b}^{\prime}(\tau)=\left(\sum_{k=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{k} | s_{k}\right)\right)(r(\tau)-b)$ 。定义随机变量 $A=\sum_{k=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{k} | s_{k}\right)$ ，$B=r(\tau)$ ，可以得到 $E(A)=0$。

这样问题变成，在 $E(A)=0$ 的前提下，寻找最优的常量 $b$，使得 $\operatorname{var}(A(B-b))$ 最小：

$$
\begin{aligned} \operatorname{argmin}(\operatorname{var}(A(B-b))) &=\operatorname{argmin}\left(E\left((A(B-b)-E(A B))^{2}\right)\right) \\ &=\operatorname{argmin}\left(E\left((A B-E(A B)-b \cdot A)^{2}\right)\right) \\ &=\operatorname{argmin}\left(b^{2} E\left(A^{2}\right)-2 b E\left(A^{2} B\right)+E\left((A B-E(A B))^{2}\right)\right) \\ &=\operatorname{argmin}\left(b^{2} E\left(A^{2}\right)-2 b E\left(A^{2} B\right)\right) \\ &=\frac{E\left(A^{2} B\right)}{E\left(A^{2}\right)} \\&=\frac{E\left(\left(\sum_{k=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{k} | s_{k}\right)\right)^{2} r(\tau)\right)}{E\left(\left(\sum_{k=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{k} | s_{k}\right)\right)^{2}\right)}\end{aligned}\tag{11.5}
$$

即式（11.4）中的结果。




# 相关

- 《百面机器学习》
