---
title: 1.07 如何尽量避免 GAN 的训练崩溃问题
toc: true
date: 2019-09-03
---


# 如何尽量避免 GAN 的训练崩溃问题？



- 归一化图像输入到 $(-1，1)$ 之间；Generator 最后一层使用 tanh 激活函数
- 生成器的 Loss 采用：min (log 1-D)。因为原始的生成器 Loss 存在梯度消失问题；训练生成器的时候，考虑反转标签，real=fake, fake=real <span style="color:red;">怎么反转标签的？</span>
- 不要在均匀分布上采样，应该在高斯分布上采样。<span style="color:red;">为什么？</span>
- 一个 Mini-batch 里面必须只有正样本，或者负样本。不要混在一起；如果用不了 Batch Norm，可以用 Instance Norm。<span style="color:red;">为什么不能混在一起？Instance Norm 是什么？跟上面提到的 layer norm 有什么区别吗？</span>
- 避免稀疏梯度，即少用 ReLU，MaxPool。可以用 LeakyReLU 替代 ReLU，下采样可以用 Average Pooling 或者 Convolution + stride 替代。上采样可以用 PixelShuffle, ConvTranspose2d + stride。<span style="color:red;">为什么要避免稀疏梯度？convolution+stride 是什么？ pixelshuffle 和 convtranspose2d+stride 是什么？</span>
- 平滑标签或者给标签加噪声；平滑标签，即对于正样本，可以使用 0.7-1.2 的随机数替代；对于负样本，可以使用 0-0.3 的随机数替代。<span style="color:red;">为什么要平滑？</span>给标签加噪声：即训练判别器的时候，随机翻转部分样本的标签。<span style="color:red;">为什么这种噪声是有用的？</span>
- 如果可以，请用 DCGAN 或者混合模型：KL+GAN，VAE+GAN。<span style="color:red;">KL 是什么？</span>
- 使用 LSGAN，WGAN-GP
- Generator 使用 Adam，Discriminator 使用 SGD。<span style="color:red;">为什么？</span>
- 尽快发现错误；比如：判别器 Loss 为 0，说明训练失败了；如果生成器 Loss 稳步下降，说明判别器没发挥作用
- 不要试着通过比较生成器，判别器 Loss 的大小来解决训练过程中的模型坍塌问题。比如：<span style="color:red;">为什么不能这样？</span>
  ```
  While Loss D > Loss A:
  Train D
  While Loss A > Loss D:
  Train A
  ```
- 如果有标签，请尽量利用标签信息来训练。<span style="color:red;">嗯，训练的时候尽量带标签是吗？</span>
- 给判别器的输入加一些噪声，给 G 的每一层加一些人工噪声。<span style="color:red;">每一层都要加人工噪声吗？加多少？</span>
- 多训练判别器，尤其是加了噪声的时候。<span style="color:red;">什么意思？</span>
- 对于生成器，在训练，测试的时候使用 Dropout。<span style="color:red;">生成器测试的时候也要用 dropout 吗？</span>
