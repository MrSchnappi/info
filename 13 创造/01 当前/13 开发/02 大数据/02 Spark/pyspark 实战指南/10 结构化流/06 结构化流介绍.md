---
title: 06 结构化流介绍
toc: true
date: 2019-07-02
---

10.6 结构化流介绍
对于 Spark 2.0，Apache Spark社区致力于通过引入结构化流（structured streaming）的概念来简化流，结构化流将 Streaming 概念与 Dataset/DataFrame相结合（如下图所示）：
如前几章中对于 DataFrame 的叙述，Spark SQL Engine（和 Catalyst Optimizer）中的 SQL 和 DataFrame 查询的执行以构建逻辑计划、构建大量物理计划为中心，引擎根据其成本优化器选择正确的物理计划，然后生成代码（即 code gen）以对性能有利的方式将结果传递。结构化流式引入的是增量执行计划（Incremental Execution Plan）的概念。当处理一系列数据块时，结构化流不断地将执行计划应用在所接收的每个新数据块集合上。通过这种运行方式，引擎可以充分利用 Spark DataFrame/Dataset所包含的优化功能，并将其应用于传入的数据流。而且还更易于与其他 Spark DataFrame优化组件集成，这些组件包括 ML Pipeline、GraphFrames、TensorFrames等等。
使用结构化流还能简化代码。例如，以下的伪代码演示了从 S3 读取数据流并存储到 MySQL 数据库的批量聚合（batch aggregation）：
以下是连续聚合（continous aggregation）的伪代码示例：
创建 sq 变量的原因是它允许你检查结构化流作业的状态并将其终止，如下所示：
我们来看一下使用 updateStateByKey 的有状态流的文字计数脚本，并将其改成一个结构化流的文字计数脚本；你可以在以下位置获取完整的 structured_streaming_word_count.py脚本：https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/structured_streaming_word_count.py。
与之前的脚本相反，我们现在要使用更为熟悉的 DataFrame 代码，如下所示：
脚本开始的几行导入必要的类，并建立当前的 SparkSession。然而与之前的流脚本相反，如接下来的几行脚本所示



取而代之的，流那部分的代码是通过调用 readStream 来初始化的（第 4 行）。
·3～8行初始化了从 9999 端口读取的数据流，如同之前的两个脚本一样。
·不使用 RDD flatMap、map和 reduceByKey 函数去读取行从而将其分割成单词，并对每个批量数据中的单词计数，我们可以使用 PySpark SQL的 explode 和 split 函数，如第 10～15行所述。
·我们可以使用熟悉的 DataFrame groupBy语句和 count（）来生成运行的文字计数，而不使用 updateStateByKey 或创建一个 updateFunc（见有状态的流文字计数脚本），如 17～18行所示。
要将这些数据输出到控制台，我们将使用 writeStream，如下所示：
这里没有使用 pprint（），我们明确地调用 writeStream 来写入流，并定义格式和输出模式。虽然写起来有点长，但这些方法和属性在语法上与其他 DataFrame 调用类似，只需要更改 outputMode 和 format 属性即可将其保存到数据库、文件系统、控制台等。最后，如第 10 行所示，我们将运行 awaitTermination 等待取消该 streaming 作业。
让我们回到第一个终端运行我们的 nc 作业：
检查以下输出。如你所见，你既能得到有状态流的优势，还能使用更为熟悉的 DataFrame API：
