---
title: 图像风格化算法综述三部曲之2
toc: true
date: 2019-11-17
---
# 图像风格化算法综述三部曲之2


![img](https://pic1.zhimg.com/80/v2-47b627fb232418ac2502582309260570_hd.jpg)

## **前言**

在上一篇文章中，主要和大家讨论图像风格化迁移的**简介**、**前传**和**起源**，目的是想让大家对风格化这个领域有一个初步的认识。其中**起源**这一部分比较重要，这里首先做一个简单回顾以及**补充**。

图像风格化迁移源于两个其他领域：**(1) 纹理建模(Visual Texture Modelling)**；**(2) 图像重建**。

其中**纹理建模(Visual Texture Modelling)**又分为两类：

**(a) 基于统计分布的参数化纹理建模方法（Parametric Texture Modelling with Summary Statistics）**

**(b) 基于MRF的非参数化纹理建模方法（Non-parametric Texture Modelling with MRFs）**

纹理建模解决了如何对风格特征进行提取的问题。

而**图像重建**解决的则是如何将给定的特征表达重建还原为一张图像。上篇文章中没有提到的一点是**图像重建**算法其实也可以分为两类：

**(a) 基于在线图像优化的慢速图像重建方法（Slow Image Reconstruction based on Online Image Optimisation）**

**(b) 基于离线模型优化的快速图像重建方法（Fast Image Reconstruction based on Offline Model Optimisation）**

由名字也可以看出这种分类方式的主要依据是图像重建的速度。第一类图像重建的方法 **(a)** 是在图像像素空间做梯度下降来最小化目标函数。这一类算法的过程可以理解为：由随机噪声作为起始图，然后不断迭代改变图片的所有像素值来寻找一个目标结果图 ![[公式]](https://www.zhihu.com/equation?tex=x%27) ，这个目标结果图的特征表达和我们作为重建目标的目标特征表达 ![[公式]](https://www.zhihu.com/equation?tex=%5CPhi%28x%29) 相似，即像素迭代的目标为 ![[公式]](https://www.zhihu.com/equation?tex=%5CPhi%28x%27%29+%5Capprox+%5CPhi%28x%29) 。由于每个重建结果都需要在像素空间进行迭代优化很多次，这种方式是很耗时的(几百乘几百的图需要几分钟)，尤其是当需要的重建结果是高清图的时候，占用的计算资源以及需要的时间开销很大。为了加速这一过程，一个直接的想法是我们能不能设计一个前向网络，用数据驱动的方式，喂给它很多训练数据去提前训练它，训练的目标就是给定一个特征表达作为输入，这个训练好的网络只需要一次前向就能输出一张重建结果图像。bingo，这种方式最终被一些德国研究者在一篇CVPR论文中证明了其有效性，即第二类方法 (**b)** 。后续这些德国研究者又融入了生成对抗网络(Generative Adversarial Network)的思想，进一步提升了效果，这个工作发表在了一篇NIPS上。

下面用这张图总结一下**图像风格化算法的起源和基石**，这篇博文的剩下部分全是建立在这些内容之上的：

![img](https://pic2.zhimg.com/80/v2-25b41cd0ae4295a20c90d98b0f195799_hd.jpg)图1、图像风格化迁移之起源

ok，下面开始这篇博文的重头戏——**图像风格化迁移(Neural Style Transfer)的发展**（章节序号紧接“图像风格化算法综述三部曲之（一）”）。

------

## **4. 发展**

前面提到，**图像风格化迁移算法=图像重建算法+纹理建模算法**，而图像重建和纹理建模又各自可以分为两类方法。于是乎，我们可以很自然地想到，如果将两类图像重建算法和两类纹理建模方法进行排列组合，不就可以得到一系列的图像风格化迁移算法嘛？事实上，粗略来讲，在图像风格化这一领域大家也确实都是这么做的。下面我先给出一个**现有图像风格化迁移的分类方法**，大家可以和前面的**图1**放在一块看，图像风格化迁移这一领域的研究脉络和套路即可了然于胸。

![img](https://pic1.zhimg.com/80/v2-189dc8d7cf5fc9e44ab11552231964d4_hd.jpg)图2、图像风格化迁移算法分类方法

下面咱们一起对每一类图像风格化算法做具体介绍和优缺点分析。为了讨论方便，涉及具体算法时直接采用上面这张图中的**引用序号**进行表示。

------

## **一、基于在线图像优化的慢速图像风格化迁移算法**

**(Slow Neural Method Based On Online Image Optimisation)**

## **1.1. 基于统计分布的参数化慢速风格化迁移算法**

**(Parametric Slow Neural Method with Summary Statistics)**

通过名字就可以看出，这一类风格化算法是由**基于在线图像优化的慢速图像重建方法**和**基 于统计分布的参数化纹理建模方法**结合而来。其中，图像风格化迁移这一领域的祖师爷**Gatys**的开山大作[4]就是属于这一类方法的。在此之前，祖师爷自己先发了一篇NIPS提出了一个新的**基于CNN的纹理建模方法**[27] [[Texture Synthesis Using Convolutional Neural Networks\]](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1505.07376)。[27]的核心思想是在图像经过预训练的VGG网络时的特征表达(feature map)上计算**Gram矩阵**，利用得到的Gram矩阵来表示一种纹理。Gram矩阵的计算方式是先将预训练VGG某一层的特征表达 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D%5E%7Bl%7D%28I%29) 由 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7BC+%5Ctimes+H+%5Ctimes+W%7D) reshape 成 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7BC+%5Ctimes+%28HW%29%7D) ，然后用reshape后的特征表达和其转置矩阵相乘 ![[公式]](https://www.zhihu.com/equation?tex=%5B%5Cmathcal%7BF%7D%5E%7Bl%7D%28I%29%5D+%5Ctimes+%5B%5Cmathcal%7BF%7D%5E%7Bl%7D%28I%29%5D%5ET)，最后得到的Gram矩阵维度为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7BC+%5Ctimes+C%7D) 。祖师爷发现这个Gram矩阵可以很好地表示大多数纹理。结合咱们在上一篇博文讨论的内容，这个Gram矩阵的纹理表示方法其实是利用了二阶统计量来对纹理进行建模。祖师爷之后用Gram矩阵来对图像中的风格进行建模和提取，再利用慢速图像重建方法，让重建后的图像以梯度下降的方式更新像素值，使其Gram矩阵接近风格图的Gram矩阵（即风格相似），然后其VGG网络的高层特征表达接近内容图的特征表达（即内容相似），实际应用时候经常再加个总变分TV项来对结果进行平滑，最终重建出来的结果图就既拥有风格图的风格，又有内容图的内容啦 [[Image Style Transfer Using Convolutional Neural Networks\]](https://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) ~

除了Gram矩阵外，还有一些其他方式对风格进行建模。[37]里

[Naiyan Wang](https://www.zhihu.com/people/f5911fddc7fa5fd74a80d5ce2c12e1a2)



老师在IJCAI2017的文章中从

Domain Adaption

的角度对风格化迁移进行解释和分析，并提出了一些其他的用于风格建模的方法



[Demystifying Neural Style Transfer]

。这里简单解释一下，Domain Adaption指的是当训练数据和测试数据属于不同的域时，我们通过某种手段利用源域有标签的训练数据训练得到的模型，去预测无标签的测试数据所在的目标域中的数据。Domain Adaption其中一个套路就是以最小化统计分布差异度量

MMD

的方式，让目标域中的数据和源域中的数据建立起一种映射转换关系。



[Naiyan Wang](https://www.zhihu.com/people/f5911fddc7fa5fd74a80d5ce2c12e1a2)



老师通过公式推导发现，最小化重建结果图和风格图的Gram统计量差异其实等价于最小化两个域统计分布之间的基于

二阶核函数

的MMD。换言之，风格迁移的过程其实可以看做是让目标风格化结果图的特征表达二阶统计分布去尽可能地逼近风格图的特征表达二阶统计分布。由此可以很自然地想到，既然是衡量统计分布差异，除了有二阶核函数的MMD外，其他的MMD核函数例如

一阶线性核函数、高阶核函数、高斯核函数

，也可能达到和Gram统计量类似的效果。实验证明也确实如此。这些计算风格特征的方式其实都是在特征表达（feature map）的

所有channel

上进行计算的，除了这些贡献外，



[Naiyan Wang](https://www.zhihu.com/people/f5911fddc7fa5fd74a80d5ce2c12e1a2)



老师还提出了一个新的用channel-wise的BN统计量去对风格进行建模的方法，即利用VGG某些层的特征表达







的

每一个channel

的均值和方差

(channel-wise)

来表示风格。 （





表示VGG中第







层的feature map的第







个channel），并取得了很好的效果，后续有很多研究者follow了这种风格建模的方式。



看到这里大家可以发现一点的是，以往风格化算法在提取特征的时候都是在高层次的CNN特征空间（feature space）中完成的，虽然这样做的效果在感知效果上（perceptually）优于利用传统的在像素空间（pixel space）计算的特征，但由于特征空间是对图像的一种抽象表达，会不可避免丢失一些低层次的如边缘等的图形信息这会导致风格化结果图中有一些不是很漂亮的变形等。

[李韶华](https://www.zhihu.com/people/ebdb20dc00a4f2864105ae5b1e1d2358)



老师为解决这一问题，在ACM MM2017的一篇文章中[40]

[Laplacian-Steered Neural Style Transfer]

，提出在风格化迁移的过程中

同时考虑像素空间和特征空间

。具体做法为在像素空间中将内容图的

拉普拉斯算子

的滤波结果和风格化重建结果图的滤波结果之间的差异作为一个新的loss，加到祖师爷Gatys提出的损失函数上面。这样的话就弥补了抽象特征空间丢失低层次图像信息的缺点。



## **1.2. 基于MRF的非参数化慢速风格化迁移算法**

**(Non-parametric Slow Neural Method with MRFs)**

另外一类慢速风格化算法就是利用**基于MRF的非参数化纹理建模方法**对风格信息进行建模了。代表性工作由浙大出身、德国美茵茨大学Postdoc [Chuan Li](https://link.zhihu.com/?target=http%3A//www.staff.uni-mainz.de/chuli/)学长完成并发表于CVPR2016 [41] [[Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis\]](https://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Li_Combining_Markov_Random_CVPR_2016_paper.pdf)。其核心思想是提出了一个取代Gram损失的新的MRF损失。思路与传统的MRF非参数化纹理建模方法相似，即先将风格图和重建风格化结果图分成若干patch，然后对于每个重建结果图中的patch，去寻找并逼近与其最接近的风格patch。与传统MRF建模方法不同之处在于以上操作是在CNN特征空间中完成的。另外还需要加一个祖师爷Gatys提出的内容损失来保证不丢失内容图中的高层语义信息。这种基于patch的风格建模方法相比较以往基于统计分布的方法的一个明显优势在于，当风格图不是一幅艺术画作，而是和内容图内容相近的一张摄影照片**（Photorealistic Style）**，这种基于patch匹配**（patch matching）**的方式可以很好地保留图像中的局部结构等信息。

------

## **二、基于离线模型优化的快速图像风格化迁移算法**

**(Fast Neural Method Based On Offline Model Optimisation)**

ok，咱们前面介绍的都是用慢速图像重建方法对风格化结果进行重建的，所以速度肯定是比较慢的，而且很吃资源，在工业界落地的成本肯定是很高的。所以另外一个大的图像风格化迁移算法分支——快速图像风格化迁移算法主要解决速度问题，核心思想就是利用**基于离线模型优化的快速图像重建方法**对风格化结果进行重建，基预先训练前向网络来解决计算量大、速度慢的问题。根据一个训练好的前向网络能够学习到多少个风格作为分类依据，我们将快速图像风格化迁移算法分为**单模型单风格 (PSPM）、单模型多风格 (MSPM)** 和 **单模型任意风格 (ASPM)** 的快速风格化迁移算法。下面先放一张不同快速风格化方法的对比图：

![img](https://pic1.zhimg.com/80/v2-cf11a804778355c8a951836c362405b8_hd.jpg)

![img](https://pic3.zhimg.com/80/v2-930aa02a239ec8b10dd34e5d2bfc5a2a_hd.jpg)

## **2.1. 单模型单风格的快速风格化迁移算法**

**(Per-Style-Per-Model Fast Neural Method)**

单模型单风格的快速风格化迁移算法是最早的一类快速风格化算法。主要想法是针对每一个风格图，我们去训练一个特定**（style specific）**的前向模型，这样当测试的时候，我们只需要向前向模型扔进去一张内容图，就可以前向出一个风格化结果了。如此这般，工业化落地就非常方便了，直接将模型打包，做成一个API，用户上传数据后直接把数据扔进去返回结果就好了（模型大小也不大，在淘宝AI Team的

[黄真川](https://www.zhihu.com/people/80bc53bef5465e5697ab59ef1a5f50cd)



哥指导下对模型进行优化并打包，最终用的时候一个tf模型才0.99MB）。



这一类算法（简称PSPM）其实可以再分成两类**（1）一类是基于统计分布的参数化快速风格化PSPM算法以及（2）基于MRF的非参数化PSPM算法**：

**（1）**这一小类算法代表性工作主要有两个，一个由斯坦福的[Justin Johnson](https://link.zhihu.com/?target=https%3A//cs.stanford.edu/people/jcjohns/)提出（CS231n的大神lecture）[42] [[Perceptual Losses for Real-Time Style Transfer and Super-Resolution\]](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.08155.pdf)，另一个由俄罗斯成立不久的Skolkovo科技研究所的[Ulyanov](https://link.zhihu.com/?target=https%3A//dmitryulyanov.github.io/about)提出（Deep Image Prior的作者，关于**Deep Image Prior和风格化迁移算法之间的关系**可以参考我这个回答 [Yongcheng Jing：如何评价Deep Image Prior这篇文章？](https://www.zhihu.com/question/263404981/answer/364372683)）[43] [[Texture Networks: Feed-forward Synthesis of Textures and Stylized Images\]](https://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v48/ulyanov16.pdf)。这两个工作的思想相同，都是用一个前向网路求学一个风格。训练数据可以用COCO的8万张图，损失函数和祖师爷Gatys的慢速风格化算法相同，用Gram统计量来进行风格建模。不同之处在于两个工作的**具体网络框架设计不同**，一个基于当时最新的残差网络设计的，一个是设计了多尺度的网络（我实验发现基于残差网络的设计能更好地最小化风格化损失函数，感兴趣的同学可以去看我arXiv上Review的实验部分）。Ulyanov后来又在CVPR2017上对其之前的工作做了改进，他们发现**Instance Normalization** 比 **Batch Normalization** 能够更快、更好地使模型达到收敛（其实就是把batch normalization的batch size设成1），但其实**Instance Normalization**的idea**最早**是由

[Naiyan Wang](https://www.zhihu.com/people/f5911fddc7fa5fd74a80d5ce2c12e1a2)

老师在

[[1603.04779\] Revisiting Batch Normalization For Practical Domain Adaptation](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.04779)

中提出的 ，即文章中的

Adaptive Batch Normalization (AdaBN)

。



[Naiyan Wang](https://www.zhihu.com/people/f5911fddc7fa5fd74a80d5ce2c12e1a2)



老师的AdaBN的思想是在BN层对不同的域使用由域内数据计算出的域特定的均值和方差统计量，以解决domain adaption的问题。而在前面我们提到，其实图像风格化也是一个domain adaption的问题，不同图像可以看做不同的域，从这个层面上来讲，其实AdaBN和IN的思想是相似的，而AdaBN的工作其实早于IN。



**（2）**第二小类基于MRF的快速PSPM风格化算法也是由[Chuan Li](https://link.zhihu.com/?target=http%3A//www.staff.uni-mainz.de/chuli/) 学长提出的[47] [[Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks\]](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1604.04382.pdf)，他们将自己之前提出的基于patch的慢速风格化算法进行了加速。同样是训练一个前向网络，[Chuan Li](https://link.zhihu.com/?target=http%3A//www.staff.uni-mainz.de/chuli/) 学长进一步利用**GAN中的判别网络**的想法来取代他们慢速风格化算法中的patch匹配（patch matching）过程。这一工作的最终效果虽然不是特别理想（由我个人的审美来看），但这篇文章的理论价值很大，其想法在另一篇具有开创意义的文章，清华出身、BAIR PhD的大牛 [朱俊彦](https://link.zhihu.com/?target=http%3A//people.eecs.berkeley.edu/~junyanz/) 的**Image-to-Image Translation with Conditional Adversarial Networks**中被进一步延伸和发展。

## **2.2. 单模型多风格的快速风格化迁移算法**

**(Multiple-Style-Per-Model Fast Neural Method)**

楼上的单模型单风格的快速风格化迁移算法对于每一个风格都要训练一个模型，这个就很不便利了，而且如果在工业落地的时候，有几百个风格的话还好说，要是有上万、百万的风格的话，所有模型占用的巨大空间开销就肯定会被产品经理拎出来说了。于是乎，学术圈的很多研究者开始研究咋利用**一个模型去学习很多个风格**，即**单模型多风格**的快速风格化迁移算法（下面简称**MSPM**）。

大家可以设身处地想一下这个问题，要是需要咱们自己去解决这个问题，可以怎么去想。首先把多个风格整合到一个模型中，理论上是合理的。比如咱们就说中国山水画，有很多著名山水画作品，但不同山水画虽然风格不尽相同，但是还是有很多相似的地方的（相似特征），所以对每一幅山水画训练得到的网络之间理论上是有**共享**的部分的。于是乎在这种情况下，对每个风格都学习一个网络本来就是一件很冗余、浪费资源的事情。沿着这个思路想，我们能不能**发掘出不同风格网络之间共享的部分，然后对于新的风格只去改变其有差别的部分，共享的部分保持不变**呢？bingo，这个就是Google Brain的众大佬们研究出来的一个MSPM算法的基本思路[48] [[A Learned Representation for Artistic Style\]](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.07629.pdf)。他们发现在训练好的一个风格化网络基础上，只通过在Instance Norlization层上做一个仿射变换（他们起了个名字叫 **Conditional Instance Normalization**，简称**CIN**），就可以得到一个具有完全不同风格的结果。这下好了，我们只需要把CIN层中仿射变换的很少的参数与每一个风格进行绑定，每个新风格只需要去训练这些参数，其余部分保持不变就ok了~ 最后实验效果显著。但说实话这个有点类似科学发现的意味，CIN层work的理由现在也没有严格的推导证明。一个大概的解释是CIN能够进行一种style normalization，能够将图像中的风格直接normalize成另外一种风格。

另一个由

[微软亚洲研究院](https://www.zhihu.com/people/27b8390344b45c7289032f3218ced120)



与中科大的联合培养博士生



[陈冬冬](https://www.zhihu.com/people/5190c670883296cc9d7b8ee1e4e3f56d)



哥提出的MSPM算法[49]



[StyleBank: An Explicit Representation for Neural Image Style Transfer]



与Google Brain这篇思路有异曲同工之妙，核心思想为把风格化网络中间的几层单独拎出来（文章中起了个名字叫

StyleBank层

），与每个风格进行绑定，对于每个新风格只去训练中间那几层，其余部分保持不变。这里也有一篇



[微软亚洲研究院](https://www.zhihu.com/people/27b8390344b45c7289032f3218ced120)



的介绍文章：

AI 创造艺术风格化：从图片到视频

。



其中特别感谢



[陈冬冬](https://www.zhihu.com/people/5190c670883296cc9d7b8ee1e4e3f56d)



哥帮我forwarding算法结果 :）



ok，上面俩工作的共同点都是把网络的一部分拿出来与每个风格进行绑定，从而实现MSPM，虽然随着风格的增加，模型大小不会大很多，但总归还是会跟着变大。所以呢，另外有一些研究者想，咱能不能试试完全用一个网络，看它能不能学到多个风格。这时候需要考虑的问题是既然只用一个网络，那就需要给网络一个信号，我们需要风格化成哪一个风格。这一思路最早由Amazon AI的

[张航](https://www.zhihu.com/people/4cc7002ae7bf956a79dab4752645670e)



哥在

2017年3月

提出[51]



[Multi-style Generative Network for Real-time Transfer]

。



[张航](https://www.zhihu.com/people/4cc7002ae7bf956a79dab4752645670e)



哥算法的核心思想是把通过VGG网络提取到的风格特征与风格化网络中的多个尺度的中间层的feature map通过提出的

Inspiration Layer

结合在一起，相当于将风格特征作为信号输入到网络中来决定要风格化成哪一个风格。最终算法的效果非常显著，由我自己实验结果来看[51]是质量上最接近PSPM算法结果的MSPM方法。在



[张航](https://www.zhihu.com/people/4cc7002ae7bf956a79dab4752645670e)



哥的博文里有对这个方法更详细的介绍：

多风格生成网络——实时风格转换



在此非常感谢



[张航](https://www.zhihu.com/people/4cc7002ae7bf956a79dab4752645670e)



哥在训练模型上给予我的帮助 :）



另外除了把风格特征作为信号外，另一个选择是把图像像素作为信号输入进去风格化网络。这一想法的可行性在浙大[李一君](https://link.zhihu.com/?target=https%3A//sites.google.com/site/yijunlimaverick/)学长的工作[50]中得到了证明 [[Diversified Texture Synthesis With Feed-Forward Networks\]](https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2017/papers/Li_Diversified_Texture_Synthesis_CVPR_2017_paper.pdf)。[李一君](https://link.zhihu.com/?target=https%3A//sites.google.com/site/yijunlimaverick/)学长首先将每一张风格图与一个随机产生的噪声图进行绑定，然后将噪声图与风格化网络中间层的feature map进行concat，作为网络进行风格选择的信号。

## **2.3. 单模型任意风格的快速风格化迁移算法**

**(Arbitrary-Style-Per-Model Fast Neural Method)**

有了上面的介绍，现在咱们已经能够做到用一个网络迁移多个风格了，虽然模型大小的问题一定程度上解决了，不过对于新的一组风格，我们仍然有额外训练时间的开销。于是有学者开始想，我们能不能搞一个模型出来，做到**Zero-shot Fast Style Transfer**，即来一个新风格不需要训练，我们就可以很快速地把风格化结果输出来？（这里我们称之为单模型任意风格，简称**ASPM**）

最早的ASPM算法由多伦多大学的Tian Qi Chen（不是UW的明星博士生陈天奇）提出的[52] [[Fast Patch-based Style Transfer of Arbitrary Style\]](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1612.04337.pdf)。这个算法是基于patch的，可以归到**基于MRF的非参数化ASPM算法**。基本思想是在CNN特征空间中，找到与内容patch匹配的风格patch后，进行内容patch与风格patch的交换**（Style Swap）**，之后用快速图像重建算法的思想对交换得到的feature map进行快速重建。但由于style swap需要一定的时间开销，[52]没有达到实时。

第一篇能达到实时的ASPM算法由康奈尔的大牛

[Xun Huang](https://www.zhihu.com/people/8bb80097ecfcec5fa9aaf1742686dc7c)



提出[46]



[Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization]

。[46]中的工作主要受到MSPM的CIN层启发，提出一个

Adaptive Instance Normalization（AdaIN）

。AdaIN的输入是通过VGG提取的风格和内容特征，用数据驱动的方式，通过在大规模风格和内容图上进行训练，让AdaIN能够直接将图像中的内容normalise成不同的风格。这一工作录用为ICCV2017的Oral。

在此非常感谢



[Xun Huang](https://www.zhihu.com/people/8bb80097ecfcec5fa9aaf1742686dc7c)



哥在实验过程中给予的帮助 :)



另外一个数据驱动的ASPM方法由CIN的提出者——Google Brain提出[53] [[Exploring the Structure of a Real-time, Arbitrary Neural Artistic Stylization Network\]](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1705.06830.pdf)。[53]可以看做是[48]的一个follow-up的工作，既然通过改变CIN层中仿射变换的参数，就可以得到不同的style，换言之，只要任意给一个风格，我们只需要知道他的CIN层中的仿射变换的参数就可以了。沿着这个思路，Google Brain的研究者设计和训练了一个**style prediction network**去专门预测每个style的**仿射变换的参数**，style prediction network需要大规模style和content图来进行训练。这个方法的缺点也很明显，数据驱动的方式不可避免地导致风格化效果与训练数据集中style的种类和数量非常相关。

由以上数据驱动ASPM算法的局限性，[李一君](https://link.zhihu.com/?target=https%3A//sites.google.com/site/yijunlimaverick/)学长进一步思考能不能用一种不需要学习训练的方式（**style learning-free**），而是单纯使用一系列特征变换来进行ASPM风格迁移 [54] [[Universal Style Transfer via Feature Transforms\]](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1705.08086.pdf)。[李一君](https://link.zhihu.com/?target=https%3A//sites.google.com/site/yijunlimaverick/)学长发现在VGG提取的特征上用**ZCA whitening transform**能够把一张图片的风格信息抹去，而保留原有高级语义信息，之后应用**coloring transform**将风格图的颜色进行迁移，即可重建出效果不错的风格化结果。这一工作发表于NIPS2017上，也是很少见的一篇NIPS上发表的application类的文章，足见学术界对Neural Style Transfer的关注。

------

**由以上讨论，大家可以发现，其实在图像风格化迁移这一领域的很多代表性工作中都有着中国研究者的身影。这些工作经过进一步地传承与发展，最终形成了这一全新的领域。前一阵子看到**

**周博磊**



老师的谈华人学者leadership问题的文章时正好在准备这篇Review，当时很有感触，在此贴上



[周博磊](https://www.zhihu.com/people/cbb829f60195651c37e4499286639be6)



老师的文章链接与大家共勉：



从CVPR‘18谈谈华人研究者的Leadership

。



------

> 下期预告：以上介绍的都是一般意义上的图像风格化迁移算法，目前学术界和工业界还出现了很多基于图像风格化算法衍生出的新的有意思的应用。下篇文章——三部曲之（三）将用一系列有意思的展示图来介绍这些有趣的扩展应用。

![img](https://pic4.zhimg.com/80/v2-6248c99c56af9e3d6c104cba5d758cbf_hd.jpg)Image from http://paintstransfer.com/

**如果我在那篇Review中遗漏了你的文章，欢迎给我发邮件(ycjing@zju.edu.cn)，我会尽快加到里面去，谢谢！:)**




# 相关

- [图像风格化算法综述三部曲之 (二) (Neural Style Transfer: A Review)](https://zhuanlan.zhihu.com/p/36346074)
