# 监督学习分类

分为：

- 判别模型与生成模型。根据模型对数据的建模方式
- 概率模型与非概率模型
- 参数模型与非参数模型

## 生成式模型与判别式模型


**判别模型**

两种：

1. 直接对输入空间到输出空间的映射进行建模，也就是学习函数 $h$：

$$
h : X \rightarrow Y, s . t . y=h(x)
$$

2. 对条件概率 $P(y | x)$ 进行建模，然后根据贝叶斯风险最小化的准则进行分类：

$$
y=\underset{y \in[1-1]}{\arg \max } P(y | x)
$$

举例：

- 感知机
- 逻辑斯特回归
- 支持向量机
- 神经网络
- K 近邻

**生成模型**

生成模型是间接地，先对 $P(x, y)$ 进行建模，再根据贝叶斯公式：

$$
P(y | x)=\frac{P(x | y) P(y)}{P(x)}
$$


算出 $\mathrm{P}(\mathrm{y} | \mathrm{x})$ ，最后根据 $y=\underset{y \in\{1,-1\}}{\arg \max } P(y | x)$ 来做分类 (由 $y=\underset{y \in\{1,-1\}}{\arg \max } P(y | x)$ 可知，实际上不需要对 $P(x)$ 进行建模)。

对于分类问题，实际上我们只需对 $P(y | x)$ 直接进行建模就行了，没必要间接地先对 $P(x, y)$ 进行建模。但是对 $P(x, y)$ 进行建模从而达到判别的目的也有它自身的一些优势。<span style="color:red;">什么优势？</span>

举例：

- 高斯判别分析
- 朴素贝叶斯



**生成式模型和判别式模型对比**

举例，猫狗分类：

- 对于判别式模型，只需要学习二者差异即可。比如说猫的体型会比狗小一点。
- 对于生成式模型，需要学习猫什么样，狗什么样。有了二者的长相以后，再根据长相去区分。

举例，人的特征与性别：

人的特征 $X$ 和性别 $Y$ 有如下关系：

<p align="center">
    <img width="26%" height="70%" src="http://images.iterate.site/blog/image/20200222/iYcFc9sLJ0ST.png?imageslim">
</p>


- 用生成式模型：统计得到联合概率分布 $P(X, Y)$ 后，可以学习一个模型，比如用二维高斯分布来拟合，得到 $X$，$Y$ 的一个联合分布。在使用模型来预测时，给一个$X$，预测其 $Y$，则需要通过贝叶斯公式得到条件概率分布才能进行推断：$P(Y|X)={\frac{P(X,Y)}{P(X)}}$ ，嗯，这个时候的 P(X) 是可以从样本中统计出来的。<span style="color:red;">怎么拟合这样的数据的？</span>
- 判别式模型：训练一个模型，输入人的特征 $X$，输出则是对于性别的判断概率，这个概率服从一个分布，分布的取值只有两个，要么男，要么女，记这个分布为 $Y$。这个过程学习了一个条件概率分布 $P(Y|X)$，即输入特征 $X$ 的分布已知条件下，$Y$ 的概率分布。

可见：

- 生成法关心的是 $X$ 和 $Y$ 之间的关系。
- 判别方法关心的是对于给定的输入 $X$，应该预测什么样的输出 $Y$。

而且：

- 判别式模型似乎要方便很多，因为生成式模型要学习一个 $X$，$Y$ 的联合分布往往需要很多数据，而判别式模型需要的数据则相对少，因为判别式模型更关注输入特征的差异性。
- 不过生成式既然使用了更多数据来生成联合分布，自然也能够提供更多的信息，现在有一个样本 $(X,Y)$，其联合概率 $P(X,Y)$ 经过计算特别小，那么可以认为这个样本是异常样本。这种模型可以用来做 outlier detection。<span style="color:red;">厉害呀。这个真的可以这样吗？</span>


**哪些是生成式模型，哪些是判别式模型？**

常见的概率图模型有：<span style="color:red;">要都总结进来。</span>

- 朴素贝叶斯
- 最大熵模型
- 贝叶斯网络
- 隐马尔可夫模型
- 条件随机场
- pLSA
- LDA
- 等

下面列出各模型所属及原因：<span style="color:red;">还是不清楚，要全部分析一遍。</span>

- 这些模型都是先对联合概率分布进行建模，然后再通过计算边缘分布得到对变量的预测，所以它们都属于**生成式模型**：
    - 朴素贝叶斯
    - 贝叶斯网络
    - pLSA
    - LDA
    - 等
- 最大熵模型是直接对条件概率分布进行建模
    - 最大熵模型是**判别式模型**
- 隐马尔可夫模型和条件随机场模型是对序列数据进行建模的方法
    - 隐马尔可夫模型属于**生成式模型**
    - 条件随机场属于**判别式模型**

## 概率模型与非概率模型

**概率模型**

概率模型指出了学习的目的是学出 $P(x,y)$ 或 $P(y|x)$，但最后都是根据 $y=\underset{y \in\{1,-1\}}{\arg \max } P(y | x)$ 来做判别归类。

对于 $P(x,y)$ 的估计，一般是根据乘法公式 $P(x,y) = P(x|y)P(y)$ 将其拆解成 $P(x|y)$,$P(y)$ 分别进行估计。

无论是对 $P(x|y)$,$P(y)$ 还是 $P(y|x)$ 的估计，都是会先假设分布的形式，

例如：<span style="color:red;">补充些别的例子。</span>

- 逻辑斯特回归就假设了 $Y|X$ 服从伯努利分布。<span style="color:red;">**？**</span>

分布形式固定以后，剩下的就是分布参数的估计问题。常用的估计有：

- 极大似然估计(MLE)
- 极大后验概率估计(MAP)
- 等。

其中，极大后验概率估计涉及到分布参数的先验概率，这为我们注入先验知识提供了途径。<span style="color:red;">极大后验概率估计怎么帮助注入先验知识的？</span>


概率模型有：

- 逻辑斯特回归
- 高斯判别分析
- 朴素贝叶斯


**非概率模型**

非概率模型指的是直接学习输入空间到输出空间的映射 $h$，学习的过程中基本不涉及概率密度的估计，概率密度的积分等操作，问题的关键在于最优化问题的求解。

通常，为了学习假设 $h(x)$，我们会先根据一些先验知识来选择一个特定的假设空间 $H$(函数空间)，例如一个由所有线性函数构成的空间，然后在这个空间中找出泛化误差最小的假设出来。

$$
h^{*}=\underset{h \in H}{\arg \min } \varepsilon(h)=\underset{h \in H}{\arg \min } \sum_{x, y} l(h(x), y) P(x, y)\tag{3}
$$

其中 $l(h(x),y)$ 是我们选取的损失函数，选择不同的损失函数，得到假设的泛化误差就会不一样。

由于我们并不知道 $P(x,y)$，所以即使我们选好了损失函数，也无法计算出假设的泛化误差，更别提找到那个给出最小泛化误差的假设。于是，我们转而去找那个使得经验误差最小的假设。

$$
g=\underset{h \in H}{\arg \min } \hat{\varepsilon}(h)=\underset{h \in H}{\arg \min } \frac{1}{m} \sum_{i=1}^{m} l\left(h\left(x^{(i)}\right), y^{(i)}\right)\tag{4}
$$


这种学习的策略叫经验误差最小化(ERM)，理论依据是大数定律：

- 当训练样例无穷多的时候，假设的经验误差会依概率收敛到假设的泛化误差。

要想成功地学习一个问题，必须在学习的过程中注入先验知识。前面，我们根据先验知识来选择假设空间，其实，在选定了假设空间后，先验知识还可以继续发挥作用，这一点体现在为我们的式子 【4】 中加上正则化项上，例如常用的 L1 正则化，L2正则化等。


$$
g=\underset{h \in H}{\arg \min } \hat{\varepsilon}(h)=\underset{h \in H}{\arg \min } \frac{1}{m} \sum_{i=1}^{m} l\left(h\left(x^{(i)}\right), y^{(i)}\right)+\lambda \Omega(h)\tag{5}
$$


正则化项一般是对模型的复杂度进行惩罚，例如我们的先验知识告诉我们模型应当是稀疏的，这时我们会选择 L1 范数。

当然，加正则化项的另一种解释是为了防止对有限样例的过拟合，但这种解释本质上还是根据先验知识认为模型本身不会太复杂。

在经验误差的基础上加上正则化项，同时最小化这两者，这种学习的策略叫做结构风险最小化(SRM)。最后，学习算法 A 根据训练数据集 $D$，从假设空间中挑出一个假设 $g$，作为我们将来做预测的时候可以用。具体来说，学习算法 A 其实是一个映射，对于每一个给定的数据集 $D$，对于选定的学习策略(ERM or SRM)，都有确定的假设与 $D$ 对应

$$
A : D \mapsto h, s . t . h=A(D)\tag{6}
$$


非概率模型有如下：

- 感知机
- 支持向量机
- 神经网络
- k近邻

线性支持向量机可以显式地写出损失函数——hinge损失。神经网络也可以显式地写出损失函数——平方损失。

时下流行的迁移学习，其中有一种迁移方式是基于样本的迁移。这种方式最后要解决的问题就是求解一个加权的经验误差最小化问题，而权重就是目标域与源域的边际密度之比。所以，线性支持向量机在迁移学习的环境下可以进行直接的推广。


**概率模型与非概率模型的对应关系**


在一定的条件下，非概率模型与概率模型有以下对应关系：

<p align="center">
    <img width="60%" height="70%" src="http://images.iterate.site/blog/image/20190828/SEtoCpiOxu09.png?imageslim">
</p>



## 参数模型与非参数模型


**参数模型**

如果我们对所要学习的问题有足够的认识，具备一定的先验知识，此时我们一般会假定要学习的目标函数 $f(x)$ 或分布 $P(y|x)$ 的具体形式。

然后，通过训练数据集，基于 ERM、SRM、MLE、MAP等学习策略，可以估计出 $f(x)$ 或 $P(y|x)$ 中含有的未知参数。

一旦未知参数估计完毕，训练数据一般来说，就失去其作用了，因为这些估计出来的参数就是训练数据的浓缩。

通过这种方式建立起来的模型就是参数模型。

参数模型的一个很重要的特点是，如果对于模型的假设正确，那么只需要很少的训练数据就可以从假设空间中学出一个很好的模型。但是，如果模型的假设错误，那么无论训练的数据量有多大，甚至趋于无穷大，学出的模型都会与实际模型出现不可磨灭的偏差。

参数模型有如下：

- 感知机
- 逻辑斯特回归
- 高斯判别分析
- 朴素贝叶斯
- 线性支持向量机


**非参数模型**

当我们对所要学习的问题知之甚少，此时我们一般不会对潜在的模型做过多的假设。在面对预测任务的时候，我们通常会用上所有的训练数据。

例如简单的核密度估计(KDE)的表达式中，就带有所有训练数据的信息。通过这种方式建立的模型就是非参数模型。

非参数模型的一个很重要的特点就是：let the data speak for itself.

正因为如此，非参数模型的存储开销、计算开销都会比参数模型大的多。

但是，由于不存在模型的错误假定问题，可以证明，当训练数据量趋于无穷大的时候，非参数模型可以逼近任意复杂的真实模型。这正是非参数模型诱人的一点。

另外需要说明的一点是，非参数模型之所以叫做非参数，并不是因为模型中没有参数。实际上，非参数模型中一般会含有一个或多个超参数，外加无穷多个普通的参数。


非参数模型如下：

- K近邻就是典型的非参数模型。


**半参数模型**


对于神经网络来说，当固定了隐层的数目以及每一层神经元的个数，它也属于参数模型。但由于隐层数目与每一层神经元个数的不确定性，很多时候，神经网络都被归类为半参数模型。

通过加大网络的深度(加大隐层数目)以及宽度(增加每一层神经元的个数)，使假设空间的复杂度得到极大的提高。复杂的假设空间有极强的表达能力，当训练数据量很大的时候，不会陷入过拟合。所以，深度学习的成功，从理论上讲，一方面来源于海量的训练数据，另一方面来源于其复杂的网络结构。
