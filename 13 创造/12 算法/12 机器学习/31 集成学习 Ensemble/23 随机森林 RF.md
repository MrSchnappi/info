---
title: 23 随机森林 RF
toc: true
date: 2018-06-28 15:20:10
---
# 随机森林

随机森林(Random Forest，简称 RF) 是 Bagging 的一个 扩展变体。RF 在以决策树为基学习器构建 Bagging 集成的基础上，进一步在 决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分 属性时是在当前结点的属性集合(假定有 $d$ 个属性)中选择一个最优属性；而在 RF中，对基决策树的每个结点，先从该结点的属性集合中隨机选择一个包含 $k$ 个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数 $k$ 控制了随机性的引入程度：若令 $k = d$ ，则基决策树的构建与传统决策树相同; 若令 $k = 1$，则是随机选择一个属性用于划分；一般情况下，推荐值 $k=log_2d$ 。

随机森林简单、容易实现、计算开销小，令人惊奇的是，它在很多现实任务中展现出强大的性能，被誉为 “代表集成学习技术水平的方法”。可以看出, 随机森林对 Bagging 只做了小改动，但是与 Bagging 中基学习器的“多样性” 仅通过样本扰动（通过对初始训练集采样）而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升.

随机森林的收敛性与 Bagging 相似。如图 8.7 所示，随机森林的起始性能 往往相对较差，特别是在集成中只包含一个基学习器时。这很容易理解，因为通 过引入属性扰动，随机森林中个体学习器的性能往往有所降低。然而，随着个体学习器数目的增加，随机森林通常会收敛到更低的泛化误差。值得一提的是，随 机森林的训练效率常优于 Bagging，因为在个体决策树的构建过程中，Bagging 使用的是 “确定型” 决策树，在选择划分属性时要对结点的所有属性进行考察， 而随机森林使用的 “随机型” 决策树则只需考察一个属性子集。

<center>

![](http://images.iterate.site/blog/image/180628/HH5bbcADlE.png?imageslim){ width=55% }


</center>



## 随机森林的随机性体现在哪里？


随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。随机森林的随机性体现在每颗树的训练样本是随机的，树中每个节点的分裂属性集合也是随机选择确定的。有了这 2 个随机的保证，随机森林就不会产生过拟合的现象了。

 随机森林是用一种随机的方式建立的一个森林，森林是由很多棵决策树组成的，每棵树所分配的训练样本是随机的，树中每个节点的分裂属性集合也是随机选择确定的。





# 相关

- 《机器学习》周志华
- [独家 | 一文读懂随机森林的解释和实现 - 知乎](https://zhuanlan.zhihu.com/p/51165358)
