---
title: 3.14 其他的输出类型
toc: true
date: 2019-08-31
---

### 2.4 其他的输出类型

<span style="color:red;">基本没看懂。</span>

之前描述的线性、sigmoid 和 softmax 输出单元是最常见的。神经网络可以推广到我们希望的几乎任何种类的输出层。最大似然原则给如何为几乎任何种类的输出层设计一个好的代价函数提供了指导。<span style="color:red;">嗯。</span>

一般的，如果我们定义了一个条件分布 $p(\boldsymbol y\mid\boldsymbol x; \boldsymbol \theta)$，最大似然原则建议我们使用 $-\log p(\boldsymbol y\mid \boldsymbol x;\boldsymbol \theta)$ 作为代价函数。

一般来说，我们可以认为神经网络表示函数 $f(\boldsymbol x;\boldsymbol \theta)$。这个函数的输出不是对 $\boldsymbol y$ 值的直接预测。相反，$f(\boldsymbol x;\boldsymbol \theta)=\boldsymbol \omega$ 提供了 $y$ 分布的参数。<span style="color:red;">提供了 $y$ 分布的参数是什么意思？</span>我们的损失函数就可以表示成 $-\log p(\mathbf y; \boldsymbol \omega(\boldsymbol x))$。<span style="color:red;">为什么可以表示为这个？</span>

例如，我们想要学习在给定 $\mathbf x$ 时，$\mathbf y$ 的条件高斯分布的方差。<span style="color:red;">为什么要学习 $\mathbf y$ 的条件高斯分布的方差？</span>简单情况下，方差 $\sigma^2$ 是一个常数，此时有一个解析表达式，这是因为方差的最大似然估计量仅仅是观测值 $\mathbf y$ 与它们的期望值的差值的平方平均。一种计算上代价更加高但是不需要写特殊情况代码的方法是简单地将方差作为分布 $p(\mathbf y\mid\boldsymbol x)$ 的其中一个属性，这个分布由 $\boldsymbol \omega=f(\boldsymbol x;\boldsymbol \theta)$ 控制。<span style="color:red;">为什么可以吧方差作为它的一个属性？</span>负对数似然 $-\log p(\boldsymbol y;\boldsymbol \omega(\boldsymbol x))$ 将为代价函数提供一个必要的合适项来使我们的优化过程可以逐渐地学到方差。在标准差不依赖于输入的简单情况下，我们可以在网络中创建一个直接复制到 $\boldsymbol \omega$ 中的新参数。<span style="color:red;">怎么创建？</span>这个新参数可以是 $\sigma$ 本身，或者可以是表示 $\sigma^2$ 的参数 $v$，或者可以是表示 $\frac{1}{\sigma^2}$ 的参数 $\beta$，取决于我们怎样对分布参数化。我们可能希望模型对不同的 $\mathbf x$ 值预测出 $\mathbf y$ 不同的方差。这被称为异方差模型。在异方差情况下，我们简单地把方差指定为 $f(\mathbf x;\boldsymbol \theta)$ 其中一个输出值。实现它的典型方法是使用精度而不是方差来表示高斯分布，就像式 3.22 所描述的。在多维变量的情况下，最常见的是使用一个对角精度矩阵：

<span style="color:red;">上面整段都没看懂。</span>

$$
\text{diag}(\boldsymbol \beta).\tag{6.34}
$$

这个公式适用于梯度下降，因为由 $\boldsymbol \beta$ 参数化的高斯分布的对数似然的公式仅涉及 $\beta_i$ 的乘法和 $\log \beta_i$ 的加法。乘法、加法和对数运算的梯度表现良好。相比之下，如果我们用方差来参数化输出，我们需要用到除法。除法函数在零附近会变得任意陡峭。虽然大梯度可以帮助学习，但任意大的梯度通常导致不稳定。如果我们用标准差来参数化输出，对数似然仍然会涉及除法，并且还将涉及平方。通过平方运算的梯度可能在零附近消失，这使得学习被平方的参数变得困难。无论我们使用的是标准差，方差还是精度，我们必须确保高斯分布的协方差矩阵是正定的。因为精度矩阵的特征值是协方差矩阵特征值的倒数，所以这等价于确保精度矩阵是正定的。如果我们使用对角矩阵，或者是一个常数乘以单位矩阵(注：译者注：这里原文是"If we use a diagonal matrix, or a scalar times the diagonal matrix..."即"如果我们使用对角矩阵，或者是一个标量乘以对角矩阵..."，但一个标量乘以对角矩阵和对角矩阵没区别，结合上下文可以看出，这里原作者误把"identity"写成了"diagonal matrix"，因此这里采用"常数乘以单位矩阵"的译法。)，那么我们需要对模型输出强加的唯一条件是它的元素都为正。如果我们假设 $\boldsymbol a$ 是用于确定对角精度的模型的原始激活，那么可以用 softplus 函数来获得正的精度向量：$\boldsymbol \beta=\boldsymbol zeta(\boldsymbol a)$。这种相同的策略对于方差或标准差同样适用，也适用于常数乘以单位阵的情况。

<span style="color:red;">上面这段没看懂。</span>

学习一个比对角矩阵具有更丰富结构的协方差或者精度矩阵是很少见的。如果协方差矩阵是满的和有条件的，那么参数化的选择就必须要保证预测的协方差矩阵是正定的。这可以通过写成 $\boldsymbol Sigma(\boldsymbol x)=\boldsymbol B(\boldsymbol x)\boldsymbol B^\top (\boldsymbol x)$ 来实现，这里 $\boldsymbol B$ 是一个无约束的方阵。如果矩阵是满秩的，那么一个实际问题是计算似然的代价是很高的，计算一个 $d\times d$ 的矩阵的行列式或者 $\boldsymbol Sigma(\boldsymbol x)$ 的逆（或者等价地并且更常用地，对它特征值分解或者 $\boldsymbol B(\boldsymbol x)$ 的特征值分解）需要 $O(d^3)$ 的计算量。


我们经常想要执行多峰回归(multimodal regression)，即预测条件分布 $p(\boldsymbol y\mid\boldsymbol x)$ 的实值，该条件分布对于相同的 $\boldsymbol x$ 值在 $\boldsymbol y$ 空间中有多个不同的峰值。在这种情况下，高斯混合是输出的自然表示。将高斯混合作为其输出的神经网络通常被称为混合密度网络。具有 $n$ 个分量的高斯混合输出由下面的条件分布定义：

$$
p(\boldsymbol y\mid\boldsymbol x) = \sum_{i=1}^n p(\mathrm c = i \mid \boldsymbol x) \mathcal N(\boldsymbol y; \boldsymbol mu^{(i)}(\boldsymbol x), \boldsymbol Sigma^{(i)}(\boldsymbol x)).
$$

神经网络必须有三个输出：定义 $p(\mathrm c=i\mid\boldsymbol x)$ 的向量，对所有的 $i$ 给出 $\boldsymbol mu^{(i)}(\boldsymbol x)$ 的矩阵，以及对所有的 $i$ 给出 $\boldsymbol Sigma^{(i)}(\boldsymbol x)$ 的张量。这些输出必须满足不同的约束：

- 混合组件 $p(\mathrm c=i\mid\boldsymbol x)$：它们由潜变量(注：我们之所以认为 $\mathrm c$ 是潜在的，是因为我们不能直接在数据中观测到它：给定输入 $\mathbf x$ 和目标 $\mathbf y$，不可能确切地知道是哪个高斯组件产生 $\mathbf y$，但我们可以想象 $\mathbf y$ 是通过选择其中一个来产生的，并且将那个未被观测到的选择作为随机变量。) $\mathrm c$ 关联着，在 $n$ 个不同组件上形成 Multinoulli 分布。这个分布通常可以由 $n$ 维向量的 softmax 来获得，以确保这些输出是正的并且和为 1。
- 均值 $\boldsymbol mu^{(i)}(\boldsymbol x)$ ：它们指明了与第 $i$ 个高斯组件相关联的中心或者均值，并且是无约束的（通常对于这些输出单元完全没有非线性）。如果 $\mathbf y$ 是个 $d$ 维向量，那么网络必须输出一个由 $n$ 个这种 $d$ 维向量组成的 $n\times d$ 的矩阵。用最大似然来学习这些均值要比学习只有一个输出模式的分布的均值稍稍复杂一些。我们只想更新那个真正产生观测数据的组件的均值。在实践中，我们并不知道是哪个组件产生了观测数据。负对数似然表达式将每个样本对每个组件的贡献进行赋权，权重的大小由相应的组件产生这个样本的概率来决定。
- 协方差 $\boldsymbol Sigma^{(i)}(\boldsymbol x)$：它们指明了每个组件 $i$ 的协方差矩阵。和学习单个高斯组件时一样，我们通常使用对角矩阵来避免计算行列式。和学习混合均值时一样，最大似然是很复杂的，它需要将每个点的部分责任分配给每个混合组件。如果给定了混合模型的正确的负对数似然，梯度下降将自动地遵循正确的过程。


有报告说基于梯度的优化方法对于混合条件高斯（作为神经网络的输出）可能是不可靠的，部分是因为涉及到除法（除以方差）可能是数值不稳定的（当某个方差对于特定的实例变得非常小时，会导致非常大的梯度）。一种解决方法是梯度截断（见第 10.11.1 节），另外一种是启发式缩放梯度。<span style="color:red;">哇塞，什么是梯度截断和启发式缩放梯度？</span>


高斯混合输出在语音生成模型和物理运动中特别有效。<span style="color:red;">这么厉害吗？</span>混合密度策略为网络提供了一种方法来表示多种输出模式，并且控制输出的方差，这对于在这些实数域中获得高质量的结果是至关重要的。混合密度网络的一个实例如图 6.4 所示。



<center>

![](http://images.iterate.site/blog/image/20190710/zBdECkIoSeVV.png?imageslim){ width=55% }

</center>

> 从具有混合密度输出层的神经网络中抽取的样本。输入 $x$ 从均匀分布中采样，输出 $y$ 从 $p_{\text{model}}(y\mid x)$ 中采样。 神经网络能够学习从输入到输出分布的参数的非线性映射。这些参数包括控制三个组件中的哪一个将产生输出的概率，以及每个组件各自的参数。每个混合组件都是高斯分布，具有预测的均值和方差。 输出分布的这些方面都能够相对输入 $x$ 变化，并且以非线性的方式改变。


一般的，我们可能希望继续对包含更多变量的、更大的向量 $\boldsymbol y$ 来建模，并在这些输出变量上施加更多更丰富的结构。例如，我们可能希望神经网络输出字符序列形成一个句子。在这些情况下，我们可以继续使用最大似然原理应用到我们的模型 $p(\boldsymbol y;\boldsymbol \omega(\boldsymbol x))$ 上，但我们用来描述 $\boldsymbol y$ 的模型会变得非常复杂，超出了本章的范畴。第十章描述了如何使用循环神经网络来定义这种序列上的模型，第三部分描述了对任意概率分布进行建模的高级技术。







# 相关

- 《深度学习》花书
