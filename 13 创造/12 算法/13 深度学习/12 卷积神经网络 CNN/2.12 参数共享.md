---
title: 2.12 参数共享
toc: true
date: 2019-08-31
---

### 参数共享（Parameter Sharing）

参数共享是指在同一个模型的不同模块中使用相同的参数，它是卷积运算的固有属性。<span style="color:red;">同一个模型的不同模块中使用相同的参数？有吗？</span>

全连接网络中，计算每层的输出时，权值参数矩阵中的每个元素只作用于某个输入元素一次；而在卷积神经网络中，卷积核中的每一个元素将作用于每一次局部输入的特定位置上。<span style="color:red;">嗯。作用于每一次局部输入的特定位置时上？嗯？</span>

根据参数共享的思想，我们只需要学习一组参数集合，而不需要针对每个位置的每个参数都进行优化，从而大大降低了模型的存储需求。

参数共享的物理意义是使得卷积层具有平移等变性。<span style="color:red;">什么意思？</span>假如图像中有一只猫，那么无论它出现在图像中的任何位置，我们都应该将它识别为猫，也就是说神经网络的输出对于平移变换来说应当是等变的。特别地，当函数 $f(x)$ 与 $g(x)$ 满足 $f(g(x))=g(f(x))$ 时，我们称 $f(x)$ 关于变换 $g$ 具有等变性。将 $g$ 视为输入的任意平移函数，令 $I$ 表示输入图像（在整数坐标上的灰度值函数），平移变换后得到 $I^{\prime}=g(I)$ 。例如，我们把猫的图像向右移动 $l$ 像素，满足 $I^{\prime} (x, y)=I(x-1, y)$。我们令 $f$ 表示卷积函数，根据其性质，我们很容易得到 $g(f(I))=f\left(I^{\prime} \right)=f(g(I))$ 。也就是说，在猫的图片上先进行卷积，再向右平移 l 像素的输出，与先将图片向右平移 $l$ 像素再进行卷积操作的输出结果是相等的。<span style="color:red;">为什么这个地方看不明白了呢？嘶，重看。</span>




## 参数共享

参数共享是指在一个模型的多个函数中使用相同的参数。

在传统的神经网络中，当计算一层的输出时，权重矩阵的每一个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。

在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（是否考虑边界像素取决于对边界决策的设计）。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。

这虽然没有改变前向传播的运行时间（仍然是 $O(k\times n)$），但它显著地把模型的存储需求降低至 $k$ 个参数，并且 $k$ 通常要比 $m$ 小很多个数量级。因为 $m$ 和 $n$ 通常有着大致相同的大小，$k$ 在实际中相对于 $m\times n$ 是很小的。因此，卷积在存储需求和统计效率方面极大地优于稠密矩阵的乘法运算。

图 9.5 演示了参数共享是如何实现的：




<center>

![](http://images.iterate.site/blog/image/20190718/PEGWj5plClDU.png?imageslim){ width=55% }

</center>

> 9.5 参数共享。黑色箭头表示在两个不同的模型中使用了特殊参数的连接。\emph{(上)}黑色箭头表示在卷积模型中对 3 元素核的中间元素的使用。因为参数共享，这个单独的参数被用于所有的输入位置。\emph{(下)}这个单独的黑色箭头表示在全连接模型中对权重矩阵的中间元素的使用。这个模型没有使用参数共享，所以参数只使用了一次。



作为前两条原则的一个实际例子，图 9
.6 说明了稀疏连接和参数共享是如何显著提高线性函数在一张图像上进行边缘检测的效率的。




<center>

![](http://images.iterate.site/blog/image/20190718/6vO07JP8oq4H.png?imageslim){ width=55% }

</center>

> 9.6 边缘检测的效率。右边的图像是通过先获得原始图像中的每个像素，然后减去左边相邻像素的值而形成的。这个操作给出了输入图像中所有垂直方向上的边缘的强度，对目标检测来说是有用的。两个图像的高度均为 280 个像素。输入图像的宽度为 320 个像素，而输出图像的宽度为 319 个像素。这个变换可以通过包含两个元素的卷积核来描述，使用卷积需要 $319\times 280\times 3 = 267,960$ 次浮点运算（每个输出像素需要两次乘法和一次加法）。为了用矩阵乘法描述相同的变换，需要一个包含 $320\times 280\times 319\times 280$ 个或者说超过 80 亿个元素的矩阵，这使得卷积对于表示这种变换更有效 40 亿倍。直接运行矩阵乘法的算法将执行超过 160 亿次浮点运算，这使得卷积在计算上大约有 60,000倍的效率。当然，矩阵的大多数元素将为零。如果我们只存储矩阵的非零元，则矩阵乘法和卷积都需要相同数量的浮点运算来计算。矩阵仍然需要包含 $2\times 319\times 280=178,640$ 个元素。将小的局部区域上的相同线性变换应用到整个输入上，卷积是描述这种变换的极其有效的方法。




# 原文与相关

- 《百面机器学习》
- 《深度学习》花书
