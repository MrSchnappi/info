---
title: 6.02 提高卷积神经网络的泛化能力
toc: true
date: 2019-09-03
---

## 5.17 提高卷积神经网络的泛化能力

<span style="color:red;">这个跟增强普通网络的泛化能力的方法有些重复。</span>

卷积神经网络与其他类型的神经网络类似，在采用反向传播进行训练的过程中比较依赖输入的数据分布，当数据分布较为极端的情况下容易导致模型欠拟合或过拟合，下表记录了提高卷积网络泛化能力的方法。

| 方法 | 说明 |
| :---: | :--- |
| 使用更多数据 | 在有条件的前提下，尽可能多地获取训练数据是最理想的方法，更多的数据可以让模型得到充分的学习，也更容易提高泛化能力 |
| 使用更大批次 | 在相同迭代次数和学习率的条件下，每批次采用更多的数据将有助于模型更好的学习到正确的模式，模型输出结果也会更加稳定 |
| 调整数据分布 | 大多数场景下的数据分布是不均匀的，模型过多地学习某类数据容易导致其输出结果偏向于该类型的数据，此时通过调整输入的数据分布可以一定程度提高泛化能力。<span style="color:red;">怎么调整数据分布？</span> |
| 调整目标函数 | 在某些情况下，目标函数的选择会影响模型的泛化能力，如目标函数 $f(y,y')=|y-y'|$ 在某类样本已经识别较为准确而其他样本误差较大的侵害概况下，不同类别在计算损失结果的时候距离权重是相同的，若将目标函数改成 $f(y,y')=(y-y')^2$ 则可以使误差小的样本计算损失的梯度比误差大的样本更小，进而有效地平衡样本作用，提高模型泛化能力。<span style="color:red;">这个一般都是用的交叉熵吧？</span> |
| 调整网络结构 | 在浅层卷积神经网络中，参数量较少往往使模型的泛化能力不足而导致欠拟合，此时通过叠加卷积层可以有效地增加网络参数，提高模型表达能力；在深层卷积网络中，若没有充足的训练数据则容易导致模型过拟合，此时通过简化网络结构减少卷积层数可以起到提高模型泛化能力的作用 |
|   数据增强   | 数据增强又叫数据增广，在有限数据的前提下通过平移、旋转、加噪声等一些列变换来增加训练数据，同类数据的表现形式也变得更多样，有助于模型提高泛化能力，需要注意的是数据变化应尽可能不破坏元数数据的主体特征(如在图像分类任务中对图像进行裁剪时不能将分类主体目标裁出边界)。 |
|  权值正则化  | 权值正则化就是通常意义上的正则化，一般是在损失函数中添加一项权重矩阵的正则项作为惩罚项，用来惩罚损失值较小时网络权重过大的情况，此时往往是网络权值过拟合了数据样本(如 $Loss=f(WX+b,y')+\frac{\lambda}{\eta}\sum{|W|}$)。<span style="color:red;">这个式子之前好像没见到吧？里面的参数是什么意思？</span> |
| 屏蔽网络节点 | 该方法可以认为是网络结构上的正则化，通过随机性地屏蔽某些神经元的输出让剩余激活的神经元作用，可以使模型的容错性更强。 |





> 对大多数神经网络模型同样通用








# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
