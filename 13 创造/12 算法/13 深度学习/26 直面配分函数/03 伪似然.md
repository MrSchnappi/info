---
title: 03 伪似然
toc: true
date: 2019-06-05
---
# 伪似然

蒙特卡罗近似配分函数及其梯度需要直接处理配分函数。有些其他方法通过训练不需要计算配分函数的模型来绕开这个问题。这些方法大多数都基于以下观察：无向概率模型中很容易计算概率的比率。这是因为配分函数同时出现在比率的分子和分母中，互相抵消：

$$
\frac{p(\mathbf{x})}{p(\mathbf{y})}=\frac{\frac{1}{Z} \tilde{p}(\mathbf{x})}{\frac{1}{Z} \tilde{p}(\mathbf{y})}=\frac{\tilde{p}(\mathbf{x})}{\tilde{p}(\mathbf{y})}
$$

伪似然正是基于条件概率可以采用这种基于比率的形式，因此可以在没有配分函数的情况下进行计算。假设我们将 $\mathbf{x}$ 分为 $\mathbf{a}$、$\mathbf{b}$ 和 $\mathbf{c}$，其中 $\mathbf{a}$ 包含我们想要的条件分布的变量，$\mathbf{b}$ 包含我们想要条件化的变量，$\mathbf{c}$ 包含除此之外的变量：

$$
p(\mathbf{a} | \mathbf{b})=\frac{p(\mathbf{a}, \mathbf{b})}{p(\mathbf{b})}=\frac{p(\mathbf{a}, \mathbf{b})}{\sum_{\mathbf{a}, \mathbf{c}} p(\mathbf{a}, \mathbf{b}, \mathbf{c})}=\frac{\tilde{p}(\mathbf{a}, \mathbf{b})}{\sum_{\mathbf{a}, \mathbf{c}} \tilde{p}(\mathbf{a}, \mathbf{b}, \mathbf{c})}
$$

以上计算需要边缘化 $\mathbf{a}$，假设 $\mathbf{a}$ 和 $\mathbf{c}$ 包含的变量并不多，那么这将是非常高效的操作。在极端情况下，a可以是单个变量，c可以为空，那么该计算仅需要估计与单个随机变量值一样多的 $\tilde{p}$。

不幸的是，为了计算对数似然，我们需要边缘化很多变量。如果总共有 $n$ 个变量，那么我们必须边缘化 $n-1$ 个变量。根据概率的链式法则，我们有

$$
\log p(\mathbf{x})=\log p\left(x_{1}\right)+\log p\left(x_{2} | x_{1}\right)+\cdots+\log p\left(x_{n} | \mathbf{x}_{1 : n-1}\right)
$$

在这种情况下，我们已经使 $\mathbf{a}$ 尽可能小，但是 $\mathbf{c}$ 可以大到 $\mathbf{x}_{2 : n}$。如果我们简单地将 $\mathbf{c}$ 移到 $\mathbf{b}$ 中以减少计算代价，那么会发生什么呢？这便产生了伪似然(pseudolikelihood)(Besag,1975)目标函数，给定所有其他特征 $\boldsymbol{x}_{-i}$，预测特征 $x_{i}$ 的值：

$$
\sum_{i=1}^{n} \log p\left(x_{i} | \boldsymbol{x}_{-i}\right)
$$

如果每个随机变量有 $k$ 个不同的值，那么计算 $\tilde{p}$ 需要 $k \times n$ 次估计，而计算配分函数需要 $k^{n}$ 次估计。

这看起来似乎是一个没有道理的策略，但可以证明最大化伪似然的估计是渐近一致的(Mase,1995)。当然，在数据集不趋近于大采样极限的情况下，伪似然可能表现出与最大似然估计不同的结果。

我们可以使用广义伪似然估计(generalized pseudolikelihood estimator)来权衡计算复杂度和最大似然表现的偏差(Huang and Ogata,2002)。广义伪似然估计使用 $m$ 个不同的集合 $\mathbb{S}^{(i)}$，$i=1, \ldots, m$ 作为变量的指标出现在条件棒的左侧。在 $m=1$ 和 $\mathrm{S}^{(1)}=1, \dots, n$ 的极端情况下，广义伪似然估计会变为对数似然。在 $m=n$ 和 $\mathbb{S}^{(i)}=\{i\}$ 的极端情况下，广义伪似然会恢复为伪似然。广义伪似然估计目标函数如下所示

$$
\sum_{i=1}^{m} \log p\left(\mathbf{x}_{\mathbb{S}^{(i)}} | \mathbf{x}_{-\mathbb{S}^{(i)}}\right)
$$

基于伪似然的方法的性能在很大程度上取决于模型是如何使用的。对于完全联合分布 $p(\mathbf{x})$ 模型的任务(例如密度估计和采样)，伪似然通常效果不好。对于在训练期间只需要使用条件分布的任务而言，它的效果比最大似然更好，例如填充少量的缺失值。如果数据具有规则结构，使得 $\mathbb{S}$ 索引集可以被设计为表现最重要的相关性质，同时略去相关性可忽略的变量，那么广义伪似然策略将会非常有效。例如，在自然图像中，空间中相隔很远的像素也具有弱相关性，因此广义伪似然可以应用于每个 $\mathbb{S}$ 集是小的局部空间窗口的情况。

伪似然估计的一个弱点是它不能与仅在 $\tilde{p}(\mathbf{x})$ 上提供下界的其他近似一起使用，例如第 19 章中介绍的变分推断。这是因为 $\tilde{p}$ 出现在了分母中。分母的下界仅提供了整个表达式的上界，然而最大化上界没有什么意义。这使得我们难以将伪似然方法应用于诸如深度玻尔兹曼机的深度模型，因为变分方法是近似边缘化互相作用的多层隐藏变量的主要方法之一。尽管如此，伪似然仍然可以用在深度学习中，它可以用于单层模型，或使用不基于下界的近似推断方法的深度模型中。

伪似然比 SML 在每个梯度步骤中的计算代价要大得多，这是由于其对所有条件进行显式计算。但是，如果每个样本只计算一个随机选择的条件，那么广义伪似然和类似标准仍然可以很好地运行，从而使计算代价降低到和 SML 差不多的程度(Goodfellow et al.,2013d)。

虽然伪似然估计没有显式地最小化 $\log Z$，但是我们仍然认为它具有类似负相的效果。每个条件分布的分母会使得学习算法降低所有仅具有一个变量不同于训练样本的状态的概率。

读者可以参考 Marlin and de Freitas(2011)了解伪似然渐近效率的理论分析。




# 相关

- 《深度学习》花书
