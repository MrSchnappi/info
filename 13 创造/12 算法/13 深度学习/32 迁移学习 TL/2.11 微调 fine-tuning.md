---
title: 2.11 微调 fine-tuning
toc: true
date: 2019-09-01
---
# 微调

Model Fine-tuning (labelled source, labelled target)


任务情况：

用源数据训练一个模型，然后用目标数据微调模型


适用场景：

- 目标数据量很少，源数据量很多。（One-shot learning：在目标域中只有几个或非常少的样例）



难点：


- 只有很有限的目标数据，所以要注意过拟合问题。


## 解决过拟合的几个方法

### Conservative Training（保留训练）

<center>

![mark](http://images.iterate.site/blog/image/20190901/7g69rLOXN7NK.png?imageslim)

</center>

在微调新模型时加入限制（regularization），比如要求微调后的新模型与旧模型针对相同的输入的输出越相似越好，或者说模型的参数越相似越好。

### Layer Transfer（层转移器）

<center>

![mark](http://images.iterate.site/blog/image/20190901/noiS2TlIKy4N.png?imageslim)

</center>
将用源数据训练好的模型的某几层取出/拷贝（连带参数），然后用目标数据去训练没有保留（拷贝出来）的层。

那么，究竟哪些层应该被转移（拷贝）呢？

针对不同的学习任务，往往是需要不一样的层：

- 在语音识别上，一般拷贝最后几层。（直觉：不同的人由于口腔结构差异，同样的发音方式得到的声音可能不同。而模型的前几层做的事是讲话者的发音方式，后面的几层再根据发音方式就可以获得被辨识的文字，即是跟具体讲话者没有太大关系的，所以做迁移时，只保留后面几层即可，而前面几层就利用新的特定讲话者的目标数据来做训练）
- 在图像识别上，一般拷贝前几层。（直觉：网络的前几层一般学到的是最简单的模式（比如直线，横线或最简单的几何模型），比较通用，而后几层学习到的模式已经很抽象了，很难迁移到其它的领域）



《How transferable are features in deep neural networks》论文的相关实验结果：做图像识别，拷贝不同的层，并对迁移模型做不同的操作，观察效果。

![mark](http://images.iterate.site/blog/image/20190901/mLa0D5ducsek.png?imageslim)

另外一个实验结果：将源数据与目标数据的域划分开，比如源数据都是自然界的东西，而目标数据都是人造的东西，那么这对迁移模型的效果会有怎样的影响。
![mark](http://images.iterate.site/blog/image/20190901/LLugsjocygPC.png?imageslim)


可以从上图看出，这对迁移模型的前几层的影响是不大的，说明迁移模型不管是识别自然界事物还是人造事物，其在前几层上面做的事情其实是很相似的。


# 相关

- [迁移学习（Transfer Learning）](https://blog.csdn.net/qq_32690999/article/details/78849565)
