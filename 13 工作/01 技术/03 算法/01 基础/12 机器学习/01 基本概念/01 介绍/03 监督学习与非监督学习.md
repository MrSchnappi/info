## 监督学习与非监督学习


## 对比

- 非监督学习一般用来学习数据集上有用的结构性质。
  - 在深度学习中，我们通常要学习生成数据集的整个概率分布，显式地，比如密度估计，或是隐式地，比如合成或去噪。还有一些其他类型的非监督学习任务，例如聚类，将数据集分成相似样本的集合。
- 监督学习一般学习如何将样本划分为不同标签。


传统上，人们将回归、分类或者结构化输出问题称为监督学习，将支持其他任务的密度估计称为非监督学习。<span style="color:red;">嗯。</span>


大致说来：

- 非监督学习涉及观察随机向量 $\mathbf{x}$ 的好几个样本，试图显式或隐式地学习出概率分布 $p(\mathbf{x})$ ，或者是该分布一些有意思的性质
- 而监督学习包含观察随机向量 $\mathbf{x}$ 及其相关联的值或向量 $\mathbf{y}$ ，然后从 $\mathbf{x}$ 预测 $\mathbf{y}$，通常是估计 $p(\mathbf{y} | \mathbf{x})$。<span style="color:red;">嗯，是的。</span>

## 界限是模糊的

例如，概率的链式法则表明对于随机向量 $\mathbf{x} \in \mathbb{R}^{n}$ ，联合分布可以分解成：

$$
p(\mathbf{x})=\prod_{i=1}^{n} p\left(\mathrm{x}_{i} | \mathrm{x}_{1}, \ldots, \mathrm{x}_{i-1}\right)
$$

该分解意味着我们可以将其拆分成 $n$ 个监督学习问题，来解决表面上的非监督学习 $p(\boldsymbol{x})$ 。<span style="color:red;">哈哈，哎呦，真的哎！</span>

另外，我们求解监督学习问题 $p(y | \mathbf{x})$ 时，也可以使用传统的非监督学习策略学习联合分布 $p(\mathbf{x}, y)$ ，然后推断：

$$
p(y | \mathbf{x})=\frac{p(\mathbf{x}, y)}{\sum_{y^{\prime}} p\left(\mathbf{x}, y^{\prime}\right)}
$$

<span style="color:red;">上面这个式子没理解。</span>




## 监督学习算法

比如对于分类问题：


- 感知机
  - 感知器算法按照最大化分类间隔思想就是 SVM
  - 另外一个分支是 Logistic 回归，把线性预测器改装了以下，用 logistic 函数影射了以下，得到一个 0~1 之间的概率值。
    - 把 Logistic 推广到多分类问题的场景，就是著名的 Softmax 回归，这个在深度学习里面是经常用到的，很多时候，我们的深度神经网络最后一层接的就是 Softmax 。
  - 从感知器诞生的另外一类分支就是人工神经网络。
- KNN 是一个大家族，里面要用到距离。
  - 引出了距离度量学习算法。
- 贝叶斯家族，
  - 第一种是朴素贝叶斯，
  - 第二是正态贝叶斯。
- 线性投影技术 LDA 。是一种有监督学习的投影，他的思想很简单，就是最大化类间差异，最小化类差异，向这个方向做投影，得到一个投影方向。
  - 它有非线性版本，加上核函数之后得到一个 KLDA 这个不做重点介绍。



## 非监督学习算法

非监督学习主要学习的是：

- 学习从分布中采样
- 学习从分布中去噪
- 寻找数据分布的流形
- 将数据中相关的样本聚类。

也就是找到数据的“最佳”表示。

非监督学习主要包含两大类学习方法：

- 数据聚类。往往是通过多次迭代来找到数据的最优分割
- 特征变量关联。特征变量关联则是利用各种相关性分析方法来找到变量之间的关系。如 数据挖掘里面的关联规则学习，有 Apriori 等。

还有就是降维：

- 最著名的就是 PCA 主成分分析，一种线性的降维技术。
  - 改进版本叫做 KPCA 核主成分分析，这种算法在很多地方会大规模使用的，这是
- 非线性的降维技术呢？就是流行学习，4 种有代表性的算法
  - 分别是 LLE 局部线性嵌入
  - 拉普拉斯特征映射
  - 等距映射
  - 局部保持投影。

还有就是聚类：

- 聚类
  - 层次聚类、
  - k-means 
  - 还有三种基于密度的聚类：DBSCAN、OPTICS、Mean shift  
  - 然后会介绍谱聚类，它是一种基于图论的算法，
  - 最后会介绍 EM 算法，是一种基于概率的算法。

到这里位置，最常用的算法应该是已经介绍完了，还可以补充进来的就是 

- 概率图模型，典型代表就是
  - 贝叶斯网络、
  - 隐马尔科夫模型 HMM
  - 条件线性随机场 等等



## 其他变种

例如：

- 半监督学习中，一些样本有监督目标，但其他样本没有。主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。
- 多实例学习中，样本的整个集合被标记为含有或者不含有该类的样本，但是集合中单独的样本是没有标记的。<span style="color:red;">这个怎么使用？</span>
- 强化学习 (reinforcement learning) 算法会和环境进行交互，所以学习系统和它的训练过程会有反馈回路。并不是训练于一个固定的数据集上。

