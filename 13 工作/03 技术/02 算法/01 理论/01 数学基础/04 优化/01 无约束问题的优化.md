
# 无约束优化问题的优化

假设有一道无约束优化问题摆在你面前：

$$
\arg \min_{\theta} L(\theta)
$$

其中目标函数 $L(\cdot)$ 是光滑的。

请问：

- 求解该问题的优化算法有哪些？
- 它们的适用场景是什么？

## 经典的优化算法

经典的优化算法可以分为两大类：

- 直接法
- 迭代法



# 直接法

直接法就是能够直接给出优化问题最优解的方法。

缺点：

- 听起来厉害，但它并不是万能的。


直接法要求目标函数满足两个条件：

1. 条件一：$L(\cdot)$ 是凸函数。若 $L(\cdot)$ 是凸函数，那么 $\theta$ 是最优解的充分必要条件是 $L(\cdot)$ 在 $\theta$ 处的梯度为 $0$，即 $\nabla L\left(\theta^{*}\right)=0$
2. 因此，为了能够直接求解出 $\theta^{\star}$ ，第二个条件是，上式有闭式解。

举例：


岭回归同时满足这两个条件。

- 其目标函数为：<span style="color:red;">补充</span>

$$
L(\theta)=\|X \theta-y\|_{2}^{2}+\lambda\|\theta\|_{2}^{2}
$$

- 最优解为（试着自己推导）：

$$
\theta^{*}=\left(X^{\mathrm{T}} X+\lambda I\right)^{-1} X^{\mathrm{T}}
$$


应用：

- 直接法要满足的这两个条件限制了它的应用范围。因此，在很多实际问题中，会采用迭代法。






# 迭代法（梯度下降法和牛顿法）

迭代法就是迭代地修正对最优解的估计。

说明：

- 对于无约束问题：$\arg \min_{\theta} L(\theta)$，假设当前对最优解的估计值为 $\theta_{t}$
- 我们希望求解下面的优化问题来得到更好的估计值 $\theta_{t+1}=\theta_{t}+\delta_{t}$。<span style="color:red;">没有很清楚。</span>


$$
\delta_{t}=\underset{\delta}{\arg \min } L\left(\theta_{t}+\delta\right)
$$

划分：

- 迭代法又可以分为一阶法和二阶法两类。

一阶法：（**梯度下降法**）

- 对 $L\left(\theta_{t}+\delta\right)$ 做一阶泰勒展开，得到近似式

$$
L\left(\theta_{t}+\delta\right) \approx L\left(\theta_{t}\right)+\nabla L\left(\theta_{t}\right)^{\mathrm{T}} \delta
$$

- 由于该近似式仅在 $\delta$ 较小时才比较准确，因此在求解 $\delta_t$ 时一般加上 L2 正则项：<span style="color:red;">$-\alpha \nabla L\left(\theta_{t}\right)$ 是怎么推导出来的？</span>

$$
\begin{aligned}
\delta_{t}&=\underset{\delta}{\arg \min }\left(L\left(\theta_{t}\right)+\nabla L\left(\theta_{t}\right)^{\mathrm{T}} \delta+\frac{1}{2 \alpha}\|\delta\|_{2}^{2}\right)\\&=-\alpha \nabla L\left(\theta_{t}\right)\end{aligned}
$$


- 由此，一阶法的迭代公式表示为如下，其中 $\alpha$ 称为学习率。

$$
\theta_{t+1}=\theta_{t}-\alpha \nabla L\left(\theta_{t}\right)
$$

梯度就是目标函数的一阶信息。


二阶法：（**牛顿法**）

- 对 $L\left(\theta_{t}+\delta\right)$ 做二阶泰勒展开，得到近似式：


$$
L\left(\theta_{t}+\delta\right) \approx L\left(\theta_{t}\right)+\nabla L\left(\theta_{t}\right)^{\mathrm{T}} \delta+\frac{1}{2} \delta^{\mathrm{T}} \nabla^{2} L\left(\theta_{t}\right) \delta
$$

- 其中 $\nabla^{2} L\left(\theta_{t}\right)$ 是函数 $L$ 在 $\theta_{t}$ 处的 Hessian 矩阵。
- 通过求解近似优化问题：<span style="color:red;">$-\nabla^{2} L\left(\theta_{t}\right)^{-1} \nabla L\left(\theta_{t}\right)$ 是怎么得到的？</span>


$$
\begin{aligned}
\delta_{t}&=\underset{\delta}{\arg \min }\left(L\left(\theta_{t}\right)+\nabla L\left(\theta_{t}\right)^{\mathrm{T}} \delta+\frac{1}{2} \delta^{\mathrm{T}} \nabla^{2} L\left(\theta_{t}\right) \delta\right)\\&=-\nabla^{2} L\left(\theta_{t}\right)^{-1} \nabla L\left(\theta_{t}\right)
\end{aligned}
$$


- 可以得到二阶法的迭代公式

$$
\theta_{t+1}=\theta_{t}-\nabla^{2} L\left(\theta_{t}\right)^{-1} \nabla L\left(\theta_{t}\right)
$$


Hessian 矩阵就是目标函数的二阶信息。


优缺点：

- 二阶法的收敛速度一般要远快于一阶法，但是在高维情况下，Hessian 矩阵求逆的计算复杂度很大，而且当目标函数非凸时，二阶法有可能会收敛到鞍点（Saddle Point）。


