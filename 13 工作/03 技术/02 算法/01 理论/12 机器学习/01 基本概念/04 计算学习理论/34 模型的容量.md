


# 模型的容量

模型的容量：

- 指其拟合各种函数的能力。

不同容量的模型：

- 容量低的模型可能很难拟合训练集。
- 容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质。

如何控制模型容量：

- 选择假设空间(hypothesis space)，即学习算法可以选择为解决方案的函数集。
  - 例如，线性回归算法将关于其输入的所有线性函数作为假设空间。广义线性回归的假设空间包括多项式函数，而非仅有线性函数。这样做就增加了模型的容量。

到目前为止，我们探讨了通过改变输入特征的数目和加入这些特征对应的参数，改变模型的容量。

事实上，还有很多方法可以改变模型的容量。容量不仅取决于模型的选择。模型规定了调整参数降低训练目标时，学习算法可以从哪些函数族中选择函数。这被称为模型的表示容量(representational capacity)。在很多情况下，从这些函数中挑选出最优函数是非常困难的优化问题。实际中，学习算法不会真的找到最优函数，而仅是找到一个可以大大降低训练误差的函数。额外的限制因素，比如优化算法的不完美，意味着学习算法的有效容量(effective capacity)可能小于模型族的表示容量。<span style="color:red;">嗯嗯，模型的表示容量，这段看的有点模糊。再看下。</span>

提高机器学习模型泛化的现代思想可以追溯到早在托勒密时期的哲学家的思想。许多早期的学者提出一个简约原则，现在广泛被称为奥卡姆剃刀 (Occam’s razor)(c.1287-1387)。该原则指出，在同样能够解释已知观测现象的假设中，我们应该挑选“最简单”的那一个。这个想法是在 20世纪，由统计学习理论创始人形式化并精确化的(Vapnik and Chervonenkis,1971;Vapnik,1982;Blumer et al.,1989;Vapnik,1995)。<span style="color:red;">为什么奥卡姆剃刀原则是正确的？它是实际意义上的正确？还是统计意义上的正确？还是真实意义上的正确？</span>


统计学习理论提供了量化模型容量的不同方法。在这些方法中，最有名的是 Vapnik-Chervonenkis 维度 (Vapnik-Chervonenkis dimension,VC)，简称 VC 维。VC 维度量二元分类器的容量。VC 维定义为该分类器能够分类的训练样本的最大数目。<span style="color:red;">这句没有很明白，VC 维定义为该分类器能够分类的训练样本的最大数目。</span>假设存在 $m$ 个不同 $\boldsymbol{x}$ 点的训练集，分类器可以任意地标记该 $m$ 个不同的 $\boldsymbol{x}$ 点，VC 维被定义为 $m$ 的最大可能值。<span style="color:red;">什么意思？VC 维被定义为 $m$ 的最大可能值。</span>

量化模型的容量使得统计学习理论可以进行量化预测。统计学习理论中最重要的结论阐述了训练误差和泛化误差之间差异的上界随着模型容量增长而增长，但随着训练样本增多而下降

这些边界为机器学习算法可以有效解决问题提供了理论验证，但是它们很少应用于实际中的深度学习算法。<span style="color:red;">嗯。为什么他们很少应用于实际的深度学习算法？</span><span style="color:blue;">哦，后面就有说。</span>一部分原因是边界太松，另一部分原因是很难确定深度学习算法的容量。由于有效容量受限于优化算法的能力，确定深度学习模型容量的问题特别困难。而且对于深度学习中的一般非凸优化问题，我们只有很少的理论分析。<span style="color:red;">哦，不过这个地方还想更多知道些。</span>


我们必须记住虽然更简单的函数更可能泛化(训练误差和测试误差的差距小)，但我们仍然需要选择一个充分复杂的假设以达到低的训练误差。<span style="color:red;">是的，还是要选择一个充分复杂的假设。</span>通常，当模型容量上升时，训练误差会下降，直到其渐近最小可能误差(假设误差度量有最小值)。通常，泛化误差是一个关于模型容量的 U 形曲线函数，如图 5.3 所示。


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190520/lejNIDu4GFSR.png?imageslim">
</p>


<span style="color:red;">嗯，这个图像非常的形象。</span>

图 5.3　容量和误差之间的典型关系。训练误差和测试误差表现得非常不同。在图的左端，训练误差和泛化误差都非常高，这是 欠拟合机制(underfitting regime)。当我们增加容量时，训练误差减小，但是训练误差和泛化误差之间的间距却不断扩大。最终，这个间距的大小超过了训练误差的下降，我们进入到了过拟合机制(overfitting regime)，其中容量过大，超过了最优容量(optimal capacity)。<span style="color:red;">容量过大，超过了最优容量。</span>


为考虑容量任意高的极端情况，我们介绍非参数(non-parametric)模型的概念。<span style="color:red;">什么是容量任意高的情况？非参数模型，之前好像么有见到过。</span>至此，我们只探讨过参数模型，例如线性回归。参数模型学习的函数在观测到新数据前，参数向量的分量个数是有限且固定的。非参数模型没有这些限制。<span style="color:red;">参数模型学习的函数在观测到新数据之前，参数向量的分量个数是有限且固定的。非参数模型没有这些限制。是这样吗？</span>

有时，非参数模型仅是一些不能实际实现的理论抽象(比如搜索所有可能概率分布的算法)。然而，我们也可以设计一些实用的非参数模型，使它们的复杂度和训练集大小有关。这种算法的一个示例是最近邻回归(nearest neighbor regression)。<span style="color:red;">哇塞！还有这样的吗？怎么使它们的复杂度和训练集大小有关？最近邻回归？之前好像没有听说过这个。</span>不像线性回归有固定长度的向量作为权重，最近邻回归模型存储了训练集中所有的 $\boldsymbol{X}$ 和 $\boldsymbol{y}$。当需要为测试点 $\boldsymbol{x}$ 分类时，模型会查询训练集中离该点最近的点，并返回相关的回归目标。<span style="color:red;">。。这个不是聚类吗？</span>换言之，$\hat{y}=y_{i}$，其中 $i=\arg \min \left\|\boldsymbol{X}_{i, :}-\boldsymbol{x}\right\|_{2}^{2}$ 。该算法也可以扩展成 $L^{2}$ 范数以外的距离度量，例如学成距离度量(Goldberger et al.,2005)。<span style="color:red;">什么是学成距离度量？</span>在最近向量不唯一的情况下，如果允许算法对所有离 $\boldsymbol{x}$ 最近的 $\boldsymbol{X}_{i,:}$ 关联的 $y_{i}$ 求平均，那么该算法会在任意回归数据集上达到最小可能的训练误差(如果存在两个相同的输入对应不同的输出，那么训练误差可能会大于零)。<span style="color:red;">这个是什么意思？这一段没有怎么明白。</span>



最后，我们也可以将参数学习算法嵌入另一个增加参数数目的算法来创建非参数学习算法。例如，我们可以想象这样一个算法，外层循环调整多项式的次数，内层循环通过线性回归学习模型。<span style="color:red;">这样也可以？！要怎么实现。想知道。</span>


理想模型假设我们能够预先知道生成数据的真实概率分布。然而这样的模型仍然会在很多问题上发生一些错误，因为分布中仍然会有一些噪声。在监督学习中，从 $\boldsymbol{x}$ 到 $y$ 的映射可能内在是随机的，或者 $y$ 可能是其他变量(包括 $\boldsymbol{x}$ 在内)的确定性函数。从预先知道的真实分布 $p(\boldsymbol{x}, y)$ 预测而出现的误差被称为贝叶斯误差(Bayes error)。<span style="color:red;">嗯嗯，这个词之前好像没有见到过。</span>

训练误差和泛化误差会随训练集的大小发生变化。泛化误差的期望从不会因训练样本数目的增加而增加。对于非参数模型而言，更多的数据会得到更好的泛化能力，直到达到最佳可能的泛化误差。任何模型容量小于最优容量的固定参数模型会渐近到大于贝叶斯误差的误差值，如图 5.4所示。<span style="color:red;">嗯，这一段说的很好，很总结。</span>

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190522/aUxVmUivhSrj.png?imageslim">
</p>

> 图 5.4　训练集大小对训练误差、测试误差以及最优容量的影响。通过给一个 5 阶多项式添加适当大小的噪声，我们构造了一个合成的回归问题，生成单个测试集，然后生成一些不同尺寸的训练集。为了描述 95% 置信区间的误差条，对于每一个尺寸，我们生成了 40 个不同的训练集。
>
> - (上)两个不同的模型上训练集和测试集的 MSE，一个二次模型，另一个模型的阶数通过最小化测试误差来选择。两个模型都是用闭式解来拟合。对于二次模型来说，当训练集增加时，训练误差也随之增大。这是由于越大的数据集越难以拟合。同时，测试误差随之减小，因为关于训练数据的不正确的假设越来越少。二次模型的容量并不足以解决这个问题，所以它的测试误差趋近于一个较高的值。最优容量点处的测试误差趋近于贝叶斯误差。训练误差可以低于贝叶斯误差，因为训练算法有能力记住训练集中特定的样本。当训练集趋向于无穷大时，任何固定容量的模型(在这里指的是二次模型)的训练误差都至少增至贝叶斯误差。
> - (下)当训练集大小增大时，最优容量(在这里是用最优多项式回归器的阶数衡量的)也会随之增大。最优容量在达到足够捕捉模型复杂度之后就不再增长了。<span style="color:red;">嗯，看明白了，不过再看下，讲的挺好的这一段。</span>



值得注意的是，具有最优容量的模型仍然有可能在训练误差和泛化误差之间存在很大的差距。在这种情况下，我们可以通过收集更多的训练样本来缩小差距。<span style="color:red;">嗯嗯，是的。</span>


<span style="color:red;">感觉上面这几段再看下。</span>







# 相关

- 《深度学习》花书
