# 备注

- <span style="color:red;">没有很清楚。为什么要有这些？</span>



# 统计假设检验

性能的比较有点麻烦：


- 想比较的是泛化性能，但是实际只能得到测试集的性能，结果可能未必相同；
- 测试集的性能与测试集本身的选择有很大关系，使用不同大小的测试集会得到不同的结果，即便用相同大小的测试集若包含的测试样例不同，测试结果 也会有不同；
- 很多机器学习算法本身有一定的随机性，即便用相同的参数设置在同一个测试集上多次运行，其结果也会有不同。

那么，是否可以将测试集上的比较结果作为依据呢？


- 统计假设检验 (hypothesis test) 为我们进行学习器性能比较提供了重要依据。


下面我们先介绍两种最基本的假设检验，然后介绍几种常用的机器学习性能比较方法。为了便于讨论，这里默认以错误率为性能度量，用 $\epsilon$ 来表示。




# 对单个学习器泛化性能的假设进行检验

## 假设检验

假设检验中的 “假设” 是对学习器泛化错误率分布的某种判断或猜想，例如 “ $\epsilon=\epsilon_0$ ”。现实任务中我们并不知道学习器的泛化错误率，只能获知其测试错误率 $\cap{\epsilon}$  ，泛化错误率与测试错误率未必相同，但直观上，二者接近的可能性应比较大，相差很远的可能性比较小。因此，我们可以根据测试错误率估推出泛化错误率的分布.

泛化错误率为 $\epsilon$ 的学习器在一个样本上犯错的概率是 $\epsilon$ ；测试错误率 $\cap{\epsilon}$  意味着在 m 个测试样本中恰有 $\cap{\epsilon}* m$ 个被误分类。假定测试样本是从样本总体分布中独立采样而得，那么泛化错误率为 $\epsilon$ 的学习器将其中 m' 个样本误分类、其 余样本全都分类正确的概率是 $\epsilon^{m'}(1-\epsilon)^{m-m'}$ ；由此可估算出其恰将 $\cap{\epsilon}* m$ 个样本误分类的概率如下式所示，这也表达了在包含 m 个样本的测试集上，泛化错误率为 $\epsilon$ 的学习器被测得测试错误率为 $\cap{\epsilon}$  的概率：

$$
P(\hat{\epsilon} ; \epsilon)=\left(\begin{array}{c}{m} \\ {\hat{\epsilon} \times m}\end{array}\right) \epsilon^{\hat{\epsilon} \times m}(1-\epsilon)^{m-\hat{\epsilon} \times m}
$$
给定测试错误率，则解 $\partial P(\cap{\epsilon};\epsilon)/\partial \epsilon=0$ 可知，$P(\cap{\epsilon};\epsilon)$ 在 $\epsilon=\cap{\epsilon}$ 时最大，$|\epsilon-\cap{\epsilon}|$ 增大时 $P(\cap{\epsilon};\epsilon)$ 减小。这符合二项(binomial)分布，如下图所示，若 $\epsilon =0.3$ ，则 10 个样本中测得 3 个被误分类的概率最大。


二项分布示意图（m=10,$\epsilon$ =0.3）：（$\alpha$ 的常用取值有 0.05、0.1 ，图中 $\alpha$ 较大是为了绘图方便）

<center>

![](http://images.iterate.site/blog/image/180727/aaliELa3ae.png?imageslim){ width=55% }


</center>


我们可使用 “二项检验” (binomial test) 来对 $\epsilon \leq 0.3$  （即“泛化错误率是否不大于 0.3” ) 这样的假设进行检验。更一般的，考虑假设 $\epsilon \leq \epsilon_0$ ，则在 $1-\alpha$ 的概率内所能观测到的最大错误率如下式计算。这里 $1-\alpha$ 反映了结论的 “置信度” (confidence)，直观地来看，相应于上图中非阴影部分的范围：



$$
\overline{\epsilon}=\max \epsilon \quad \text { s.t. } \quad \sum_{i=\epsilon_{0} \times m+1}^{m}\left(\begin{array}{c}{m} \\ {i}\end{array}\right) \epsilon^{i}(1-\epsilon)^{m-i}<\alpha
$$


注：s.t. 是 “subject to” 的简写，使左边式子在右边条件满足时成立。



此时若测试错误率 $\cap{\epsilon}$  小于临界值 $\overline{\epsilon}$  ，则根据二项检验可得出结论：在 $\alpha$ 的显著度 下，假设 $\epsilon \leq \epsilon_0$ 不能被拒绝，即能以 $1-\alpha$ 的置信度认为，学习器的泛化错误率不大于 $\epsilon_0$；否则该假设可被拒绝，即在 $\alpha$ 的显著度下可认为学习器的泛化错误率大于 $\epsilon_0$。



在很多时候我们并非仅做一次留出法估计，而是通过多次重复留出法或是交叉验证法等进行多次训练/测试，这样会得到多个测试错误率，此时可使用 “t检验”（t-test）。假定我们得到了 k 个测试错误率，$\cap{\epsilon_1},\cap{\epsilon_2},\cdots ,\cap{\epsilon_k}$，则平均测试错误率 $\mu$ 和方差 $\sigma^2$ 为：

$$
\mu=\frac{1}{k} \sum_{i=1}^{k} \hat{\epsilon}_{i}
$$
$$
\sigma^{2}=\frac{1}{k-1} \sum_{i=1}^{k}\left(\hat{\epsilon}_{i}-\mu\right)^{2}
$$


考虑到这 k 个测试错误率可看作泛化错误率 $\epsilon_0$ 的独立采样，则变量：

$$
\tau_{t}=\frac{\sqrt{k}\left(\mu-\epsilon_{0}\right)}{\sigma}
$$

服从自由度为 t-1 的 t 分布，如下图所示：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/f5Ac9fIA53.png?imageslim">
</p>

对假设 $\mu=\epsilon_0$ 和显著度 $\alpha$ ，我们可计算出当测试错误率均值为 $\epsilon_0$ 时，在  $1-\alpha$ 概率内能观测到的最大错误率，即临界值。这里考虑双边（two-tailed）假设，如上图所示，两边阴影部分各有 $\alpha/2$ 的面积；假定阴影部分范围分别为 $[-\infty,t_{-\alpha/2}]$ 和 $[t_{\alpha/2},\infty]$ 。若平均错误率 $mu$ 与 $\epsilon_0$ 之差 $|\mu-\epsilon_0|$ 位于临界值范围 $[t_{-\alpha/2},t_{\alpha/2}]$ 内，则不能拒绝假设 “ $\mu=\epsilon_0$ ”，即可认为泛化错误率为 $\epsilon_0$ ，置信度为 $1-\alpha$ ，否则可拒绝该假设，即在该显著度下可认为泛化错误率与 $\epsilon_0$ 有显著不同。$\alpha$ 常用取值有 0.05 和 0.1 。下表给出了一些常用临界值：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180727/iHfl8Hcela.png?imageslim">
</p>


上面介绍的两种方法都是对关于单个学习器泛化性能的假设进行检验，而在现实任务中，更多时候我们需对不同学习器的性能进行比较，下面将介绍适用于此类情况的假设检验方法.




# 对不同学习器的性能进行比较

## 交叉验证 t 检验


对两个学习器 A 和 B ，若我们使用 k 折交叉验证法得到的测试错误率分 别为 $\epsilon_{1}^{A}, \epsilon_{2}^{A}, \ldots, \epsilon_{k}^{A}$ 和 $\epsilon_{1}^{B}, \epsilon_{2}^{B}, \ldots, \epsilon_{k}^{B}$ ，其中 $\epsilon_{i}^{A}$ 和 $\epsilon_{i}^{B}$ 是在相同的第 $i$ 折训练/测 试集上得到的结果，则可用 $k$ 折交叉验证“成对 t 检验” (paired t-teste)来进行 比较检验。这里的基本思想是若两个学习器的性能相同，则它们使用相同的训 练/测试集得到的测试错误率应相同，即 $\epsilon_{i}^{A}=\epsilon_{i}^{B}$ .

具体来说，对 $k$ 折交叉验证产生的 $k$ 对测试错误率：先对每对结果求差 $\Delta_{i}=\epsilon_{i}^{A}-\epsilon_{i}^{B}$ ， 若两个学习器性能相同，则差值均值应为零。因此，可根据差值 $\Delta_{1}, \Delta_{2}, \dots, \Delta_{k}$ , 来对“学习器 A 与 B 性能相同”这个假设做 t 检验，计算出差值的均值 $\mu$ 和方差 $\sigma^{2}$ ，在显著度 $\alpha$ 下，若变量

$$
\tau_{t}=\left|\frac{\sqrt{k} \mu}{\sigma}\right|
$$

小于临界值 $t_{\alpha / 2, k-1}$，则假设不能被拒绝，即认为两个学习器的性能没有显著差 别；否则可认为两个学习器的性能有显著差别，且平均错误率较小的那个学习 器性能较优。这里 $t_{\alpha / 2, k-1}$ 是自由度为 $k-1$ 的 $t$ 分布上尾部累积分布为 $\alpha / 2$ 的临界值.<span style="color:red;">最后这句没明白。</span>

欲进行有效的假设检验，一个重要前提是测试错误率均为泛化错误率的独 立采样。然而，通常情况下由于样本有限，在使用交叉验证等实验估计方法时， 不同轮次的训练集会有一定程度的重叠，这就使禧测试错误率实际上并不独立， 会导致过高估计假设成立的概率。为缓解这一问题，可采用“ $5 \times 2$ 交叉验证”法[Dietterich，1998].

5x2交叉验证是做 5 次 2 折交叉验证，在每次 2 折交叉验证之前随机将数 据打乱，使得 5 次交叉验证中的数据划分不重复。对两个学习器 A 和 B，第 i 次 2折交叉验证将产生两对测试错误率，我们对它们分别求差，得到第 1 折上的差 值 $\Delta_{i}^{1}$ 和第 2 折上的差值 $\Delta_{i}^{2}$ 。为缓解测试错误率的非独立性，我们仅计算第 1 次 2 折交叉验证的两个结果的平均值 $\mu=0.5\left(\Delta_{1}^{1}+\Delta_{1}^{2}\right)$ ，但对每次 2 折实验的 结果都计算出其方差 $\sigma_{i}^{2}=\left(\Delta_{i}^{1}-\frac{\Delta_{i}^{1}+\Delta_{i}^{2}}{2}\right)^{2}+\left(\Delta_{i}^{2}-\frac{\Delta_{i}^{1}+\Delta_{i}^{2}}{2}\right)^{2}$ 。 变量

$$
\tau_{t}=\frac{\mu}{\sqrt{0.2 \sum_{i=1}^{5} \sigma_{i}^{2}}}\tag{2.32}
$$

服从自由度为 5 的 t 分布，其双边检验的临界值 $t_{\alpha / 2,5}$ 当 $\alpha=0.05$ 时为 2.5706, $\alpha=0.1$ 时为 2.0150.

## McNemar 检验

对二分类问题，使用留出法不仅可估计出学习器 A 和 B 的测试错误率，还可获得两学习器分类结果的差别，即两者都正确、都错误、一个正确另一个错 误的样本数，如“列联表” (contingency table) 2.4所示.

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190705/SrR0yztU5B8u.png?imageslim">
</p>



若我们做的假设是两学习器性能相同，则应有 $e_{01}=e_{10}$ ，那么变量 $\left|e_{01}-e_{10}\right|$ 应当服从正态分布，且均值为 1，方差为 $e_{01}+e_{10}$ 。因此变量

$$
\tau_{\chi^{2}}=\frac{\left(\left|e_{01}-e_{10}\right|-1\right)^{2}}{e_{01}+e_{10}}\tag{2.33}
$$

> 中文称为卡方分布.
> 临界值 $\chi_{\alpha}^{2}$ 在 R 语 言中可通过 `qchisq(l — a, fc—1)` 计算，在 Matlab 中 是 `icdf ('Chisquare',1 — a,k — 1)`。这里的 k = 2 是进行比较的算法个数.

服从自由度为 $1$ 的 $\chi^{2}$ 分布，即标准正态分布变量的平方。给定显著度 $\alpha$，当以 上变量值小于临界值 $\chi_{\alpha}^{2}$ 时，不能拒绝假设，即认为两学习器的性能没有显著差 别；否则拒绝假设，即认为两者性能有显著差别，且平均错误率较小的那个学习 器性能较优。自由度为 1 的 $\chi^{2}$ 检验的临界值当 $\alpha=0.05$ 时为 3.8415, $\alpha=0.1$ 时为 2.7055.

## Friedman 检验 与 Nemenyi 后续检验

交叉验证 t 检验和 McNemar 检验都是在一个数据集上比较两个算法的 性能，而在很多时候，我们会在一组数据集上对多个算法进行比较。当有多个 算法参与比较时，一种做法是在每个数据集上分别列出两两比较的结果，而在 两两比较时可使用前述方法；另一种方法更为直接，即使用基于算法排序的 Friedman 检验.

假定我们用 $D_{1}, D_{2}, D_{3},D_{4}$ 四个数据集对算法 A、B、C 进行比较. 首先，使用留出法或交叉验证法得到每个算法在每个数据集上的测试结果，然后在每个数据集上根据测试性能由好到坏排序，并赋予序值 $1,2, \dots$ ，若算法的 测试性能相同，则平分序值。例如，在 $D_{1}$ 和 $D_{3}$ 上，A最好、B其次、C最差， 而在 $D_{2}$ 上，A最好、B与 C 性能相同，……，则可列出表 2.5，其中最后一行通过对每一列的序值求平均，得到平均序值.

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190705/1SRymxfb6uky.png?imageslim">
</p>


然后，使用 Friedman 检验来判断这些算法是否性能都相同。若相同，则它 们的平均序值应当相同。假定我们在 $N$ 个数据集上比较 $k$ 个算法，令 $r_{i}$ 表示第 $i$ 个算法的平均序值，为简化讨论，暂不考虑平分序值的情况，则 $\boldsymbol{r}_{i}$ 服从正态分 布，其均值和方差分别为 $(k+1) / 2$ 和 $\left(k^{2}-1\right) / 12$.<span style="color:red;">为什么是服从这个正态分布？</span>

变量

$$
\begin{aligned}
\tau_{\chi^{2}}&=\frac{k-1}{k} \cdot \frac{12 N}{k^{2}-1} \sum_{i=1}^{k}\left(r_{i}-\frac{k+1}{2}\right)^{2}\\&=\frac{12 N}{k(k+1)}\left(\sum_{i=1}^{k} r_{i}^{2}-\frac{k(k+1)^{2}}{4}\right)
\end{aligned}\tag{2.34}
$$


在 $k$ 和 $N$ 都较大时，服从自由度为 $k-1$ 的 $\chi^{2}$ 分布.

然而，上述这样的“原始 Friedman 检验”过于保守，现在通常使用变量

$$
\tau_{F}=\frac{(N-1) \tau_{\chi^{2}}}{N(k-1)-\tau_{\chi^{2}}}\tag{2.35}
$$

其中 $\tau_{\chi^{2}}$ 由式(2.34)得到. $\tau_{F}$ 服从自由度为 $k-1$ 和 $(k-1)(N-1)$ 的 $F$ 分布。表 2.6给出了一些常用临界值.

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190705/GrIAvLp1RUAe.png?imageslim">
</p>


若 “所有算法的性能相同” 这个假设被拒绝，则说明算法的性能显著不 同。这时需进行“后续检验”(post-hoc test)来进一步区分各算法。常用的有 Nemenyi后续检验.

Nemenyi检验计算出平均序值差别的临界值域

$$
C D=q_{\alpha} \sqrt{\frac{k(k+1)}{6 N}}\tag{2.36}
$$

> $q_{\alpha}$ 是 Tukey 分布的临 界值。

表 2.7给出了 $\alpha=0.05$ 和 0.1时常用的 $q_{\alpha}$ 值。若两个算法的平均序值之差超出了临界值域 CD，则以相应的置信度拒绝“两个算法性能相同”这一假设.

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190705/EwNU8ac9sgbM.png?imageslim">
</p>


以表 2.5中的数据为例，先根据式(2.34)和(2.35)计算出 $\tau_{F}=24.429$ ，由表 2.6可知，它大于 $\alpha=0.05$ 时的 $F$ 检验临界值 5.143，因此拒绝“所有算法性 能相同”这个假设.

然后使用 Nemenyi 后续检验，在表 2.7中找到 $k=3$ 时 $q_{0.05}=2.344$ ，根据式(2.36)计算出临界值域 $C D=1.657$ ，由表 2.5中的平均序值可知，算法 A 与 B 的差距，以及算法 B 与 C 的差距均未超过临界值域，而算 法 A 与 C 的差距超过临界值域，因此检验结果认为算法 A 与 C 的性能显著不同，而算法 A 与 B、以及算法 B 与 C 的性能没有显著差别.

上述检验比较可以直观地用 Friedman 检验图显示。例如根据表 2.5的序 值结果可绘制出图 2.8，图中纵轴显示各个算法，横轴是平均序值。对每个算法， 用一个圆点显示其平均序值，以圆点为中心的横线段表示临界值域的大小。然 后就可从图中观察，若两个算法的横线段有交叠，则说明这两个算法没有显著 差别，否则即说明有显著差别。从图 2.8中可容易地看出，算法 A 与 B 没有显著 差别，因为它们的横线段有交叠区域，而算法 A 显著优于算法 C，因为它们的 横线段没有交叠区域.

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20190705/XtR00i9MWcsu.png?imageslim">
</p>


