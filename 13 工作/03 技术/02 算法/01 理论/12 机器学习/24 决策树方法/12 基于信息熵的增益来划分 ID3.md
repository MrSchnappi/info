---
title: 12 基于信息熵的增益来划分 ID3
toc: true
date: 2019-08-27
---

### 信息增益

#### 没有划分的时候，样本集的信息熵

说道信息增益，首先，我们要知道什么是信熵。

"信息熵" (information entropy) 是度量样本集合纯度最常用的一种指标。假定当前样本集合 $D$  中第 $k$  类样本所占的比例为  $p_{k}(k=1,2, \ldots,|\mathcal{Y}|)$ ，则 $D$ 的信息熵定义为


$$
\operatorname{Ent}(D)=-\sum_{k=1}^{ | \mathcal{Y |}} p_{k} \log _{2} p_{k}\tag{4.1}
$$

> $$Ent(D) =-\sum_{k=1}^{|y|}p_klog_{2}{p_k}$$
> [解析]：
>
> 熵是度量样本集合纯度最常用的一种指标，代表一个系统中蕴含多少信息量，信息量越大表明一个系统不确定性就越大，就存在越多的可能性。
>
> 假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k(k =1,2,...,|y|)$ ，则 $D$ 的信息熵为：
>
> $$
> Ent(D) =-\sum_{k=1}^{|y|}p_klog_{2}{p_k}
> $$
>
> 其中，当样本 $D$ 中 $|y|$ 类样本均匀分布时，这时信息熵最大，其值为
> $$
> Ent(D) =-\sum_{k=1}^{|y|}\frac{1}{|y|}log_{2}{\frac{1}{|y|}} = \sum_{k=1}^{|y|}\frac{1}{|y|}log_{2}{|y|} = log_{2}{|y|}
> $$
> 此时样本 D 的纯度越小；
>
> 相反，假设样本 D 中只有一类样本，此时信息熵最小，其值为
> $$
> Ent(D) =-\sum_{k=1}^{|y|}\frac{1}{|y|}log_{2}{\frac{1}{|y|}} = -1log_21-0log_20-...-0log_20 = 0
> $$
> 此时样本的纯度最大。



<span style="color:red;">嗯，这个式子是怎么来的呢？</span>

> $\operatorname{Ent}(D)$ 的最小值为 $0$ ，最大值为 $log_2|\mathcal{Y}|$。

$\operatorname{Ent}(D)$ 的值越小，则 $D$ 的纯度越高。比如，只有一个类别的时候，$\operatorname{Ent}(D)$ 就是 0。

注意：在计算信息熵的时候，我们约定：若 $p=0$，则 $p \log _{2} p=0$。

上面我们就介绍了一个样本集合的信息熵。OK，要注意，这是一个没有进行分类的集合的信息熵。

#### 按照某个属性进行划分后，样本集的信息熵。

现在，我们看一下这个样本集的某个属性的属性值。

我们假设，离散属性 $a$ 有 $V$ 个可能的取值 $\left\{a^{1}, a^{2}, \ldots, a^{V}\right\}$，如果我们使用属性 $a$ 来对整个样本集 $D$ 进行划分，就会产生 $V$ 个分支结点，其中第 $v$ 个分支结点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^{v}$ 的样本，记为 $D^v$ 。

OK，也就是说，经过对属性 $a$ 进行划分，我们得到了 $V$ 个分支，每个分支的样本数为 $D^v$ 。

我们把其中的一个分支 $v$ 拿出来看，它包含的样本数量为 $D^v$ ，我们但看这个样本集，它其实也可以计算出一个信息熵的 $Ent(D^v)$ 。

OK，那么我们就可以算出这个划分后的总的样本集的信息熵：

$$\sum_{v=1}^{V}\frac{|D^v|}{D}Ent(D^v)$$

其中，我们给每个分支样本集的信息熵赋予权重 $\left|D^{v}\right| /|D|$。

<span style="color:red;">嗯，不知道这个权重有什么理论依据吗？虽然看起来好像是 OK 的。</span>

#### 这种划分引起的信息增益

OK，我们得到了划分后的信息熵，那么与没有划分时候的信息熵进行比较久得到了对这个属性进行划分所引起的熵的变化：

$$
\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)\tag{4.2}
$$

> $$
> Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent({D^v})
> $$
> [解析]：假定在样本 D 中有某个**离散特征** $a$ 有 $V$ 个可能的取值 $(a^1,a^2,...,a^V)$，若使用特征 $a$ 来对样本集 $D$ 进行划分，则会产生 $V$ 个分支结点，其中第 $v$ 个分支结点包含了 $D$ 中所有在特征 $a$ 上取值为 $a^v$ 的样本，样本记为 $D^v$，由于根据离散特征 a 的每个值划分的 $V$ 个分支结点下的样本数量不一致，对于这 $V$ 个分支结点赋予权重 $\frac{|D^v|}{|D|}$，即样本数越多的分支结点的影响越大，特征 $a$ 对样本集 $D$ 进行划分所获得的“信息增益”为
> $$
> Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent({D^v})
> $$
> 信息增益越大，表示使用特征 a 来对样本集进行划分所获得的纯度提升越大。
>
> **缺点**：由于在计算信息增益中倾向于特征值越多的特征进行优先划分，这样假设某个特征值的离散值个数与样本集 $D$ 个数相同（假设为样本编号），虽然用样本编号对样本进行划分，样本纯度提升最高，但是并不具有泛化能力。


这个就是所谓的信息增益。

一般而言，信息增益越大，则意味着使用属性 $a$  来进行划分所获得的"纯度提升"越大。是的，因为信息增益越大，说明划分后的信息熵越小，即说明样本集越纯。最小为 0。

OK，因此，我们可用信息增益来进行决策树的划分属性选择，即，当我们想要对这个样本集进行划分时，我们选择什么属性进行划分呢？我们选择能够使信息增益最大的属性：

$$
a_{*}=\underset{a \in A}{\arg \max } \operatorname{Gain}(D, a)
$$

著名的 ID3 决策树学习算法就是以信息增益最大为准则来选择要划分的属性的。

OK，到这里，我们对以信息增益为划分的形式还是比较熟悉了，下面我们介绍一个例子，看看实际划分的时候的具体计算过程。

#### 一个例子

OK，我们来根据上面的样本举个例子：

<center>

![](http://images.iterate.site/blog/image/180626/Liia4JEC7j.png?imageslim){ width=55% }

</center>

以上表中的西瓜数据集为例：该数据集包含了 17 个训练样本，用来学习一棵西瓜决策树。显然，$|\mathcal{Y}|=2$。

在决策树学习开始时，根结点包含 $D$ 中的所有样例，其中正例占 $p_1 =\frac{8}{17}$，反例占的 $p_2 =\frac{9}{17}$。

于是，计算出根结点的信息熵为：

$$
\operatorname{Ent}(D)=-\sum_{k=1}^{2} p_{k} \log _{2} p_{k}=-\left(\frac{8}{17} \log _{2} \frac{8}{17}+\frac{9}{17} \log _{2} \frac{9}{17}\right)=0.998
$$


然后，我们要计算出当前属性集合  {色泽，根蒂，敲声，纹理，脐部，触感}  中每个属性的信息增益。

以属性"色泽"为例：它有 3 个可能的取值:  {青绿，乌黑，浅自}，如果使用该属性对 $D$ 进行划分，则可得到 3个子集，分别记为:

- $D^1$ (色泽=青绿)，
- $D^2$ (色泽=乌黑)，
- $D^3$ (色泽=浅白)。

分析每个子集：


- 子集 $D^1$ 包含编号为 { 1, 4, 6, 10, 13, 17 } 的 6 个样例，其中正例占 $p_1 =\frac{3}{6}$ ，反例占 $p_2 =\frac{3}{6}$;
- 子集 $D^2$ 包含编号为 { 2, 3, 7, 8, 9, 15 } 的 6 个样例，其中正、反例分别占 $p_1 =\frac{4}{6}$， $p_1 =\frac{2}{6}$
- 子集 $D^3$ 包含编号为 { 5, 11, 12, 14, 16 } 的 5 个样例，其中正、反例分别占 $p_1 =\frac{1}{5}$，$p_1 =\frac{4}{5}$


可以计算出用 "色泽" 划分之后所获得的 3 个分支结点的信息熵为：
$$
\operatorname{Ent}\left(D^{1}\right)=-\left(\frac{3}{6} \log _{2} \frac{3}{6}+\frac{3}{6} \log _{2} \frac{3}{6}\right)=1.000
$$
$$
\operatorname{Ent}\left(D^{2}\right)=-\left(\frac{4}{6} \log _{2} \frac{4}{6}+\frac{2}{6} \log _{2} \frac{2}{6}\right)=0.918
$$
$$
\operatorname{Ent}\left(D^{3}\right)=-\left(\frac{1}{5} \log _{2} \frac{1}{5}+\frac{4}{5} \log _{2} \frac{4}{5}\right)=0.722
$$


因此，可以计算出属性 "色泽" 的信息增益为


$$
\begin{aligned}
\operatorname{Gain}(D，色泽) &=\operatorname{Ent}(D)-\sum_{v=1}^{3} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right) \\ &=0.998-\left(\frac{6}{17} \times 1.000+\frac{6}{17} \times 0.918+\frac{5}{17} \times 0.722\right) \\& =0.109
\end{aligned}
$$

类似的，我们可计算出其他属性的信息增益:

- Gain(D，根蒂) = 0.143
- Gain(D，敲声) = 0.141
- Gain(D，纹理) = 0.381
- Gain(D，脐部) = 0.289
- Gain(D，触感) = 0.006

显然，属性 "纹理" 的信息增益最大，于是它被选为划分属性。下图给出了基于 "纹理" 对根结点进行划分的结果，各分支结点所包含的样例子集显示在结点中.

<center>

![](http://images.iterate.site/blog/image/180626/BeF6FHbm3G.png?imageslim){ width=55% }


</center>


OK，现在，第一步的划分已经结束了，我们使用的特是纹理。接下来，我们对每个分支结点做进一步划分。

以上图第一个分支结点 ( "纹理=清晰" ) 为例，该结点包含的样例集合 $D^1$ 中有编号为 {1 ,2, 3, 4, 5, 6, 8, 10, 15} 的 9 个样例，可用属性集合为{色泽，根蒂，敲声，脐部，触感}。那么基于 $D^1$ 我们可以计算出各属性的信息增益：

- Gain($D^1$ ，色泽) = 0.043;
- Gain($D^1$ ，根蒂) = 0.458;
- Gain($D^1$ ，敲声) = 0.331;
- Gain($D^1$ ，脐部) = 0.458;
- Gain($D^1$ ，触感) = 0.458.

可见，"根蒂"、 "脐部"、 "触感" 3 个属性均取得了最大的信息增益，这里我们可以任选其中之一作为划分属性。

类似的，我们对每个分支结点都进行上述操作，最终得到的决策树如图所示：

<center>

![](http://images.iterate.site/blog/image/180626/k0cg9dm8cm.png?imageslim){ width=55% }

</center>

<span style="color:red;">嗯，还是很清晰的。这种原理配例子的讲解方式还是很清楚的。要推广。</span>

到这里，关于以信息增益为划分原则的方式已经介绍完了。还是很清晰的。








# 相关

- 《机器学习》周志华
