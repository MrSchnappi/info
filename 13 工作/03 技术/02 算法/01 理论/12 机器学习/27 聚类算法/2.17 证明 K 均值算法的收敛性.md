---
title: 2.17 证明 K 均值算法的收敛性
toc: true
date: 2019-03-31
---

# 可以补充进来的

- 感觉还是有些地方不明白的，要自己亲手推下，并且把公式重新写下。


## 证明 K 均值算法的收敛性

首先，我们需要知道 K 均值聚类的迭代算法实际上是一种最大期望算法（Expectation-Maximization algorithm），简称 EM 算法。

EM算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。

假设有 m 个观察样本，模型的参数为θ，最大化对数似然函数可以写成如下形式：

![](http://images.iterate.site/blog/image/20190401/I47P8tyjwBkv.png?imageslim){ width=55% }（5.5）

当概率模型中含有无法被观测的隐含变量时，参数的最大似然估计变为：

![](http://images.iterate.site/blog/image/20190401/2pzgBnnPmFWQ.png?imageslim){ width=55% }（5.6）

由于 $z^{(i)}$ 是未知的，无法直接通过最大似然估计求解参数，这时就需要利用 EM 算法来求解。假设 $z^{(i)}$ 对应的分布为 $Q_i(z^{i)})$，并满足 $\sum_{z(i)}Q_i(z^{(i)})=1$。利用 Jensen 不等式，可以得到：

![](http://images.iterate.site/blog/image/20190401/SghPSKiBaTrC.png?imageslim){ width=55% }
![](http://images.iterate.site/blog/image/20190401/PfEjJC6pj0rb.png?imageslim){ width=55% }（5.7）

要使上式中的等号成立，需要满足 ![](http://images.iterate.site/blog/image/20190401/9smgI58q0KBx.png?imageslim){ width=55% }，其中 $c$ 为常数，且满足 ![](http://images.iterate.site/blog/image/20190401/jOKu5iT7W04N.png?imageslim){ width=55% }；


因此，![](http://images.iterate.site/blog/image/20190401/80w9gY1Dp7cT.png?imageslim){ width=55% }，不等式右侧函数记为 $r(x|\theta)$。当等式成立时，我们相当于为待优化的函数找到了一个逼近的下界，然后通过最大化这个下界可以使得待优化函数向更好的方向改进。


图 5.5是一个θ为一维的例子，其中棕色的曲线代表我们待优化的函数，记为 $f(θ)$，优化过程即为找到使得 $f(θ)$ 取值最大的θ。在当前θ的取值下（即图中绿色的位置），可以计算![](http://images.iterate.site/blog/image/20190401/y992cue601TN.png?imageslim){ width=55% }，此时不等式右侧的函数（记为 $r(x|\theta)$）给出了优化函数的一个下界，如图中蓝色曲线所示，其中在θ处两条曲线的取值时相等的。接下来找到使得  $r(x|\theta)$ 最大化的参数 $\theta′$，即图中红色的位置，此时 $f(\theta')$ 的取值比 $f(\theta)$ （绿色的位置处）有所提升。可以证明， $f(\theta')\geq r(x|\theta)=f(\theta)$ ，因此函数是单调的，而且 $P(x^{(i)},z^{(i)}|\theta)\in(0,1)$ 从而函数是有界的。根据函数单调有界必收敛的性质，EM算法的收敛性得证。但是 EM 算法只保证收敛到局部最优解。当函数为非凸时，以图 5.5为例，如果初始化在左边的区域时，则无法找到右侧的高点。

![](http://images.iterate.site/blog/image/20190401/yGM76Ccn6q4s.png?imageslim){ width=55% }

由上面的推导，EM算法框架可以总结如下，由以下两个步骤交替进行直到收敛。


（1）E步骤：计算隐变量的期望
![](http://images.iterate.site/blog/image/20190401/Ei43odaeV3Kx.png?imageslim){ width=55% }

（2）M步骤：最大化．
![](http://images.iterate.site/blog/image/20190401/Vxcv5Gv6cvjd.png?imageslim){ width=55% }（5.9）

剩下的事情就是说明 K 均值算法与 EM 算法的关系了。K均值算法等价于用 EM 算法求解以下含隐变量的最大似然问题：

![](http://images.iterate.site/blog/image/20190401/BSDz6J0Mfrg1.png?imageslim){ width=55% }（5.10）

其中![](http://images.iterate.site/blog/image/20190401/l3zNGcsOSoTk.png?imageslim){ width=55% }是模型的隐变量。直观地理解，就是当样本 x 离第 k 个簇的中心点 $\mu_k$ 距离最近时，概率正比于 $exp(-||x-\mu_z||_2^2)$ ，否则为 0。


在 E 步骤，计算

![](http://images.iterate.site/blog/image/20190401/0NnBrMLFQxmW.png?imageslim){ width=55% }（5.11）

这等同于在 K 均值算法中对于每一个点 $x(i)$ 找到当前最近的簇 $z(i)$。

在 M 步骤，找到最优的参数![](http://images.iterate.site/blog/image/20190401/nbUAnIhegkB7.png?imageslim){ width=55% }，使得似然函数最大：

![](http://images.iterate.site/blog/image/20190401/DRVr7lqffodU.png?imageslim)(5.12){ width=55% }

经过推导可得：

![](http://images.iterate.site/blog/image/20190401/SyfAJX2LuKUy.png?imageslim){ width=55% }（5.13）

因此，这一步骤等同于找到最优的中心点![](http://images.iterate.site/blog/image/20190401/Ef4VibUEoJoF.png?imageslim)，使得损失函数![](http://images.iterate.site/blog/image/20190401/fwXkAtkizboc.png?imageslim){ width=55% }达到最小，此时每个样本 $x^{(i)}$ 对应的簇 $z^{(i)}$ 已确定，因此每个簇 k 对应的最优中心点 $\mu_k$ 可以由该簇中所有点的平均计算得到，这与 K 均值算法中根据当前簇的分配更新聚类中心的步骤是等同的。




# 原文及引用

- 《百面机器学习》
