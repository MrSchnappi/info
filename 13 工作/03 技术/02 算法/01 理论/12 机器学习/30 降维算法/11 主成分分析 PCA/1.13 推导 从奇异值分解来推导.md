---
title: 1.13 推导 从奇异值分解来推导
toc: true
date: 2019-08-27
---

# 主成分分析


<center>

![](http://images.iterate.site/blog/image/20190524/DVP6JeyDcWGN.png?imageslim){ width=55% }

</center>

> PCA 学习一种线性投影，使最大方差的方向和新空间的轴对齐：
>
> - (左)原始数据包含了 $x$ 的样本。在这个空间中，方差的方向与轴的方向并不是对齐的。
> - (右)变换过的数据 $z=x^{\top} W$ 在轴 $z_1$ 的方向上有最大的变化。第二大变化方差的方向沿着轴 $z_2$。


如上图所示，PCA 将输入 $\boldsymbol{x}$ 投影表示成 $\boldsymbol{z}$ ，学习数据的正交线性变换。


在下文中，我们将研究 PCA 表示如何使原始数据表示 $\boldsymbol{X}$ 去相关的。




假设有一个 $m \times n$ 的设计矩阵 $\boldsymbol{X}$，数据的均值为零，$\mathbb{E}[\boldsymbol{x}]=0$。若非如此，通过预处理步骤使所有样本减去均值，数据可以很容易地中心化。<span style="color:red;">嗯。</span>

$\boldsymbol{X}$ 对应的无偏样本协方差矩阵给定如下

$$
\operatorname{Var}[\boldsymbol{x}]=\frac{1}{m-1} \boldsymbol{X}^{\top} \boldsymbol{X}\tag{5.85}
$$

<span style="color:red;">为什么是 m-1？</span>

PCA 通过线性变换找到一个 $\operatorname{Var}[\boldsymbol{z}]$ 是对角矩阵的表示 $\boldsymbol{z}=\boldsymbol{W}^{\top} \boldsymbol{x}$ 。


## 主成分也可以通过奇异值分解(SVD)得到

在第 2.12 节，我们已知设计矩阵 $\boldsymbol{X}$ 的主成分由 $\boldsymbol{X}^{\top} \boldsymbol{X}$ 的特征向量给定。从这个角度，我们有


$$
\boldsymbol{X}^{\top} \boldsymbol{X}=\boldsymbol{W} \boldsymbol{\Lambda} \boldsymbol{W}^{\top}
$$

本节中，我们会探索主成分的另一种推导。主成分也可以通过奇异值分解(SVD)得到。具体来说，它们是 $\boldsymbol{X}$ 的右奇异向量。为了说明这点，假设 $\boldsymbol{W}$ 是奇异值分解 $\boldsymbol{X}=\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{W}^{\top}$ 的右奇异向量。以 $\boldsymbol{W}$ 作为特征向量基，我们可以得到原来的特征向量方程：

$$
\boldsymbol{X}^{\top} \boldsymbol{X}=\left(\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{W}^{\top}\right)^{\top} \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{W}^{\top}=\boldsymbol{W} \boldsymbol{\Sigma}^{2} \boldsymbol{W}^{\top}
$$

SVD有助于说明 PCA 后的 $\operatorname{Var}[z]$ 是对角的。使用 $\boldsymbol{X}$ 的 SVD 分解，$\boldsymbol{X}$ 的方差可以表示为

$$
\begin{aligned} \operatorname{Var}[\boldsymbol{x}] &=\frac{1}{m-1} \boldsymbol{X}^{\top} \boldsymbol{X} \\ &=\frac{1}{m-1}\left(\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{W}^{\top}\right)^{\top} \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{W}^{\top} \\ &=\frac{1}{m-1} \boldsymbol{W} \boldsymbol{\Sigma}^{\top} \boldsymbol{U}^{\top} \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{W}^{\top} \\ &=\frac{1}{m-1} \boldsymbol{W} \boldsymbol{\Sigma}^{2} \boldsymbol{W}^{\top} \end{aligned}
$$


其中，我们使用 $\boldsymbol{U}^{\top} \boldsymbol{U}=\boldsymbol{I}$，因为根据奇异值的定义矩阵 $\boldsymbol{U}$ 是正交的。这表明 $z$ 的协方差满足对角的要求：

$$
\begin{aligned} \operatorname{Var}[\boldsymbol{z}] &=\frac{1}{m-1} \boldsymbol{Z}^{\top} \boldsymbol{Z} \\ &=\frac{1}{m-1} \boldsymbol{W}^{\top} \boldsymbol{X}^{\top} \boldsymbol{X} \boldsymbol{W} \\ &=\frac{1}{m-1} \boldsymbol{W}^{\top} \boldsymbol{W} \boldsymbol{\Sigma}^{2} \boldsymbol{W}^{\top} \boldsymbol{W} \\ &=\frac{1}{m-1} \boldsymbol{\Sigma}^{2} \end{aligned}
$$

其中，再次使用 SVD 的定义有 $\boldsymbol{W}^{\top} \boldsymbol{W}=\boldsymbol{I}$。

以上分析指明当我们通过线性变换 $\boldsymbol{W}$ 将数据 $\boldsymbol{x}$ 投影到 $\boldsymbol{z}$ 时，得到的数据表示的协方差矩阵是对角的(即 $\boldsymbol{\Sigma}^{2}$ )，立刻可得 $\boldsymbol{z}$ 中的元素是彼此无关的。

PCA这种将数据变换为元素之间彼此不相关表示的能力是 PCA 的一个重要性质。它是**消除数据中未知变化因素**的简单表示示例。在 PCA 中，这个消除是通过寻找输入空间的一个旋转(由 $\boldsymbol{W}$ 确定)，使得方差的主坐标和 $\boldsymbol{z}$ 相关的新表示空间的基对齐。

虽然相关性是数据元素间依赖关系的一个重要范畴，但我们对于能够消除更复杂形式的特征依赖的表示学习也很感兴趣。对此，我们需要比简单线性变换更强的工具。<span style="color:red;">什么工具？</span>




# 相关

- 《深度学习》花书
