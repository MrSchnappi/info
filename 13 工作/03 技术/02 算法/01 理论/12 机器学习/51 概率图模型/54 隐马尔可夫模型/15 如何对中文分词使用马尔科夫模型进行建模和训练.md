---
title: 15 如何对中文分词使用马尔科夫模型进行建模和训练
toc: true
date: 2019-08-27
---

## 如何对中文分词问题用隐马尔可夫模型进行建模和训练？

下面用一个简单的例子来说明隐马尔可夫模型的建模过程。

假设有 3 个不同的葫芦，每个葫芦里有好药和坏药若干，现在从 3 个葫芦中按以下规则倒出药来。

1. 随机挑选一个葫芦。
2. 从葫芦里倒出一颗药，记录是好药还是坏药后将药放回。
3. 从当前葫芦依照一定的概率转移到下一个葫芦。
4. 重复步骤 2 和 3。

在整个过程中，我们并不知道每次拿到的是哪一个葫芦。

用隐马尔可夫模型来描述以上过程：

- 隐状态就是当前是哪一个葫芦，隐状态的取值空间为{葫芦 1，葫芦 2，葫芦 3}
- 观测状态的取值空间为{好药，坏药}
- 初始状态的概率分布就是第（1）步随机挑选葫芦的概率分布
- 隐状态间的转移概率就是从当前葫芦转移到下一个葫芦的概率
- 而隐状态到观测状态的输出概率就是每个葫芦里好药和坏药的概率
- 记录下来的药的顺序就是观测状态的序列
- 而每次拿到的葫芦的顺序就是隐状态的序列。

<span style="color:red;">是的，很清晰</span>

隐马尔可夫模型包括概率计算问题、预测问题、学习问题三个基本问题：

1. 概率计算问题：已知模型的所有参数，计算观测序列 $Y$ 出现的概率，可使用前向和后向算法求解。
2. 预测问题：已知模型所有参数和观测序列 $Y$，计算最可能的隐状态序列 $X$，可使用经典的动态规划算法-维特比算法来求解最可能的状态序列。<span style="color:red;">维特比算法，又看到了。</span>
3. 学习问题：已知观测序列 $Y$，求解使得该观测序列概率最大的模型参数，包括隐状态序列、隐状态之间的转移概率分布以及从隐状态到观测状态的概率分布，可使用 Baum-Welch 算法进行参数的学习，Baum-Welch 算法是最大期望算法的一个特例。<span style="color:red;">Baum-Welch 是什么算法？最大期望算法是什么？都总结下，不然又不清楚了。</span>


上面提到的问题和算法在此不多做介绍，感兴趣的读者可以查阅相关资料。<span style="color:red;">好吧，还是要都总结进来的。</span>

下面回到开头的问题。

隐马尔可夫模型通常用来解决序列标注问题，因此也可以将分词问题转化为一个序列标注问题来进行建模。

例如：

- 可以对中文句子中的每个字做以下标注，B 表示一个词开头的第一个字，E 表示一个词结尾的最后一个字，M 表示一个词中间的字，S 表示一个单字词，则隐状态的取值空间为 {B,E,M,S}。<span style="color:red;">嗯，很在理。</span>
- 同时对隐状态的转移概率可以给出一些先验知识，B 和 M 后面只能是 M 或者 E，S 和 E 后面只能是 B 或者 S。
- 而每个字就是模型中的观测状态，
- 取值空间为语料中的所有中文字。

完成建模之后，使用语料进行训练可以分有监督训练和无监督训练：<span style="color:red;">什么意思？这个地方没懂，要把完整的过程总结进来。</span>

- 有监督训练即对语料进行标注，相当于根据经验得到了语料的所有隐状态信息，然后就可以用简单的计数法来对模型中的概率分布进行极大似然估计。
- 无监督训练可以用上文提到的 Baum-Welch 算法，同时优化隐状态序列和模型对应的概率分布。

<span style="color:red;">上面这最后的讲解没有很懂，之前没有怎么看过这部分。</span>






# 相关

- 《百面机器学习》
