---
title: 4.02 循环神经网络为什么会出现梯度消失或梯度爆炸 有哪些改进方案
toc: true
date: 2019-04-19
---
# 循环神经网络的梯度消失问题


细究深度学习的发展历史，其实早在 1989 年，深度学习先驱 Lecun 就已经提出了基于反向传播算法的卷积神经网络 LeNet，将其用于数字识别任务，并取得了良好的效果。但 Lenet在当时并没有取得广泛的关注与重视，也少有人接着这项工作继续研究，提出更新颖、突出的模型。另外深度神经网络模型由于缺乏严格的数学理论支持，在 20 世纪 80 年代末，这股浪潮便渐渐退去，走向平淡。

1991年，深度学习的发展达到了一个冰点。在这一年，反向传播算法被指出存在梯度消失（Gradient Vanishing）问题，即在梯度的反向传播过程中，后层的梯度以连乘方式叠加到前层。由于当时神经网络中的激活函数一般都使用 Sigmoid 函数，而它具有饱和特性，在输入达到一定值的情况下，输出就不会发生明显变化了。而后层梯度本来就比较小，误差梯度反传到前层时几乎会衰减为 0，因此无法对前层的参数进行有效的学习，这个问题使得本就不景气的深度学习领域雪上加霜。

在循环神经网络中，是否同样存在梯度消失的问题呢？

梯度消失，梯度爆炸（Gradient Explosion）

## 循环神经网络为什么会出现梯度消失或梯度爆炸？有哪些改进方案？

循环神经网络模型的求解可以采用 BPTT（Back Propagation Through Time，基于时间的反向传播）算法实现，BPTT 实际上是反向传播算法的简单变种。<span style="color:red;">BPTT 算法还是要看下具体是怎么计算的。</span>

如果将循环神经网络按时间展开成 $T$ 层的前馈神经网络来理解，就和普通的反向传播算法没有什么区别了。

循环神经网络的设计初衷之一就是能够捕获长距离输入之间的依赖。从结构上来看，循环神经网络也理应能够做到这一点。

然而实践发现，使用 BPTT 算法学习的循环神经网络并不能成功捕捉到长距离的依赖关系，这一现象主要源于深度神经网络中的梯度消失。传统的循环神经网络梯度可以表示成连乘的形式，


$$
\frac{\partial n e t_{t}}{\partial n e t_{1}}=\frac{\partial n e t_{t}}{\partial n e t_{t-1}} \cdot \frac{\partial n e t_{t-1}}{\partial n e t_{t-2}} \cdots \frac{\partial n e t_{2}}{\partial n e t_{1}}\tag{10.4}
$$

其中：

$$
net_{t}=U x_{t}+W h_{t-1}\tag{10.5}
$$

$$
h_{t}=f\left(\text { net }_{t}\right)\tag{10.6}
$$

$$
y=g\left(V h_{t}\right)\tag{10.7}
$$


$$
\begin{aligned}\frac{\partial n e t_{t}}{\partial n e t_{t-1}}&=\frac{\partial n e t_{t}}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial n e t_{t-1}}=W \cdot \operatorname{diag}\left[f^{\prime}\left(n e t_{t-1}\right)\right]\\&=\left( \begin{array}{ccc}{\mathrm{w}_{11} f^{\prime}\left(n e t_{t-1}^{1}\right)} & {\cdots} & {\mathrm{w}_{1 n} f^{\prime}\left(n e t_{t-1}^{n}\right)} \\ {\vdots} & {\ddots} & {\vdots} \\ {\mathrm{w}_{n 1} f^{\prime}\left(n e t_{t-1}^{1}\right)} & {\cdots} & {\mathrm{w}_{n n} f^{\prime}\left(n e t_{t-1}^{n}\right)}\end{array}\right)\end{aligned}\tag{10.8}
$$



其中 $n$ 为隐含层 $h_{t−1}$ 的维度（即隐含单元的个数），$\frac{\partial n e t_{t}}{\partial n e t_{t-1}}$ 对应的 $n×n$ 维矩阵，又被称为雅可比矩阵。<span style="color:red;">再看下</span>

由于预测的误差是沿着神经网络的每一层反向传播的，因此：

- 当雅克比矩阵的最大特征值大于 1 时，随着离输出越来越远，每层的梯度大小会呈指数增长，导致梯度爆炸；
- 反之，若雅克比矩阵的最大特征值小于 1，梯度的大小会呈指数缩小，产生梯度消失。

对于普通的前馈网络来说，梯度消失意味着无法通过加深网络层次来改善神经网络的预测效果，因为无论如何加深网络，只有靠近输出的若干层才真正起到学习的作用。**这使得循环神经网络模型很难学习到输入序列中的长距离依赖关系。** <span style="color:red;">是的。</span>


那么怎么解决这个梯度爆炸和梯度消失问题呢？

- 梯度爆炸的问题可以通过梯度裁剪来缓解，即当梯度的范式大于某个给定值时，对梯度进行等比收缩。<span style="color:red;">怎么进行梯度裁剪？怎么对梯度进行等比收缩？现实场景中真的会这么做吗？</span>
- 而梯度消失问题相对比较棘手，需要对模型本身进行改进：
    - 深度残差网络是对前馈神经网络的改进，通过残差学习的方式缓解了梯度消失的现象，从而使得我们能够学习到更深层的网络表示；<span style="color:red;">是的，之前有介绍过的。的确是不错的。</span>
    - 而对于循环神经网络来说，长短时记忆模型[23]及其变种门控循环单元（Gated recurrent unit，GRU）[24]等模型通过加入门控机制，很大程度上弥补了梯度消失所带来的损失。<span style="color:red;">嗯。好的。</span>




# 相关

- 《百面机器学习》
