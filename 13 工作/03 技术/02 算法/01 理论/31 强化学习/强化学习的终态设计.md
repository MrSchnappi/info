强化学习的终态设计始终是强化学习入门者经常踩的一个坑，甚至有的熟悉强化学习的同学也经常掉入这个陷阱。 终态设计不当可能导致模型完全不收敛。本文会详细介绍终态的类型，以及应对方案，同时分析很多人在终态处理时容易犯的一些错误

**终态**（Terminal）的概念，通常对应于一个Episode的结束。 我们需要首先对终态进行分类，包含两类不同的终态：**真实终态**和**伪终态。 **真实终态比较好理解，为什么存在伪终态呢？存在大量的MDP（Markov Decision Process）问题属于Infinite Horizon。而近期的强化学习算法越来越多地使用阶段性地优化。 为了能让训练切成一个一个阶段，有必要把无限长的交互序列拆解成有限的长度。而这种拆解就会导致**伪终态**的产生。这种伪终态不对应任何一个特殊的状态，而仅仅是由于人为切断导致。而这种伪终态处理不当，可能引入不必要的bias，下面以一个非常经典的case - MountainCar来引出我们的讨论

![img](https://pic3.zhimg.com/80/v2-178eb0f1c74451c5f6c95dc6ba96958e_1440w.jpg)

Mountain Car的设定是，如果控制小车到达山顶，则Episode结束，否则每一步一直得到-1的reward. 在优化过程中，如果小车一直不能达到山顶，则Episode一直持续。如果不能探索到到达峰顶，Episode将无限长。 这对于很多算法显然不能接受，例如TRPO。为此，我们尝试人为地设置一个Episode截断长度（比如我们允许一个Episode最长200个step，否则就停止），这里就产生了伪终态的问题。

![img](https://pic2.zhimg.com/80/v2-6961f0b9c3a3c495735d198c5b0a3f6d_1440w.jpg)

上图展示了3条不同轨迹A, B, C, 其中，A轨迹是200步内一直不能到达山顶的轨迹，B是A经过了人为截断的轨迹，C是能够到达山顶，正常结束一个Episode的轨迹。 如果使用A轨迹来计算其Q值，那每一步的Q值都是-100 （如果一直得到r = -1, 并且 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%3D0.99) ，其Q值可以通过 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7Br%7D%7B1-%5Cgamma%7D) 得到）。 如果我们在A轨迹这人为截断，就得到轨迹B。但是如果我们不对轨迹B做任何处理，我们会发现它的Q值的分布和轨迹C，即成功到达了山顶是完全一致的，这和A的Q值分布产生了巨大的差别。

但问题在于，B轨迹的最后一个状态，和A轨迹对应状态一样，Q都是-100, 但我们却让系统错误地认为，B轨迹最后到达了目标，这就给训练引入了非常大的Bias，也就是模型会认为，超过200步一定能到达山顶。单单这样其实还不足以引起灾难性的效果，然而这里还引入了一个额外的问题：你用的模型压根就不知道自己当前step数，因为你并没有将step数作为特征引入模型，因此，你的数据里第200步的状态和第一步可能没有什么不同，但Q值却差得很远，这直接会导致不收敛的情况。

如何避免这类“伪终态陷阱”？这里介绍几类常用的方法

1. 把你的step数（或者时间t）作为状态观测量之一。这样，一定程度上改变了问题的设定，但同样能达到效果。相当于你把问题变成了一个限定时间内完成任务的问题（参看ICML 2018（[Time Limits in Reinforcement Learning](https://link.zhihu.com/?target=http%3A//proceedings.mlr.press/v80/pardo18a.html)））。模型一旦对于当前step有数，就完全能正常处理“靠近终止态，Q值会升高/降低”这类冲突。然而加入时间特征实际增大了你的观测空间，很多时候是不划算的。
2. 取决于你使用的算法，假如你使用的是1-step return的TD(0), Loss Function一般如下所示 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%3D%5Cmathbb%7BE%7D%5B%28y_t-Q%28s_t+%2Ca_t%29%29%5D%5C%5C+%5Ctext%7B+if+%7Ds_%7Bt%2B1%7D%5Ctext%7B+is+terminal%2C+%7D+y_t%3D+r_t+%5C%5C+%5Ctext%7B+else%2C+%7D+y_t+%3D+r_t+%2B+Q%27%28s_%7Bt%2B1%7D%2C+a_%7Bt%2B1%7D%29) 一个样本通常用 ![[公式]](https://www.zhihu.com/equation?tex=%28s_t%2C+a_t%2C+s_%7Bt%2B1%7D%2C+r_t%29+) 来表示，假如 ![[公式]](https://www.zhihu.com/equation?tex=s_T) 是终止态，一个非常简单的办法就是直接把最后一个样本![[公式]](https://www.zhihu.com/equation?tex=%28s_%7BT-1%7D%2C+a_%7BT-1%7D%2C+s_T%2C+r_T%29+)去掉。 这样做为什么就解决了？原因就在于，你用上面的式子，实际隐性用了 ![[公式]](https://www.zhihu.com/equation?tex=Q%28s_T%2Ca%29%3D0) 这样一个假设。而这个错误值的backup导致了前面一串值都有问题。只要截断这个错误，就不会有问题。
3. 如果你使用了n-step return的TD( ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) )甚至于monte carlo这种方法，相对起来就非常trick。如果你使用了n-step return, 就需要对episode最后n-1个step都进行特殊处理, 对于MonteCarlo方法来说，你甚至需要在最后人为补一个值。总的来说，你需要保障不会使用![[公式]](https://www.zhihu.com/equation?tex=Q%28s_T%2Ca%29%3D0) 这样一个隐含假设。以monte carlo值估计为例子，你原来可能使用 ![[公式]](https://www.zhihu.com/equation?tex=y_t+%3D+r_t+%2B+%5Cgamma+r_%7Bt%2B1%7D+%2B+...+%2B+%5Cgamma%5E%7BT-t%7D+r_T) , 现在你需要在后面补上 ![[公式]](https://www.zhihu.com/equation?tex=s_T)的Value Function，即![[公式]](https://www.zhihu.com/equation?tex=y_t+%3D+r_t+%2B+%5Cgamma+r_%7Bt%2B1%7D+%2B+...+%2B+%5Cgamma%5E%7BT-t%7D+r_T+%2B+%5Cgamma%5E%7BT+%2B+1+-+t%7D+V%27%28s_T%29+)
4. 如果使用的是像REINFORCE这种暴力求解方法，更好的方法是重新做reward shaping. 例如，对于伪终态，我们给一个 ![[公式]](https://www.zhihu.com/equation?tex=r_T+%3D+-100) （和不成功完全接近）的reward，就能大概率减缓这个问题，但仍然可能存在风险，有可能需要和方法1联合起来使用。

前面讲的**伪终态**不单单针对人为截断Episode的问题，也针对一类设定中本身就存在伪终态的环境。**伪终态**可以确切定义为，**从状态观测上，实际不能和其他普通状区分开的一切终态**。我们可以举出一些其他例子： 比如cartpole(倒立摆)中，如果环境设定就是是200步内不倒获得一个正收益，那这个环境中，倒掉的是真终态，而200步没倒获得收益却是一个伪终态，同样要注意上面说的问题。

接下来说说**真终态**处理同样存在一些坑。真终态一般存在两种处理方式，

1. 传统处理方式，即 ![[公式]](https://www.zhihu.com/equation?tex=+y_T%3D+r_T)
2. 轮回方式， 即 ![[公式]](https://www.zhihu.com/equation?tex=y_T+%3D+r_T+%2B+%5Cgamma+V%28s_0%29) , 这里 ![[公式]](https://www.zhihu.com/equation?tex=s_0) 是初始状态或者某种特殊状态

终态处理是一个和reward shaping极其相关的课题。这里存在一些陷阱，如果终态设定和reward设定不搭配好，可能导致最优解实际不是你想要的。以一个简单的路径规划躲避障碍为例子，很多人喜欢设置每走一步 reward = -1， 然后如果碰到障碍或者掉进陷阱设置一个终止态，并且设置一个惩罚如reward = -100。但这其实可能导致灾难性的后果。如果我们设置 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma+%5Cgeq+0.99) , 就会使得不碰到障碍的情况下， ![[公式]](https://www.zhihu.com/equation?tex=Q+%5Cleq+-100) ； 而如果你使用了前面提到的1这种终态处理方式，agent选择碰撞障碍物或者进入陷阱的 value function是 ![[公式]](https://www.zhihu.com/equation?tex=Q+%3D+-100) 。 因此，这样设置实际会鼓励agent去碰撞障碍或者掉入陷阱而不是回避他们！

最后，你用哪种方式来处理终态必须非常谨慎分析再确定。