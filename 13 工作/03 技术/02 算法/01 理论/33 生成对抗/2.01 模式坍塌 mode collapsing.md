---
title: 2.01 模式坍塌 mode collapsing
toc: true
date: 2019-09-03
---

# 什么是模式坍塌 mode collapsing


模式坍塌：

- 生成器在训练好之后，只能产生固定几个模式的图片。

真实的数据分布空间其实是很大的，但是模型崩塌到这个空间的若干个点上。

示意如下：

<center>

![](http://images.iterate.site/blog/image/20190722/XlTFrFgHqKRX.png?imageslim){ width=55% }

</center>

上图左侧的蓝色五角星表示真实样本空间，黄色的是生成的。生成样本缺乏多样性，存在大量重复。比如上图右侧中，红框里面人物反复出现。

## 解决方法

最近一年针对这个问题也提出了比如标签平滑、Mini-Batch 判别器等启发式方法来解决生成器模型崩塌的问题并取得了一定效果。



<span style="color:red;">三种方法都挺好，不知道真正使用哪种。</span>

方法一：**针对目标函数的改进方法**

为了避免前面提到的由于优化 maxmin 导致 mode 跳来跳去的问题，<span style="color:red;">上面说的是 mode 跳来跳去的问题吗？</span>UnrolledGAN 采用修改生成器 loss 来解决。具体而言，UnrolledGAN 在更新生成器时更新 $k$ 次生成器，参考的 Loss 不是某一次的 loss，是判别器后面 $k$ 次迭代的 loss。注意，判别器后面 $k$ 次迭代不更新自己的参数，只计算 loss 用于更新生成器。这种方式使得生成器考虑到了后面 $k$ 次判别器的变化情况，避免在不同 mode 之间切换导致的模式崩溃问题。<span style="color:red;">嗯，听起来好像是可以的。</span>此处务必和迭代 $k$ 次生成器，然后迭代 $1$ 次判别器区分开[8]。DRAGAN 则引入博弈论中的无后悔算法，改造其 loss 以解决 mode collapse 问题[9]。<span style="color:red;">什么是无后悔算法？</span>前文所述的 EBGAN 则是加入 VAE 的重构误差以解决 mode collapse。<span style="color:red;">什么是 VAE 的重构误差？</span>

方法二：**针对网络结构的改进方法**

Multi agent diverse GAN(MAD-GAN)采用多个生成器，一个判别器以保障样本生成的多样性。具体结构如下：

<center>

![](http://images.iterate.site/blog/image/20190722/vjACGwol4Qcm.png?imageslim){ width=55% }

</center>


相比于普通 GAN，多了几个生成器，且在 loss 设计的时候，加入一个正则项。正则项使用余弦距离惩罚三个生成器生成样本的一致性。<span style="color:red;">怎么判定一致性？两张图片的一致性吗？</span>

MRGAN 则添加了一个判别器来惩罚生成样本的 mode collapse 问题。具体结构如下：

<center>

![](http://images.iterate.site/blog/image/20190722/WBB1eEjEGfaF.png?imageslim){ width=55% }

</center>

输入样本 $x​$ 通过一个 Encoder 编码为隐变量 $E(x)​$，然后隐变量被 Generator 重构，训练时，Loss有三个。$D_M​$ 和 $R​$（重构误差）用于指导生成 real-like 的样本。而 $D_D​$ 则对 $E(x)​$ 和 $z​$ 生成的样本进行判别，显然二者生成样本都是 fake samples，所以这个判别器主要用于判断生成的样本是否具有多样性，即是否出现 mode collapse。<span style="color:red;">没有明白这个，但是觉得很好。补充下。</span>

方法三：**Mini-batch Discrimination**

Mini-batch discrimination 在判别器的中间层建立一个 mini-batch layer 用于计算基于 L1 距离的样本统计量，通过建立该统计量，实现了一个 batch 内某个样本与其他样本有多接近。这个信息可以被判别器利用到，从而甄别出哪些缺乏多样性的样本。对生成器而言，则要试图生成具有多样性的样本。<span style="color:red;"></span>
