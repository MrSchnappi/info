---
title: 06 类别不平衡时候的再平衡
toc: true
date: 2018-06-26 09:24:58
---

# 可以补充进来的

- 欠采样亦称“下采样” (downsampling)，过采样亦称“上采样” (upsampling)

# 类别不平衡问题

如果不同类别的训练样例数目稍有差别，通常影响不大，但若差别很大，则会对学习过程造成困扰。例如有 998 个反例，但正例只有 2 个，那么学习方法只需返回一个永远将新样本预测为反例的学习器，就能达到 99.8%的精度；然而这样的学习器往往没有价值，因为它不能预测出任何正例。<span style="color:red;">嗯，是呀</span>

对 OvR、MvM 来说，由于对每个类进行了相同的处理，其拆解出的二分类 任务中类别不平衡的影响会相互抵消，因此通常不需专门处理.<span style="color:red;">没明白</span>

类别不平衡(class-imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情况。为了不失一般性，本节假定正类样例较少，反类样例较多. 在现实的分类学习任务中，我们经常会遇到类别不平衡，例如在通过拆分 法解决多分类问题时，即使原始问题中不同类别的训练样例数目相当，在使用 OvR、MvM 策略后产生的二分类任务仍可能出现类别不平衡现象，因此有必要了解类别不平衡性处理的基本方法.

从线性分类器的角度讨论容易理解，在我们用 $y=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$ 对新样本 $\boldsymbol{x}$ 进行分类时，事实上是在用预测出的 $y$ 值与一个阈值进行比较，例如通常在 $y> 0.5$ 时判别为正例，否则为反例。实际上表达了正例的可能性，几率 $\frac{y}{1-y}$ 则反映了正例可能性与反例可能性之比值，阈值设置为 0.5恰表明分类器认为 真实正、反例可能性相同，即分类器决策规则为

$$
若 \frac{y}{1-y}>1 则预测为正例 \tag{3.46}
$$

然而，当训练集中正、反例的数目不同时，令 $m^+$ 表示正例数目， $m^-$ 表示反例数目，则观测几率是 $\frac{m^{+}}{m^{-}}$，由于我们通常假设训练集是真实样本总体的无偏釆样，因此观测几率就代表了真实几率。于是，只要分类器的预测几率高于观测 几率就应判定为正例，即


$$
若 \frac{y}{1-y}>\frac{m^+}{m^-} 则预测为正例 \tag{3.47}
$$

但是，我们的分类器是基于式(3.46)进行决策，因此，需对其预测值进行调整，使其在基于式(3.46)决策时，实际是在执行式(3.47)。要做到这一点很容易，只需令

$$
\frac{y^{\prime}}{1-y^{\prime}}=\frac{y}{1-y} \times \frac{m^{-}}{m^{+}}\tag{3.48}
$$


这就是类别不平衡学习的一个基本策略——“再平衡”(rebalance).


再平衡的思想虽简单，但实际操作却并不平凡，主要因为“训练集是真实样本总体的无偏采样”这个假设往往并不成立。也就是说，我们未必能有效地基于训练集观测几率来推断出真实几率。现有技术大体上有三类做法：

- 第一类是直接对训练集里的反类样例进行“欠采样”(undersampling)，即去除 一些反例使得正、反例数目接近，然后再进行学习；
- 第二类是对训练集里的 正类样例进行“过采样”(oversampling)，即增加一些正例使得正、反例数目接近，然后再进行学习；
- 第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将式(3.48)嵌入到其决策过程中，称为“阈值移动” (threshold-moving).

关于欠采样法和过采样法：

- 欠采样法的时间开销通常远小于过采样法，因为前者丢弃了很多反例，使得分类器训练集远小于初始训练集，欠采样法若随机丢弃反例，可能丢失一些重要信息；欠采样法的代表性算法 EasyEnsemble [Liu et al.5 2009]则是利用集成学习机制，将反例划分为若干个 集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在全局来看却不会丢失重要信息.<span style="color:red;">怎么实现的？</span>
- 而过采样法増加了很多正例，其训练集大于初始训练集。需注意的是，过采样法不能简单地对初始正例样本进行重复采样，否则会招致严重的过拟合；过采样法的代表性算法 SMOTE [Chawla et al.5 2002]是通过对训练集里的正例进行插值来产生额外的正例<span style="color:red;">怎么实现的？</span>


值得一提的是，“再平衡” 也是 “代价敏感学习”(cost-sensitive learning) 的基础. 在代价敏感学习中将式(3.48)中的 $m^{-} / m^{+}$ 用 $\operatorname{cost}^{+} / \cos t^{-}$ 代替即可，其中 $cost^+$ 是将正例误分为反例的代价，$cost^-$ 是将反例误分为正例的代价.<span style="color:red;">没明白？</span>







# 相关

- 《机器学习》周志华
