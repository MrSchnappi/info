---
title: 03 卷积层
toc: true
date: 2019-05-30
---
# 卷积层

卷积是一种局部操作，通过一定大小的卷积核作用于局部图象区域，从而得到图象的局部信息。下面以一维卷积层为例：

```py
import torch
from torch import autograd
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 3)
        self.conv2 = nn.Conv2d(20, 4, 3)

    def forward(self, x):
       x = F.relu(self.conv1(x))
       return F.relu(self.conv2(x))

inputs = autograd.Variable(torch.randn(2, 1, 7,6))
mod=Model()
output=mod(inputs)
print(inputs)
print(mod)
print(output)

print("\n")

m = nn.Conv1d(4, 3, 3, stride=2)
inputs = autograd.Variable(torch.randn(2, 4, 5))
output = m(inputs)
print(inputs)
print(m)
print(output)

print("\n")

m = nn.MaxPool1d(3, stride=2)
inputs = autograd.Variable(torch.randn(2, 3, 5))
output = m(inputs)
print(inputs)
print(m)
print(output)
```

输出：

```
tensor([[[[ 0.2527,  0.9059,  0.3875,  0.5003,  0.7819, -0.2161],
          [ 1.1903, -0.7685,  0.7659,  0.4519, -0.8815, -0.4115],
          [-1.1377, -0.4293, -1.4989,  0.5423,  0.5220,  1.4691],
          [ 0.7675,  0.6207, -1.6024, -0.7190, -0.0300,  0.4756],
          [ 0.7925,  0.3510, -0.8285, -0.6407,  0.7847, -1.0686],
          [-1.2434, -0.0485,  1.0425, -1.1364,  0.9726,  0.6865],
          [ 1.2502, -1.2026, -1.0652,  1.4207, -0.8950,  1.2504]]],


        [[[ 0.9058,  0.4763, -0.0198,  0.4173,  1.2277,  0.7972],
          [ 0.5370, -0.5664,  0.6119,  1.1392, -0.5117,  0.0167],
          [-1.6757,  0.3266,  0.5386, -1.0772, -0.0941, -0.5119],
          [-0.7009,  0.8967,  0.3706, -0.8387,  0.6135,  0.3583],
          [ 1.2170, -0.2839, -0.5595, -0.8334, -0.6451, -1.4147],
          [ 1.1355, -1.0992,  0.4449,  0.9773,  0.5081, -0.0375],
          [ 1.4799, -0.2714, -0.4348, -0.1604,  0.4418,  1.1469]]]])
Model(
  (conv1): Conv2d(1, 20, kernel_size=(3, 3), stride=(1, 1))
  (conv2): Conv2d(20, 4, kernel_size=(3, 3), stride=(1, 1))
)
tensor([[[[0.0000, 0.0000],
          [0.0000, 0.0000],
          [0.0000, 0.0000]],

         [[0.0000, 0.0000],
          [0.0000, 0.0000],
          [0.0000, 0.0000]],

         [[0.0000, 0.0000],
          [0.0000, 0.0150],
          [0.0000, 0.0000]],

         [[0.4085, 0.0000],
          [0.6253, 0.0000],
          [0.4554, 0.0318]]],


        [[[0.0000, 0.0000],
          [0.0000, 0.0000],
          [0.0000, 0.0000]],

         [[0.0000, 0.0000],
          [0.0000, 0.0000],
          [0.0000, 0.0000]],

         [[0.0000, 0.0000],
          [0.0000, 0.0000],
          [0.0149, 0.0000]],

         [[0.0317, 0.3184],
          [0.1715, 0.2838],
          [0.1042, 0.2971]]]], grad_fn=<ReluBackward>)


tensor([[[-0.5448,  0.2654,  1.3167, -0.5525,  1.6755],
         [ 0.2433, -0.8126, -0.0788,  0.0561,  1.1728],
         [-0.3729, -0.5496, -1.2274,  0.5352,  0.9140],
         [-0.8072, -1.3103,  0.0099, -0.2422, -0.1679]],

        [[-0.2556, -0.2332, -0.1281, -0.2124, -1.1474],
         [ 0.3492,  1.5358, -0.2478,  1.0252,  0.9372],
         [-0.3162,  0.4835,  0.1149, -0.8258,  0.9493],
         [ 0.0063,  0.0799,  0.5150, -0.4592, -0.5731]]])
Conv1d(4, 3, kernel_size=(3,), stride=(2,))
tensor([[[-0.2152, -0.7821],
         [-0.3097,  0.0648],
         [ 0.2641, -0.3394]],

        [[-0.1388, -0.0892],
         [-0.3060, -0.0460],
         [-0.1508, -0.3630]]], grad_fn=<SqueezeBackward1>)


tensor([[[-0.0101, -0.2253,  0.5103, -0.4498,  1.0100],
         [-0.1033,  0.4632,  0.6030, -1.9225, -0.0780],
         [ 2.4814,  0.1323,  0.4371,  1.4305,  1.2624]],

        [[-1.9155, -0.7742,  2.1208,  0.0939,  0.5217],
         [-0.2796, -0.0061, -0.5401, -1.3831,  0.9548],
         [ 1.7043,  0.8059, -0.7640,  0.7627, -0.5131]]])
MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
tensor([[[ 0.5103,  1.0100],
         [ 0.6030,  0.6030],
         [ 2.4814,  1.4305]],

        [[ 2.1208,  2.1208],
         [-0.0061,  0.9548],
         [ 1.7043,  0.7627]]])
```

一维卷积层：

class torch.nn.Conv1d(in_channels,out_channels,kernel_size,stride=1, padding=0,dilation=1,groups=1,bias=True)

一维卷积层，输入的尺度是（N,C_in,L），输出尺度（N,C_out,L_out）
参数说明如下。


- in_channels（int）：输入信号的通道。
- out_channels（int）：卷积产生的通道。
- kerner_size（int or tuple）：卷积核的尺寸。
- stride（int or tuple,optional）：卷积步长。
- padding（int or tuple,optional）：是否对输入数据填充 0。Padding 可以将输入数据的区域改造成是卷积核大小的整数倍，这样对不满足卷积核大小的部分数据就不会忽略了。通过 padding 参数指定填充区域的高度和宽度。
- dilation（int or tuple,`optional`）：卷积核之间的空格。<span style="color:red;">卷积核之间的空格是什么？</span>
- groups（int,optional）：将输入数据分成组，in_channels 应该被组数整除。<span style="color:red;">为什么要将数据分组？</span>
- bias（bool,optional）：如果 bias=True，添加偏置。


池化层：

class torch.nn.MaxPool1d（kernel_size,stride=None,padding=0,dilation=1, return_indices=False,ceil_mode=False）

对于输入信号的输入通道，提供一最大池化（Max Pooling）操作。

参数说明如下。
- kernel_size（int or tuple）:Max Pooling的窗口大小。
- stride（int or tuple,optional）:max pooling的窗口移动的步长。默认值是 kernel_size。
- padding（int or tuple,optional）：输入的每一条边补充 0 的层数。
- dilation（int or tuple,optional）：一个控制窗口中元素步幅的参数。
- return_indices：如果等于 True，会返回输出最大值的序号，对于上采样操作会有帮助 <span style="color:red;">返回输出的最大值是什么？</span>
- ceil_mode：如果等于 True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作。<span style="color:red;">为什么要有向上取整和向下取整的操作？有什么区别吗？</span>


下面我们创建一个小型 ConvNet。所有的网络都是从基类 nn.Module中构造函数。

```py
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F


class MNISTConvNet(nn.Module):
    def __init__(self):
        super(MNISTConvNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, 5)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(10, 20, 5)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, input):
        x = self.pool1(F.relu(self.conv1(input)))
        x = self.pool2(F.relu(self.conv2(x)))
        return x


net = MNISTConvNet()
print(net)
input = Variable(torch.randn(1, 1, 28, 28))
out = net(input)
print(input.size())
print(out.size())
```

输出：

```
MNISTConvNet(
  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fc1): Linear(in_features=320, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=10, bias=True)
)
torch.Size([1, 1, 28, 28])
torch.Size([1, 20, 4, 4])
```

`torch.nn` 包装仅支持作为小批量样品的输入，而不是单个样品。<span style="color:red;">嗯。</span>


奇怪了，这个书上的 `net.conv1.weight.grad.size()` `net.conv1.weight.grad.data.norm()` 这个怎么跟例子不配套呢？简直了，感觉这个还是会要经常使用到的。嗯把相关的例子补充下。


# 相关

- 《深度学习框架 Pytorch 快速开发与实战》
