---
title: 2.08 语义分割效果优化方法
toc: true
date: 2019-09-01
---


# 语义分割效果优化方法

**1.Astrous Convolutions（扩张卷积）**

空洞卷积（dilated convolution）是针对图像语义分割问题中下采样会降低图像分辨率、丢失信息而提出的一种卷积思路。利用添加空洞扩大感受野，让原本 3x3 的卷积核，在相同参数量和计算量下拥有 5x5（dilated rate =2）或者更大的感受野，从而无需下采样。扩张卷积又名空洞卷积（atrous convolutions），向卷积层引入了一个称为 “扩张率(dilation rate)”的新参数，该参数定义了卷积核处理数据时各值的间距。换句话说，相比原来的标准卷积，扩张卷积多了一个 hyper-parameter（超参数）称之为 dilation rate（扩张率），指的是 kernel 各点之前的间隔数量，【正常的 convolution 的 dilatation rate为 1】。

**2.空间金字塔池化等下采样技术。**

如上述模型中的 ASPP 模块及 JPU 模块，并且可以以此技术来相互组合达到更好的效果。

**3.CRF（条件随机场）**

在基于深度学习的语义图像分割体系结构,CRF是一个有用的后处理模块，但是主要缺点是不能将其用作端到端体系结构的一部分。在标准 CRF 模型中，可以表示成对电位用加权高斯函数的和。但是由于精确的极小化是否考虑了 CRF 分布的平均场近似用一个简单的产品版本来表示发行版独立的边际分布。它的平均场近似原生形式不适合反向传播。

**4.Loss函数的更改**

**(1)二分类**

大名鼎鼎的 focal loss，focal loss的提出是在目标检测领域，为了解决正负样本比例严重失衡的问题。是由 log loss改进而来的，公式如下：

![img](https://pic2.zhimg.com/80/v2-7f510d6c0a12f9c7c2c17e43d1633a1d_hd.jpg)

其中 gamma>0

在 Focal Loss中，它更关心难分类样本，不太关心易分类样本，比如：

若 gamma = 2，

对于正类样本来说，如果预测结果为 0.97那么肯定是易分类的样本，所以就会很小；

对于正类样本来说，如果预测结果为 0.3的肯定是难分类的样本，所以就会很大；

对于负类样本来说，如果预测结果为 0.8那么肯定是难分类的样本，就会很大；

对于负类样本来说，如果预测结果为 0.1那么肯定是易分类的样本，就会很小。

另外，Focal Loss还引入了平衡因子 alpha，用来平衡正负样本本身的比例不均。

alpha取值范围 0~1，当 alpha>0.5时，可以相对增加 y=1所占的比例。实现正负样本的平衡。

虽然何凯明的试验中，lambda为 2 是最优的，但是不代表这个参数适合其他样本，在应用中还需要根据实际情况调整这两个参数。

**(2)多分类**

多分类常见的是交叉熵，其他的损失函数也比较多，也可以对交叉熵加权。

如 Dice：

```python3
##Keras
def dice_coef(y_true, y_pred, smooth=1):
    mean_loss = 0;
    for i in range(y_pred.shape(-1)):
        intersection = K.sum(y_true[:,:,:,i] * y_pred[:,:,:,i], axis=[1,2,3])
        union = K.sum(y_true[:,:,:,i], axis=[1,2,3]) + K.sum(y_pred[:,:,:,i], axis=[1,2,3])
    mean_loss += (2. * intersection + smooth) / (union + smooth)
    return K.mean(mean_loss, axis=0)
```

**5.标签平滑**

简单理解就是对预测结果进行惩罚。

```text
def label_smoothing(inputs, epsilon=0.1):
    K = inputs.get_shape().as_list()[-1]    # number of channels
    return ((1-epsilon) * inputs) + (epsilon / K)
```

**6.可利用的训练策略，主要是学习率策略**

每隔一段时间 warm restart学习率，这样在单位时间内能得到多个而不是一个 converged local minina，做融合的话手上的模型会多很多。

![img](https://pic4.zhimg.com/80/v2-2ecb37ad307937e1fdaff387844331ef_hd.jpg)

**7.更高级的优化器**

**LookAhead等优化器。**

Lookahead 算法与已有的方法完全不同，它迭代地更新两组权重。直观来说，Lookahead 算法通过提前观察另一个优化器生成的 fast weights序列，来选择搜索方向。该研究发现，Lookahead 算法能够提升学习稳定性，不仅降低了调参需要的功夫，同时还能提升收敛速度与效果。

![img](https://pic4.zhimg.com/80/v2-54fe7bfce2f4658006e0a9188f1ab097_hd.jpg)

**8.数据增强技术**

无论对于什么样的数据集，规模多大的数据集，在语义分割任务中，数据随机缩放给模型带来的收益永远是最大的。我们在训练网络时，如果将 padding 给的过大，而卷积核大小不变你会发现我们的卷积核能力被弱化，过大的 padding 增加了数据集的单一性，假设你对一张大小为 16x16 的图像增加了 64x64 的 padding，甚至更大，大到我们本来的图像在在增加 padding 之后显的很渺小，那这新的图像对于卷积核来看，基本上就是一张呼呼的图像，如果整个数据集都是这种图像，那对于卷积核来讲，太单一了，当然也没有人会这样做。数据集的多样性，包括尺度多样性，光照多样性，目标姿态多样性。尺度多样性不仅在深度学习中使用，在传统计算机视觉中也是很重要的方法。

对于随机翻转，镜像翻转，色彩偏移等数据增强技术手段为模型所带来的收益加一起也不一定有尺度的随机缩放所带来的效果好。

**9.更高明的数据预处理**

**最常见的就是标准化与归一化**

数据的标准化（normalization）是**将数据按比例缩放**，**使之落入一个小的特定区间**。在某些比较和评价的指标处理中经常会用到，**去除数据的单位限制**，将其转化为无量纲的纯数值，**便于不同单位或量级的指标能够进行比较和加权**。目前数据标准化方法有多种，归结起来可以分为直线型方法(如极值法、标准差法)、折线型方法(如三折线法)、曲线型方法(如半正态性分布)。不同的标准化方法，对系统的评价结果会产生不同的影响，然而不幸的是，在数据标准化方法的选择上，还没有通用的法则可以遵循。

数据的归一化处理，即将数据统一映射到[0,1]区间上。

\1. 把数变为（0，1）之间的小数
主要是为了数据处理方便提出来的，把数据映射到 0～1范围之内处理，更加便捷快速，应该归到数字信号处理范畴之内。
\2. 把有量纲表达式变为无量纲表达式
归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。

**10.模型的集成**

已知神经网络的集合比单个网络更健壮和准确。然而，为模型平均训练多个深度网络在计算上是昂贵的。前面有讲，通过循环调整网络的学习率可使网络依次收敛到不同的局部最优解也可达到模型集成的作用，此处所讲的主要是**多模型集成**。

（1）**直接平均法**

直接平均法是最简单有效的多模型集成方法，通过直接平均不同模型产生的类别置信度得到最后额预测结果

（2）**加权平均法**

加权平均法是直接平均的基础上加入权重来调节不同模型输出间的重要程度。

（3）**投票法（voting）**

投票法中最常用的是多数表决法。表决前需先将各自模型返回的预测置信度 sisi 转化为预测类别，即最高置信度对应的类别标记 cici ∈ {1, 2, … , C} 作为该模型的预测结果。多数表决法中在得到样本 x 的最终预测时，若某预测类别获得一半以上模型投票，则该样本预测结果为该类别; 若对该样本无任何类别获得一半以上投票，则拒绝作出预测。投票法中另一种常用方法是相对多数表决法 , 与多数表决法会输出“拒绝预测”不同的是，相对多数表决法一定会返回某个类别作为预测结果, 因为相对多数表决是选取投票数最高的类别作为最后预测结果。

（4）**堆叠法**

堆叠法又称“二次集成法”是一种高阶的集成学习算法。在刚才的例子中，样本 x 作为学习算法或网络模型的输入, sisi 作为第 i 个模型的类别置信度输出，整个学习过程可记作一阶学习过程。堆叠法则是以一阶学习过程的输出作为输入开展二阶学习过程, 有时也称作“元学习”。拿刚才的例子来说，对于样本 x, 堆叠法的输入是 N 个模型的预测置信度，这些置信度可以级联作为新的特征表示。之后基于这样的”特征表示”训练学习器将其映射到样本原本的标记空间。此时学习器可为任何学习算法习得的模型，如支持向量机，随机森林 ，当然也可以是神经网络模型。不过在此需要指出的是，堆叠法有较大过拟合风险。

**11.Dropout等技术。**

在神经网络中 DropOut 层的主要作用是防止权值过度拟合，增强学习能力。DropOut层的原理是，输入经过 DropOut 层之后，随机使部分神经元不工作（权值为 0），即只激活部分神经元，结果是这次迭代的向前和向后传播只有部分权值得到学习，即改变权值。

因此，DropOut层服从二项分布，结果不是 0，就是 1，在 CNN 中可以设定其为 0 或 1 的概率来到达每次只让百分之几的神经元参与训练或者测试。在 Bayesian SegNet中，SegNet作者把概率设置为 0.5，即每次只有一半的神经元在工作。因为每次只训练部分权值，可以很清楚地知道，DropOut层会导致学习速度减慢。




# 相关

- [2019年最新基于深度学习的语义分割技术讲解](https://zhuanlan.zhihu.com/p/76418243)
