---
title: 张钹 朱松纯 黄铁军等同台激辩 人工智能的能与不能
toc: true
date: 2019-11-17
---


情感伴侣




人工智能的知识来源：

- 知识和经验。特别是常识
- 数据

尝试：

- 推理模型，不成功


当前基于数据的学习的局限：

- 第一个条件是信息基本上是完全确定的；
- 第二是环境基本上是结构化的，如果有变化，变化需要是可预测的；
- 第三是限定领域；
- 第四是关键重要的场景不能用


主要是因为这个技术有四大缺点：不可靠、不可信、不安全、难以推广，这就是单纯数据驱动造成的问题。


因此需要加入知识和常识。

比如：

- 现在所有的翻译系统，“说你行的人行”这句话一定翻译不好，最好的翻译系统也翻得一塌糊涂。为什么出现这样的错误？因为没有常识。
- 自动驾驶也是如此，关键场景不可靠，经常会犯人类不可能犯的大错。


下一步最重要的是要把数据与知识结合起来，这样才能做好系统。




还有一个分别：

- 至于“能”和“不能”，如果能够把任务定义清楚，即某一个单一任务，并且有数据，基本上都可以做。我极力反对百度、谷歌宣称在某些任务上超过人类的做法，因为那只是定义了一个任务，如果重新定义新的任务，人工智能可能就不奏效了。这就是核心的分水岭，即能够做多大的任务、多少任务，是否能自己定义任务。




第一个问题是实现人工智能有几个基本的要素？
第二个问题是实现人工智能的自我意识最基本的元素是什么？

- 一个载体。一个类似甚至超越人类的神经系统的物理载体。类似于神经系统的可以被训练、可以被改变调整的物理载体。
- 一个复杂的环境，其实这个很容易。地球就是一个很复杂的环境，要有一个自然的环境，或者一个训练仿真的环境，无论如何要让它有尝试失败、成功的环境。



有了这两个条件之后，就可以进行交互、尝试，然后不断地去训练。在这个过程中会发生很多种可能性，包括自我意识的产生。


不了解，并不是说没有可能做出来：

- 中国发明指南针的时候，谁告诉我们有一种东西能够沿着地磁的方向去指南？没有。那时候科学还没有、还不存在。那时候根本不知道电磁学、地球、磁场，所有的知识理论都没有。但是这没有影响中国人发明指南针。
  - 在宋朝的时候绝对解释不清楚司南为什么老往南指，我们觉得很神秘，但是是可重复、可实现的装置。这种装置引导着人类前行进入了航海时代，发现了地球是圆的，对科学的发展起到了很大的作用。今天我们才明白了为什么是指南的。

回过头：

- 我们做一个有自我意识的系统，前提是先去做这个系统，而不是解释人智能背后的原理问题，它不是科学问题、哲学问题，而是技术问题。












我们是否能抛开意识形态、自我意识做一些更具体的解释？比如是否能自动进行任务分解，分解以后针对不同的任务用一种大神经网络、小神经网络，甚至自动机器学习的方法全部自动分解以后做自动学习？这样是否就可以实现技术层面的人工智能？


我觉得人的意识是说不清楚的，因为人的进化系统不是最优的系统，是多少年沉淀慢慢积累下来的系统，很多东西是跟我们的神经系统相关的。很大程度上它依赖于我们这个特定的系统。

我同意黄铁军说的情况，这样 人工智能来解决意识的话本身不是一个问题。你可以认为里面所有的状态，都可以随机被调用、抽取出来，也就是意识的表达，这倒不是一个很大的问题。

回到刚才的问题，技术能不能把任务分解，用每个不同的任务去训练，然后总结在一块。我不否认这种路线，就是说我们人脑里面有很多不同的区，我的同事用不同的芯片来做不同的任务，他发现比如说人做加法和减法都是完全不同的区域在做，这就是一个问题。

至于特定的怎么总和起来达成一个自洽的解释，这有另外的算法来做。这种计算的机理我们也考虑过，数据驱动的算法，每一个边缘分布，相当于每一个子任务都可以来算数据系统，但是互相之间要保证自洽性、完整性，必须要有联合的推理过程，这个过程是另外一个区域完成的。我倒不完全否定大数据的训练方式在工程上的作用，可能大部分的学生被黄铁军的训练集吸引过去了，或者调参去了，这是一个陷阱。

黄铁军：刚才的问题我想是否定的回答，我可以肯定地说“NO”。因为我想这位听众想问的问题，实际上是能不能设计一种机器，把任意的任务分解，分别有其他的机器完成，最后是全自动了，智能问题就解决了，我想他背后的意思是这个，这个显然是“NO”。因为只要读一下图灵的论文就知道答案，1936年的文章说的很清楚，因为不可计算理论的奠基才有了计算机。

我没有时间展开， 我的意思是结论很清楚，不可能设计一台机器把所有可计算的过程来自动完成，不存在这样的机器，更不存在你说的一个机器能分配任务。你想象中强大的机器是不存在的，逻辑上就不可能，而不是说任务的难度和不难的问题。

张钹：其实大家说的意见是一致的，刚才黄教授也表示争论机器是否意识的条件不成熟；第二，我并不反对做研究，只是说不要去争论，按照自己的观点去实践，我们应该鼓励。最后有一点不同的，哲学家肯定会去讨论这个问题，我们也无法阻止他们去讨论。而且我认为这个讨论肯定是有益处的，哲学会讨论出一些很有启发性的思想。

黄铁军：最后这一点我同意张老师，哲学家不仅可以讨论，确实需要讨论。我希望他们讨论的是如果技术走的太快，捅出篓子来怎么可以和谐的问题，但是让哲学家给我们提出路线，我觉得不太现实。



# 实现AI不会一蹴而就，AI发展应该分级吗？

唐杰：很多人工智能技术可以实现，有的不容易实现，人工智能的发展不可能一蹴而就，一下子就拥有自我意识，甚至有些技术自动化也是有必要的。我们是否有必要和自动驾驶一样对人工智能发展进行分级，甚至是认证，比如哪家公司的某项技术，或者某个学校的技术达到人工智能一级、二级、三级、四级、五级？这个分级怎么分？

文继荣：我记得上次跟张老师在 AI Time 讨论的时候就在想，一个程序到底为什么是人工智能的程序，而不是一个普通程序？我不知道各位想没想过这个问题，你怎么判断一个程序是人工智能还是普通的？我后来琢磨了这个事情，其实有一个隐含的判断标准，这个程序是不是具有一些人才有的特质。比如说人跟传统的程序比起来有一些特性包括了什么？

- 第一是灵活性，它能够处理各种不确定性，有各种不确定性，灵活性、可泛化等等。
- 第二是有自适应性，能够适应不同的变化，能够自学习。
- 第三是可解释性。其实可解释性的事情对人是很重要的，背后的含义是人的理性。我们做任何东西背后是有理性的支持，这是可解释性。
- 第四，还有创造性和自主性等等。

我自己根据这些特性，把人工智能做了一个分级。

- Level 0 是能够完成固定逻辑任务的，就是预定义逻辑任务，其实就是现在说的普通的程序。
- Level 1 是定义为完成特定领域任务，不管用什么方法，用大数据方法或其他方法也好，它可以处理不确定的输入、可泛化，像人脸识别。
- Level 2 是可以对结果行为可解释，能够从相关到因果。这个事情很重要，为什么要有理性，其实是因为我们想结果可控。刚才大家讨论了很多，基于大数据深度学习和可解释性是有一些矛盾和冲突的。
- Level 3 是刚才朱教授谈到的通用人工智能，实际上这一块就需要能够具有多领域的自适应性，并且具有常识推理能力等等，使得能够在多个领域自由游走。
- Level 4 应该是让系统具有一些创造性。AI产生的一些新的、有意义的东西，比如真的能够写出一部小说了，但是这个东西必须是要有意义的，所以这是创造性，也是人很重要的特性。
- Level 5 是具有自我意识，刚才已经进行了哲学层面的讨论，如果到了这一级是真的有生命了。

我立了一个靶子，请各位老师来批判。

朱松纯：我觉得这个分级在各个垂直应用行业是非常有意义的，因为目标和任务非常清楚。但是作为一个通用人工智能的分级，整体的分级我觉得目前还很难。因为到底人工智能包含有多少个模块，包含有多少个功能，在整体的框架没有说清楚之前，在维度没有说清楚之前，去分析的话可能会引起很大的争论。

关于这个问题，更简单的情况下把它说成是智商。比如说智商到底有多少个维度，为什么我们测人的智商是从0到 150，这其实就是个分级，人也可以分级。我们最近也在研究这个事情，美国也在研究这个事情，为什么我们比动物聪明，为什么有的人比我聪明，基本的机制在哪儿？天才到底是因为什么变成了天才，比如数学天才或者下棋的天才。这就是有很多种维度，这个维度在大的空间里面到底是有限的还是无限的，那就说到了任务的边界在哪儿。到底有多少个任务，智能的边界在哪儿，智能极限在那，或者学习的终极极限在哪儿。

原来图灵有一个停机问题，刚才黄铁军也提到了。我们其实在学习也有个停机问题，我提出了学习的极限。 是什么东西决定了我们智商的极限、学习的极限，这个是可以作为一个科学的问题来研究。

张钹：我基本上同意他的意见，对于具体问题要进行一些分级，这对于我们的研究很意义。比如自动驾驶现在分成五级，这样就可以知道这个阶段究竟可以做到什么程度。

但是，我认为对整个智能进行分级很难，因为这涉及到了很多方面。什么叫聪明？智能在不同方面的表现不同，牵涉的面很宽，所以分级的可能性较小、较难。特别是对于通用人工智能一定要谨慎，要有明确的定义。最早看到他（朱松纯）讲的题目，我觉得他的胆子够大了。后来听他讲了，感觉就不一样了，等于是做大任务。我们对通用人工智能的使用要谨慎，这在人工智能历史上有过激烈的争论，到现在为止都没有搞清楚。

黄铁军：我就补充一点， 我觉得我们除了功能，还要考虑性能的维度。为什么呢？比如说下围棋，现在毫无疑问机器比人强大得多，不能说机器就超越人了，因为它靠的是速度、性能的维度。刚才提到几个级别的时候，性能一件事就足以颠覆我们对功能的分级。比如说我们昨天给朱老师看视觉芯片，速度比人眼快一千倍。这样的传感器如果照着这样的感知，动作也是这样，推理也是这样的，就可能远远超越人类了。

我们原来很多时候想的以人的功能为标准，制定了级别，又考虑场景，在被性能冲击的时候可能完全就不适用了。



# AI隐私问题

唐杰：今天的主题是讨论“能”与“不能”，刚才大量的时间都在讨论第一个“能”，第二个”能”还没有讨论。第一个“不能”和第二个”能”的含义不同。现在随着AI技术的发展，我们可以做到很多事情，比如国内的在课堂通过视频自动观察学生有没有认真听课，深圳的交警窗口把闯红灯者的照片、姓名自动打在大屏幕上，看起来非常厉害。

今年5月份，美国加州通过了网络隐私法保护，禁止在任何公共场所使用拍照的人脸，甚至包括警察抓小偷。关于隐私问题，几位嘉宾是否有相关考虑，你们是支持更多的保护隐私，还是在当下，尤其是中国当前的情况下，可以放开隐私（问题）做更多的技术探索？

黄铁军：我认为人工智能技术嵌入到社会生活中，嵌入到方方面面过程中一定有适应过程。但是这个适应过程是双向的，既要技术适应人类社会，人类有时候也要一定程度上适应技术，构成一个和谐的共同体。

可能对于技术应该适应人类，我觉得不用论证，大家多数都会同意。但是从人类要适应技术的这一点，我们心理上也要做一定的调整。我就不举那么多例子了，比如说图灵之所以1954年去世，就是因为当年的伦理给了他致命一击。 如果没有人类伦理限制的话，今天的计算机科学、人工智能可能完全是另外一个状态，所以人类自己适当的调整是必要的。

张钹：这个问题涉及广泛，属于人工智能治理的问题。 我认为涉及到了三个层面，第一是误用人工智能技术，第二是滥用人工智能技术，第三是有益利用人工智能技术进行国家与国家之间的对抗、集团与集团之间的对抗，这三个层面是不同的，应该采取不同的治理方法。

朱松纯：我觉得隐私和自由，隐私是侵犯了自由的空间，人家看到你的时候，你做事情就不自由了。 我觉得这个问题应该是由社会中大部分人来决定。以前我说了伦理道德不是绝对的，是相对的，在人和人之间交流过程中达成的共识。道德的“德”字我原来解释过，就是一大堆人有十个人眼睛看着你心里怎么想的，这个东西合适是道德，不合适就是不道德，这是随着时间在改变的事情。

我觉得自由的话，像人脸识别系统，我们的机场、火车站都非常方便，也是大家选择的一个问题，要由公众选择。公众如果通过法律不让你这么做，那就不能做。

文继荣：第一点，这次在 AI Time 时张老师提了一个观点，我分享一下。大家可以看到在（美国）加州禁止了人脸的东西，但在中国可以看见各种人脸监控还是比较普遍的，跟这两个国家的特点有关系。可能西方更强调的是个人自由。在咱们这儿更强调集体主义。在中国一直提倡可以为了集体、国家的利益，甚至可以牺牲一些个人利益，这对这个社会大多数人来说是接受这一点，这是不同点。

在中国的话，我们不能简单说加州不让做，中国就一定要这样做，其实不是这样的。但是边界在哪里？我同意朱老师说的，这要交给人民去决定。比如说前两天看新闻，比教室监控更厉害，教室是用摄像头，而他们是给学生戴一个东西，可以监测你的脑电波，然后看你走神没走神，最后给你弄个大数据来分析你，这是不是有点走的太远了，所以有一个边界。

第二，隐私安全另一点比较重要的是人工智能要可控。现在深度学习黑箱的问题是很麻烦的事情，我是比较担心的。比如说武器，如果里面的可解释性、可控性解决不了，这才是真正危险的东西。

刘知远：实际上，随着AI技术的深入应用，已经出现了很多隐私、安全和伦理相关问题，各位嘉宾都非常认可AI技术应用应该有一些边界。很多国家和机构也都意识到了这个问题，纷纷发起了如何以负责任的态度发展人工智能的行动。

唐杰：最近各很多国家发表了AI宣言，北京也发布了《人工智能北京共识》，因为黄铁军参加了很多相关工作，我们请他简单解读一下《人工智能北京共识》。

黄铁军：发展人工智能，让它造福、服务于人类，这样的宗旨我相信是大家的共识。我不仅参加了《人工智能北京共识》的讨论，也参加了国家新一代人工智能治理委员会的一些讨论。这些原则，我想既然是大家讨论的结果，就是一个多数人的共识，我也是同意的。

但是像我刚才说的，其实在刚才的两个委员会里面我都是另类，都是提反对意见比较多的。我的一个观点，一方面在现阶段或者在未来的一段时间之内，我们确实要从人的利益、幸福感出发来考虑这个问题，但这不是永远的。

从长远来看，我个人的观点还是要跳出以人为中心的思维惯性，而要考虑智能在地球、宇宙的意义下的长远未来，这是同样要考虑的问题。在那个时候就不是说智能怎么服务于人，而是人和智能系统如何共处、共融，共同融合在一起在未来去发展下去的问题。



# AI技术安全和伦理的边界在哪里？

唐杰：人工智能的发展离不开伦理，请几位嘉宾简单论述对于伦理和 AI发展边界的观点。

张钹：一两句话说不清楚，治理应该分成几个层面，有的涉及到安全，有的涉及到伦理，作为宣言，笼统地提倡是可以的。但是具体到技术问题，还需要具体界定，这比较复杂。

朱松纯：这个问题确实是非常难，现在大概花不到1000 美金就可以造一个杀人武器，无人机和摄像头有人脸识别，定位某个人，就可以开枪了，其实这是非常危险的事情，肯定也无法阻止有人这么做。既然这个工具产生了，他拿这个工具砸银行或者干什么是别人的事情。我想科学家在这个方面上阻止不了他，只能通过社会去做。

将来最好的办法是计算机能够形成一定的伦理，它能够判断这么去做会造成的社会后果是什么，这是一般人基本的能力。因为我们社会上的每一个人都可能做一些违法的事情，但是它会考虑到后果。如果机器没有考虑到这个后果的话，每一个都是自杀 炸 弹性质的机器人，这是危险所在。

文继荣：其实这个问题特别难，我有一个很朴素的想法，大家为什么会讨论人工智能伦理，是因为大家担心让人工智能去做一些人不能做，或者不敢做的事情，或者不愿意做的事情，将来交给机器或者人工智能去做。一个简单、朴素的边界在哪里？就是在这个地方。 如果让人工智能去做对人的伦理评价这么大事情的话，实际上是要三思一下。

唐杰：医疗领域的容错率很低，在医疗领域中，未来五年哪些AI技术真的可以实现在这个场景中落地？

张钹：现在的技术实际上用在了非关键的场景，如分诊、医疗健康管理、医院管理、医疗系统的支撑系统等。现在的技术还难以支持人命关天的医疗场景，比如计算机疾病诊断用起来大家就很慎重。简单来讲，现在大家做了很多医疗图像识别系统，识别率都做的很高，有的声称超过人类。但是医生还是不敢用，为什么？因为不可解释，做出的判断解释不了原因，医生敢签字吗？所以，在关键领域还要下工夫，人命关天的问题还是要慎重，大家还没有真正普遍使用。

唐杰：我们现在研究人工智能有两个思路，一个思路是用计算机模拟人脑的思维方式，然后用人脑的思维方式求解这个问题。另一个思路是作为纯算法，然后用纯算法的思路来求解，比如可确定性的问题，或者人很难求解，但是计算可以求解的问题。这两个思路哪个在当下最可行，为什么？

黄铁军：我觉得两个思路好像是一个思路。当然我知道有点区别，一个是人把规则定下来然后去做，另外一个是训练，总的来说也没有绕出计算机的范围。因为时间已经到了，我就简单说两句。其实我昨天晚上被唐杰抓来做找茬的，只看到了题目，不知道今天讨论的不能是那个意思。实际上我脑子里想的“不能”是人工智能作为一种技术，它不能的边界在什么地方，这个事情跟你刚才提的问题有关系。

计算机的能与不能有很多讨论，也有很多著作、文章可以去看。最基本的是图灵在1936年论文说计算边界有了很清楚的结论，绝对不是无所不能，计算的范围是很有限的。因此你刚才说的两个问题在人工智能是一种方法，但是它是一种可能的有限范围内的方法。

但这并不等于人工智能永远就只能限制在这样的范围之内，我们完全可以去制造、去创造全新的系统。这个系统不一定非要计算，或者它超越了图灵机，所谓超越了图灵机因为不是在机械计算的方式在进行的。从这个意义上讲，它的能力比刚才那样的方法要大得多。 我认为人也有不能，因为人类神经元有几百亿，人也有一个智能上限的天花板。天花板的突破要靠未来的机器，机器从物理载体的能力上超越我们，这是完全有可能的。

当然谁要敢给一个说“不能”，因为说“不能”都是最伟大的贡献，你敢肯定一件事不能，能说人工智能的“不能”，那一定也是一个伟大的贡献。但是我认为说“不能”时代的远远还没有到来，而是有巨大的探索空间。

唐杰：非常感谢黄老师，最后的“不能”讲得非常好。今天我们用了一个小时探讨了人工智能的“能”与“不能”，但其实这没有标准答案，很多 AI 相关技术和应用场景，还有很长的路要走。


# 相关

- [张钹、朱松纯、黄铁军等同台激辩：人工智能的“能”与“不能”](http://blog.itpub.net/69946223/viewspace-2662881/)
