---
title: 深度学习的缺点以及如何弥补
toc: true
date: 2019-10-01
---
举个栗子，AlphaGo 虽然4：1击败了李世石，但那是学习了三千万盘棋才获得的胜利，李世石同学终其一生也才能下几万盘，李世石同学用少得多的样本，习得了接近AlphaGo的围棋棋力，而且李世石下棋时耗费的那点生物能量相比AlphaGo运行需要开启不计其数的GPU，高下立判。

人脑的高效，围绕着非常少的样本，输入一点点数据，就能够训练出在计算机看来不可思议的能力。这种高效，就是大脑的超级泛化能力。

而但凡对人工神经网络有那么一点研究的人都知道，**无论是CNN、DBN、CDBN，限制各式神经网络极限的，都是过拟合**：它们对训练数据有非常好的识别较果，此时模型会复杂且非常“不灵活”，只要换一些精心甄别的、不一样的样本，它们立刻就变成瞎子。

> 深度神经网络其实需要极其庞大的数据，才能训练出一些泛化能力，因为近些年互联网时代大数据爆发，机器不知疲倦的特性，才实现了在特定任务上、非常狭窄范围内接近或者超过人类的表现。

神经网络只是人脑复杂认知功能一部分解剖学成果，而Vicarious 的方案，是从根本上模拟和接近人脑的能力，在适应新环境、新情况、习得新技能上，这种超级泛化能力是智能的根本所在：

**Vicarious 的牛逼之处，深度神经网络用230万个样本才获得识别验证码的能力，Vicarious 仅仅才用了260个。**

![img](https://pic4.zhimg.com/80/v2-8bfe8ad3b907238eb64baae9c54614db_hd.jpg)

这是如何做到的？Vicarious 的研究人员 Dileep George 在EmTech 的一场公开演讲中谈到，

这不是像深度学习一样，训练计算机从海量样本找到CAPTCHA共有的匹配特征，而是重新让计算机自己构建一个CAPTCHA。

![img](https://pic2.zhimg.com/80/v2-4375d1f4e2ee3a859e92b188b8f9a7f5_hd.jpg)

> **“深度神经网络是从每一个样本字母a中抽象特征，Vicrious 的方法关键是让计算机自己‘创造’一个字母 a 的模型”**

Vicarious 把生物智能分为两个阶段，old brain 时代和 new brain时代，两者的区别就如下图中的青蛙一样，青蛙只看到了面前有“虫子”，不断地尝试吃，这是一种对输入信息的神经动作“映射”，而**青蛙无法理解为什么吃不到，因为它碰到了新情况时无法“虚拟”出一个屏幕的概念...**

![img](https://pic1.zhimg.com/80/v2-2ac8d0256e7ba53485d96f2c19012fa8_hd.jpg)

这种说法，很容易让人联想起《人类简史》从头到尾强调的东西：人类之所以特别，就是因为近二十万年foxP2基因的突变导致语言的出现，大脑可以虚拟一些自然界并不存在的概念，比如龙、宗教、国家、社会组织、意识形态等等。

“Vicarious 就像 AI 界的扫地僧，”

其实，只要你了解 Vicarious 老大 Scott Phoenix 对神经网络的深刻洞察，你一定会多多少少认同上面的评价。这家不太喜欢抛头露面的公司，几乎找不到第二个案例，他们一直专注在AI 根本性的突破上：

> “深度神经网络（DNN)需要大量的训练数据，不能很好地适用于新的任务或环境。”
>
> “此外深度学习往往侧重于学习输入感知与输出动作之间的映射（如用于做分类决策或者是围棋、Atari游戏上的移动的决策），对大脑功能的模拟，太过单一。”
>
> **“我们认为智能的本质是能够学习一个所处在世界的心理模型(mental model )，然后能否在这个模型上进行模拟（所谓想象力）。”**

Vicarious 强调，实现通用人工智能的路径，一是要突破神经认知科学，二需要专注于样本数据的学习效率和泛化“通用”任务的能力。

![img](https://pic3.zhimg.com/80/v2-49995b285556e12a3c8f0f4ae9164b5e_hd.jpg)


# 相关


- [Vicarious说，现在的深度学习都是渣渣](https://zhuanlan.zhihu.com/p/26035695)
