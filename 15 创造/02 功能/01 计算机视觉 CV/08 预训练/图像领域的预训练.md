---
title: 图像领域的预训练
toc: true
date: 2019-09-29
---
# 图像领域的预训练

自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，这种做法很有效，能明显促进应用的效果。

## 预训练介绍

<center>

![mark](http://images.iterate.site/blog/image/20190926/uhGH92c61MAI.png?imageslim)


</center>




那么图像领域怎么做预训练呢，上图展示了这个过程：

我们设计好网络结构以后，对于图像来说一般是 CNN 的多层叠加网络结构，可以先用某个训练集合比如训练集合 A 或者训练集合 B 对这个网络进行预先训练，在 A 任务上或者 B 任务上学会网络参数，然后存起来以备后用。

假设我们面临第三个任务 C，网络结构采取相同的网络结构，在比较浅的几层 CNN 结构，网络参数初始化的时候可以加载 A 任务或者 B 任务学习好的参数，其它 CNN 高层参数仍然随机初始化。

之后我们用 C 任务的训练数据来训练网络，此时有两种做法：

- 一种是浅层加载的参数在训练 C 任务过程中不动，这种方法被称为“Frozen”
- 另外一种是底层网络参数尽管被初始化了，在 C 任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的 C 任务。

一般图像或者视频领域要做预训练一般都这么做。


## 预训练的好处

这么做有几个好处：

- 首先，如果手头任务 C 的训练集合数据量较少的话，现阶段的好用的 CNN 比如 Resnet/Densenet/Inception等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如 ImageNet 预先训练好直接拿来初始化大部分网络结构参数，然后再用 C 任务手头比较可怜的数据量上 Fine-tuning过程去调整参数让它们更适合解决 C 任务，那事情就好办多了。这样原先训练不了的任务就能解决了。
- 即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度。


## 为什么预训练的思路是可行的



<center>

![mark](http://images.iterate.site/blog/image/20190926/Ws4QjMPX36f0.png?imageslim)

</center>



目前我们已经知道，对于层级的 CNN 结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。

正因为此，所以**预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性**，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用 Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。

## 一般用来做预训练的数据集


一般我们喜欢用 ImageNet 来做网络的预训练，主要有两点：

- 一方面 ImageNet 是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；
- 另外一方面因为 ImageNet 有 1000 类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。分量足的万金油当然老少通吃，人人喜爱。




# 相关

- [从 Word Embedding到 Bert 模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)
