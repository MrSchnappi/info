---
title: 01 卷积神经网络
toc: true
date: 2018-08-29
---
# 第 4 章深度卷积神经网络

## 可以补充进来的

- <span style="color:red;">这个也没有进行整理，把神经网络部分的内容融合之后，把这个整理进去</span>



。关于分布式表征的更详细内容将在第 4 章进行介绍

通过前 3 章的讲述，我们已经知道其实深度学习并不是什么新鲜概念了，就是多层的 神经网络而已，只不过 2006 年以前并没有变得实用起来。目前，深度学习获得成功的领域 主要是图像和语音，而比起语音来，图像领域基于深度学习的应用又是最多样化的。

本章将介绍基于深度学习的图像和计算机视觉中最基础的结构：卷积神经网络。并通 过介绍 4 种最典型的结构来一步步揭开深度卷积神经网络的面纱。

4.1卷积神经网络

本节介绍现在主流卷积神经网络中最基本的概念和结构，以及从深度学习角度的定性 理解。

4.1.1 基本概念

第 1 章里讲述神经网络历史的时候就提到过，早在 1989 年，LeCun就发明了卷积神经 网络，并且被广泛应用于美国的很多银行系统中，用来识别支票上的手写数字。

卷积神经网络是受到了动物视觉神经系统的启发，所以先来简单了解一下视觉神经系 统。眼睛是一个成像系统，图像通过瞳孔、晶状体最终在视网膜上成像，这一部分是视觉 的光学系统。视网膜上布满了大量的光感受细胞，可以把光刺激转换成神经冲动，从这些 细胞开始，就进入了视觉神经系统。神经冲动经过视觉通路(视觉神经-4见交叉一视束一 外侧膝状体一视放射)传递进大脑的初级视觉皮层。从视觉皮层开始，对图像信息的处理 也是分层的，这些层分别是从 VI 到 V5 (V代表 Visual)，每一层处理一部分特定的信息, 比如 VI 主要对一些边缘纹理响应，V4则对一些简单的形状响应，而信号传递的大概顺序 也是从 VI—V5，需要注意的是这种大概顺序并不是一层层的简单连接，比如 V2 和其他所 有层都有连接。

上面只是大概提一下神经科学中的分层结构，真正对现在的计算机视觉尤其是对卷积 神经网络启发很大的发现，是加拿大科学家大卫•休伯尔(David Hubei)瑞典科学家托斯 坦•维厄瑟尔(Torsten Wiesel)从 1958 年起对猫视觉皮层的研究。他们在 VI 皮层里发现 了两种细胞，简单细胞(Simple Cells)和复杂细胞(Complex Cells)，这两种细胞都有一 个特点就是每个细胞只对特定方向的条形图样刺激有反应，也就是说，这些细胞是有方向 性选择的。这两种细胞的主要区别是简单细胞对应的视网膜上的光感受细胞所在的区域很 小，而复杂细胞则对应更大的区域，这个区域被称作感受野(Receptive Field)。这种结构 如果简化一下形象理解如图 4-1所示。

模拟这种概念的结构早在 1980 年计算机界已经有人提出来了，是日本人 Kunihiko Fukushima提出的 Neocognitron。现在的卷积神经网络结构深受 Neocognitron 的影响，简 单来说就是卷积层用来模拟对特定图案的响应，而池化层模拟感受野。基本的结构如图 4-2

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-183.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-184.jpg)

图 4-2卷积神经网络中卷积层+池化层的基本结构

图 4-2中所示的是卷积神经网络中最基础的一种结构，输入首先经过卷积层得到响应， 然后经过激活层做非线性变换。根据在第 1 章中的讨论，卷积本身也可以看做是一种线性 变换，再考虑到偏置项，这种放射变换+非线性变换的操作其实就是一般神经网络中基本 操作的一种特殊情况。所以相对来说更特殊一点的是激活层后面的'池 2 七层。接下来的小节 就稍微深入一些来分别讨论卷积层和池化层。

4.1.2卷积层和特征响应图

第 2 章已经讨论过了卷积，深度学习中使用的卷积和第 2 章讨论的卷积基本一致，唯 一可能的区别是在不同的深度学习框架中，有的卷积实现就是卷积，而有的卷积实现是互 相关。第 2 章中也讨论过了互相关，本质上和卷积并无不同只是顺序相反，并且不满足交 换律。互相关的好处是卷积核在视觉上更加直观，理解起来也更容易，所以本书中演示卷 积原理的例子都是用互相关。在深度学习中，我们把和卷积核做卷积之后得到的结果叫 做特征响应图(feature map)，从名字上理解，如果认为卷积核是代表着某种特征的话， 那么卷积的结果就是输入对这种特征的响应。下面来看一个例子帮助直观理解，如图 4_3 所示。

图 4-3中是一棵椰树的照片，其中椰树的叶子部分都是条状的纹理。我们用图像处理 中常用来做边缘检测的滤波核函数 Gabor 核函数，按照不同朝向旋转后，和椰树的图像做 卷积。得到的结果就是下面一排图像，为了方便展示，响应图像取了绝对值，白色是值比 较大的部分，可以看到不同的卷积核得到的不同的响应。和 Gabor 核函数朝向一致或相近 的叶子纹理，在特征响应图中都比较明显。这就模拟了 Hubei试验中神经细胞对不同方向 线条的响应现象，也是卷积操作最容易直观理解的一个功能。图 4-3虽然用了最简单的卷 积核，不过响应图有些杂乱，为加深理解，再来看一下图 4-4所示的例子。

*



abs^



*



abs^

abs\J^



图 4-3图像对不同朝向的 Gabor 卷积核的响应图



图 4-4图像对稍微复杂的卷积核的响应

图 4-4中上面一排的照片的画面中是 5 架战斗机，把其中一架战斗机的图像截取出来, 减去均值作为卷积核。然后和原图像做卷积，得到结果如下面一排图像所示。为了方便画 图，同样取了绝对值。可以很明显看到，每架战机所在的位置得到了一个极大响应。总结

一下就是卷积核可以找到图像中和自身纹理最相似的部分，并且相似度越高，得到的响应

值越大。其实在第 2 章我们已经得到过类似的结论。

同时还注意到第一张画面中 5 架战机刚刚飞入画面，得到的响应图里五个极大值点也

是对应 5 架战机的位置，第二张画面中 5 架战机飞到了画面更中央的位置，对应的 5 个极 大值点也到了画面中央。卷积的这个性质有个挺正式的名称叫做同变性(equivariance)。 简单来说就是，如果想象有个框可以在图像上“移动”，不管是先对整张图做卷积再移动 框到指定位置获得响应图，还是先移动框到指定位置，再做卷积得到响应图，结果都一样。 更白话一些就是卷积操作能够一定程度上保留位置信息，定性来看是个显而易见的结论。

4.1.3参数共享

同变性的来源是卷积的另一个性质，在很多教科书中都会讲到，叫做参数共享 (parameter sharing)。名字虽然取得像模像样，其实意思就是说卷积核在任何一个位置都 是不变的。所以在卷积核所在的区域，所有像素和卷积核对应位置相乘求和的过程中，相

当于都是和同一套权重相乘，这就是“共享”的意思，如图 4-5所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-190.jpg)

| 0    | 1    | 0    |
| ---- | ---- | ---- |
| 1    | -4   | 1    |
| 0    | 1    | 0    |

| 255   | 254  | 255  | 255   | 253  | 255  | 250   | 255  | 252  | 255  | 253  | 255  | 255  | 255  | 253  | 255  |
| ----- | ---- | ---- | ----- | ---- | ---- | ----- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 255   | 253  | 250  | 255   | 253  | 254* | 255   | 255  | 255  | 255  | 254  | 252  | 255/ | 253  | 255  | 252  |
| 254   |      | 188  | 31    | 0    | 2    | 0     | 1    | 0    | 0    | 31   | 163  | 254  | 252  | 254  | 255  |
|       | 250  | 32   | 0     | 1    | 0    | 2     | 0    | 4    | 0    | 0    | 7    | 254  | 255  | 255  | 252  |
| 255   | 255  | 134  | 3     | 0    | 3    | 0     | 0    | 0    | 0    | 0    | 107  | 255  | 251  | 254  | 255  |
| 255   | 253  | 252  | 255   | 戶   | 178  | 18    | 0    | 4    | 0    | 110  | 255  | 251  | 255  | 254  | 255  |
| 252   | 255  | 255  | 2X；  | 180  | 20   | 1     | 0    | 0    | 106  | 255  | 250  | 255  | 255  | 252  | 253  |
| Z5S-  | -解- |      | "255^ | 25   | 0    | 0     | 2    | 104  | 254  | 253  | 255  | 255  | 253  | 255  | 255  |
| 255   | 25,/ | 255  | 150； | 3    | 0    | 0     | 108  | 255  | 255  | 254  | 255  | 251  | 255  | 255  | 253  |
| T     | /254 | 187  | 16 :  | 0    | 5    | 87    | 252  | 253  | 255  | 255  | 255  | 255  | 254  | 255  | 255  |
| .'254 | 184  | 16   | 0 ；  | 6    | 0    | 250   | 255  | 255  | 255  | 253_ | 。讲• |      |      | 254  | 255  |
| 254   | 27   | 0    | 1 !   | 3    | 0    | .-0-- |      | 丫   | 1    | 0    | 3    | 1    | 34   | 162  | 253  |
| 255-  |      | 一   | 一 4」 | 0    | 0    | 5     | 2    | 1    | 2    | 2    | 0    | 0    | 0    | 8    | 255  |
| 255   | 134  | 3    | 0     | 2    | 1    | 0     | 0    | 0    | 0    | 1    | 0    | °    | 7    | 99   | 255  |
| 255   | 253  | 255  | 255   | 25S  | 251  | 255   | 255  | 255  | 254  | 254  | 255  | 255  | 249  | 255  | 252  |
| 255   | 255  | 匕   | 254   | 255  | 255  | 254   | 255  | 254  | 255  | 255  | 253  | -    | 255  | 255  | 255  |

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-191.jpg)

、/

| 188x0 | 31x1 | 0x0  |
| ----- | ---- | ---- |
| 32x1  | ox-4 | 1x1  |
| 134x0 | 3x1  | 0x0  |

| 255x0  | 255x1  | 253*0 |
| ------ | ------ | ----- |
| 255»JL | 251x~4 | 255«1 |
| 255x0  | 255x1  | 254*0 |

图 4-5参数共享

图 4-5中左上角是一幅灰度图像，图中是个字母 z

。图像的每个像素在计算机中用一

个 8 位的无符号整数表示，取值范围是 0〜255。其中越亮的部分值越大，所以在计算机看 来，图像是如图 4-5中右边所示的一系列值。让这个图像和 Laplacian 算子做卷积，可以得 到边缘强度的响应（Laplacian算子从定义是在求梯度的散度）。这里为了和右边的数值相 对应，直接画出了响应图的大小，白色代表值越大，黑色代表值越小，0值是灰色。

回到正题来看参数共享，其实很简单就是图 4-5中两个虚线框，虽然位置不同，像素 值不同，但是对应位置乘的系数都是一样的，这个表示在图 4-5中右下角的两个对应位置 乘积的示意图里了。这个显而易见的性质之所以叫“共享”，是因为在一般的神经网络中， 每层输出节点对应的参数是没有任何共享或是说关联的。比如把卷积前的每一个像素看作 输入节点，卷积后得到的响应图的每一个像素看作是输出节点的话，在一般的全连接情况 下，显然响应图中的每个节点对应的权重是可以不一样的。



4.1.4稀疏连接

第 2 章已经讨论过用矩阵乘法来表示卷积，变换矩阵是一个块循环矩阵。这种矩阵的 一个性质就是稀疏性，除非卷积核和图像差不多大，否则这种矩阵中绝大多数元素都为 0。 这种稀疏性也是卷积神经网络相比一般神经网络的一个巨大优点，稀疏性不仅让模型的拟 合能力得到了限制，而且还极大降低了模型的计算量。我们通过一个简单的例子来理解一 下稀疏连接的性质，如图 4-6所示。



图 4-6卷积神经网络稀疏性示意图

图 4_6中是一个 4x4=16的二维输入和 3x3=9的二维输出。图 4-6中上图展示的全连接 的情况，每条边代表一个权值，一共有 16x9=144个权值。图 4-6中下图代表的是卷积的情 况，卷积的形式是 valid，卷积核大小是 2x2=4, —共是 4x9=36个权值，只有全连接的 1/4, 从视觉上就一眼能看到下图中的边数目少了很多。为了方便作图演示这个例子用的是 4x4 输入，假设 256x256=65536的输入，则输出的数目是 255x255=65025，于是全连接情况下 一共需要 65536x65025=4261478400个权值；而卷积则只需要卷积核大小乘以输出数目也 就是 4x65025=260100个权值，一下子小了 5个数量级。总之和全连接情况比起来，卷积 需要的权值数目的比例就是卷积核大小和输入大小的比值，这还是最一般形式的卷积。在 第 2 章讲过卷积核“滑动”是可以按照一定间隔的，这个间隔叫 stride，如果考虑 stride〉l 的情况，则需要的权值数目还会更少，连接更加稀疏。

4.1.5多通道卷积

深度学习中无论是输入，还是中间产生的特征响应图，通常都不是单一的二维图，而 是多个，每一张图称为一个通道（channel）。计算机的图像表示中，通道是一个非常基础 的概念，比如一幅彩色图像通常是由红色（Red）、绿色（Green）和蓝色（Blue）三个通 道构成，分别代表了一幅彩色图像中红、绿和蓝三种成分的值，这样的三，渾道图像就是 RGB 图像，是最常见的一种图像表示。

对于多通道的输入，处理非常简单，就是给每个通道用不同的卷积核做卷积，然后结 果加一块就可以了。所以卷积层本身也成了多通道的，再加上每过一个卷积层通常会输出 多个特征响应图，比如图 4-3中的例子，3个 Gabor 卷积核对应了 3张不同的响应图，也 就是说输出也是多通道的。这样一来卷积层的权值就成了一个 4 维的张量。

对于一个一般的卷积层，我们考虑输入为 X，其中每个通道为和输出为 y，其中每个 通道为的话，从 x 计算卷积得到多通道特征响应图^的公式如下：

yj =    * %）    （公式 4-1）

其中 wy 代表第 y 个特征响应图对应的卷积层中，与第/个输入通道进行卷积的卷积核， g是激活函数。

4.1.6激活函数

讲到这里还没有讲过卷积层的激活函数，其实和全连接层没有大的区别，就是给卷积 结果做个非线性变换，比如还是以图 4-4中战机照片为例，之前为了方便作图，都是直接 把得到的特征响应图取了绝对值，这个操作其实已经是一种非线性变换，可以看作是一种 激活函数。不过在卷积神经网络中，更常用的是 ReLU 激活函数，也就是只取响应大于 0 的部分。具体到图 4-4中的例子就是把小于 0 的响应都忽略掉，只保留相关程度大的信息， 这种对比如图 4-7所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-193.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-194.jpg)

ReLU

+&[=>



图 4-7激活函数在卷积层中的作用

可以看到，和绝对值激活的结果比起来，ReLU的结果少了战机位置周围那些像雾一 样笼罩的区域，这些区域正好是没有什么用的信息。如果更近一步在激活之前找到一个合 适的偏置值，得到的结果就是图 4-7中最下面的图像。背景中少量的噪音也没有了，剩下 的只是 5 架战机的大概位置。

4.1.7池化、不变性和感受野

池化(pooling)大概是卷积神经网络和普通神经网络最不同的地方，池化这个概念 本身并不是一个具体的操作，而是代表着一种对统计信息的提取，从字面上看就是把东 西放到一起的意思。比如，一堆样本不好处理，我们求一下样本的平均值来代表这些样 本，这就是一种池化。又比如给了-堆数字，按照一定的区间画出这些数字分布的直方 图，这也是一种池化。在卷积神经网络中，池化一般代表对特征响应图上的一个给定区 域求出一个能代表这个区域特点的值，在深度学习中最常见的两种池化大概是最大值池 化(max-pooling)和平均值池化(average-pooling), 一起来看图 4-8所示的例子理解

一下。



max-pooling | ■■

| 255  | 255  | 255  |
| ---- | ---- | ---- |
| 255  | 31   | 2    |
| 2S5  | 255  | 178  |



图 4-8池化示意图

图 4-8左边是一个卷积完之后得到的响应图的例子，是一个 6x6 的图。我们做一个 2x2 的池化，也就是按照 2x2 的大小将 6x6 的图划分为 9 个 2x2 的小区域，然后对每个小区域 执行池化操作。最大值池化就是取这个小区域里的最大值作为结果，这个结果展示在图 4-8 右上图。平均值池化就是取这个小区域里的平均值，结果展示在图 4-~右下图。

另外在第 2 章讲卷积的时候介绍过，卷积执行的时候可以按照，定的间隔来“划动采 样” 一个小区域，这个间隔叫做 stride。池化操作这点和卷积类似，也可以按照一定的 stride 来进行。图 4-8中的例子是 stride=2，和池化区域的大小相同，是为了举例子可视化方便。 实际的网络结构中，池化的 stride 常常要小于池化区域的边长，这样能使相邻池化区域有 一定的重叠。最常见的情况是 stride 等于池化区域边长减 1，这种间隔下池化一定程度上还 起到了降采样的作用。

池化层最直接的作用是引入了不变性，比如最常用的最大值池化，因为取一片区域的 最大值，所以这个最大值在该区域内无论在哪，最大值池化之后都是它，所以在池化区域 大小内的任何位移都不会对结果产生影响，相当于对微小位移的不变性。如图 4-9a中演示 了这种性质，图 4-9中的例子是对一个 6x6 的响应图按照 3x3 大小进行池化，最后会生成 一个 2x2 的新的响应图。例子中演示了两种情况的响应图，第一种情况在 3x3 区域内最大 值 11 出现的位置是左上角，第二种情况最大值 11 出现在中间一列最下面一行。因为最大 值池化的操作就是取最大值，所以只要最大值 11 出现在这个 3x3 区域内，无论具体位置 在哪，最后在池化后的响应图中都不会变，这就是对微小位移的不变性。从仿生的角度理 解这个现象，池化相当于模拟了感受野，感受野内任何一点出现了响应，这个响应都能够 传播到池化后的层里。下面还是通过图 4-9来理解一下。

图 4-9中画出了新的特征响应图里左上角对应的 3x3 区域，这个区域内对任何特征 出现的一个值很大的响应，都会传播到 2x2 响应图中左上角的值里。这里画出的只是一 个池化层，想象一下如果是更大的特征响应图，后面接了更多的卷积一激活一池化层的 话，那么越是高的层里的一个节点，就会在低层中对应越大的一个感受野，如图 4-9b 所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-198.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-199.jpg)

4.1.8 分布式表征(Distributed Representation)

分布式表征并不是卷积神经网络特有的性质，而是神经网络最重要的性质之一。从名 字并不容易看出来到底是什么意思，其实就是个特别的自然概念，举个例子来理解一下。 比如人们去体检的时候，最基本的信息是要填写性别、年龄、身高和体重，如表 4-1所示。

表 4-1体检中用身高体重年龄描述个人基本信息

| 姓    名      | 性    另 IJ | 年    龄 | 身    高 | 体    重 |
| ------------- | ---------- | -------- | -------- | -------- |
| 张三          | 男         | 18       | 172      | 66       |
| 尼古拉斯•赵四 | 男         | 49       | 171      | 70       |
| 王五          | 女         | 30       | 166      | 53       |

其实这就可以看作是一种分布式的表征，描述一个人的个体的时候用到了多种属性， 另外每种属性又用来描述多个不同的人的个体。这是一种多对多的描述关系，所以叫分布 式(distributed)。表 4-1所示的例子中，如果定义性别为男表示为 1，女表示为 0，则在 这种表示形式下，每个个体可以被表达成一个向量，比如赵四就是(1,49,171，70)，王五是 (0, 30, 166, 53)。因为都被表达成了定长的向量，在这个向量所在的空间里就可以比较两个 个体的相似性。

这种多对多的形式，恰好和神经网络是一样的。比如全连接网络，每层的输入都会对 所有输出产生影响，而每个输出也会收到所有输入的影响。分布式表征拥有强大的表达能 力，比如考虑每个神经元把空间划分为两个区域，那么〃个神经元就可以表达 2”个不同的 概念。和分布式表征相对的是局部表征(localist representation)或者叫符号式表征(symbolic representation)，这些奇怪的名字的来源其实和第 1 章讲的符号派和连接派有一定联系。 局部表征的意思就是一个概念用一个编码或样本来表达，最被人熟知的就是独热编码 (one-hot)编码，比如下面表 4-2就是对十进制数字 0〜7的编码，采用二进制编码就是分 布式表征，采用独热编码就是局部表征。

表 4-2独热编码和二进制编码

| 数    字 | 独热编码 | 二进制编码 |
| -------- | -------- | ---------- |
| 0        | 10000000 | 000        |
| 1        | 01000000 | 001        |
| 2        | 00100000 | 010        |
| 3        | 00010000 | 011        |
| 4        | 00001000 | 100        |
| 5        | 00000100 | 101        |
| 6        | 00000010 | 110        |
| 7        | 00000001 | 111        |

可以看到分布式的二进制编码只用长度为 3 的编码就表达了 8个概念，而独热编码则 是多少个概念就需要多少长度的编码。神经网络既然就是分布式表征，那是否也能学到编 码呢？答案是肯定的，事实上表 4-2所示的例子就是神经网络学习编码的经典例子，在第 2章里讲降维的时候也提到过，如图 4-10所示。



图 4-10学习二进制编码的神经网络

通过输入独热编码的值，然后学习过程是让输出重现输入，最后会发现当某一个编码 输入时，如果把隐藏层三个单元的激活值输出，激活值就能学习到不同的二进制编码。比 如输入是 10000000 时，隐藏层可能是最上边的单元激活，其他两个单元的激活值为 0；而 输入是 00000001 时，隐藏层可能是三个单元都激活。总之中间隐藏层的单元是否激活看作 0和 1 的话，学习到的编码就能够和输入一一对应。

这个例子学习的是编码，但更一般地我们认为神经网络能够学习到底层用于产生数据 分布的根本因素。比如描述一个“穿着带花裙子的女孩”，把这看成一个样本的话，其有 3个属性，即服装的颜色（带花）、服装的类型（裙子）、性别（女孩）。如果每个属性 只考虑两个取值：带花/不带花、穿裙子/不穿裙子、女孩/不是女孩，那么和 3 位二进制的 编码的例子就没区别了。

4.1.9分布式表征和局部泛化

分布式表征在在泛化角度也和局部泛化有很大区别。下面通过图 4-11所示的例子定性 理解一下。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-201.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-202.jpg)

图 4-11分布式表征和局部表征的泛化示意

假设有一些描述一种动物特点的样本，图 4-11中所示的二维平面是 6 个样本在某个特 征空间上对应的点。图 4-11中左边的情况在分布式表征下，根据描述的 3 种属性，即是否 会飞、是否会游泳、是否有腿，可以用 3 种特征来将空间划分为 8 个区域。而在图 4-11右 图中则是局部泛化的思路，我们基于距离用最近邻的策略对空间进行了划分，一共可以划 分为 6 个区域。可以看到两种策略下对空间的划分是非常不一样的。比如图 4-11右上角的 A点在分布式表征的划分中对应的是有腿会飞不会游泳的动物，而在最近邻划分中则是不 会游泳且没腿的动物。又比如图 4-11中 B 点则是有腿会飞会游泳的全能型动物，而在最近 邻划分中则是不会飞的。

假如在这个二维空间中，样本已经是线性可分的，则显然分布式表征的划分会有更好 的泛化。并且通过这个例子还能看到分布式表征的一个优点，即使样本中有信息缺失，或 是样本数量并不足，也可以根据已有的属性学到不错的泛化能力。其根本在于各个不同的 特征可以分别根据样本进行学习，比如图 4-11左图的例子，每个特征都学到了一个对应的 线性分类边界，组合在一起对 3 种属性对应的 8 种全部情况都能够在空间中找到对应的区 域。而局部泛化因为是基于函数在空间中连续的假设，所以在样本量足够的情况下也许能

很好泛化，而样本不足的时候则只能在当前的空间内基于现有样本做出类似插值的近似。 这里举出的是 3 个特征，每个特征二分类的例子。在实际应用中，特征的数目和每个

特征对应的值域都会更加丰富。并且因为每个样本都会对应一个由相同特征维度表示的向 量，这个向量中的距离比样本在原始空间中的距离会对相似性等性质有更好的度量（比如 图 4-11的例子），所以分布式表征的一个重要的意义相当于把样本从原始空间投影到一个 更好的特征空间中。

4.1.10分层表达

其实回顾一下，本节讲的内容和第 3 章讲的神经网络基础有很多类似。神经网络每经 过一层（仿射变换+非线性变换），都把样本重新在一个新的空间内表示。这个表示可以 再经过一层在另一个新的空间内表示。如果把每层中的一个维度都看作是分布式表征中的 一个维度，则相当于数据在一层一层的传播中，都在不同层面的分布式特征下得到了重新 表示。

这种分层的表达就是深度学习中的重要思想之一。其实这种思想和人的认知有很多相 同的地方，比如在学习汉字的时候，先学习了笔画，才能通过笔画组合成汉字。学会了汉 字就可以组成一些词语甚至成语，每个短的词语都能表达一个新的意 g。进一步通过词语 组成了句子，句子构成文章，文章构成更长的文章……通过一层一层&合出来的概念，能 够表达许多特别复杂而丰富的新概念。

回到本书主题，在计算机中，我们理解一幅图像，也可以考虑按照如下的层次：像 素一边缘一基本形状一纹理一复杂图案一更复杂图案……示意图如图 4-12所示。



图 4-12图像的分层次表达示意图

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-204.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-205.jpg)

如图 4-12所示，最底层的表示自然是像素，通过像素能够组成一些简单线条，线条进 一步能够构成纹理，纹理为基础则构成一些更复杂的结构，比如像轮子一样的图案，再进 -步则构成了一个像汽车一样的图像。这种表达的方式，其实正是需要多层神经网络，也 就是深度学习的-*个基本动机。

4.1.11卷积神经网络结构

我们把分层的思想结合前面两节讲的卷积层和池化层结构在一起，就是最常见的卷积 神经网络结构了。当然这只是一个示意图，实际的卷积神经网络中，未必会有这么分明的 特征分层，不过大致上特征的复杂程度确实是一层层变得更复杂，并且每一层的特征构成 都是基于前-层特征。比如图 4-13所示，画出了一个基于受限玻尔兹曼机的，3个卷积层 的卷积神经网络每层学到的特征。



图 4-13卷积神经网络中，每一层学习到的特征的例子(Honglak Lee, ICML 09)

图片来源是 2009 年 ICML 的论文《Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations》，是 3 个卷积层对人脸这个类别学习 到的特征。虽然和流行的卷积神经网络的方式有些不同，不过表达的意思和图 4-12中所示 的是一回事，后层卷积学习到的特征都是基于前面层的组合。

4.2    LeNet—第一个卷积神经网络

LeNet就是前面提到的 LeCim 在 1989 年提出的网络结构，是卷积神经网络的鼻祖。

如今在各大深度学习框架中自带的用作 Demo 目的的 LeNet 结构，是简化改进版的 LeNet-5，和原始的 LeNet-5有一些微小的差别，比如把 tanh 激活函数换成了 ReLU等，原 版的 LeNet-5 可以参考 LeCun 的 LeNet 页面 [http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/%e3%80%82%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84%e7%a4%ba)[。网络结构示](http://yann.lecun.com/exdb/lenet/%e3%80%82%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84%e7%a4%ba) 意如图 4-14所示。



fc2

I10

Softmax

I

output

在 30 年前，这是一个比较深的网络了。LeNet-5和现有的 conv—pool—ReLU的套路 还不太一样，是 convl-*pool 1 -►convZ-^poo^，然后才接上全连接层的结构。不过这种先 卷积+池化，最后再过全连接层的基本思路就奠定了下来。

LeNet-5解决的是手写数字识别问题，输入的图像均为单通道灰度图，分辨率为 28x28。 说到这里就不得不提几乎是绝大多数深度学习入门者必用的 Hello world级的数据：MNIST 数据集。MNIST 的全称是 Mixed National Institute of Standards and Technology，是 NIST 数 据集中的一个子集。MNIST—共 70 000个样本，其中 60 000个是训练集，10 000个是测 试集。这些样本都是数字已经置于图像中央的分辨率为 28x28 的灰度图，其中一半是书写 比较工整的，另一半相对潦草，这两种书写在训练集和测试集中也分别各占一半。在 MNIST 上训练一个模型，目的是区分和识别手写的 0〜9，也就是一个十分类问题。LeNet-5这么 简单的结构在 MNIST 上可以很轻松地达到 98%以上的分类准确率。

回到正题，输入图像进入网络之后，第一层卷积核为 5x5, stride为 1，输出通道数 为 20，分辨率为 24x24 (24=28-5+1)的特征响应图(图 4-14中的箭头旁边的数字表示： 通道数 x 图像高度 x 图像宽度)。然后经过一个没有重叠的区域大小为 2x2 的池化层， 到这响应图的分辨率降低到 12x12。然后这样的操作又重复一遍就是 COiw2 和 pool2, 只是通道数变为 50，响应图的分辨率变成了 4x4。最后就是全连接(fully connected, fc)两层了，其中第一层的单元数为 500。第二层输出分类个数为 ip，然后接 Softmax， 输出最终结果。

总 Tips:这个结构在后面章节的入门例子中会来训练 MNIST。

4.3 新起点-AlexNet

AlexNet在第 1 章已经提过，关于这个经典网络的辉煌战绩就不再介绍了。AlexNet 对几年前的卷积神经网络结构影响很深，无论是后来 2013 年的冠军结构 ZF-Net，还是名 声也很响但屈居老二的 VGGNet，其背后其实都是 AlexNet。本节来仔细了解一下这个经典 的卷积神经网络结构。

4.3.1网络结构

AlexNet针对的是 ILSVRC 的分类问题，输入图片是 256x256 的三通道彩色图片。为 了增强泛化能力，训练的时候 Alex 采用的数据增加手段中包含随机位置裁剪，具体就是在 256x256的图片中，随机产生位置裁剪一块 224x224 的子区域，如图 4-15所示。

所以输入的维度是 3x224x224。整个网络的结构如图 4-16所示，因为层数和参数比 LeNet多了好多，这里就不详细一层层讲解了，只讲一下前两层来了解一下如何通过卷积 参数计算响应图的大小。第一层卷积和 LeNet 中卷积唯一的不同是 stride 变成了 4，不过在 第 2 章中我们已经了解了 stride的概念，所以计算卷积后的响应图边长应该就是先算卷积 能有效计算的边长为(224-11)/4=53.25。这意味着如果第一个核的卷积巾心位置在第一个像 素上的话，最后一个卷积核并不在最后一个像素上。对这种情况比较通用的做法是在最后 一个像素位置上再求一次响应，作为响应图的最后一个像素。所以是[(224-11)/4] + 1 = 54, 这和文章里的维度对不上。有不少读文章细致的人在读 Alex 的论文时都为此感到困惑， 这到现在还是个未解之谜，斯坦福的 CS231n 课猜测是 Alex 用了边缘零填充，又或是 其实他用了更大的输入训练网络，只不过论文上写错了。总之，这是一个无关紧要但又 让人困惑的细节，本书就不纠结于此，按照 Alex 论文(《ImageNet Classification with Deep Convolutional Neural Networks》)中的结构和数据为准，计算响应图大小的方法就是 这样。



对于 pooling 层也可以用同样的办法，如果加了零填充，也就是 padding，那么计算的 时候等效边长就是原边长加上 2 倍的 padding 大小。比如从 conv4 到 conv5 的时候，卷积 大小为 3x3, padding为 1，stride为 1 所以无须考虑非整数的问题，因此响应图大小是 13+2x1-3+1=13，也就是卷积之后大小无变化。

AlexNet还有个特殊的地方是卷积的时候采用了分组的方法，在图 4-16中表现为 conv2 和 conv4—conv5的两个分叉。这样做主要是因为当时 Alex 的显卡不够强大(GTX580), 不过有两块，为了减少计算量同时方便并行，所以采用了分组计算，对于性能而言没什么 特殊意义。除非一定要分通道进行卷积，否则现在几乎已经没人用这种方法了。

另外需要提-下的是在 dropout 的使用，是在 fc6 和 fc7 两层。

input size:3x224x224



conv5 kernel:3x3 stride: 1 pad:l

conv5 kernel:3x3 stride: 1 pad:l 1192x13x13 relu5

|192x13x13

poo!5 kernel:3x3 stride:2 type:MAX

192x13x13

|192x13x13

poo!5 kernel:3x3 stride:2 type:MAX



11000

Softmax

Ol4)Ut

图 4-16 AlexNet结构示意图

4.3.2 局部响应归一化（Local Response Normalization, LRN）

AlexNet除了更深的网络结构，和之前的大部分卷积神经网络比起来较独特的一些改 进主要是：ReLU、Dropout和局部响应归一化。ReLU和 Dropout 已经在第 3 章中讨论过， 本节介绍一下局部响应归一化。

局部响应归一化是在某一层得到了多通道的响应图后，对响应图上某一位置和临近通 道的值按照如下公式做归一化：

^7    （公式 4-2）

k+a X （<）

V    j=i-nl2    y

其中是特征响应图第 Z•个通道上在（x，y>位置上的值，I C（和々均为超参数。需要 注意的是，《为局部通道的总数，比如 AlexNet 中，所以《/2取整为 2。如果定义下 标从 0 开始，/+«/2超过了通道总数 7V-1则截止在 7V-1，如果 i-n/2小于 0 则截止在 0。局部 响应归一化模拟的是动物神经中的横向抑制效应，从公式 4-2中也可以看出，如果在该位 置，该通道和临近通道的绝对值都比较大的话，归一化之后值会有变得更小的趋势。另外 需要提一句，在深度学习中的通道之间，邻近的概念和神经元的横向临近是完全不一样的， 因为通道之间的临近仅仅是根据该通道对应的下标定义的。

在 Alex 的论文中，局部响应归一化还是对指标提升有帮助的，不过随着之后更深结构 的提出，局部响应归一化被认为并没有什么作用，比如 VGG 组的《Very Deep Convolutional Networks for Large-Scale Image Recognition》发现在 11 层的网络中局部响应归一化就已经 起了副作用。

另外，在 AlexNet 中，局部响应归一化层在池化层之前，这在计算上并不经济。Caffe 自带的模型中有个 Caffenet 模型把这两层顺序换了 一下，算是 AlexNet 的改进版。

4.4更深的网络——Goog LeNet

GoogLeNet 由 Google 的 Christian Szegedy 发表在 CVPR2015 的论文《Going Deeper with Convolutions》中。作为深度卷积神经网络发展中一个重要的结构，不仅仅因为 GoogLeNet 把层数推进到了 22层并且接近了人类在 ImageNet 数据上的识别水平，也因为 GoogLeNet 跳出了 AlexNet的基本结构，创新地提出了构建网络的单元 Inception 模块。

4.4.1    1x1 卷积和 Network In Network

在正式了解 GoogLeNet 之前，先来了解一种比较特殊的卷积方式 1><1卷积。这种卷积 方式在深度学习中的讨论最早见于论文《Network In Network》（NIN），作者是新加坡国 立大学的深度学习“巨头”颜水成教授的研究组。顾名思义，1M卷积就是卷积核大小为 1。所以很多人对这种卷积的第一反应是：不就是给特征响应图的每个值都乘了同一个系数

吗？如果一通道输入，一通道输出的情况下的确如此，但多通道输入，多通道输出的情况 下就不同了。下面来看图 4-17所示的例子。





图 4-17中画出的是一组 m 通道的特征响应图通过 1M 卷积得到《通道的新的《通道 的特征响应示意图。我们来关注特征图上某一个位置的像素，或者是同一位置的一组 M 个像素，经过 1x1 卷积后在对应位置会得到一组新的《个像素，就是 ft 4-17下半部分的示 意图。所以如果只看特征图上指定位置的像素的话，其实就是一个全连接层，我们用 x,•表 示第 f 个输入通道上指定位置的像素值，功表示第 y 个输出通道对应位置的像素值，则公 式如下：

x'j. = w7lx, + wy2x2 + …^jmxm + bj = WjXT +bj    (公式 4-3)

其中 x=(x7,x2,    是把所有对应位置的像素看作是一个向量，进一步考虑所有输出通道

对应位置像素的向量：

xJ +b    （公式 4-4）

因为卷积核是对每位置像素进行同样的操作，所以 1x1 卷积相当于对所有的输入特征 响应图做了一次线性组合，然后输出新的一组特征响应图。特别是如果的情况下，通 过训练之后相当于降维，这样再接新的卷积层就只需要在更少的 n 个通道上做卷积，节省 了计算资源。进一步把《通道上的激活函数也考虑进来，就成了一个感知机了。再进一步 再来一次 1x1 卷积+激活函数，就成了一个对响应图指定位置，跨通道的一组像素作为输 入的一个神经网络。这就是最基本的 Network In Network，网络里边还有网络。

NIN论文里还提出了另一种被广泛应用的方法叫做全局平均池化(Global Average Pooling)，就是对最后一层卷积的响应图，每个通道求整个响应图的均值，这个就是全局 池化。然后再接一层全连接，因为全局池化后的值相当于一像素，所以最后的全连接其实 就成了一个加权相加的操作。这种结构比起直接的全连接更直观，并且泛化性更好，在第 10章将结合一个实例对这种结构进行更多介绍。

4.4.2 Inception 结构

这个 Inception 和盗梦空间关系不大。Inception模块的基本思想是源于前面提过的 NIN, 如果把卷积+激活看作是一种广义线性模型(Generalized Linear Model)，那么从这个角度， 既然可以用广义线性模型抽取特征，何不用个更好的模型呢？那干脆就像 4.4.1节末尾提到 那样，用个小型的神经网络。NIN并不是本书要详细讨论的内容，有兴趣的读者可以自行 找到出处论文进行了解。总之意思就是用更有效的结构代替单纯的卷积+激活操作。 Inception模块的结构如图 4-18所示。

因为所有卷积的 stride 都是 1，所以在图 4-18中没有特意标明，另外对于 3x3 卷积、 5x5卷积和 3x3 池化，为了保持特征响应图大小一致，都用了零填充(3x3的填充为 1，5x5 的填充为 2)。最后每个卷积层后面都立刻接了个 ReLU 层，在图 4-18中没有单独画出， 而是和卷积层放一起。在输出前有个叫 concatenate 的层，直译过来就是“并置”。这个操 作的意思是把 4 组不同类型但是大小相同的特征响应图一张张“并排叠” 一起，形成新的 一组特征响应图。



所以通过图 4-18可以看到，Inception里面主要做了两件事：第一件事是通过 3><3池化， 以及 1x1、3x3和 5x5 这 3 种不同尺度的卷积核，一共 4 种方式对输入的特征响应图做了 特征抽取。第二件事是为了降低计算量，同时让信息通过更少的连接传递以达到更加稀疏 的特性，采用 1x1 卷积核进行降维。图 4-18中每条箭头边旁的数字是以 GoogLeNet 最低 层的 Inception 模块为例的通道数。可以看到，对于计算量略大的 3x3 卷积，把 192 通道的 特征响应图降到了原来的一半 96 通道，而对于计算量更大的 5x5 卷积，则降到了更少， 只有 16 通道。

4.4.3网络结构

GoogLeNet的结构如图 4-19所示，对于卷积和池化层，没有写出 stride 和 pad 的话则 说明 stride 为默认值 1，以及没有零填充。限于篇幅，Inception的内部结构就不再画出， Inception模块内部的通道数传递写在了示意图 4-19中，比如 cl 后面跟的是输出的通道数,

c3和 c5 跟的是 lxl 卷积降维后的通道数和最后输出的通道数，比如 c5:16-〉32标明先由 lxl 卷积产生 16 个通道，再做 5x5 卷积得到 32 通道的特征响应图。p3是指 3x3 池化后输出的 通道数。计算层数的时候只按照有需要训练的参数的算作一层，则每个 Inception 里有两层， 一共有 9 个 Inception 模块，所以是 18 层，其他层里则是三个卷积层加 Softmax 前的全连 接层，一共 22 层。

input size:3x224x224

conv+retu kernel:7x7 stride:2



conv+relu kernel:lxl j192x56x56

conv+relu kernel:3x3 pad:l |192x56x56

norml size:5 alpha:0,0001 beta:0.75

[192x56x56



pool kernel：3x3 stride:2 type：MAX

pool kernel:3x3 stride:2 type:MAX 1480x14x14

inception cl： 192 c3:%->208 c5; l6->43 p3;64

| inception ci :J60 c3:U2->224 c5:24->64 p3:64 | pool kernel:5x5 stride：3 type:AVERAGE |         |         |
| -------------------------------------------- | -------------------------------------- | ------- | ------- |
|                                              | 512x14x14                              |         | 512x4x4 |
| inception cl:128 c3:12S->256 c5:24->64 p3;64 | conv+relu kernel: ixl                  |         |         |
|                                              | 512x14x14                              | 128x4x4 |         |

option cl;il2c3;l44->288 c5:32->64 p3;64

J1024

inception cl:256 c3:160->32Q c5:32->128 p3:128 pool kernel: 5x5 stride: 3 type .-AVERAGE    ReLU(dropout)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-216.jpg)

jl024

fc

11000 Softmax

J832xl4xl4 • pool kernel:3x3 stride:2 type:MAX

1832x7x7

inception cl:256 c3:160->320 c5:32->128 p3:128

i1024x7x7

inception cl;384 c3;1^2->384 cS'.48->128 p3:l28

J832x7x7

pool kernel:7x7 type:MAX j1024x1x1

fc(dropout)

11000

Softmax

outputrloss3

图 4-19 GoogLeNet结构示意图

首先注意到的是结构中出来了 3个 loss 单元，也就是训练中计算代价函数的对应单元。 这样做的目的是为了帮助网络的收敛。虽然 ReLU 单元能够一定程度解决梯度消失问题， 但是并不能完全解决深层网络难以训练的问题，这个在第 3 章中也有过讨论。所以越是远 离最后输出的层，就会不如靠近输出的层训练得好。在中间层加入辅助的计算 loss 的单元， 目的是计算损失的时候让低层的特征也有很好的区分能力，从而让网络更好地被训练，而 且低层学到了好的特征也能加速整个网络的收敛。在论文中，这两个辅助的 loss （lossK loss2）的计算被乘以系数 0.3，然后和最后的 loss （loss3）相加作为最终的损失函数来训练 网络。

其次值得一提的是，最后一个 Inception 模块输出 7x7 大小的 832 通道的特征响应图后， 并没有像 AlexNet 那样降维然后过两个全连接层，而是直接对每个特征响应图求了平均值， 得到了一个 1024 维的向量，然后再过一个全连接得到和输出数目对应的 1000 维向量用于 分类，从某种程度上讲，这算是舍弃了经典的最后一/二层全连接的做法。第一节中也讨论 过，全连接因为是一种密集的连接，所以和卷积层比起来参数量巨大，往往占据了一个网 络中大多数的参数，并且还有过拟合的倾向。通过这种方式去掉全连接后，不仅参数的数 量减小了很多，而且模型准确率还有提升（论文中是 0.6%）。和 AlexNet —比这种趋势更 加明显，AlexNet的参数有 6000 多万（其中全连接层的参数占了约 94%） , GoogLeNet的 参数总量不到 700 万。

4.4.4 批规一■化（Batch Normalization，BN）

其实批规一化和第一版的 GoogLeNet 没什么联系，不过作为一种更有效训练深层网 络的常见手段，并>且作者就是 GoogLeNet 的作者 Christian Szegedy，所以放到本节简单 讲一讲。

从字面上看 Batch Normalization就是要对每一批数据进行归一化，确实如此，对于训 练中某一个 batch 的数据加，x2，...，x山注意这个数据可以是输入也可以是网络中间某一层 的输出，BN的前 3 步如下：

1 m

-一 Yx,.    （公式 4-5）

1讯

o1 =—y（^z-A）    （公式 4-6）

/=!

<--f 一"    （公式 4-7）

yJO2 +£

到这步为止就是一个标准的数据减均值除方差的归一化流程。这样的归一化有什么作 用呢？来看图 4-20a定性理解一下。



图 4-20批归一化的定性理解

图 4-20中左边是没有任何处理的输入数据，曲线是激活函数的曲线，比如 Sigmoid。 如果数据如图 4-20所示在梯度很小的区域，那么学习速率就会很慢甚至陷入长时间的停 滞。减去均值再除以方差之后，数据被移到了中心区域，就是图 4-20a中右边的情况。 对于大多数激活函数而言，这个区域的梯度都是最大的或是有梯度的（比如 ReLU），这 可以看作是一种对抗梯度消失的手段。对于一层是如此，如果对于每一层数据都这么操 作，那么数据的分布就总是在随输入变化敏感的区域，相当于不用考虑数据分布变来变 去了，这样训练起来效率就高多了。不过到这里问题并没有结束，因为减均值除方差未 必是最好的分布。比如数据本身就很不对称，或者激活函数未必是对方差为 1 的数据有 最好的效果，比如 Sigmoid 激活函数，在-1〜1之间的梯度变化不大，那么这样非线性变 换的作用有可能就不能很好体现。所以，在前面三步之后加入最后一步完成真正的 Batch Normalization。

Z    + P    （公式 4-8）

其中 Y 和 P 是两个需要学习的参数，所以其实 BN 的本质就是利用优化变一下方差大 小和均值的位置，示意图如图 4-20b所示。因为需要统计方差和均值，而这两个值是在每 个 batch 的数据上计算的，所以叫做 Batch Normalization。当然训练模型时，数据分布的均 值和方差应该尽量贴近所有数据的分布，所以在训练过程中记录大量数据的均值和方差， 得到整个训练样本的均值和方差期望值，训练结束后作为最后使用的均值和方差。

注意前面写的都是对于一般的情况，对于卷积神经网络有些许不同。因为卷积神经网 络的特征是对应到一整张特征响应图的，所以做 BN 的时候也是以响应图为单位而不是按 照各个维度。比如在某一层，batch大小为 w，响应图大小为 wx//，则做 BN 的数据量为 ”jxwxfio

4.5更深的网络-ResNet

2015年时，还在 MSRA 的何恺明祭出 ResNet 这个“大杀器”，在 ISLVRC 和 COCO 上“横扫” 了所有对手，可以说是顶级高手用必杀技进行了一场华丽的演出。除了耀眼的 成绩，更重要的意义是启发了对神经网络结构更多的思考。

4.5.1困难的深层网络训练：退化问题

关于深层网络的训练难题，第 3 章其实已经讲了不少，在 ResNet 的论文里，何恺明要 解决的是训练网络的另一个角度的问题：退化问题。退化问题简单来说就是，随着层数的 加深到一定程度之后，越深的网络反而效果越差，并且并不是因为更深的网络造成了过拟 合，也未必是因为梯度传播的衰减，因为已经有很多行之有效的方法来避免这个问题(关 于这一点，原文并没能够很好论证，其实梯度的衰减仍然存在)。而随着网络层数的加深， 训练效果真的变差了。

何恺明用了一个例子来说明：考虑一个训练好的网络结构，如果加深层数的时候，不 是单纯地堆叠更多的层，而是堆上去一层使得堆叠后的输出和堆叠前的输出相同，也就是 单位映射(identity mapping)，然后再继续训练。比方说用单位矩阵初始化，从卷积考虑 的话就是分通道的 1X1 卷积核，初始化值为 1。这种情况下，按理说训练得到的结果不应 该更差，因为在训练开始前已经达到了加层数之前的水平作为初始了，然而实验结果表明 在网络层数达到了定深度后，结果会变得更差，这就是退化问题。

4.5.2残差单元

首先回顾下残差指的是预测值和观测值之间的差异，很多人容易把残差和误差混淆， 误差是指观测值和真实值的差异。回到正题，为了解决退化问题，论文中提出了残差模块。 大体思路是，既然单位映射在梯度下降框架下不起作用，那么索性直接把输入传到输出端， “强行”作为单位映射的部分，让可学习的网络作为另一部分，这就是残差学习的模块， 如图 4-21a所示。

可以看到，数据经过了两条路线，一条是和一般网络类似的经过两个卷积层再传递到 输出，另一条则是实现单位映射的直接连接的路线，这个路线被称为 shortcut。这样做之后， 如果像前面所说那样，前面层的参数已经到了一个很好的水平，那么再基本构建模块时， 输入的信息通过 shortcut 得以一定程度的保留。

通过实验，这个模块很好地应对了退化问题，并且让可有效训练的网络层数更进一步。 至于为什么有效，论文中的解释是，如果把网络中一个模块的输入输出关系看作是 那么直接通过梯度方法求 H(x)就会面临提到的退化问题。所以作者假设通过这种接入 shortcut的方法，那么可变参数部分的优化目标就不再是//(x)，如图 4-21a虚线框部分所示, 用 F(x)来代表需要优化的部分的话，则 H(x)=F(x)+x，也就是 F(x>H(x)-x。因为在单位映射 的假设中尸 x 就相当于观测值，所以 F(x)就对应着残差，因而叫残差网络。

F(x)



![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-219.jpg)

batch normalization



batch normalization

batch noimalizdtion

cow 1x1

a）    b）

c)

图 4-21 ResNet的基本构建单元

图 4-2la是一个很原始的模块，实际使用的时候，残差模块和 Inception 模块一样希望 能够降低计算消耗。所以论文中又进一步提出了“瓶颈（BottleNeck）”模块，思路和 Inception 一样，就是先通过 lxl 卷积降维，然后正常是 3x3 卷积层，最后再 1x1 卷积将维度和 shortcut 对应上，这就是图 4-21b的模块，也是最终被用到 2015 年 ILSVRC 中并获得第一的结构。

再后来，何恺明对残差网络进行了改进，主要是把 ReLU 给移到了 coiw层之前，相应 地 shortcut 不再经过 ReLU，相当于输入输出直连。并且论文中对 ReLU, BN和卷积层的 顺序做了实验，最后确定了改进后的残差网络基本构造模块，如图 4-21c所示。因为对于 每个单元，激活函数到了仿射变换之前，所以论文中将这种改进叫做预激活残差单元

（pre-activation residual unit） o

论文中通过实验+数据的方式来比较各种层顺序的优劣，但其实这种比较很难说清楚 是否网络提升的真正原因，因为换掉 ReLU 层的顺序必然会影响 shortcut 是否直连。最根 本的改进来源应该说还是因为 shortcut 的直连。比如考虑从多个堆叠的预激活残差单元， 对于第 Z*个单元，令 X/为输入，又/+1为输出，贝 IJ 有：

xM =xt +F（xz.）    （公式 4-9）

把第什 2 个单元也考虑进来，则有：

么 2 =）=x,.+F（xz.）+F（xm）

更一般地，基于公式 4-8，对则有:

xi+。=x/+SF（x/+y）    （公式 4-10）

7=0

通过第 3 章的讲解知道，直接堆叠的网络相当于一层层地做放射变换一非线性变换， 而仿射变换这一步主要就是矩阵乘法。所以总体来说直接堆叠的网络相当于是乘法性质的 计算。而在 ResNet 中，相对于直接堆叠的层数的网络，因为 shortcut 的出现，计算的性质 从乘法变成了加法，计算变得更加简单稳定。当然这些是从前向计算的角度，从后向传播 的角度，如果代价函数用 J 表示，则有：

n-\

w-1

dJ _ dJ dxt dxi dxi+n dxj dxt

dJ

y=o

i+j

dx.

dJ dJ ——+ ― dx: dx；

<>=° ，

dxf.

（公式 4-11）

也就是说，无论是哪层，更高层的梯度成分#都可以直接传过去。这样一来梯度的 oxi

衰减得到进一步抑制，并且加法性的计算让训练的稳定性和容易性也得到了提高，所以可 训练网络的层数也大大增加。基于这种单元，作者构建了 1001层的残差网络，网络的描述 可以在何惜明的 github 上见到地址为 [https://github.com/KaimingHe/resnet-lk-layers](https://github.com/KaimingHe/resnet-lk-layers%e3%80%82)[。](https://github.com/KaimingHe/resnet-lk-layers%e3%80%82)

4.5.3深度残差网络

因为 ResNet 脾实层数太多，本书就不画出来了。比较经典的 ResNet—般是 3 种结构： 即 50 层、101层和 152 层。其中 152 层的网络就是在 2015 年 ISLVRC 种大放异彩的结构。 具体结构参见何借明的论文原文《Deep Residual Learning for Image Recognition》，地址为 [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385%ef%bc%8c%e4%bb%a5%e5%8f%8a%e4%bd%95%e6%83%9c%e6%98%8e%e7%9a%84github%e4%b8%bb%e9%a1%b5%e4%b8%8a%e5%bc%80%e6%ba%90%e7%9a%84%e7%bb%93%e6%9e%84%e6%8f%8f%e8%bf%b0https://github.com/KaimingHe/deep-residual-networks%e3%80%82)[，以及何惜明的 github 主页上开源的结构描述 https://github.com/KaimingHe/deep-residual-networks。](https://arxiv.org/abs/1512.03385%ef%bc%8c%e4%bb%a5%e5%8f%8a%e4%bd%95%e6%83%9c%e6%98%8e%e7%9a%84github%e4%b8%bb%e9%a1%b5%e4%b8%8a%e5%bc%80%e6%ba%90%e7%9a%84%e7%bb%93%e6%9e%84%e6%8f%8f%e8%bf%b0https://github.com/KaimingHe/deep-residual-networks%e3%80%82)

另外值得一提的是，其实残差网络之前已经有训练到 100 层以上的网络，是名字很难 拼写的 Jtirgen“大神”组发表的《Highway Networks》，ResNet中的残差单元，就是 Highway Networks中基本单元的一个特例，并且是一种更容易优化计算量更小的特例。Highway Networks的细节就不说了，有兴趣的读者可以自行查阅。总之残差网络算是非常巧妙的， 从单位映射问题到优化残差的角度，给出了 Highway Networks的另一个种解释，尽管这个 解释并不是很直观易懂。

4.5.4从集成的角度看待 ResNet

ResNet虽然强大，但是残差的解释未必触到了本质。是否网络真的是越深越好呢？针 对可计算性的改善是否是一个正确的方向呢？于是对于残差网络结构的分析很快就出现 了。这些研究中，最有代表性也是对残差网络理解最直观的，是康奈尔（Cornell）大学的 Serge Belongie教授研究组发表的从模型集成角度看待残差网络的文章《Residual Networks Behave Like Ensembles of Relatively Shallow Networks》。

说到这里不由得讲讲 Serge Belongie的传奇生涯，其学术背景“顶天”，本科毕业于 加州理工学院，然后在加州大学伯克利分校拿到博士学位，学术能力超群。Serge Belongie 毕业后去了加州大学圣迭戈分校执教，从 2014 年开始到了康奈尔大学担任教授，美国学术 界的各种奖项如 NSF CAREER、MIT科技评论“35岁以下创新奖”等也都轻松拿到手。 在学术的主线任务上做到这样已经相当厉害了，这位“大神”在各种支线任务上也做得有 声有色。先是商业方面，毕业之后先后创立了 4家公司，其中 3 家已被收购，也算赚得“盆 满钵满”。其在音乐方面也颇有造诣，精通吉他，30岁之后组建了摇滚乐队 SO3，并担任 主唱以及贝斯手。另外，其语言天赋也很好，据说其在巴西为期两个月访学期间，为了展 现自己的聪明才智，报了学习班，零基础开始学葡萄牙语，最后真的学会了。

回到正题，为什么残差网是一种集成呢？来看图 4-22所示。



图 4-22是把一个三个残差单元串联的网络展开的示意图，每个单元都有两条路径，所 以很自然地，这样一个例子展开后有 8 条路径。比起拟合残差的解释，这种展开就直观多 了。整个网络就像是一个完全二叉树结构，每一个高层节点其实都是由低层节点路径构成 的一棵子树，因为路径非常多所以等效于由底层路径构成的网络的一种集成。为了验证这 个思路是否正确，用两种方法来测试 ResNet 是否会发生和集成相似的性质，一个是随机去 掉某些单元，还有就是随机交换某些单元对。对于集成方法来说，这种结果中的“投票” 特性，网络的性能会随着这种对网络的“破坏”程度平滑变化，而路径单一的网络则不然， 去掉某层的这种“破坏”往往直接就让网络的预测结果随机了。实验的结果符合预期,ResNet 对于这两种手段的破坏，表现出的特性和集成一致。

在 Serge Belongie的论文中，还发现如果以卷积层作为有效路径层数的话，随着层数 增加，梯度还是在衰减的。所以单纯从残差拟合解决单位映射问题的角度来解释更深层网 络可以被有效训练，并不是那么站得住脚的。总之，从集成角度对 ResNet 的理解不仅仅是 一种新的角度，也是更直观的理解。同时也发现另一个事实，就是 ResNet 并没有那么深层。 对于最经典的残差单元串联结构，如果用卷积部分来计算层数的话，对于《个单元串联的 网络，路径上出现卷积部分的概率是个二项分布，也就是说平均来说路径上的平均有效层 数是最高层数的一半。在第一版 ResNet 的论文中曾对比 34 层的 ResNet 和 18 层的传统结 构，ResNet胜出，而其实平均有效层数还不到 18 层，况且还是第一版 shortcut 路径上带 ReLU的版本。在 ResNet 基础上，有人还开始研究更胖而不是更深的网络(《wide residual network》，by Sergey Zagoruyko, et, al.)，那么难道不是更深的网络带来了更好的性能吗？ 深度学习诞生至今，追求更深的网络一直是个大趋势，而更深的网络是否真的意味着更 好呢？

4.5.5结构更复杂的网络

4.5.3小节末的问题应该说还在研究当中，并且相关的研究越来越多。从 2012 年 AlexNet 一骑绝尘至今，己经有 5 年，纵观各种和深度学习相关的论文，大多数还是利用网络结合 特定问题的研究，因为非常容易出成果。但随着这种类型研究爆发式的增长渐渐趋稳，一 些不容易立刻产出，但是更低层的方向开始吸引着更多的研究。比如前面章节中提到过的 理论相关的问题，深度学习的优化是否真的非凸，还有泛化能力到底能不能有好的理论支 撑，这是一方面。而另一方面则是朝着对网络结构探索和本质的探讨，比如从 Highway Networks到 ResNet 再到从集成角度看 ResNet 的相关研究。有一个趋势就是，信息流的传 递趋向于更加多样化。

本章 4.1节中我们就提到过，生物的神经网络中，信息传递远不是一层层传下去这么 简单，其连接的方式和稠密程度比现在的人工神经网络复杂得多。比如视觉神经系统中， V2层和后面的层都有连接。如果从模拟这种复杂性的角度来看，Highway Networks和 ResNet就是一个很好的尝试。在 ResNet 开了头之后，尝试更复杂信息传递的网络改进也 越来越多。比如 2016 年夏，一个非常有意思的研究是芝加哥大学发表的分形网络

《FractalNet: Ultra-Deep Neural Networks without Residuals》。还有发表在 2016 年秋，在 ResNet基础上把 shortcut 从跨层串联，变成每个节点和多个节点用 shortcut 相连的《Densely Connected Convolutional Networks》，这个网络在 CIFAR 上的指标全面超越 ResNet。2016 年 ICLR 的 workshop 上，旧金山加州大学的一个小组在短文章《ResNet in ResNet: Generalizing Residual Architectures》中提出了 ResNet in ResNet 结构，让每个基础的残差模 块中的信息流传递式都比起标准残差模块有了量级的增加。2016年底，MSRA的研究小 组将 ResNet 中的 Ensemble 思想进一步加强，在论文《On the Connection of Deep Fusion to Ensembling》中提出了 Merge-n-Run的结构，让更浅层的 ResNet 也能达到和更深的标准 ResNet同等的效果。

早在神经网络还不深的时代，受到生物神经网络启发，就已经涌现了很多关于神经网 络结构变化的研究，如今深度学习把神经网络的可用性大幅提高，相信未来会有更多类似 的研究成果出现。

第 2 篇 实例精讲

第 5 章 python 基础

第 6 章 OpenCV 基础

第 7 章 Hello World!

第 8 章最简单的图片分类——手写数字识別

第 9 章利用 Caffe 做回归

第 10 章迁移学习和模型微调

第 U 章目标检测

第 12 章度量学习

第 13 章图像风格迀移





# 相关

- 《深度学习与计算机视觉》
