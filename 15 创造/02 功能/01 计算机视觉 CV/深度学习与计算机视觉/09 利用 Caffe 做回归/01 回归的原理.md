---
title: 01 回归的原理
toc: true
date: 2018-08-29
---
# 可以补充进来的

- 这个还没有进行整理


##### 第 9 章利用 Caffe 做回归

第 9 章实现了一个基于 Caffe 的用卷积神经网络做回归的例子，并介绍了如何制作 HDF5 格式数据，如何用 GPU 批量对数据进行运算，以及如何实现可视化训练的模型。

<span style="color:red;">想知道怎么制作 HDF5 格式的数据？以及怎么使用 GPU 批量对数据进行运算。</span>

通过前面两章的例子，我们已经了解了用 Caffe 做分类的基本流程。在机器学习的应 用中，除了分类任务，回归(regression)任务也是很常见的，比如人脸关键点回归，物体 位置回归，甚至通过人脸预测年龄的回归等。本章将通过一个玩具小例子了解如何在 Caffe 中做回归。

9.1回归的原理

经典的回归场景就是要让模型预测的实数值和标签的值尽量一致，因为没有了类别， 所以损失函数也变得不一样。本节一起来了解 Caffe 中用于计算损失函数的欧式距离和对 应的 EuclideanLossLayer 层。

9.1.1预测值和标签值的欧式距离

用于分类的经典的卷积神经网络结构，一般是前面部分是若干各种方式连接的卷积层， 最后接一两个全连接层，得到一个向量，然后经过 Softmax 层输出预测的分类概率。如果 把图像的矩阵也看成是一个向量，卷积神经网络中无论是卷积层还是全连接层，就是不断 地把一个向量变换成另一个向量。事实上对于单个的 filter/feature channel，Caffe里最基础 的卷积实现就是向量和矩阵的乘法，可参考<https://github.eom/Yangqing/caffe/wiki/> Convolution-in-Caffe:-a-memo o

最后输出就是一个把指定分类的类目数作为维度的概率向量。因为神经网络的风格算 是黑盒子学习，所以很直接的想法就是把经过 Softmax 层之前的向量的值直接拿来做回归， 最后优化的目标函数直接基于实数值的误差。

在第 2.5.5节中，讲线性回归的时候已经讲过，可以用预测值和标签值的差的平方作为 损失函数。那么如果每个样本要回归的标签有多个呢？自然我们考虑把每个标签值和预测 值的差的平方累积起来：

1 W    n

Z=1

如果把多个标签合一起看成是一个向量的话，只需要神经网络输出的预测值也是一个 等维度的向量，则计算的损失函数其实就相当于这两个向量之间欧氏距离的平方，也可以 看作是差向量的 L2 范数(L2norm)。

9.1.2 EuclideanLoss 层

执行 9.1.1节说的实数向量的回归要用到 Caffe 中的 EuclideanLoss 层，这个层可以把 某一个层输出，和设定好的多维度实数值标签作为输入，然后计算差向量的 L2 范数作为 loss。用法在形式上和 SoftmaxWithLoss 基本相似。

9.2预测随机噪声的频率

本节用一个预测随机噪声频率的玩具例子，来说明用 Caffe 做回归的基本流程。

9.2.1生成样本：随机噪声

首先需要生成如图 9-1中所示的随机噪声图像。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-267.jpg)



OOO59_O.97_O.82.jpg



00000_0.34_0.92.jpg



00001_0.57_0.12.jpg



图 9-1横纵轴频率随机的二值噪声图像

如图 9-1所示，就是要产生随机噪声，同时也不是完全无规律的噪声，而是沿着横纵

两个方向有不同频率。比如左上角的图，是个低频噪声的例子，其中沿着横轴的频率比起

纵轴稍高一些，而左下角则是个高频噪声的例子。频率的高低用一个 0〜1之间的数字表示,

0表示完全没有变化，1表示每个像素之间都是不相关的最高频率变化。如图 9-1中，文件

名的第二和第三个字段分别就是定义的频率沿着横轴和纵轴的值。这个例子中使用了

100x100的灰度图像，用一种非常简单的思路产生不同频率噪声。首先用随机整数发生器

得到一个 1〜100的随机数，分别作为原始噪声的横轴和纵轴长度。然后按照这个大小产生

一个 0〜1之间均匀分布的随机浮点数矩阵作为原始图像。最后把这个随机噪声的原始图像

放大到 100x100 就得到了不同频率的噪声图像。根据这个思路产生图像的代码如下：

import os import sys import datetime import cv2

from multiprocessing import Process, cpu_count import numpy as np

import matplotlib.pyplot as pit

\#定义长宽和样本数量

H_IMG, W_IMG =    100,    100

SAMPLE_SIZE =    70000

\#最后岳存样本的文件夹名

SAMPLES_DIR = 'samples*

\#产生—噪声并保存到 samples 文件夹

def make_noise(index):

\#    _随机生成宽和高

h = np.random.randint(1, H_IMG) w = np.random.randint(1, W_IMG)

\#生成原始噪声图像

noise = np.random.random( (h, w))

\#用 cubic 插值放大让图像显得更平滑

noisy_img = cv2.resize(noise, (H_IMGZ W—IMG), interpolation。 cv2.INTER_CUBIC)

\#    1始图像的宽高和要放大到的宽高的比值就是相对频率

fx = float(w)    / float(W—IMG)

fy = float(h)    / float(H_IMG)

\#文件名包含 3 个字段，分别是序_号、横轴频率和纵轴频率

filename =    *{}/{：0>5d}_{}_{}.jpg*.format(SAMPLES_DIR, index, fx,

fy)

pit. imsave (filename, noisy_img, cmap= * gray *)    /

def make_noises (iO, il):

\#    _产生下标 iO 到 il 的噪声图像

np. random, seed(datetime.datetime.now() .microsecond) for i in xrange (iO, il):

make_noise (i)

print (* Noises from { } to { } are made ! * . format (i0 + l, il)) sys.stdout.flush()

def main ():

\#建立保存图像的文件夹

cmd = 'mkdir -p {}*.format(SAMPLES_DIR) os.system(cmd)

\#默认获取所有可用 CPU 核数作为多进程数量 n_procs = cpu_count()

print(*Making noises with {} processes ... 1.format(n_procs))

\#产生尽量均匀的任务列表

length = float (S7kMPLE_SIZE) /float (n_procs)

indices = [int (round (i * length)) for i in range (n^procs + 1)]

\#定义每个进程

processes = [Process(target=make_noises, args=(indices[i]z indices [i+1])) for i in range(n_procs)]

for p in processes: p. start()

for p in processes: p. join()

print('Done!')

if _name_ ==    *_main_':

main ()

运行这段代码就得到了一个叫做 samples 的文件夹，这里照搬 MNIST 的习惯，一共 7 万个样本。

9.2.2制作多标签 HDF5 数据

对于多标签+回归的任务，最简单的生成数据的方式还是 HDF5。对于 9.2.1节中产生 的 7 万个样本，还是划分出 5 万作为训练集，1万作为验证集，1万作为测试集。首先产生 3个数据集的列表，代码如下：

import os

\#取文件名不包含后缀部分，用下画线分隔后两个字段

filename2score = lambda x: x[:x.rfind(•.')].split (•_•) [-2:]

\#列出 samples 下所有图片文件名 filenames = os.listdir(* samples *)

\#创建训练集列表

with open(* train.txt',    * w *) as f_train_txt:

for filename in filenames[:50000]:

fx, f y = filename2score(filename)

line =    'samples/{}    {}    {}\nformat(filename, fx,

fy)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-272.jpg)

f_train_txt.write(line)

\#创建验证集列表_

with open(* val.txt',    * w1) as f_val_txt:

for filename in filenames[50000:60000]:

fx, f y = filename2score(filename)

line =    * samples/{}    { }    {}\n1.format(filename, fx,

f_val_txt.write(line)

\#创建测试集列表

with open(* test.txt',    * w *) as f_test_txt:

for filename in filenames[60000:]:

line =    * samples/{}\n *.format(filename)

f_test_txt.write(line)

注意，和训练集/验证集不同的是，测试集只有文件路径列表没有标签。执行后，就得 到了 train.txt, val.txt和 test.txt包含了文件路径和对应的频率的值，格式如下：

samples/04375_0.31_0.35.jpg 0.31 0.35 samples/08750_0.22_0.85.jpg 0.22 0.85 samples/26250_0.58_0.36.jpg 0.58 0.36

基于这 3 个 txt 文件中的列表，接下来产生对应的 HDF5 文件。

import sys

import numpy as np

import matplotlib.pyplot as pit

import h5py

\#图像大小

IMAGE_SIZE =    (100,    100)

\#减遍值帮助收敛

MEAN_VALUE =    128

\#获 1输入的图像和标签列表文件名

filename = sys.argv[1]

\#获取文件名不包含后缀部分作为数据集名称

setname, ext = filename.split(*.*)

\#读取所有行

with open (filename, * r *) as f: lines = f.readlines()

\#乱序

np.random.shuffle(lines)

\#样本数量

sample_size = len(lines)

\#图像＞据格式，样本总量 X 通道数 X 高 X 宽

imgs = np.zeros((sample_size, 1,)    + IMAGE一 SIZE, dtype=np.float32)

\#频率标签格式，样本总量 x _标签数

freqs = np.zeros((sample_size, 2), dtype=np.float32)

\#生成 h5 文件名

h5一 filename =    *{ } «h5 *.format(setname)

\# _将数据写入 h5 文件，图像对应的数据名称为 data，频率标签对应的数据名称为 freq

with h5py. File (h5_f ilename, * w *) as h:

for i, line in enumerate (lines):

image_name, fx, fy = line[:-1]•split ()

\#因 $ 直接用 plt.imread()读取的图片是三通道，所以只取第一个通道 img = pit. imread (image_name) [:,    :,    0] . astype (np. f loat32)

img = img. reshape ( (1,    ) -i-img. shape)

img -= MEAN_VALUE imgs[i]    = img

freqs[i]    = [float(fx), float(fy)]

if (i+1)    %    1000    ==    0:

print('Processed { } images!'.format(i+1)) h. create_dataset(* data *, data=imgs) h.create一 dataset('freq*, data=freqs)

\#生成 h5 文件列爰

with open (* {}_h5 . txt' . format (setname) ,    ' w *) as f:

f.write(h5_filename)

将段代码保存为 gen_hdf5.py，然后依次执行以下命令：

» python gen_hef5.py train.txt

\>> python gen_hdf5.py val.txt

这样就得到了训练集和验证集的 h5 文件及对应的列表文件。

9.2.3网络结构和 Solver 定义

网络结构是试着在 LeNet-5上加了 -层，然后改动一下卷积核大小和 stride，让最后进 入全连接前的维度不会太高。另外为了方便回归到 0〜1之间，最后一层的激活函数是 Sigmoid()，最后再和标签一起在 EuclideanLoss 层：

name : "RegressionExample** layer {

name: "data" type: "HDF5Data" top: "data" top: "freq" include {

phase: TRAIN

}

hdf 5_data_parain {

source : '*train_h5 . txtn batch_size:    50

}

}

layer {

name: "data" type: "HDF5Data" top: "data" top: "freq" include {

phase: TEST

}

hdf 5_data_param {

source: "val_h5.txt" batch_size:    50

}

layer {

name: nconvln type: "Convolution" bottom: "data" top:    "convl"

param {

stride: 2 weight_filler {

typp: "gaussian” std: 0.01

}

bias_filler {

type: ’’constant" value:    0

}

}

}

layer {

name: "relul*' type: "ReLU" bottom: ’’convl" top: "convl"

}

layer {

name: "pooll" type: "Pooling" bottom: "convl" top: "pooll" pooling_param {

pool: MAX kernel_size:    3

stride: 2

}

}

layer {

name: "conv2" type:    "Convolution"

bottom: "pooll" top:    "conv2"

param {

}

}

}

layer {

name: nrelu2" type:    "ReLU"

bottom:    "conv2"

top: nconv2"

}

layer {

name: "pool2” type: "Pooling" bottom: "conv2" top: "pool2n pooling_param {

pool: MAX kernel_size:    3

stride:    2

}

}

layer {

name: nconv3" type:    "Convolution**

bottom: "pool2" top: "conv3" param {

}

convolution_param { num_output:    128

pad: 1

kernel_size:    3

weight_filler {

type: "gaussian" std: 0.01

}

bias_filler {

type: "constant" value:    0

}

}

}

layer {

name:    ** relu3*'

type: "ReLUn bottom: nconv3" top: "conv3"

}

layer {

name: "pool3" type: "Pooling*， bottom: "conv3" top: "pool3" pooling_param {

weight_filler {

type: "gaussian'’ std: 0.005

}

bias_filler {

type: "constant" value:    0

}

}

layer {

name: nrelu4" type: "ReLU" bottom: "fc4" top: "fc4"

}

layer {

name: "drop4" type:    " Dropout'*

weight_filler {

type: "gaussian" std: 0.005

}

bias一 filler {

type: "constant" value:    0

}

}

}

layer {

name: "sigmoid5" type: "Sigmoid" bottom: "fc5" top: "pred"

}

layer {

name: "loss"

type: "EuclideanLoss"

bottom: "pred"

bottom: "freq"

top: "loss"

}

用 Caffe 的 draw net.py得到的可视化结构如图 9-2所示。



图 9-2训练噪声图像频率打分的网络结构

相应的 Solver 定义如下：

net:    "./train_val.prototxt"

momentum: 0.9

weight_decay:    0.00001

snapshot_pref ix:    . /f req_regression"

solver_mode: GPU

type: "Nesterov"

这回采用的梯度下降是 Nesterov 方法，文件里没有指定 snapshot 的周期，则默认训练 结束后自动保存一个参数和 Solver 状态的存档。

9.2.4训练网络

首先还是像之前 Caffe 的例子一样，运行下面命令：

\>> /path/to/caffe/build/tools/caffe train -solver solver.prototxt

就可以训练网络了，没有指定 GPU 的情况下，默认使用序号为 0 的 GPU。训练这个网络 也并不费时，收敛后得到 freq_regression_iter_10000.caffemodel的模型参数。笔者的笔记本 电脑上训练的曲线如图 9-3所示。

-3.5

,A    Loss vs. Iters

-3.0

O

«2.

{SSO-I0I60I



0

2000

4000    6000

Iterations

8000 10000

图 9-3训练/验证的 loss 随迭代次数的变化

在图 9-3中纵轴为了方便查看，用的是 logl0(loSS)，可以清楚看到模型收敛了，并且 验证集的 loss 更低，可能是训练集中极端样本出现的可能性更高的原因。

因为回归通常比分类更难训练，所以用 Caffe 的时候很容易出现不收敛的问题，或者 收敛一个很大的 loss 上。这通常是两个原因引起的：第一，参数设置不合适，这吋可以尝

试减小参数初始化时的浮动大小，减小学习率或改变梯度下降的方法等；第二，查看一下 是否使用了 cuDNN，这是很多人使用 Caffe 时都遇到过的问题，通常的表现是 GPU 不训 练不收敛，换成 CPU 或者删除 cuDNN 就可以了。解决方法是升级 cuDNN 或者直接不使 用 cuDNN o

9.2.5批量装载图片并利用 GPU 预测

直接用 train_val.prototxt测试的方法在第 8 章已经讲过，这里就不重复了。直接利用

python脚本在图片文件上测试的方法在第 8 章也有提到，但讲的是每次运行一张图片。在

图片数据量大的情况下，如果又有 GPU，其实可以利用更加经济的方式，就是把多张图片

同时装载到显存中，然后运行一次得到多张图片的结果，实现并行。首先需要改写

train val.prototxt生成一个专门用来部署的网络结构描述文件 deploy.prototxt，网络结构部

分和 train_val.prototxt 一样，只需要改动开头和结尾部分即可。

name:    "RegressionExample"

layer {

name: "data" type: "Input" top: "data" input_param {

| shape: | {    |
| ------ | ---- |
| dim:   | 100  |
| dim:   | 1    |
| dim:   | 100  |
| dim:   | 100  |

}

}

} ， layer {

name: "convl"

..。中间部分省略...

}

layer {

name: "sigmoid5" type: "Sigmoid" bottom: ”fc5" top: "pred"

}

注意，在 deploy.prototxt的 Input 层中，第-个维度是 100，这是要使用的每次装载的

图片数量，也就是每次 GPU 前向计算的 batch 大小。基于此，deploy.prototxt用来批量从

图片文件预测噪声频率的代码如下：

import sys

import numpy as np

sys.path.append(*/path/to/caffe/python1) import caffe

WEIGHTS_FILE =    * freq_regression_iter_10000.caffemodel1

DEPLOY_FILE =    1 deploy.prototxt'

MEAN_VALUE =    128

\#caffe.set_mode_cpu()

net = caffe.Net(DEPLOY_FILE, WEIGHTS_FILE, caffe.TEST)

\#    Transformer是 Caffe 中 1 于对图片进行预处并转换成分通道形式的模块

\#    用 deploy.prototxt 中的 shape 作为图像的 shape

transformer = caffe.io.Transformer({'data *: net.blobs['data1].data.shape}) #转换高度 x 宽度 x 通道--> 通道 x 高度 x 宽度 transformer.set_transpose('data',    (2,0,1))

\#减去均值

transformer.set_mean(* data1, np.array([MEAN_VALUE]))

\#缩放系数

transformer.set_raw_scale('data *,    255)

image_list = sys.argv[1]

\# 获取 batch size

batch_size = net.blobs[* data *].data.shape[0] with open (image_list, ' r *) as f:

i =    0    ~

filenames =    []

for line in f.readlines():

filename = line[:-1] filenames.append(filename)

\#读取一张图片，默认读取图片为 0〜1之间的浮点数矩阵

image = caffe.io.load_image(filename, False)

\#按照 transformer 的设置"转化为网络接受的格式并进行减均值等预处理 transformed_image    =    transformer.preprocess('data',

image)

freqs):

{:.2f} and

\#按对应位置装载到 data 的 blob 里

net .blobs[* data *].data[i,    . . . ]    = transformed_image

i += 1

\#装载数据够一个 batch 后，执行一次前向计算    <

if i == batch_size:

output = net.forward() freqs = output[*pred*]

\#从结果中读取该 batch 所有结果并显示

for filename, (fxz    fy)    in zip(filenames,

print('Predicted frequencies for {:.2f} is {}'.format(filename, fx, fy))

\#重新开始给 batch 计数 i = 0

filenames =    []

需要注意的是，在第 8 章中我们对图像的处理比较粗暴，直接用 cv2 读取了单通道图 像然后减去均值，原因是因为单通道的例子比较简单。不过更好的做法是用 Caffe 的 10 模 块(背后实现是 scikit-image)读取图片，然后用 Caffe 的 Transformer 模块对图片进行减均 值、通道变换等预处理。将上面代码保存为 predict.py，然后执行：

» python predict.py test.txt

输出结果如下：

| Predicted0.31 | frequencies | for  | samples/04349_ | _0.77_ | _0.31 .jpg | is   | 0.76 | and  |
| ------------- | ----------- | ---- | -------------- | ------ | ---------- | ---- | ---- | ---- |
| Predicted0.96 | frequencies | for  | samples/15275  | _0.33_ | 0.98.jpg   | is   | 0.32 | and  |
| Predicted     | frequencies | for  | samples/02137  | _0.53_ | 0.68.jpg   | is   | 0.54 | and  |

0.69

从结果来看，基本还是准确的。

9.2.6卷积核可视化

为什么卷积神经网络也能判断出频率大小呢？我们可以考虑把第一个卷积层的每个卷

积核可视化出来，代码如下：

import sys

import numpy as np

import matplotlib.pyplot as pit

import cv2

sys.path.append('/path/to/caffe/python*) import caffe

\#卷积核大小为 5x5，放大方便观察

ZOOM_IN_SIZE =    50

\#每 X 积核放大后之间的间隔

PAD_SIZE =    4

WEIGHTS_FILE =    'freq_regression_iter_10000.caffemodel*

DEPLOY一 FILE =    'deploy.prototxt *

net =~ caffe.Net(DEPLOY_FILEZ WEIGHTS_FILEZ caffe.TEST)

\#通过 param 属性取得指定层的参数的值 kernels = net.params[1convl1][0].data #归一化到 0〜1之间方便可视化

kernels -= kernels.min() kernels /- kernels.max()

\#    进行缩放，由 5x5―>50x50

zoomed_in_kernels =    []

for kernel in kernels:

zoomed_in_kernels.append(cv2.resize(kernel[0] ,    (ZOOM_IN_SIZE,

ZOOM_IN_SIZE)~ interpolation=cv2.INTER_NEAREST))

half:pa5 = ,PAD_SIZE /    2

padded_size x= ZOOM_IN_SIZE+PAD_SIZE

\#利用 numpy 的 pad 方沄直蚕在卷积核函像的两边各补上 half_pad这么多像素的白边 padding =    ((0,    0), (half_pad, half_pad), (half_pad, half_pad))

padded_kernels = np.pad(zoomed_in_kernelsf padding, * constant *, constant一 values=l)

\#    一共 96 个卷积核，画成 8x12 的排列

padded_kernels = padded_kernels.reshape(8,    12, padded_size,

padded_size).transpose(0,    2,    1,    3)

kernels_img    =    padded_kernels.reshape((8*padded_size,

12*padded_size))[half_pad:-half_pad, half_pad: -half_pad]

\#可视化生&的图像，包含〒所有第一层_96个卷积核_

pit.imshow(kernels_img, cmap='gray', interpolation:•nearest’) pit.axis (* off *) pit.show()

执行代码，可视化结果如图 9-4所示。

可以看到第一层的卷积核成功学到了各种不同频率的成分，这也是例子中判断频率高 低的关键，高频的成分通过高频的卷积核响应向下一层传递，低频则通过变化缓慢的卷积 核响应传向下一层。

虽然模型是用噪声训练出来的，但也可以试试看用到普通图像上是否管用。如图 9-5 所示，从左至右分别是青海湖边的油菜花、四川西部深山的树林和北京(雾霾中)的高楼， 图片下方是用模型预测得到的分数。

##### ■闢 na 醱汸蝙钃山鼸；国

■mn，繚钃扑 b 灯覉喊较 .■ J鬵鱷!■■鞔鼷 PSHrai

似巧關翳 BISHHP 鎂

##### ■⑥聚曜國蟛韉囑 sum 鈐 觀誦鵬職繼觀 騙醒龍麵醺\

图 9-4第一层卷积核的可视化



横轴频率：0.65    横轴频率：0.77    横轴频率：0.19

纵轴频率：0.73    纵轴频率：0.68    纵轴频率：0.22

图 9-5噪声训练的模型在正常图像上的预测分数

可以看到，定性来说大体趋势是正确的。




# 相关

- 《深度学习与计算机视觉》
