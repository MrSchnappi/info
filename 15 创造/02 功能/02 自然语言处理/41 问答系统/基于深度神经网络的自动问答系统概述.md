---
title: 基于深度神经网络的自动问答系统概述
toc: true
date: 2019-11-17
---
# 基于深度神经网络的自动问答系统概述


- - \1. 引子
  - \2. 门派
  - - 2.1. 背景
    - 2.2. 脉络
  - \3. 章法
  - - 3.1. GA-Reader
    - - 模型结构
      - 杀手锏
    - 3.2. Match-LSTM
    - - 模型结构
      - 杀手锏
    - 3.3. Bi-DAF
    - - 模型结构
      - 杀手锏
    - 3.4. R-Net
    - - 模型结构
      - 杀手锏
    - 3.5. QA-Net
    - - 模型结构
      - 杀手锏
    - 3.6. S-Net
    - - 模型结构
      - 杀手锏
    - 3.7. R3
    - - 模型结构
      - 杀手锏


学术圈的生存之道，无非『挖坑灌水』四字真言。大仙鼻祖挖坑，徒子徒孙灌水。坑越挖越大，水越灌越多，终成一片繁荣的小江湖。而『深度神经网络+自动问答系统』，就是这样一个好坑。而本文，将带着各位读者，趟一趟这个大坑，会一会各路大仙们的套路。作为一篇科普文，

**本文会探讨：**

模型结构和对比、简单的公式。

**本文不会涉及:**

复杂的公式，

比赛刷榜的奥义（包括：数据集、特征工程、调参技巧）。









2

门派







2.1. 背景



问答系统的实现机制多种多样，基于信息检索 (IR: Information Retrieval) 的、基于问答知识库 (KB: Knowledge Base) 的、基于知识图谱 (KG: Knowledge Graph) 的等等，一个相对完善的问答系统往往是多种机制的组合。

而本文提到的问答系统，特指:

> 给定一个问题 Q 和一个与 Q 相关的文档 D，自动得到 Q 对应的答案 A

这样的系统有一个更学术的名字: 机器阅读理解 (MRC: Machine Reading Comprehension)，它既可以作为一个独立的问答系统，也可以看成是一个完整问答系统的一部分: 即，在 IR 检索出跟 Q 相关的文档 D 的基础上，机器去阅读并理解 Q 和 D，得到答案 A。

将问答系统用一图以蔽之:

![img](https://mmbiz.qpic.cn/mmbiz_jpg/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDu3RHjto1gGe8lGGGWJmzc6vUs7l95kib25j5IsYcLJvZ3qfvQwlGhGrQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)(图片来自网络，侵权请告知)

2.2. 脉络



用简单的数学符号来表达问答系统要做的事情：

> f(Q, D) = A

(题外话: 如果我们去掉 Q，便成了 f(D) = A，也就是一个摘要系统。所以，问答系统的模型稍加改造，可以无缝对接到『自动摘要』上去。)

回想一下，我们人类是怎么解决阅读理解问题的？

我们会仔细阅读问题和文章，带着问题去文章里找答案，如果文章里有现成的答案片段，我们直接抽取出来，如果没有，我们需要组织自己的语言去生成一个回答。

机器其实跟我们的套路是一样的：

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1gow4icLKCTALRbAwcVxo6XHiaudeicibu8ho276LjSOBUDtrw1tCImUficO3g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这，便是深度问答系统、机器阅读理解的『**套路**』，可以用一个更简单明了的图来表示：

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDu3KGpXRCT3CUVP53nPZOIIrJ8nBzJnGUicCibDuTMORce4fO7cEiaDssiaQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因此，按照模型各个组成部分的不同，可以将其大致分成以下几类『门派』：

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1go8dIhEfKYJb7dQKfYBrZ7SyyVUc7Lfuw9Ih89gJVnibdskHpudyXiansA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

每种模型都或多或少融合了不同门派的风格，既有各自的特色，更有相互的传承，且待下文慢慢道来。

这里要说明一下，decoder 的不同主要因为数据集的不同，有些数据集里的答案 A 可以直接从 D 里面抽取出来，比如 SQuAD 数据集, 有些则需要抽取+生成，比如 MS-MARCO 、 DuReader 和最新的 SQuAD 2.0 数据集。（P.S. 我们在 DuReader 比赛中，用单模型（不做特征工程），刷到过第三）

章法







我们将一网打尽当前常见问答系统、阅读理解的各路模型，细数他们套路章法的特别之处。 若有漏网之鱼，读者朋友务必留言指出。

为避免不必要的江湖纷争，特此声明：

> 模型有门派之异，但无优劣之分。以下模型的排序乃随机抽样的结果，与诞生时间、模型先进性无任何关系。

鉴于每种模型的大框架都是上节提到的 『套路』，所以接下来我们只提及模型的特别之处，具体的公式细节乃至源代码，请各位自行参考相关论文。

3.1. GA-Reader



秘籍传送门: GA-Reader

模型结构







![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDuF189P4KWb4xUGDicY9Nym4WjdD9grfAbadIoJk97lfSiabkenROUrd4A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



杀手锏





1.) 在每层 encoder 里都加入了一个 『GA (Gated Attentnion)』 结构，其公式如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDuKEEAicdgNeHicK1WDEa3tXSTDUXHicpBBfmQ9hNwQJcicpgDibzRmuicnumQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)顾名思义，GA 就是用 attention 的思想，用 Q 的 encoding 给 D 里每个词 di 的 encoding 加一个 『gate』 ai，再传入下一层。

 2.) 加深 encoder 的深度，在每一层都设置 GA 结构，使得 Q 能在不同粒度上被融合进 D。

 3.) decoder 用 softmax 函数来预测答案里的所有词，比如文档 D 是 [ '水', '能', '载舟', '亦', '可', 'citing']， A 是 ['能', '载舟', '亦']，则 label 就是 [0, 1, 1, 1, 0, 0]，本质上就是个 multi-label 问题，损失函数用 cross entropy。

3.2. Match-LSTM



秘籍传送门: Match-LSTM

模型结构







![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDu1Nn1OK4NCJ7CQhvjD1R8qaiafvbBNGRTAlqiavqoq0Oq7k1micZkLT6XA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



杀手锏







1.) encoder 第一部分将 D (图中用 P 表示，代表 Passage) 、Q 分别经过 bi-LSTM 得到的 hp、 hq 拼接在一起，实现 Q 和 D 的第一次融合 2.) encoder 第二部分是重头戏，应用到前人提出的 match-LSTM 模块，公式如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1gom94GW17chRevibO9qa4s2keMH4eicsQFgu2ml4ibkmJdC6Xs7iau6t3ZjA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

通过 attention 机制，得到 D 中每个词 i 关于 Q 里所有词的权重，即公式中的 ai，接着

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1govZT2lkq9xIJyQLUBE5a0Jh5LFTqUTYutWCmYfXtDnlKibomgjA0icB5g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

将 ai 作用到 Q 的 encoding Hq 上，得到一个新的向量，可以认为这个向量是 D 融合了 Q 之后的结果，再跟 Hp 拼接在一起，实现 Q 和 D 的第二次融合。

注意，这里的 attention 机制跟 GA-Reader 有区别，这里得到的是 D 里每个词对 Q 里每个词 两两的权重，而不像 GA-Reader 里 GA 的，得到的是整个 Q 对 D 里每个词的权重。 这也是 Match-LSTM 的特别之处。 输入到下一时刻的向量包含三部分： attention vector, 上一层的 hidden state, 上一时刻的 hidden state，如下所示：

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1go1vrLuGbHJYbqIbEz6OXSjIX8flS2HC3B2qiavXzrMqB5ZGzscDUnZfg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1gopjKybkjiayfWnQ3xHodfYylIbl2ianv3ADUDib8vW75TUJJkvtFBd7OAA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

3.) 设计了两种 decoder，即图中的 answer pointer layer 模块：

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDukSq6H2keTfPSEkhpTlNG6CqvG3vf902ibDyELJDMTSsnAQySEiahptZA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这两种 decoder 将答案预测看做是一个序列问题，而不是像 GA-Reader 一样看做 multi-label，所以用到了 pointer network 去选择答案。 第一种 decoder 是按顺序预测整个答案序列，仍以 D 是 [ '水', '能', '载舟', '亦', '可', 'citing']， A 是 ['能', '载舟', '亦'] 为例，在第一种 decoder 下，label 是 [1, 2, 3]，代表 A 在 D 中的位置，而第二种 decoder 是预测答案边界，即起始位置， label 是 [1, 3]。 文章的实验表明，预测边界效果最好，这也几乎成了后续抽取式模型的『标配』。

（题外话: 关于 pointer network，简言之就是用 attention 的机制，预测目标序列在原文中的位置，每一步解码的概率，就是 attention score 经过 softmax 之后的概率分布。 具体请参考原论文 pointer-network 。）

3.3. Bi-DAF



秘籍传送门: Bi-DAF

模型结构







![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDutibCY2qmzBCn4diarNrRTvVwt0icfW1g4bSSlnia34RicwMtuJHXVnI2dkQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



杀手锏







1.) encoder 跟前面俩模型一样， 考虑到了用 Q 融合进 D、 去 attend D 里的每个词，即图中的 Context2Query 模块，这部分没什么特别的，只是在计算 attention 的时候，多加了 h 和 u 的点乘项，如下：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/75DkJnThACksI7ZicxVwHSeb1Igapu1goe7njLKUO5fgVlBJILrhhYJquzVvOy4ia12K1Zx9EV1r4dSU0CbRjqicw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

除此之外，还创新性地用 D 去 attend Q，即图中的 Query2Context 模块，跟 Context2Query 的本质一样，唯一区别是对 attention score 取了 max 后再对 h 加权平均，得到的是单个向量，可以看成是 D 融合进 Q 之后的结果，参与到 encoder 的每个时间步的输入。

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDu0ibZKtfzeQsmwINUSkNZG18m0w39f8JERib5DoE6fibG74oNrJA3vAj6g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

2.) encoder 输出层做了个类似 『特征工程』 的 trick，将 h u 等向量进行拼接、两两相乘

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1gowDHGrpwibjQamlnMDGdpMpA7tZvBvNXA79HAMLzic1DLibr97ibia5YUAnw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

3.) decoder 部分看似是新的，但我个人认为本质上还是个 pointer network，即利用 Q 和 D 通过 decoder 每一步出来的 M，去 attend encoder 的输出 G，然后用 softmax 作用，去预测起始位置。 只不过在预测终止位置时，M 又额外经过了一层 LSTM 得到 M2。 损失函数仍然是 cross entropy。

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1gomr7cMNQdrzaYuHJBZjVfbwz9fvYB4nKLj7d5LSnVhL3a0CX61VpqqQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1goZrXC0yT290RwJIY0vdFmLcDmJDXolGWxlOGHZGJ7lky1s0ys9kgWmA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1goBcGpXsc4VpBmSUzVUEKRLzNHBiadicWBDGRiblj1G57YTCQ1xXAgriavJA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

3.4. R-Net



秘籍传送门: R-Net

模型结构







![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDuEvpAHcrfvlSkrsIoqsiajVgcnqJhib5y1nkIxHwvwAznOnoaTRvud8dA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



杀手锏







1.) encoder 中加入了一个 Gated Match-LSTM 模块，这也是模型主要 claim 的一个点， 但我认为相对 Match-LSTM，改动并不大。 回顾下 Match-LSTM 里的每步输入：

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1gojJCSGDQicBFoImic8msxeCAYQcAAjTHc9Crlob7DicsSBia0d9UXWh3kag/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1govM4boGx9tzzZFOZDayQwyrauDrDaibibGQ851Cic4dXp6rEic26dCaGDKg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

再看看 Gated Match-LSTM 的输入：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/75DkJnThACksI7ZicxVwHSeb1Igapu1goUAP9XGIVZU2aDQUL3tr2BMeG3wRibN1llnpr2cEicE3Fia1Te94jlu6Sw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1goOEGHxziaBibvlfrmyWBlWtnIjNoE3Bd4icPNWAX33vWt9iaQSRxQAhxicIQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可见唯一的区别就是对输入多加了个 gate gt。 这么做的直观解释是，通过 gt 来过滤掉输入中跟问题和答案不相关的部分。

2.) encoder 中还加入了 self-attention 机制，使得文章内部的词与词之间相互融合，通过自身的上下文信息辅助筛选有价值的词，公式如下：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/75DkJnThACksI7ZicxVwHSeb1Igapu1goiakCzu5b3OicOXQMTuwYozAT4HBRR7m7Vy47NLicsU5KDOoIGibkhOAw5Q/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1goHZfJHFKR8v5IHgcicic0gIZ8LicMEgaE6ibme8UIP4eGQtZf9tbUREYLhw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

注意这里的 attention vector ct，是由当前词 vj 和所在文章里的所有词 vt 做 attention 得到的：

3.) decoder 仍然采用 pointer network， 但额外引入了 question 的信息，作为 decoder hidden state 的初始输入，如下图：

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDuPWib9X1awTKzsCKeVicibfgFiarglGz0n2D6UK8QbzeOXMIq77GXmzIpPw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

3.5. QA-Net



秘籍传送门: QA-Net

模型结构







![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDu0HicOTQHKatK77X2ibWLdVSvodww034iaT5ftG3Tib6csqOt5RibrpLKtLg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



杀手锏







相信在看过四个 rnn 派的模型后，这个模型一定让大家耳目一新，所以他的杀手锏就是：

1.) encoder 用 cnn 来实现，由多个结构一样的 encoder block 来实现，每个 encoder block 长下面这样：

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDu7vbnqdp7YPwl3iatwfCBH15YKud3ibSmFoURcUpKyaC2GiaWUiaX2ljReg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

每个 block 里又包含多个小单元，包含了 layer-norm、 convolution 和 self-attention 等，其中：

a.) layer-norm 可以看做是 batch-norm 的变种，简单理解，batch-norm 是对一个 batch 里 x 的特征进行归一化，而 layer-norm 是对一层的 x 输出归一化。

b.) convolution 采用了 depthwise separable convolution，如下图：

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDusFvHFtnicKKkWnhdURv5mT0CPLyrOL7MQqo54Q4QSIgpRmqOiaQTokxQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

看似复杂，其实简单理解，就是把一个 H x W x D 的卷积核，分解成 H x W x 1 和 1 x D 俩矩阵，从而降低矩阵的秩，减少参数量。 这是模型压缩很常见的一个做法。

2.) encoder 里也有一层 context to query 的 attention，做法与 Bi-DAF 有点类似但不一样，作者说他是借鉴自一个叫 DCN 的模型，计算 attention score 的公式如下，额外加入了 q 和 c 的点乘。

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1gouibciagpYdQPeMsB0xrPJx5eHWKBIkjpwZc9lricvp4uryMAxjZbV9Q4w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

3.) decoder 沿用 Bi-DAF 的，这里不赘述了。

4.) 除了模型层面的改进，QA-Net 还额外做了数据扩充，采用 English-French-English 的双向翻译模型来扩充语料。 作者号称效果变好，对此我表示存疑，毕竟翻译误差的累积会导致语料变差。

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDu6xDKic3uE8oQ20iayORTYUgjYKDELtANFjFT6mjCMDAXicSWO0AR1e4kQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

3.6. S-Net



秘籍传送门: S-Net

模型结构







![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDusROqvZIALZofvJCZ00HyTIRfXcARA1On8vJJp9rPK9gfrym8VAmfOg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



杀手锏







#### 前面所有模型都是端到端的抽取式模型，而 S-Net 是分成了两部分的生成式模型：先从文章中抽取出所谓的 evidence snippets， 再通过生成式的 decoder 生成答案，具体如下：

1.) 第一部分： 直接 train 一个抽取式模型，如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDuSicBx1raLKbSj3O19iczsqDfxLJibGzwxbTPglUdtam171QNCV1GOtKKQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

a.) encoder 部分借鉴了 R-Net 里的 Gated Match-LSTM。

b.) decoder 部分采用了 multi-task training，是一个双任务结构，一个目标是传统的抽取式过程，采用了 R-Net 的 decoder 用来预测答案，一个是 passage ranking，将目标 passage 排序靠前，辅助抽取式 decoder。

2.) 第二部分: 将第一部分预测的答案起始位置作为重要信息放到词的 embedding 里，然后采用 seq2seq 的结构来生成答案。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/75DkJnThACksI7ZicxVwHSeb1Igapu1got7my3rPjgUoBokKk8aF6eu7IlKn1iamMhvSdFVZZibicLyMVCD9rWr1qQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这里用 seq2seq 的方法比较简单粗暴，encoder 部分直接拿 passage 和 question 的 last hidden state 做个拼接作为 decoder 的初始状态，decoder 就是一个采用了 attention 机制的 GRU。

S-Net 这种设计方式使得模型结构非常灵活，既可以应用在生成式答案的数据集，如 MS-MARCO, SQuAD 2.0，也可以用于传统的抽取式数据集，如 SQuAD。 值得注意的是，虽然 S-Net 号称处理多篇文档，但在 encoder 层面依然是把多篇文档拼接在一起作为一整篇文档，并没有做文档排序。

3.7. R3

秘籍传送门: R3-Net



模型结构



![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDuBdMQ5IRYUia2kiccu7vMUwNiaCSXWCmgcAB1DvGgkmdpZ8Dsm5l2SQQKA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

杀手锏







回顾一下我们在前言部分提到的，一个相对完整的问答系统——包含了文章检索、文章排序和答案生成三部分，前面我们讨论的所有问答模型主要集中在答案生成，所针对的数据集都是：

> 给定一个问题 Q 和一个与 Q 相关的文档 D，自动得到 Q 对应的答案 A

即便是 S-Net，也是基于这样的数据集，文章排序不过是一个辅助任务，但 R3 要解决的问题是，

> 给定一个问题 Q 和通过 IR 检索出的相关文档集合 Dset，自动得到 Q 对应的答案 A

这是个典型的 open-domain QA 系统，一般都会先对 Dset 排序，再做答案生成，但 R3 用强化学习的框架统一了文章排序 (Ranker) 和答案生成 (Reader) 两部分，是一个端到端的模型。

1.) Ranker 和 Reader share 一个 encoder， 用来对输入文章建模，结构如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDu0rvV6L95fvkX6Ojg4vMHrknFhF6urWJdag3smOH2c1X7senY9OIHfw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这里有个小 trick，跟 Bi-DAF 有点类似，对 Hp 和 Hq 做了些类似特征工程的操作—— 将 Hp Hq 进行点乘和相减，再作为下一层 layer 的输入：

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1gokJKEs9vTlI2v2ly1FX8NzP7adyDnkJXRRuibTZ7mytLibukADib3SXZdQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

2.) 把 Ranker 和 Reader 放到强化学习(RL) 的框架中， Ranker 负责产生 policy，而 Reader 负责执行 policy 并产生 reward。 Ranker 产生 policy 的过程如下图：

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDujs3s6dnaDvkA448RY9HKiccCr9f260faoG0kpZb2iaCPicjLd2Tl6pLVw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

对应的公式如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1gopftO1kPw5e21ibmXzXKkEwvVYADdIudB7EiaviaQpw3VQFL7LHabIuzXw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

最后的 policy 其实就是 Dset 里文章的概率分布。 在得到 policy 后，所要采取的 action 就是根据概率分布抽取文章，作为 Reader 的输入，如下所示：

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDuDShHNqdozavAbXhLH8kHIHnmKvkMjkDtFlG2C8PLggDicP6XgsGryZQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这部分其实就是典型的阅读理解模型，可以用前面任何一个模型替代，只不过文中的 encoder 和 decoder 来自 Match-LSTM。 注意到除了 sample 概率最高的文章，还额外加进去一些概率不高的文章作为『负样本』，一起输入 Reader，这么做的原因是使得 softmax 出来的正确答案位置更 sharp，加快模型收敛：

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACksI7ZicxVwHSeb1Igapu1gohM7MicUMgib0cCCiaiceCnZ7ch0EsRib81Ticm9x3zQakM8OMNGtlVMD3PPA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Ranker 跟 Reader 分别用 policy gradient 和 supervised gradient 这两种更新策略来训练，这部分没太多好说的，都是典型的强化学习框架。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/75DkJnThACksI7ZicxVwHSeb1Igapu1goB4QNjiaJToav9jtrUrNLj8Ee1fRibZoWV3GGFL6CZBxuhIN7jx5ZicpSA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

唯一注意的是这里 reward 不能直接用 reader 的 decoder 出来的 cross entropy：

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDuPWhHZKnmF5uLRUGpgmljJPM6Yf5LJtqbHAK1iaGGYicnIRQsjll0ATMg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因为这个 reward 是没有上下界的，梯度变化会很大，所以需要重新设计一个 reward，文中简单采用了真实答案和预测答案序列的 f1 score，如下所示：

![img](https://mmbiz.qpic.cn/mmbiz_png/nWWpiaGcoHE56amzFmjSUnMT88JjzsoDuicYfiaJqPkv9q1BKcgGB6h1FzhPt98LmicsrooSvmDS7ljh5eUMVGX7lw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

当真实答案和预测答案有交集时，用 f1 score 作为 reward；当两者完全一致时，f1 score 是1，但这里 reward 直接设为 2，我理解是为了使收敛更快，更快地找到最优策略；当两者没有交集时，f1 score 是0，但这里 reward 设为 -1 。







参考文献链接

[GA-Reader]

(http://www.aclweb.org/anthology/P17-1168)

[Match-LSTM]

(http://www.aclweb.org/anthology/P17-1018)

[Bi-DAF]

(https://arxiv.org/abs/1611.01603)

[R-Net](https://www.microsoft.com/enus/research/wpcontent/uploads/2017/05/r-net.pdf)

[QA-Net]

(https://arxiv.org/pdf/1804.09541.pdf)

[S-Net]

(https://arxiv.org/pdf/1706.04815.pdf)

[R3-Net]

(https://arxiv.org/pdf/1709.00023.pdf)


# 相关

- [基于深度神经网络的自动问答系统概述](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247485842&idx=1&sn=d7485054d6e93225b6ac0c77f8706cf7&chksm=fdb69405cac11d1355b84f692c2cbe49a3852a10e074b6941c95618598caea6ed64103c4ee4c&mpshare=1&scene=1&srcid=0803WrakensB0P9gCoXpb7Gf#rd)
