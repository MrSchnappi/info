# 大纲


### 1.1 探索与利用

当向用户推荐时，会产生超出普通监督学习范围的问题，并进入强化学习的领域。理论上，许多推荐问题最准确的描述是 contextual bandit(Langford and Zhang,2008; Lu et al.,2010)。问题是，当我们使用推荐系统收集数据时，我们得到是一个有偏且不完整的用户偏好观：我们只能看到用户对推荐给他们项目的反应，而不是其他项目。此外，在某些情况下，我们可能无法获得未向其进行推荐的用户的任何信息(例如，在广告竞价中，可能是广告的建议价格低于最低价格阈值，或者没有赢得竞价，因此广告不会显示)。更重要的是，我们不知道推荐任何其他项目会产生什么结果。这就像训练一个分类器，为每个训练样本 $\boldsymbol{x}$ 挑选一个类别 $\hat{y}$(通常是基于模型最高概率的类别)，然后只能获得该类别正确与否的反馈。显然，每个样本传达的信息少于监督的情况(其中真实标签 $y$ 是可直接访问的)，因此需要更多的样本。更糟糕的是，如果我们不够小心，即使收集越来越多的数据，我们得到的系统可能会继续选择错误的决定，因为正确的决定最初只有很低的概率：直到学习者选择正确的决定之前，该系统都无法学习正确的决定。这类似于强化学习的情况，其中仅观察到所选动作的奖励。一般来说，强化学习会涉及许多动作和许多奖励的序列。bandit 情景是强化学习的特殊情况，其中学习者仅采取单一动作并接收单个奖励。bandit问题在学习者知道哪个奖励与哪个动作相关联的时候更容易。在一般的强化学习场景中，高奖励或低奖励可能是由最近的动作或很久以前的动作引起的。术语 contextual bandit 指的是在一些输入变量可以通知决定的上下文中采取动作的情况。例如，我们至少知道用户身份，并且我们要选择一个项目。从上下文到动作的映射也称为策略(policy)。学习者和数据分布(现在取决于学习者的动作)之间的反馈循环是强化学习和 bandit 研究的中心问题。<span style="color:red;">挺好。</span>

强化学习需要权衡探索(exploration)与利用(exploitation)。利用指的是从目前学到的最好策略采取动作，也就是我们所知的将获得高奖励的动作。探索是指采取行动以获得更多的训练数据。如果我们知道给定上下文 $\boldsymbol{x}$，动作 $a$ 给予我们 $1$ 的奖励，但我们不知道这是否是最好的奖励。我们可能想利用我们目前的策略，并继续采取行动 $a$ 相对肯定地获得 $1$ 的奖励。然而，我们也可能想通过尝试动作 $a^{\prime}$ 来探索。我们不知道尝试动作 $a^{\prime}$ 会发生什么。我们希望得到 $2$ 的奖励，但有获得 $0$ 奖励的风险。无论如何，我们至少获得了一些知识。<span style="color:red;">是的，会获得一些知识。</span>

探索可以以许多方式实现，从覆盖可能动作的整个空间的随机动作到基于模型的方法(基于预期回报和模型对该回报不确定性的量来计算动作的选择)。

许多因素决定了我们喜欢探索或利用的程度。最突出的因素之一是我们感兴趣的时间尺度。如果代理只有短暂的时间积累奖励，那么我们喜欢更多的利用。如果代理有很长时间积累奖励，那么我们开始更多的探索，以便使用更多的知识更有效地规划未来的动作。<span style="color:red;">时间尺度的影响。</span>

监督学习在探索或利用之间没有权衡，因为监督信号总是指定哪个输出对于每个输入是正确的。我们总是知道标签是最好的输出，没有必要尝试不同的输出来确定是否优于模型当前的输出。

除了权衡探索和利用之外，强化学习背景下出现的另一个困难是难以评估和比较不同的策略。强化学习包括学习者和环境之间的相互作用。这个反馈回路意味着使用固定的测试集输入评估学习者的表现不是直接的。策略本身确定将看到哪些输入。Dudik et al.(2011)提出了评估 contextualb andit 的技术。<span style="color:red;">怎么评估的？想知道。</span>
