---
title: 01 介绍
toc: true
date: 2018-07-26 17:58:04
---
终于到了深度学习这一课，看看到底比赛的时候是什么样的。


前面几课题目一直在变，但是基本思路没有变，就是使用一些机器学习算法啦分类，预测，等等。

从这节课之后就开始关注一些深度学习的项目

深度学习就是一种 end-to-end 的 learning ，不需要人为定义和提取特征，把一切都交给算法本身解决，让算法只拿到自己最原始的输入的 input 和最原始的 output。剩下的中间的所有的过程都让算法本身来做。



主要内容格式两部分：


- 记忆⾥的过去
    - RNN
    - LSTM
- 眼前的五光⼗⾊
    - 图⽚特征
    - CNN


第一部分与 NLP 相关，第二部分与图像相关。

普通的神经网络：


![](http://images.iterate.site/blog/image/180726/4G5fK2KkAc.png?imageslim){ width=55% }


对于用神经网络来解决的分类问题而言，input 中进来的每一批的 x 之间的关系是没有办法捕捉到的，如果想让每一批的输入之间的相关性被捕捉到，就可以使用 RNN：



RNN


RNN的⽬的是让有 sequential 关系的信息得到考虑。

什么是 sequential 关系？就是信息在时间上的前后关系。

相⽐于普通神经⽹络：

![](http://images.iterate.site/blog/image/180726/6J3690k5G9.png?imageslim){ width=55% }

简单来说，对于 t=5 来说，其实就相当于把⼀个神经元拉伸成五个。换句话说， s 就是我们所说的记忆（因为把 t 从 1-5 的信息都记录下来了）

<span style="color:red;">说实话，还是不明白，为什么这种 RNN 就一定是科学的？有什么理论依据吗？还是说它是一个根本的模型？</span>

很容易就能想到，这个在语言这种顺序信息比较重要的场景中的使用。


可⻅， RNN可以带上记忆。

假设，⼀个『⽣成下⼀个单词』的例⼦：

『这顿饭真好』 ——>『吃』

很明显，我们只要前 5 个字就能猜到下⼀个字是啥了

但是有个情况，比如我给你讲了整个葫芦娃的故事，然后我问你，穿⼭甲说了什么？，你能回答嘛？

这就是一个问题了：

即：如果我们是一段很长很长的对话，我们单纯的使用 RNN，那么他就需要把所有的信息都融入到我们的 s 里面去，这种的计算量是非常大的。

因此，科学上发明了另外一种神经网络 LSTM

![](http://images.iterate.site/blog/image/180726/98Kd2EAH1J.png?imageslim){ width=55% }

这个东西与 RNN 本身长得差不多。

LSTM 比 RNN 复杂在他自己指向自己的时候，要经过一些处理。

这套关系

为什么说这套记忆纽带能够对长时间之前的信息进行记忆。

![](http://images.iterate.site/blog/image/180726/Hm96KL75dH.png?imageslim){ width=55% }

LSTM中最重要的就是这个 Cell State，它⼀路向下，贯穿这个时间线，代表了记忆的纽带。

它会被 XOR 和 AND 运算符搞⼀搞，来更新记忆

![](http://images.iterate.site/blog/image/180726/D2jid5AGEi.png?imageslim){ width=55% }


⽽控制信息的增加和减少的，就是靠这些阀⻔： Gate 阀⻔嘛，就是输出⼀个 1 于 0 之间的值：

- 1 代表，把这⼀趟的信息都记着
- 0 代表，这⼀趟的信息可以忘记了


下⾯我们来模拟⼀遍信息在 LSTM⾥跑跑~

#### 第⼀步：忘记门

来决定我们该忘记什么信息。

它把上⼀次的状态 $h_{t-1}$ 和这⼀次的输⼊ $x_t$ 相⽐较，通过 gate 输出⼀个 0 到 1 的值（就像是个 activation function⼀样），

- 1 代表：给我记着！
- 0 代表：快快忘记！

![](http://images.iterate.site/blog/image/180726/E4L1fLmKK4.png?imageslim){ width=55% }

<span style="color:red;">门是被谁控制的？ $h_{t-1}$ 和 $x_t$ 是怎么混合在一起的？直接加吗？ 这个门过去的信息是要被忘记的吗？因为接到的是异或的节点上。 </span>


#### 第⼆步：记忆⻔

哪些该记住

这个⻔⽐较复杂，分两步：

1. 第⼀步，⽤sigmoid决定什么信息需要被我们更新（忘记旧的）
2. 第⼆部，⽤Tanh造⼀个新的 Cell State（更新后的 cell state）

![](http://images.iterate.site/blog/image/180726/JGc7meHjaj.png?imageslim){ width=55% }


#### 第三步：更新⻔

把⽼cell state更新为新 cell state

⽤XOR和 AND 这样的⻔来更新我们的 cell state：

![](http://images.iterate.site/blog/image/180726/ff1KHLImGi.png?imageslim){ width=55% }


#### 第四步：输出⻔

记忆纽带只是我们用作记忆的纽带，不是我们的 lstm 的输出，我们每一层 lstm 的输出用 $h_t$ 来表示。

由记忆来决定输出什么值

我们的 Cell State 已经被更新，于是我们通过这个记忆纽带，来决定我们的输出：（这⾥的 $o_t$ 类似于我们刚刚 RNN ⾥直接⼀步跑出来的 output）

![](http://images.iterate.site/blog/image/180726/FBBdj765fK.png?imageslim){ width=55% }



<span style="color:red;">上面讲的还是没怎么理解，还是要融入到之前我们的 lstm 的总结里面去，再好好理解下。关键是想知道里面的数字是怎么变化的，为什么有这些门，以及记忆纽带的作用。实际上有很多问题想问。</span>



## 案例

题⽬原型： What’s Next？

可以⽤在不同的维度上：

维度 1：给你一篇文章，以字母为单位让 RNN 进行学习，那么下⼀个字⺟是什么？
维度 2：给你一篇文章，以单词为单位让 RNN 进行学习，那么下⼀个单词是什么？同样：给你一个单词，让你写接下去写一篇文章。像模仿郭敬明写小说，就是，把文章给 lstm 学习之后，只要随便给个开头，他就会往下写。
维度 3：下一个句子是什么。
维度 N: 下一个图片、音符、。。是什么。比如整个电影都给了一个加入 cnn 的 rnn 进行学习，然后给一张图片，他就会给你生成一个视频。音符就比如，学习了很多的音乐之后，给一个起音符，就直接把整个乐章普出来。

所以这种带记忆的学习算法在日常生产生活中还是用处非常大的。但是，他也是非常耗硬件的。


OK，下面我们开始讲案例



### 图⽚的特征

跟⽂本⼀样，图⽚也有特征，因为图⽚本身可以表示成⼀个矩阵，我们对图⽚的特征计算/创造/统计也会更加直观，包括：

SIFT, HOG, LBP, Haar, 等等等等

基本就是把图⽚中的 颜⾊，形状，深浅，等等信息各种组合

我们直接看⼀个案例：


其实整个深度学习受益最多的就是图片特征方面，因为图片特征信息量复杂度太高了，而现在使用深度学习已经变成了一个统一化的处理步骤，几乎所有的图片特征处理都会用到 CNN，只是每个人的 cnn 的构造不同而已。


在深度学习没有流行的时代，人们是如何提取出特征的。

这些特征是很快的，但是在准确度上，深度学习已经统治了图片领域。




鉴于时间关系，我们不深展开图⽚特征提取的各种算法。

⼤家都知道，深度学习的意义就在于，不依靠⼈为创造的特征，⽽是让机器⾃⼰去学特征。


![](http://images.iterate.site/blog/image/180726/694FilLHcj.png?imageslim){ width=55% }



在图片领域必用的算法就是 CNN，

大数据量的图像匹配算法通常是什么呢？一般就是这种用了卷积神经网络的深度学习算法。


什么是卷积：

![](http://images.iterate.site/blog/image/180726/4I2004BBi1.png?imageslim){ width=55% }

![](http://images.iterate.site/blog/image/180726/lGIAh31h5k.png?imageslim){ width=55% }

就是两个方程相遇的过程。


![](http://images.iterate.site/blog/image/180726/ha9l16m2dC.png?imageslim){ width=55% }

相当于一个滤镜。

![](http://images.iterate.site/blog/image/180726/bFHcam87LC.png?imageslim){ width=55% }

这种 全是 1的，这个滤镜就把周边的颜色混在了一起，这样他就使得图片虚化了。

![](http://images.iterate.site/blog/image/180726/2cmK2fJ8AH.png?imageslim){ width=55% }

这个滤镜照过之后，相同颜色的区块就会变成黑色，不同颜色的区块即边界就会保留下来。<span style="color:red;">厉害，这样就把边框保留了下来。</span>

![](http://images.iterate.site/blog/image/180726/cEaBFg1IL7.png?imageslim){ width=55% }

可见，如果使用不同的矩阵进行卷积，就相当于使用不同的滤镜来对图片进行处理，就得到了这个图片的一堆不同的特征。


Pooling

![](http://images.iterate.site/blog/image/180726/EGfF9d186I.png?imageslim){ width=55% }


MaxPooling

![](http://images.iterate.site/blog/image/180726/2He5ddfk6e.png?imageslim){ width=55% }

averagePooling

![](http://images.iterate.site/blog/image/180726/I8111d18IF.png?imageslim){ width=55% }

业界主要用的还是 MaxPooling，因为 averagepooling 会创造出新的，没有太大的关系。

综合成一个模型：

![](http://images.iterate.site/blog/image/180726/3m4j12lgLG.png?imageslim){ width=55% }


对比特征提取+神经网络

![](http://images.iterate.site/blog/image/180726/c65KKkmLab.png?imageslim){ width=55% }
