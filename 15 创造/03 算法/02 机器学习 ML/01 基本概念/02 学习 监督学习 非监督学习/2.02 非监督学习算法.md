---
title: 2.02 非监督学习算法
toc: true
date: 2019-05-24
---
# 可以补充进来的

- 特征变量关联是什么？

# 非监督学习算法

非监督算法只处理“特征”，不操作监督信号。通俗地说，非监督学习的大多数尝试是指从不需要人为注释的样本的分布中抽取信息。

注意：监督和非监督算法之间的区别没有规范严格的定义，因为没有客观的判断来区分监督者提供的值是特征还是目标。<span style="color:red;">是的。</span>



非监督学习主要包含两大类学习方法：

- 数据聚类
- 特征变量关联

其中：

- 聚类算法往往是通过多次迭代来找到数据的最优分割
- 特征变量关联则是利用各种相关性分析方法来找到变量之间的关系。如 数据挖掘里面的关联规则学习，有 Apriori 等。

降维 ，最著名的就是 PCA ，主成分分析，改进版本叫做 KPCA 核主成分分析，这种算法在很多地方会大规模使用的，这是一种线性的降维技术。
那么非线性的降维技术呢？就是流行学习，我们会讲述 4 种有代表性的算法，分别是 LLE，局部线性嵌入，拉普拉斯特征映射、等距映射、局部保持投影。

然后我们说下聚类算法，聚类是非监督学习里面使用最广的一类算法，我们会介绍 层次聚类、k-means 还有三种基于密度的聚类：DBSCAN、OPTICS、Mean shift  然后会介绍谱聚类，它是一种基于图论的算法，最后会介绍 EM 算法，是一种基于概率的算法。

到这里位置，最常用的算法应该是已经介绍完了，还可以补充进来的就是 概率图模型，概率图模型的典型代表就是 贝叶斯网络、隐马尔科夫模型、条件线性随机场 等等。我们在这门课里面重点会介绍隐马尔科夫模型 HMM。







该术语通常与密度估计相关：

- 学习从分布中采样
- 学习从分布中去噪
- 寻找数据分布的流形
- 将数据中相关的样本聚类。<span style="color:red;">厉害了，寻找数据分布的流行是什么，一直感觉流行这个词挺厉害的。要总结下。</span>

一个经典的非监督学习任务是找到数据的“最佳”表示。“最佳”可以是不同的表示，但是一般来说，是指该表示在比本身表示的信息更简单或更易访问而受到一些惩罚或限制的情况下，尽可能地保存关于 $\boldsymbol{x}$ 更多的信息。<span style="color:red;">有些厉害，最佳表示是指在比本身的表示的信息更简单或更易访问而受到一些惩罚或者限制的情况下，尽可能的保存的信息。嗯。这概念 nice。</span>

有很多方式定义较简单的表示。最常见的 3 种包括：<span style="color:red;">别的表示也要总结进来。</span>

- 低维表示
- 稀疏表示
- 独立表示

分别如下：

- 低维表示尝试将 $\boldsymbol{x}$ 中的信息尽可能压缩在一个较小的表示中。
- 稀疏表示将数据集嵌入到输入项大多数为零的表示中。稀疏表示通常用于需要增加表示维数的情况，使得大部分为零的表示不会丢失很多信息。这会使得表示的整体结构倾向于将数据分布在表示空间的坐标轴上。<span style="color:red;">什么意思？需要增加表示维数的情况是什么情况？表示的整体结构倾向于将数据分布在表示空间的坐标轴上是什么意思？好像没有呀？</span>
- 独立表示试图分开数据分布中变化的来源，使得表示的维度是统计独立的。<span style="color:red;">这个之前好像没有怎么接触过。</span>

当然，这 3 个标准并非相互排斥的。低维表示通常会产生比原始的高维数据具有较少或较弱依赖关系的元素。这是因为减少表示大小的一种方式是找到并消除冗余。识别并去除更多的冗余使得降维算法在丢失更少信息的同时显现更大的压缩。<span style="color:red;">嗯嗯，是的。</span>

表示的概念是深度学习核心主题之一。


在学习相关算法的时候要仔细对照着三个标准，一些额外的表示学习算法会以不同方式处理这 3 个标准或是引入其他标准。




# 相关

- 《深度学习》花书
