---
title: 3.03 概率模型与非概率模型
toc: true
date: 2019-08-28
---
# 概率模型与非概率模型

## 概率模型

概率模型指出了学习的目的是学出 $P(x,y)$ 或 $P(y|x)$，但最后都是根据 $y=\underset{y \in\{1,-1\}}{\arg \max } P(y | x)$ 来做判别归类。

对于 $P(x,y)$ 的估计，一般是根据乘法公式 $P(x,y) = P(x|y)P(y)$ 将其拆解成 $P(x|y)$,$P(y)$ 分别进行估计。

无论是对 $P(x|y)$,$P(y)$ 还是 $P(y|x)$ 的估计，都是会先假设分布的形式，

例如：<span style="color:red;">补充些别的例子。</span>

- 逻辑斯特回归就假设了 $Y|X$ 服从伯努利分布。<span style="color:red;">确认下。</span>

分布形式固定以后，剩下的就是分布参数的估计问题。常用的估计有：

- 极大似然估计(MLE)
- 极大后验概率估计(MAP)
- 等。

其中，极大后验概率估计涉及到分布参数的先验概率，这为我们注入先验知识提供了途径。<span style="color:red;">极大后验概率估计怎么帮助注入先验知识的？</span>


概率模型有：

- 逻辑斯特回归
- 高斯判别分析
- 朴素贝叶斯




## 非概率模型

非概率模型指的是直接学习输入空间到输出空间的映射 $h$，学习的过程中基本不涉及概率密度的估计，概率密度的积分等操作，问题的关键在于最优化问题的求解。

通常，为了学习假设 $h(x)$，我们会先根据一些先验知识(prior knowledge) 来选择一个特定的假设空间 $H$(函数空间)，例如一个由所有线性函数构成的空间，然后在这个空间中找出泛化误差最小的假设出来。

$$
h^{*}=\underset{h \in H}{\arg \min } \varepsilon(h)=\underset{h \in H}{\arg \min } \sum_{x, y} l(h(x), y) P(x, y)\tag{3}
$$

其中 $l(h(x),y)$ 是我们选取的损失函数，选择不同的损失函数，得到假设的泛化误差就会不一样。

由于我们并不知道 $P(x,y)$，所以即使我们选好了损失函数，也无法计算出假设的泛化误差，更别提找到那个给出最小泛化误差的假设。于是，我们转而去找那个使得经验误差最小的假设。

$$
g=\underset{h \in H}{\arg \min } \hat{\varepsilon}(h)=\underset{h \in H}{\arg \min } \frac{1}{m} \sum_{i=1}^{m} l\left(h\left(x^{(i)}\right), y^{(i)}\right)\tag{4}
$$


这种学习的策略叫经验误差最小化(ERM)，理论依据是大数定律：

- 当训练样例无穷多的时候，假设的经验误差会依概率收敛到假设的泛化误差。

要想成功地学习一个问题，必须在学习的过程中注入先验知识。前面，我们根据先验知识来选择假设空间，其实，在选定了假设空间后，先验知识还可以继续发挥作用，这一点体现在为我们的式子 【4】 中加上正则化项上，例如常用的 L1 正则化，L2正则化等。


$$
g=\underset{h \in H}{\arg \min } \hat{\varepsilon}(h)=\underset{h \in H}{\arg \min } \frac{1}{m} \sum_{i=1}^{m} l\left(h\left(x^{(i)}\right), y^{(i)}\right)+\lambda \Omega(h)\tag{5}
$$


正则化项一般是对模型的复杂度进行惩罚，例如我们的先验知识告诉我们模型应当是稀疏的，这时我们会选择 L1 范数。

当然，加正则化项的另一种解释是为了防止对有限样例的过拟合，但这种解释本质上还是根据先验知识认为模型本身不会太复杂。

在经验误差的基础上加上正则化项，同时最小化这两者，这种学习的策略叫做结构风险最小化(SRM)。最后，学习算法 A 根据训练数据集 $D$，从假设空间中挑出一个假设 $g$，作为我们将来做预测的时候可以用。具体来说，学习算法 A 其实是一个映射，对于每一个给定的数据集 $D$，对于选定的学习策略(ERM or SRM)，都有确定的假设与 $D$ 对应

$$
A : D \mapsto h, s . t . h=A(D)\tag{6}
$$


非概率模型有如下：

- 感知机
- 支持向量机
- 神经网络
- k近邻

线性支持向量机可以显式地写出损失函数——hinge损失。神经网络也可以显式地写出损失函数——平方损失。

时下流行的迁移学习，其中有一种迁移方式是基于样本的迁移。这种方式最后要解决的问题就是求解一个加权的经验误差最小化问题，而权重就是目标域与源域的边际密度之比。所以，线性支持向量机在迁移学习的环境下可以进行直接的推广。


## 概率模型与非概率模型的对应关系


在一定的条件下，非概率模型与概率模型有以下对应关系：

<center>

![mark](http://images.iterate.site/blog/image/20190828/SEtoCpiOxu09.png?imageslim)

</center>





# 相关

- [监督学习的分类：判别模型与生成模型，概率模型与非概率模型、参数模型与非参数模型](https://zhuanlan.zhihu.com/p/26012348)
