---
title: 2.04 数据变换的方法
toc: true
date: 2019-09-20
---


# 数据变换方法的含义与应用


Rescaling（重缩放/归一化）：通常是指增加或者减少一个常数，然后乘以/除以一个常数，来改变数据的衡量单位。例如：将温度的衡量单位从摄氏度转化为华氏温度。

Normalizing（正则化）：通常是指除以向量的范数。例如：将一个向量的欧氏长度等价于 1 。在神经网络中，“正则化”通常是指将向量的范围重缩放至最小化或者一定范围，使所有的元素都在 $[0,1]$ 范围内。通常用于文本分类或者文本聚类中。

Standardizing（标准化）：通常是为了消除不同属性或样方间的不齐性，使同一样方内的不同属性间或同一属性在不同样方内的方差减小。例如：如果一个向量包含高斯分布的随机值，你可能会通过除以标准偏差来减少均值，然后获得零均值单位方差的“标准正态”随机变量。

那么问题是，当我们在训练模型的时候，一定要对数据进行变换吗？

这得视情况而定。很多人对多层感知机有个误解，认为输入的数据必须在 $[0,1]$ 这个范围内。虽然标准化后在训练模型效果会更好，但实际上并没有这个要求。但是最好使输入数据中心集中在 0 周围，所以把数据缩放到[0,1]其实并不是一个好的选择。

如果你的输出激活函数的范围是 $[0,1]$(sigmoid函数的值域)，那你必须保证你的目标值也在这个范围内。但通常情况下，我们会使输出激活函数的范围适应目标函数的分布，而不是让你的数据来适应激活函数的范围。<span style="color:red;">是的。</span>

当我们使用激活函数的范围为 $[0,1]$ 时，有些人可能更喜欢把目标函数缩放到 $[0.1,0.9]$ 这个范围。我怀疑这种小技巧的之所以流行起来是因为反向传播的标准化太慢了导致的。但用这种方法可能会使输出的后验概率值不对。如果你使用一个有效的训练算法的话，完全不需要用这种小技巧，也没有必要去避免溢出（overflow）。



# 相关

- [机器学习基础与实践（二）----数据转换](https://www.cnblogs.com/charlotte77/p/5622325.html)
