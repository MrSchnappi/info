---
title: 2.11 均方误差损失函数 MSE
toc: true
date: 2019-09-03
---
# 可以补充进来的

- 感觉写法不正规



# 均方误差损失函数 MSE

又叫做：

- 均方误差损失
- 平方损失损失
- L2 损失
- quadratic cost
- MSE

公式：



$$
L(Y, f(x)) = \sum_N{(Y-f(x))}^2
$$

使用场景：

- 是最常用的回归损失函数


说明：

均方误差（MSE）度量的是预测值和实际观测值间差的平方的均值。它只考虑误差的平均大小，不考虑其方向。但由于经过平方，与真实值偏离较多的预测值会比偏离较少的预测值受到更为严重的惩罚。再加上 MSE 的数学特性很好，这使得计算梯度变得更容易。

平方损失函数是光滑函数，能够用梯度下降法进行优化。然而，当预测值距离真实值越远时，平方损失函数的惩罚力度越大，因此它对异常点比较敏感。<span style="color:red;">是的。</span>

## 使用在逻辑回归中时


使用一个样本为例简单说明，此时二次代价函数为：

$$
J = \frac{(y-a)^2}{2}
$$

假如使用梯度下降法（Gradient descent）来调整权值参数的大小，权值 $w$ 和偏置 $b$ 的梯度推导如下：

$$
\frac{\partial J}{\partial b}=(a-y)\sigma'(z)
$$

其中，$z​$ 表示神经元的输入，$\sigma​$ 表示激活函数。权值 $w​$ 和偏置 $b​$ 的梯度跟激活函数的梯度成正比，激活函数的梯度越大，权值 $w​$ 和偏置 $b​$ 的大小调整得越快，训练收敛得就越快。

注：神经网络常用的激活函数为 sigmoid 函数，该函数的曲线如下图所示：

<center>

![](http://images.iterate.site/blog/image/20190722/4hydKl8lFODO.jpg?imageslim){ width=55% }

</center>


如上图所示，对 0.88 和 0.98两个点进行比较：
​
- 假设目标是收敛到 1.0。0.88离目标 1.0比较远，梯度比较大，权值调整比较大。0.98离目标 1.0比较近，梯度比较小，权值调整比较小。调整方案合理。
- 假如目标是收敛到 0。0.88离目标 0 比较近，梯度比较大，权值调整比较大。0.98离目标 0 比较远，梯度比较小，权值调整比较小。调整方案不合理。
​
原因：在使用 sigmoid 函数的情况下, 初始的代价（误差）越大，导致训练越慢。<span style="color:red;">嗯，初始的代价误差越大，导致训练越慢。还是有些没有理解。</span>

# 相关

- [分位数回归的代码](https://github.com/groverpr/Machine-Learning/blob/master/notebooks/09_Quantile_Regression.ipynb)
