---
title: 2.15 分位数损失
toc: true
date: 2019-09-03
---
# 分位数损失

在大多数现实世界预测问题中，我们通常希望了解预测中的不确定性。

清楚预测的范围而非仅是估计点，对许多商业问题的决策很有帮助。

当我们更关注区间预测而不仅是点预测时，分位数损失函数就很有用。使用最小二乘回归进行区间预测，基于的假设是残差（y-y_hat）是独立变量，且方差保持不变。

一旦违背了这条假设，那么线性回归模型就不成立。<span style="color:red;">是的。</span>

但是我们也不能因此就认为使用非线性函数或基于树的模型更好，而放弃将线性回归模型作为基线方法。

这时，分位数损失和分位数回归就派上用场了，因为即便对于具有变化方差或非正态分布的残差，基于分位数损失的回归也能给出合理的预测区间。

下面让我们看一个实际的例子，以便更好地理解基于分位数损失的回归是如何对异方差数据起作用的。

## 分位数回归与最小二乘回归


![mark](http://images.iterate.site/blog/image/20190902/j4SnaE7CSRyh.png?imageslim)

左：b/wX1和 Y 为线性关系。具有恒定的残差方差。右：b/wX2和 Y 为线性关系，但 Y 的方差随着 X2 增加。（异方差）


![mark](http://images.iterate.site/blog/image/20190902/82lCIDfQoubr.png?imageslim)


橙线表示两种情况下 OLS 的估值

![mark](http://images.iterate.site/blog/image/20190902/5rfTapV0mJWi.png?imageslim)

分位数回归。虚线表示基于 0.05和 0.95分位数损失函数的回归

## 理解分位数损失函数

如何选取合适的分位值取决于我们对正误差和反误差的重视程度。损失函数通过分位值（$\gamma$）对高估和低估给予不同的惩罚。例如，当分位数损失函数 $\gamma =0.25$ 时，对高估的惩罚更大，使得预测值略低于中值。

$$
L_{\gamma}\left(y, y^{p}\right)=\sum_{i : y_{i}<y_{i}^{p}}(1-\gamma)\left|y_{i}-y_{i}^{p}\right|+\sum_{i : y_{i} \geq y_{i}^{p}} \gamma\left|y_{i}-y_{i}^{p}\right|
$$

$\gamma$ 是所需的分位数，其值介于 0 和 1 之间。

![mark](http://images.iterate.site/blog/image/20190902/l5b55IkcuykL.png?imageslim)

## 这个损失函数也可以在神经网络或基于树的模型中计算预测区间

这个损失函数也可以在神经网络或基于树的模型中计算预测区间。以下是用 Sklearn 实现梯度提升树回归模型的示例。

使用分位数损失（梯度提升回归器）预测区间：

![mark](http://images.iterate.site/blog/image/20190902/7HcbnQaPBOVP.png?imageslim)



上图表明：在 sklearn 库的梯度提升回归中使用分位数损失可以得到 90％的预测区间。其中上限为 $\gamma=0.95$，下限为 $\gamma=0.05$。



# 相关

- [分位数回归的代码](https://github.com/groverpr/Machine-Learning/blob/master/notebooks/09_Quantile_Regression.ipynb)
