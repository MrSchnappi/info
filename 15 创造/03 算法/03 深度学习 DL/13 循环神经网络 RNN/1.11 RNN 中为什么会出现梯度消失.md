---
title: 1.11 RNN 中为什么会出现梯度消失
toc: true
date: 2019-09-03
---

## 6.9 RNN 中为什么会出现梯度消失？

首先来看 tanh 函数的函数及导数图如下所示：

<center>

![](http://images.iterate.site/blog/image/20190722/ixiOvxKjmRsR.jpg?imageslim){ width=45% }

</center>


sigmoid函数的函数及导数图如下所示：

<center>

![](http://images.iterate.site/blog/image/20190722/OEtthch7MiF8.jpg?imageslim){ width=65% }

</center>


从上图观察可知，sigmoid函数的导数范围是(0,0.25]，tach函数的导数范围是(0,1]，他们的导数最大都不大于 1。

基于 6.8中式（9-10）中的推导，RNN的激活函数是嵌套在里面的，如果选择激活函数为 $tanh$ 或 $sigmoid$，把激活函数放进去，拿出中间累乘的那部分可得：
$$
\prod_{j=k+1}^{t}{\frac{\partial{h^{j}}}{\partial{h^{j-1}}}} = \prod_{j=k+1}^{t}{tanh^{'}}\cdot W_{s}
$$

$$
\prod_{j=k+1}^{t}{\frac{\partial{h^{j}}}{\partial{h^{j-1}}}} = \prod_{j=k+1}^{t}{sigmoid^{'}}\cdot W_{s}
$$

**梯度消失现象**：基于上式，会发现累乘会导致激活函数导数的累乘，如果取 tanh 或 sigmoid 函数作为激活函数的话，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于 $0$，这就是“梯度消失“现象。<span style="color:red;">是的。</span>

实际使用中，会优先选择 tanh 函数，原因是 tanh 函数相对于 sigmoid 函数来说梯度较大，收敛速度更快且引起梯度消失更慢。<span style="color:red;">嗯。梯度较大的就越好吗？越使得收敛快而且梯度消失更慢吗？</span>








# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
