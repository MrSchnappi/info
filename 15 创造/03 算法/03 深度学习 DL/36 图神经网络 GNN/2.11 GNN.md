---
title: 2.11 GNN
toc: true
date: 2019-09-01
---
# 原始 GNN


本节将描述 The graph neural network model (Scarselli, F., et al., 2009) [1] 这篇论文中的算法，这是第一次提出 GNN 的论文，因此通常被认为是原始 GNN。

在节点分类问题设置中，每个节点 $v$ 的特征 $x_v$ 与一个 ground-truth 标签 $t_v$ 相关联。给定一个部分标记的 graph $G$，目标是利用这些标记的节点来预测未标记的节点的标签。它学习用包含邻域信息的 $d$ 维向量 $h_v$ 表示每个节点。即：

$$
\mathbf{h}_{v}=f\left(\mathbf{x}_{v}, \mathbf{x}_{c o[v]}, \mathbf{h}_{n e[v]}, \mathbf{x}_{n e[v]}\right)
$$

其中 $x_co[v]$ 表示与 $v$ 相连的边的特征，$h_ne[v]$ 表示 $v$ 相邻节点的嵌入，$x_ne[v]$ 表示 $v$ 相邻节点的特征。函数 $f$ 是将这些输入映射到 $d$ 维空间上的过渡函数。由于我们要寻找 $h_v$ 的唯一解，我们可以应用 Banach 不动点定理，将上面的方程重写为一个迭代更新过程。

$$
\mathbf{H}^{t+1}=F\left(\mathbf{H}^{t}, \mathbf{X}\right)
$$

$H$ 和 $X$ 分别表示所有 $h$ 和 $x$ 的串联。

通过将状态 $h_v$ 和特性 $x_v$ 传递给输出函数 $g$，从而计算 GNN 的输出。
$$
\mathbf{o}_{v}=g\left(\mathbf{h}_{v}, \mathbf{x}_{v}\right)
$$

这里的 $f$ 和 $g$ 都可以解释为前馈全连接神经网络。L1 loss 可以直接表述为：

$$
\operatorname{loss}=\sum_{i=1}^{p}\left(\mathbf{t}_{i}-\mathbf{o}_{i}\right)
$$

可以通过梯度下降进行优化。

然而，原始 GNN 存在三个主要局限性：

- 如果放宽 “不动点” (fixed point)的假设，那么可以利用多层感知器学习更稳定的表示，并删除迭代更新过程。这是因为，在原始论文中，不同的迭代使用转换函数 $f$ 的相同参数，而 MLP 的不同层中的不同参数允许分层特征提取。
- 它不能处理边缘信息 (例如，知识图中的不同边缘可能表示节点之间的不同关系)
- 不动点会阻碍节点分布的多样性，不适合学习表示节点。

为了解决上述问题，研究人员已经提出了几个 GNN 的变体。不过，它们不是本文的重点。



# 相关

- [掌握图神经网络 GNN 基本，看这篇文章就够了](https://posts.careerengine.us/p/5c64eebe4337430d41ceae7a)
