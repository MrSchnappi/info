---
title: 3.11.1 RNN 中的 Attention 机制
toc: true
date: 2019-09-03
---

# 图解 RNN 中的 Attention 机制


在上述通用的 Encoder-Decoder 结构中，Encoder 把所有的输入序列都编码成一个统一的语义特征 $c​$ 再解码，因此，$c​$ 中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。<span style="color:red;">是呀。</span>

如机器翻译问题，当要翻译的句子较长时，一个 $c​$ 可能存不下那么多信息，就会造成翻译精度的下降。<span style="color:red;">是的。</span>

Attention 机制通过在每个时间输入不同的 $c​$ 来解决此问题。<span style="color:red;">怎么在不同的时间输入不同的 $c​$ 的？</span>

引入了 Attention 机制的 Decoder 中，有不同的 $c$，每个 $c​$ 会自动选择与当前输出最匹配的上下文信息，其示意图如下所示：<span style="color:red;">这些不同的 $c​$ 是哪里来的？</span>

<center>

![](http://images.iterate.site/blog/image/20190722/qmpfl6NthoBH.jpg?imageslim){ width=35% }

</center>


**举例**，比如输入序列是“我爱中国”，要将此输入翻译成英文：

<span style="color:red;">这个例子没看懂。</span>

假如用 $a_{ij}$ 衡量 Encoder 中第 $j$ 阶段的 $h_j$ 和解码时第 $i$ 阶段的相关性，$a_{ij}$ 从模型中学习得到，和 Decoder 的第 $i-1$ 阶段的隐状态、Encoder 第 $j$ 个阶段的隐状态有关，比如 $a_{3j}​$ 的计算示意如下所示：<span style="color:red;">这个地方没有怎么看懂。</span>

<center>

![](http://images.iterate.site/blog/image/20190722/aixWU1gkAgEv.jpg?imageslim){ width=40% }

</center>


最终 Decoder 中第 $i$ 阶段的输入的上下文信息 $c_i$ 来自于所有 $h_j$ 对 $a_{ij}$ 的加权和。

其示意图如下图所示：

<center>

![](http://images.iterate.site/blog/image/20190722/mfnABsXO7z8x.jpg?imageslim){ width=75% }

</center>


在 Encoder 中，$h_1,h_2,h_3,h_4$ 分别代表“我”，“爱”，“中”，“国”所代表信息。翻译的过程中，$c_1$ 会选择和“我”最相关的上下文信息，如上图所示，会优先选择 $a_{11}$，以此类推，$c_2$ 会优先选择相关性较大的 $a_{22}$，$c_3$ 会优先选择相关性较大的 $a_{33}，a_{34}$，这就是 attention 机制。









# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
