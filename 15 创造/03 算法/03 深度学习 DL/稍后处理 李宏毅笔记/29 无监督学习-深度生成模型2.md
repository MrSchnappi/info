---
title: 29 无监督学习-深度生成模型2
toc: true
date: 2019-08-18
---
![mark](http://images.iterate.site/blog/image/20190818/MSwsiAHrXzmV.png?imageslim)


![mark](http://images.iterate.site/blog/image/20190818/uwOcnxNAf9Jt.png?imageslim)


## 为什么要用 VAE？

我们来看 intuitive reason，为什么要用 VAE？如果是原来的 auto-encoder的话，做的事情是：把每一张 image 变成一个 code，假设现在的 code 是一维(图中红色的线)。你把满月这个图变为 code 上的一个 value，在做 decoder 变回原来的图，半月图也是一样的。假设我们在满月和半月 code 中间，sample一个点，然后把这个点做 decoder 变回 image，他会变成什么样子呢？你可能会期待说：可能会变成满月和半月之间的样子，但这只是你的想象而已。因为我们今天用的 encoder 和 decoder 都是 non-linear的，都是一个 neural network，所以你很难预测在这个满月和半月中间到底会发生什么事情。


那如果用 VAE 会有什么好处呢？VAE做事情是：当你把这个满月的图变成一个 code 的时候，它会在这个 code 上面再加上 noise，它会希望加上 noise 以后，这个 code reconstruct以后还是一张满月。也就是说：原来的 auto-encoder，只有中间这个点需要被 reconstruct 回满月的图，但是对 VAE 来说，你会加上 noise，在这个范围之内 reconstruct 回来都要是满月的图，半月的图也是一样的。你会发现说：在这个位置，它同时希望被 reconstruct 回满月的图，又希望被 reconstruct 回半月的图，可是你只能 reconstruct 一张图。肿么办？VAE training的时候你要 minimize mean square，所以这个位置最后产生的图会是一张介于满月和半月的图。所以你用 VAE 的话，你从你的 code space上面去 sample 一个 code 再产生 image 的时候，你可能会得到一个比较好的 image。如果是原来的 auto-encoder的话，得到的都不像是真实的 image。

![mark](http://images.iterate.site/blog/image/20190818/smkABOOUeHIB.png?imageslim)






所以这个 encoder output m代表是原来的 code，这个 c 代表是加上 noise 以后的 code。decoder要根据加上 noise 以后的 code 把它 reconstruct 回原来的 image。 $\sigma$ 代表了 noise 的 variance，e是从 normal distribution sample出来的值，所以 variance 是固定的，当你把 $\sigma$ 乘上 e 再加上 m 的时候就等于你把原来的 code 加上 noise，e是从 normal distribution sample出来的值，所以 variance 是固定的，但是乘上一个 $\sigma$ 以后，它的 variance 大小就有所改变。这个 variance 决定了 noise 的大小，这个 variance 是从 encoder 产生的，也就是说：machine在 training 的时候，它会自动去 learn 这个 variance 会有多大。

![mark](http://images.iterate.site/blog/image/20190818/YqjHItXNyMea.png?imageslim)

但是如果还是这样子还是不够的，假如你现在的 training 只考虑：现在 input 一张 image，中间有加 noise 的机制，然后 dec oder reconstruct回原来的 image，然后 minimize 这个 reconstruct error，你只有做这件事情是不够的，你 training 的出来的结果并不会如你所预期的样子。

因为这个 variance 是自己学的，假设你让 machine 自己决定说 variance 是多少，它一定会决定说 variance是 0 就好了(就像让自己决定自己的分数的话，得 100 分就好了)。所以 variance 只让 machine 自己决定的话，variance是 0 就 ok 了，那就等于原来的 auto-encoder。所以你要这个 variance 上面去做一些限制，强迫它的 variance 不可以太小。所以我们另外加的这一项 $\sum_{i=1}^{3}(exp(\sigma_i)-(1+\sigma_i)+(m_i)^2)$ ，这一项其实就是对 variance 做了一下限制



这边有 $exp(\sigma_i)-(1+\sigma_i)$ ， $exp(\sigma_i)$ 是图中蓝色的线， $(1+\sigma_i)$ 是图中红色的这条线。把蓝色线减去红色线得到的是绿色这条线，绿色这条线的最低点是落在 $\sigma=0$ 的地方， $\sigma=0$ 的话，exp( $\sigma$ )=1，意味着 variance=1( $\sigma=0$ 的时候 loss 最后，意味着 variance=1的时候 loss 最低)。所以 machine 就不会说：让 variance=0，然后 minimize reconstruct error，它还要考虑 variance 不能够太小。最后这一项 $m_i^2$ 就相当于加了是 L2-Norm。我们常常在 training auto-encoder的时候，你就会在你的 code 上面加上 regularization，让它的结果不会 overfitting。

![mark](http://images.iterate.site/blog/image/20190818/ECRGNTvVGs4X.png?imageslim)

刚才是比较直观的理由，正式的理由这样的，以下是 paper 上比较常见的说法。假设我们回归到我们要做的事情是什么，你要 machine generate 这个 pokemon 的图，那每一张 pokemon 的图都可以想成是高维空间中的一个点。假设它是 20*20的 image，在高维空间中也就是 400 *400维的点，在图上我们用一维描述它，但其实是一个高维的空间。那现在要做的事情就是 Estimate 高维空间上的几率分布，我们要做的事情就是 estimate 这个 p(x)，只要我们 estimate p(x)的样子，那我们就可以根据 p(x)去 sample 出一张图，找出来的图就会是像宝可梦的样子(p(x)几率最高的比较容易被 sample1 出来)。这个 p(x)理论上在有 pokemon 的地方，它的几率是最大的，若在怪怪的图上，几率是低的。如果我们今天能够 estimate the probability distributon就结束了。

![mark](http://images.iterate.site/blog/image/20190818/63CRVMbBu70N.png?imageslim)


咋样 estimate the probability distributon呢？我们可以用 gaussion mixture model。guassion mixture model：我们现在有一个 distribution(黑色的线)，这个黑色的 distribution 其实是很多的 gaussion(青蓝色)用不同的 weight 叠合起来的结果。如果你的 gaussion 数目够多，你就可以产生很复杂的 distribution，公式为 $p(x)=\sum_{m}p(m)p(x|m)$ 。

###  高斯混合模型

如果你要从 p(x)sample出一个东西的时候，你先要决定你要从哪一个 gaussion sample东西，假设现在有 100gaussion(每一个 gaussion 都有自己的一个 weight)，你根据每个 gaussion 的 weight 去决定你要从哪一个 gaussion sample data。所以你要咋样从一个 gaussion mixture model smaple data呢？首先你有一个 multinomial distribution，你从 multinomial distribution里面决定你要 sample 哪一个 gaussion，m代表第几个 gaussion，它是一个 integer。你决定好你要从哪一个 m sample gaussion以后，，你有了 m 以后就可以找到 $\mu ^m,Σ^m$ (每一个 gaussion 有自己的 $\mu ^m,Σ^m$ )，根据 $\mu ^m,Σ^m$ 就可以 sample 一个 x 出来。所以 p(x)写为 summation over所有的 gaussion，哪一个 gaussion 的 weight 乘以有一个 gaussion sample出 x 的几率


每一个 x 都是从某一个 mixture 被 sample 出来的，这件事情其实就很像是做 classification 一样。我们每一个所看到的 x，它都是来自于某一个分类。但是我们之前有讲过说：把 data 做 cluster 是不够的，更好的表示方式是用 distributed representation，也就是说每一个 x 它并不是属于某一个 class，而是它有一个 vector 来描述它的各个不同面向的特性。所以 VAE 就是 gaussion mixture model distributed representation的版本。

![mark](http://images.iterate.site/blog/image/20190818/xtLI7az98JjT.png?imageslim)





首先我们要 sample 一个 z，这个 z 是从 normal distribution sample出来的。这个 vector z的每一个 dimension 就代表了某种 attribute，假设是 z 是这样的(如图)，现在图上假设它是低维的，但是在实际上它是高维的，到底是几维你自己决定。接下来你 Sample z以后，根据 z 你可以决定 $\mu(z),variance$ ，你可以决定 gaussion 的 $\mu,\sigma$ 。刚才在 gaussion model里面，你有 10 个 mixture，那你就有 10 个 $\mu,\sum $ ，但是在这个地方，你的 z 有无穷多的可能，所以你的 $\mu,variance $ 也有无穷多的可能。那咋样找到这个 $\mu,  variance $ 呢？做法是：假设 $\mu,variance $ 都来自于一个 function，你把 z 带到产生 $\mu$ 的这个 function $N(\mu(z),\sigma(z))$ ， $\mu(z)$ 代表说：现在如果你的 attribute 是 z 的时候，你在 x space上面的 $\mu$ 是多少。同理 $\sigma(z)$ 代表说：variance是多少

其实 P(x)是这样产生的：在 z 这个 space 上面，每一个点都有可能被 sample 到，只不过是中间这些点被 sample 出来的几率比较大。当你 sample 出来点以后，这个 point 会对应到一个 guassion。至于一个点对应到什么样的 gaussion，它的 $\mu,\sum$ 是多少，是由某一个 function 来决定的。所以当 gaussion 是从 normal distribution所产生的时候，就等于你有无穷多个 gaussion。

另外一个问题就是：我们肿么知道每一个 z 应该对应到什么样的 $\mu,\sum$ (这个 function 如何去找)。我们知道 neural network就是一个 function，所以你就可以说：我就是在 train 一个 neural network，这个 neural network就是 z，它的 output 就是两个 vector( $\mu(z),\sigma(z)$ )。第一个 vector 代表了 input 是 z 的时候你 gaussion 的 $\mu$ ， $\sigma$ 代表了 variance

我们有一个 neural network可以告诉我们说：在 z 这个 space 上面的每一个点对应到 x 的 space 时，你的 $\mu,variance$ 分别是多少。

![mark](http://images.iterate.site/blog/image/20190818/dQwJygHEj0Nv.png?imageslim)

p(x)的 distribution 会这样的：P(z)的几率跟我们知道 z 的时候 x 的几率，在对所有可能的 z 做积分(因为 z 是 continue 的)。那你可能会困惑，为什么是 gaussion 呢？你可以假设任何形状的，这是你自己决定的。你可以说每一个 attribute 的分布就是 gaussion，极端的 case 总是少的，比较没有特色的东西总是比较多的。你不用但心说：你如果假设 gaussion 会不会对 p(x)带来很大的限制。其实不用担心这个问题，NN是非常 powerful 的，NN可以 represent 任何的 function。所以就算你的 z 是 normal distribution，最后的 p(x)最后也可以是很复杂的 distribution。



### 最大化可能性
p(z) is a normal distribution，我们先知道 z 是什么，然后我们就可以决定 x 是从咋样的 $\mu,variance$ function里面被 sample 出来的， $\mu,variance$ 中间是关系是不知道的。咋样找呢？它的 equation 就是 maximizing the likelihood，我们现在手上已经有一笔 data x，你希望找到一组 $\mu$ 的 function 和 $\sigma$ 的 function，它可以让你现在已经有的 image(每一个 x 代表一个 image)，它的 p(x)取 log 之后相加是被 maximizing。所以我们要做的事情就是，调整 NN 里面的参数(每个 neural 的 weight bias)，使得 likehood 可以被 maximizing。



引入另外一个 distribution，叫做 $q(z|x)$ 。也就是我们有另外一个 $NN'$ ，input一个 x 以后，它会告诉你说：对应 z 的 $\mu',\sigma'$ (给它 x 以后，它会决定这个 z 要从什么样的 $\mu,variance$ 被 sample 出来)。上面这个 NN 就是 VAE 里的 decoder，下面这个 $NN'$ 里的 encoder

![mark](http://images.iterate.site/blog/image/20190818/2kPfoUaJiRrM.png?imageslim)



 $logP(x)=\int_{z}q(z|x)logP(x)dz$ ，因为 $q(z|x)$ 它是一个 distribution，对任何 distribution 都成立。假设 $q(z|x)$ 是路边捡来的 distribution(可以是任何的 distribution)，因为这个积分是跟 P(x)无关的，然后就可以提出来，积分的部分就会变成 1，所以左式就等于右式。

最后得到式子如图所示，右边的式子中 $q(z|x)$ 是一个 distribution， $logP(x)=\int_{z}q(z|x)logP(x)dz$ 也是一个 distribution，KL divergence代表这两个 distribution 相近的程度(KL divergence越大，代表这个 distribution 越不像，KL divergence衡量一个距离的概念)，右边这个式子是距离，所以一定是大于等于 0，所以 L 一定会大于等于 $\int_{m}q(z|x)log(\frac{p(x|z)p(z)}{q(z|x)})$ 这一项，这一项即是 lower bounud，称为 $L_b$ 。

![mark](http://images.iterate.site/blog/image/20190818/u5hEEzoBCJS1.png?imageslim)



我们要 maximizing 的对象是由这两项加起来的结果，在 $L_b$ 这个式子中，p(z)是已知的，我们不知道的是 $p(x|z)$ 。我们本来要做的事情是要找 $p(x|z)$ 跟 $q(z|x)$ ，让这个 likehood 越大越好，现在我们要做的事情变成要找找 $p(x|z)$ 跟 $q(z|x)$ ，让 $L_b$ 越大越好。如果我们只找这一项的话( $p(x|z)$ )，然后去 maximizing  $L_b$  的话，你增 j 加 $L_b$ 的时候，你有可能会增加你的 likehood，但是你不知道你的 likehood 跟 lower bound之间到底有什么样的距离。你希望你做到的是：当你的 lower bound上升的时候，你的 likehood 也跟着上升。但是有可能会遇到糟糕的事情是：你的 lower bound上升的时候，likehood反而下降(因为不知道它们之间的差距是多少)。

所以引入这一项( $L_b$ )可以解决刚才说的那个问题。因为：如图蓝色的是 likehood， $likehood=L_b+KL$ ，如果你今天调 $q(z|x)$ maximizing  $L_b$ 的话。你会发现说 $q(z|x)$ 跟 log p(x)是没有关系的(log p(x)。但是我们却 maximizing  $L_b$ 代表说你 minimize 这个 KL divergence，也就是说你让 lower bound跟 likehood 越来越接近(maximize  $q(z|x)$ )。加入你今天去固定住 $p(x|z)$ 这一项，去调 $q(z|x)$ 这一项的话，你会让 $L_b$ 一直上升，这个 KL divergence会完全不见。因为你的 likehood 一定要比 lower bound大，所以你确定 likehood 一定会上升。


今天我们也会得到一个副产物，你当你 maximize $q(z|x)$ 这一项的时候，你会让 KL divergence越来越小，意味着说：你会让 $q(z|x)$ 跟 $p(z|x)$ 越来越接近。所以接下来做的事情就是找 $p(x|z)$ 跟 $q(z|x)$ ，可以让 $L_b$ 越大越好，就等同于让 likehood 越来越大。在最后你顺便会找到 $q(z|x)$  approximation p(z|x)。

![mark](http://images.iterate.site/blog/image/20190818/sn1BMIx6hXu4.png?imageslim)



 $p(z)$ 是一个 distribution， $q(z|x)$ 也是一个 distribution，所以 $\int_{z}q(z|x)log\frac{p(z)}{q(z|x)}$ 是 KL divergence。复习一下，q是一个 neural network，当你给 x 的时候，它会告诉你： $q(z|x)$ 是从什么样的 $\mu,variance$ gaussion sample出来的。

![mark](http://images.iterate.site/blog/image/20190818/DT6PWmGWI6zi.png?imageslim)





### 与网络连接
你要 minimizing KL $(q(z|x)||p(z))$ 的话，你就是去调 q 对应的 neural network产生的 distribution 可以跟 normal distribution越接近越好。minimize这一项其实就是我们刚才在 reconstruction error另外加的那一项，它要做的事情就是 minimize KLdivergence，它要做的事情就是：希望 $q(z|x)$ 的 output 跟 normal distribution是接近的

![mark](http://images.iterate.site/blog/image/20190818/qRANdk0rWGD7.png?imageslim)

还有另外一项 $\int_{z}q(z|x)log P(x|z)dz$ ，可以写成 $logP(x|z)$ 根据 $q(z|x)$ 的期望值。从 $q(z|x)$ 去 sample data，然后让 $logP(x|z)$ 的几率越大越好，其实这件事情就是 auto-encoder在做的事情。咋样理解从 $q(z|x)$ 去 sample data：你把 x 丢进 neural network里面去，产生一个 $\mu,variance$ 。根据 $\mu,variance$ 你就可以 sample 出来一个 z。接下来 maximize 产生 $logP(x|z)$ 的几率，其实就是把 z 丢到另外 neural network，产生一个 $\mu,variance$ 。咋样让这个几率越大越好呢？假设我们忽视 variance(一般在实做里面不会把 variance 这件事考虑进去)，只考虑 $\mu$ 这一项的话。你要做的就是让 $\mu$ 跟 x 越接近越好。现在是 gaussion distribution，在 $\mu$ 的几率是最大的，所以你要 NN output这个 $\mu$ 等于 x 的话，那 $logP(x|z)$ 这一项是最大的。

所以整个 case 就变成说：input一个 x，然后产生两个 vector，产生一个 z，再根据这个 z，产生另外一个 vector 跟原来的 x 越接近越好。其实这件事情就是 auto-encoder在做的事情，你要你的 input 跟 output 越接近越好。所以这两项合起来就是前面看到的 VAE 的 loss function

![mark](http://images.iterate.site/blog/image/20190818/reiO5zge3a2Y.png?imageslim)
### 有条件的 VAE

还有一种 conditional VAE，如果你让 VAE 可以产生手写的数字，就是给它一个 digit，然后它把这个 digit 的特性抽取出来(笔画的粗细等等)，然后丢进 encoder 的时候一方面给它有关这个数字特性的 distribution，另外一方面告诉 decoder 它是什么数字。那你就可以根据这一个 digit，generate跟它 style 相近的 digit。你会发现说 conditional VAE可以根据某一个 digit 画出跟它 style 相近的数字。


![mark](http://images.iterate.site/blog/image/20190818/Xv92RRQkERhF.png?imageslim)
这是一些 reference 提供参考：
Carl Doersch, Tutorial on Variational Autoencoders

Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling,"Semi-supervised learning with deep generative models."NIPS,2014.

Sohn, Kihyuk, Honglak Lee, and Xinchen Yan,"Learning Structured Output Representation using Deep Conditional Generative Models."NIPS,2015.

Xinchen Yan, Jimei Yang, Kihyuk Sohn, Honglak Lee,"Attribute2lmage: Conditional Image Generation from Visual Attributes", ECCV,2016

Cool demo:
· http://vdumoulin. github. io/morphing_faces/
· http://fvae. ail. tokyo/



### VAE的问题

VAE其实有一个很严重的问题就是：它从来没有真正学咋样产生一张看起来像真的 image，它学到的东西是：它想要产生一张 image，跟我们在 database 里面某张 image 越接近越好。但它不知道的是：我们 evaluate 它产生的 image 跟 database 里面的相似度的时候(MSE等等)，decoder output跟真正的 image 之间有一个 pixel 的差距，不同的 pixel 落在不同的位置会得到非常不一样的结果。假设这个不一样的 pixel 落在 7 的尾部(让 7 比较长一点)，跟落在另外一个地方(右边)。你一眼就看出说：右边这是怪怪的 digit，左边这个搞不好是真的(只是长了一点而已)。但是对 VAE 来说都是一个 pixel 的差异，对它来说这两张 image 是一样的好或者是一样的不好。

所以 VAE 学的只是咋样产生一张 image 跟 database 里面的一模一样，从来没有想过：要真的产生可以一张以假乱真的 image。所以你用 VAE 来做 training 的时候，其实你产生出来的 image 往往都是 database 里面的 image linear combination而已。因为它从来都没有想过要产生一张新的 image，它唯一做的事情就是模仿而已


![mark](http://images.iterate.site/blog/image/20190818/7JRjR0zALtrV.png?imageslim)


所以接下来有了一个方法 Generative Adversarial Network(GAN)，最早出现在 2014 年。

![mark](http://images.iterate.site/blog/image/20190818/GSOX5C1k99xF.png?imageslim)
这里引用了 Yann LeCun's comment，有人问了：what are some recent and potentially upcoming breakthrough in unsupervised learning，Yann LeCun's 亲自来回答：adversarial training is the coolest thing since sliced bread(since sliced bread__有始以来)

![mark](http://images.iterate.site/blog/image/20190818/xTedxRq9jeo5.png?imageslim)
![mark](http://images.iterate.site/blog/image/20190818/bEGepvYnebfj.png?imageslim)
## GAN

GAN的概念像是拟态的演化，如图是一个枯叶蝶(长的像枯叶一样)，枯叶蝶咋样变得像枯叶一样的呢？也许一开始它长是这个样子(如左图)。但是它有天敌(麻雀)，天敌会吃这个蝴蝶，天敌辨识是不是蝴蝶的方式：它知道蝴蝶不是棕色的，所以它吃不是棕色的东西。所以蝴蝶就演化了，它就变成了棕色的了。它的天敌会跟着演化，天敌知道蝴蝶是没有叶脉的，所以它会吃没有叶脉的东西。所以蝴蝶演化变成枯叶蝶(有叶脉)，它的天敌也会演化，蝴蝶和它的天敌会共同的演化，枯叶蝶会不断的演化，直到和枯叶无法分别为止。


![mark](http://images.iterate.site/blog/image/20190818/s4dfl10kfA1S.png?imageslim)
所以 GAN 的概念是非常类似的，首先有第一代的 generator，generate一些奇怪的东西(看起来不是像是 image)。接下来有第一代的 Discriminator(它就是那个天敌)，Discriminator做的事情就是：它会根据 real images跟 generate 产生的 images 去调整它里面的参数，去评断说：这是真正的 image 还是 generate 产生的 image。接下来 generator 根据 discriminator 调整了下它的参数，所以第二代 generator 产生的 digit 更像是真的 image，接下来 discri minator会根据第二代 generator 产生的 digit 跟真正的 digit 再 update 它的参数。接下来产生第三代 generator，产生的 digit 更像真正的数字(第三代 generator 产生的 digit 可以骗过第二个 discri minator)，但是 discri minator也会演化，等。

### The evolution of generation 　

你要注意的一个地方就是：这个 Generator 它从来没有看过真正的 image 长什么样子，discri- minator有看过真正的 digit 长什么样子，它会比较正真正的 image 和 generator 的不同，它要做的就是骗过 discri-minator。generator没有看过真正的 image，所以 generate 它可以产生出来的 image 是 database 里面从来都没有见过的，这比较像是我们想要 machine 做的事情。

![mark](http://images.iterate.site/blog/image/20190818/EeF51UbPrLx0.png?imageslim)

我们来看 discriminator 是咋样 train 的：这个 discriminator 就是一个 neural network，它的 input 就是一张 image，它的 output 就是一个 number(可以通过 sigmoid function让值介于 0-1之间，1代表 input 这一张 image 是真正的 image，0代表是 generator 所产生的)。generator在这里的架构其实就跟 VAE 的 decoder 是一摸一样的，它也是 neural network，它的 input 就是从一个 distribution sample出来的 vector，你把 sample 出来的 vector 丢到 generator 里面，它就会产生一个数字(image)，你给它不同的 vector，它就会产生不同样子的 image，先用 generator 产生一堆 image(假的)。我们有真正的 image，discriminator就是把 generator 所产生的 image 都 label 为 0，把真正的 image 都 label 为 1。








### GAN-辨别器

现在有可第一代的 discriminator，咋样根据 discriminator update 第一代的。首先我们随便输入一个 vector，它会随便产生一张 image，这张 image 没有办法骗过 discriminatior，把 generator 产生的 image 丢到 discriminatior 里面，它得出有 0.87。接下来我们就得调这个 generator 的参数，让现在的 discriminator 会认为说：generator出来的 image 是真的，也就是说：generator出来的 image 丢到 discriminator 里面，discriminator output越接近 1 越好，所以你希望 generator 出来是这样的 image，discriminator output是 1.0觉得它是真正的 image。


![mark](http://images.iterate.site/blog/image/20190818/h33aIUpBUJR2.png?imageslim)
### GAN-生成器

generator就是一个 neural network，discriminator也是一个 neural network，你把 generator output当做 discriminator 的 input，然后再让它产生一个值。这件事情就好像是：你有一个很大很大的 neural network，你丢一个 randomly vector，它的 output 就是一个值，所以 generator 和 discriminator 合起来就是一个很大的 neural network，你要让这个 network 再丢进一个 randomly vector，output 1这件事是很容易的，你做 gradient descent就好了。你就 gradient descent调整参数，希望丢进这个 vector 的时候，它的 output 是要接近 1 的。但是你要注意的事情是：你在调整这个参数的时候，你只能够调整 generator 的参数(只能算 generator 的参数对 output 的 gradient)，必须 fix the discriminator。如果你没有 fix the discriminator的话会发生：对 discriminator 来说，要让它 output 1很简单，在最后 output 的 bias 设为 1，其他 weight 都设 0，output就是 1 了。

所以你要整个 network input randomly vector，output是 1 的时候，你要 fix the discriminator的参数，只调 generator 的参数，这样才会骗过。

![mark](http://images.iterate.site/blog/image/20190818/nmO7NfXhhxRu.png?imageslim)
### GAN-Toy Example

这是来自 GAN paper的 Toy example，Toy example是这样子的：z(z是 one dimension)丢到 generator 里面，会产生另外一个 one dimension的东西(这个 z 可以从任何的 distribution sample出来，在这个例子是从 uniform distribution sample出来的)，每一个不同的 z 会得到不同的 x，x的分布就绿色这条个分布。现在要做的事情是：希望这个 generator 的 output 可以越像 real data越好，real data就是黑色的这些点，也就绿色这条 distribution 可以跟黑色的点越接近越好。按照 GAN 的概念的话，你就把 generator 的 output x跟 real data丢到 discriminator 里面，然后让 discriminator 去判断来自真正 data 的几率跟 generator output几率(如果是真正的 data 几率就是 1，反之就是 0)



假设 generator 还很弱，产生的 green distribution是这样子(中间图)，discriminator根据 real data跟 generator distribution得出的是蓝色的线，这条蓝色的线告诉我们，如果是在右半区，比较有可能是假的(generator产生的)，如果是在左半区，比较有可能是 real data。接下来 generator 根据 discriminator 的结果去调整它的参数(generator要做的事情就是要骗过 discriminator)，既然 discriminator 认为在中间那个图左半区比较有可能是 real data，那 generator output就往左边移，也有可能移太多偏到左边去了，所以要小心的调整参数，generator会骗过它产生新的 distribution。这个 process 会一直进行下去，直到最后 generator output跟 real data一模一样，discriminator没有办法分辨真正的 data。

问题：你不知道 discriminator 是不是对的，你说 discriminator 得到一个很好的结果，那有可能是 generator 太废，有时候 discriminator 得到一个很差的结果，它认为每个地方抖没法分辨是不是 real value，这个时候并不能说：generator很像，有可能是 discriminator 太弱了，所以这是一个还没有 solution 的难题。



所以在 train GAN的时候，你会一直坐在电脑旁边看它产生的 image，因为你从 generator 跟 discriminat 的 loss 你看不出 generate 是否好，所以你 generator updata一次参数，你就去看看 generate 一些 image 看看有没有比较好，如果方向走错了，在重新调整一下参数，所以这是非常困难的。


![mark](http://images.iterate.site/blog/image/20190818/oduAjt4mC4vL.png?imageslim)
最大的问题就是你没有一个很明确的 signal，它可以告诉现在的 generator 现在做的什么样子。在 standard NNS里面，你就看那个 loss，loss越来越小就代表 train 越来越好。但是在 GAN 里面，你其实要做的事情是：keep generator跟 discriminator 是 well-matched
，不断处于一个竞争的状态。

这就很麻烦，你要让 generator 跟 discriminator 一直处于竞争的状态，所以你就要一个不可思议的平衡感来调整参数。
当 discriminator fail的时候，我们 train 的终极目标是希望 generator 产生的 output 被 discriminator 无法分别的(正确率为 0)，但是往往当你 discriminator fail的时候，并不代表说：generator真正 generate 出很好的 image，往往遇到的状况是 generator 是太弱了。





# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)


