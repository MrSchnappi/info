---
title: 1.12 分层深度强化学习算法
toc: true
date: 2019-09-04
---

## 10.6 分层深度强化学习算法

分层强化学习可以将最终目标分解为多个子任务来学习层次化的策略，并通过组合多个子任务的策略形成有效的全局策略。

- Kulkarni 等提出了分层 DQN(hierarchical deep Q-network，h--DQN) 算法。h--DQN 基于时空抽象和内在激励分层，通过在不同的时空尺度上设置子目标对值函数进行层次化处理。顶层的值函数用于确定宏观决策，底层的值函数用于确定具体行动。
- Krishnamurthy 等在 h--DQN 的基础上提出了基于内部选择的分层深度强化学习算法。该模型结合时空抽象和深度神经网络，自动地完成子目标的学习，避免了特定的内在激励和人工设定中间目标，加速了智能体的学习进程，同时也增强了模型的泛化能力。
- Kulkarni等基于后续状态表示法提出了深度后续强化学习(deep successor reinforcement learning，DSRL)。DSRL 通过阶段性地分解子目标和学习子目标策略，增强了对未知状态空间的探索，使得智能体更加适应那些存在延迟反馈的任务。
- Vezhnevets等受封建(feudal)强化学习算法的启发，提出一种分层深度强化学习的架构 FeUdal 网络(FuNs)[49]。FuNs框架使用一个管理员模块和一个工人模块。管理员模块在较低的时间分辨率下工作，设置抽象目标并传递给工人模块去执行。FuNs 框架创造了一个稳定的自然层次结构，并且允许两个模块以互补的方式学习。实验证明，FuNs有助于处理长期信用分配和记忆任务，在 Atari 视频游戏和迷宫游戏中都取得了不错的效果。

<span style="color:red;">也补充下。</span>





# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
