---
title: 02 池化层
toc: true
date: 2019-06-15
---
# 池化层

下采样层也叫池化层，池化层最常用的方法包括最大池化和平均池化，主要是用来降低网络的复杂度。池化层的输入一般来源于上一个卷积层，主要作用是提供很强的鲁棒性并且减少参数的数量，防止过拟合现象的发生。

- 如果取区域均值（Mean-pooling），往往能保留整体数据的特征，能突出背景的信息
- 而如果取区域最大值（Max-pooling），则能更好地保留纹理上的特征。例如 Max-pooling 是取一小块区域中的最大值，此时若此区域中的其他值略有变化，或者图像稍有平移，pooling 后的结果仍不变。<span style="color:red;">之前好像没注意这两种 Pooling 有这种说法，拆分出去。</span>

下采样层是对上一层特征图的一个采样处理，采用最大池化处理，采用 2×2 小区域的均值结果，如图 7.4所示。

<center>

![](http://images.iterate.site/blog/image/20190615/iKJvTgyL1pKO.png?imageslim){ width=55% }

</center>

卷积神经网络采用反向传输调整权重，虽然看来基本思想跟 BP 神经网络算法一样，都是通过最小化残差来调整权重和偏置，但 CNN 的网络结构比较复杂，而且权重共享，使得计算残差变得很困难。

卷积神经网络里面有时候会用到各种各样的归一化层，尤其是早期的研究，经常能见到它们的身影，不过近些年来的研究表明，似乎这个层级对最后结果的帮助非常小，有时候甚至干脆去掉不用了。<span style="color:red;">好像之前也有看到说归一化层不咋用了，确认下，为什么呢？</span>

在卷积神经网络的最后，往往会出现一两层全连接层，全连接一般会把卷积输出的二维特征图转化成一维的一个向量，全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。传统的网络我们的输出都是分类，也就是几个类别的概率，甚至就是一个类别，那么全连接层就是高度提纯的特征了，它方便最后的分类器或者回归使用。<span style="color:red;">嗯。</span>



# 相关

- 《深度学习框架 Pytorch 快速开发与实战》
