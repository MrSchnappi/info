---
title: 大纲
toc: true
date: 2019-07-02
---

# 大纲


为什么要翻译这本书
年初的时候我们从机械工业出版社华章公司那里知道有一本关于大数据的书正在征集翻译，在看过英文版并翻译了样章后，我们几位志同道合的软件工程师一块儿接受了《Learning PySpark》的翻译工作。我们都非常兴奋，因为作为软件工程师，能有机会把当前最热最新的技术介绍给大家是何其荣幸。
python是数据分析最常用的语言之一，而 Apache Spark是一个开源的强大的分布式查询和处理引擎。本书用详尽的例子介绍了如何使用 python 来调用 Spark 的新特性，如何处理结构化和非结构化的数据，如何使用 PySpark 中一些基本的可用数据类型，如何生成机器学习模型、操作图像、阅读串流数据以及在云上部署你的模型。
数据是每个人身边都存在的，理解学习比较容易，但是数据量足够大才是一个相对准确的学习平台。在实践中，如何确定训练集合、如何将脏数据处理为清洁数据、如何填充数据等等，需要结合本书的知识理论，清楚了解待处理的大数据特性。每一种数据的特征或特性都不一致，所以前期的准备和调研必不可少。本书不仅仅是一本工具书，也是一本能深入浅出、结合简单实例来介绍 PySpark 语言的书。不管使用什么语言和工具，万变不离其宗。希望阅读此书的人，除了看懂示例，还能够结合实际经验来推敲，这样就能明白作者举这些例子的良苦用心。
希望大家喜欢这本书，因为译者的水平有限，翻译中的错漏缺点在所难免，希望读者批评指正。读者对象
本书适合以下几类读者阅读：
·对大数据的前沿技术非常感兴趣的人。
·有志于成为一名数据科学家的从业人员。
·有一定算法和编程基础的技术爱好者。译者分工
本书由来自 IBM 中国开发中心的软件工程师联合翻译完成。其中：
·栾云杰（目前就职于 IBM 中国开发中心）翻译了第 5 章、第 6 章。
·陈瑶（原 IBM 工程师，现就职于某大数据公司）翻译了第 2 章、第 3 章、第 4 章、第 11 章。
·刘旭斌（目前就职于 IBM 中国开发中心）翻译了第 7 章、第 8 章、第 9 章。
另外，第 1 章由栾云杰、陈瑶、刘旭斌三人共同翻译，第 10 章由栾云杰、陈瑶两人共同翻译。致谢
感谢华章公司引进了该书的中译本版权，这是该中译本得以面市的最核心要素。
感谢华章公司的和静老师给予我们的支持和信任。因为这份信任，我们才有机会来翻译这本关于大数据和 Apache Spark的书籍。
感谢本次翻译组的小伙伴们。翻译本书的过程，是一种学习与思考的结合，也是和伙伴合作与交流的经历。非常庆幸遇到了睿智又勤奋的伙伴，即使在繁忙的工作和节奏极快的生活中，也努力完成了翻译和审阅计划。
另外，也要感谢我们的家人对我们的支持，正是有他们的支持和鼓励，我们才能坚持下来。




序
感谢你选择这本书开始 PySpark 之旅，希望你像我一样兴奋。当 Denny Lee第一次告诉我这本新书的时候，我非常高兴。Apache Spark既支持 Java、Scala、JVM世界，又支持 python（以及近来的 R）世界，这是它成为一个如此非凡的平台最为重要的原因。以前很多书籍都集中于核心语言，或者主要关注在 JVM 语言上，所以很高兴看到由如此有经验的 Spark 教育工作者来专门为 PySpark 出书，使 PySpark 有机会绽放光芒。PySpark通过支持这两个不同的世界，使我们能够成为更高效的数据科学家和数据工程师，同时得以借鉴彼此社区的那些绝佳想法。
很荣幸有机会浏览这本书的早期版本，这使我对该项目的兴趣更为浓厚。我曾有幸参加过一些类似的会议和聚会，看着作者将 Spark 世界的新概念介绍给不同的观众（从新人到经验丰富的老手），并且他们提取自身的经验写出这本书，他们真是太棒了。从阐述知识到各个主题的覆盖，无一不体现了作者们的丰富经验。除了简单介绍 PySpark 之外，他们还花时间从社区中找来了日渐重要的包，如 GraphFrames 和 TensorFrames。
在决定使用哪些工具时，我觉得社区是经常被忽视的一部分，python拥有一个很棒的社区，我期待着你加入 python Spark社区。所以，来享受你的冒险之旅吧；我知道你会和 Denny Lee以及 Tomek Drabas有很好的联系。我真的相信，通过拥有多样化的 Spark 用户社区，我们将能够创造出对每个人都有用的更好的工具，所以我希望能够在某个会议、聚会或邮件列表中见到你！
Holden Karau




前言
据估计，2013年全世界产生了大约 4.4ZB（词头 Z 代表 1021）信息量的数据！而到 2020 年，预计人类将会产生 10 倍于 2013 年的数据量。随着字面上的数字越来越大，加上人们需求的日益增长，为了使这些数据更有意义，2004年来自 Google 的 Jeffrey Dean和 Sanjay Ghemawat发表了一篇开创性的论文《MapReduce：Simplified Data Processing on Large Clusters》。至此，利用这一概念的技术开始快速增多，Apache Hadoop也开始迅速变得流行起来，最终创建了一个 Hadoop 的生态系统，包括抽象层的 Pig、Hive和 Mahout，都是利用了 map 和 reduce 的简单概念。
然而，即使拥有每天都分析过滤海量数据的能力，MapReduce始终是一个限制相当严格的编程框架。此外，大多数的任务还要求读取、写入磁盘。认识到这些缺点，2009年 Matei Zaharia将 Spark 作为他博士课题的一部分开始研究。Spark在 2012 年首次发布。虽然 Spark 是基于和 MapReduce 相同的概念，但其先进的数据处理方法和任务组织方式使得它比 Hadhoop 要快 100 倍（对于内存计算）。
在这本书中，我们将指导你使用 python 了解 Apache Spark的最新性能，包括如何看懂结构化和非结构化的数据，如何使用 PySpark 中一些基本的可用数据类型，生成机器学习模型，图像操作，阅读串流数据，在云上部署模型。每一章力求解决不同的问题，并且我们也希望看完这本书之后，你可以掌握足够的知识来解决其他我们还没来得及在书中讲解的问题。本书的主要内容
第 1 章通过技术和作业的组织等概念提供了对 Spark 的介绍。
第 2 章介绍了 RDD、基本原理、PySpark中可用的非模式化数据结构。
第 3 章详细介绍了 DataFrame 数据结构，它可以弥合 Scala 和 python 之间在效率方面的差距。
第 4 章引导读者了解 Spark 环境中的数据清理和转换的过程。
第 5 章介绍了适用于 RDD 的机器学习库，并回顾了最有用的机器学习模型。
第 6 章涵盖了当前主流的机器学习库，并且提供了目前可用的所有模型的概述。
第 7 章引导你了解能轻松利用图解决问题的新结构。
第 8 章介绍了 Spark 和张量流（TensorFlow）领域中深度学习（Deep Learning）的连接桥梁。
第 9 章描述 Blaze 是如何跟 Spark 搭配使用以更容易地对多源数据进行抽象化的。
第 10 章介绍了 PySpark 中可用的流工具。
第 11 章一步步地指导你运用命令行界面完成代码模块化并提交到 Spark 执行。
其他一些详细信息，我们提供了以下额外的章节：
安装 Spark：https://www.packtpub.com/sites/default/files/downloads/InstallingSpark.pdf。
免费提供 Spark Cloud：https://www.packtpub.com/sites/default/files/downloads/FreeSparkCloud Offering.pdf。本书需要的软/硬件支持
阅读本书，需要准备一台个人电脑（Windows、Mac或者 Linux 任一系统都行）。运行 Apache Spark，需要 Java 7＋并且安装配置 python 2.6＋版本或者 3.4＋版本的环境；本书中使用的是 Anaconda python3.5版本，可以在 https://www.continuum.io/downloads下载。
本书中我们随意使用了 Anaconda 的预装版 python 模块。GraphFrames和 TensorFrames 也可以在启动 Spark 实例时动态加载：载入时你的电脑需要联网。如果有的模块尚未安装到你的电脑里，也没有关系，我们会指导你完成安装过程。本书的读者对象
想要学习大数据领域发展最迅速的技术即 Apache Spark的每一个人，都可以阅读此书。我们甚至希望还有来自于数据科学领域更高级的从业人员，能够找到一些令人耳目一新的例子以及更有趣的主题。本书约定警告或重要的笔记提示和技巧下载代码示例
你可以从 http://www.packtpub.com下载代码文件。你也可以访问华章图书官网：http://www.hzbook.com，通过注册并登录个人账号，下载本书的源代码。下载本书彩图
我们还提供了一个 PDF 文件，其中包含本书中使用的截图和彩图，可以帮助读者更好地了解输出的变化。您可以从此下载文件 https://www.packtpub.com/sites/default/files/downloads/LearningPySpark_ColorImages.pdf。



# 相关

- 《PySpark 实战指南 利用 python 和 Spark 构建数据密集型应用并规模化部署》
