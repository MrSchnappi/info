---
title: 01 语言模型
toc: true
date: 2019-09-29
---
# 语言模型



在说 Word Embedding 之前，先更粗略地说下语言模型，因为一般 NLP 里面做预训练一般的选择是用语言模型任务来做。


## 什么是语言模型

<center>

![mark](http://images.iterate.site/blog/image/20190926/YJmiHws5oA6B.png?imageslim)

</center>



什么是语言模型？其实看上面这张 PPT 上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数 P 的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。

句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。

## 神经网络语言模型 NNLM

那么假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？


<center>

![mark](http://images.iterate.site/blog/image/20190926/CXykBqoPOeua.png?imageslim)

</center>


你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名 NNLM 的网络结构，用来做语言模型。它生于 2003，火于 2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在 2013 年又被 NLP 考古工作者从海底湿淋淋地捞出来了祭入神殿。

OK，我们回来讲一讲 NNLM 的思路。先说训练过程，现在看其实很简单，见过 RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词 $W_{t}=^{\prime \prime} B e r t^{\prime \prime}$ 前面句子的 $t-1$ 个单词，要求网络正确预测单词 Bert，即最大化：

$$
\left.P(W_{t}=^{\prime \prime} \operatorname{Bert}^{\prime \prime} | W_{1}, W_{2}, \ldots W_{(t-1) }  ; \theta\right)
$$

前面任意单词 $W_{i}$ 用 Onehot 编码（比如：0001000）作为原始单词输入，之后乘以矩阵 $Q$ 后获得向量 $C\left(W_{i}\right)$ ，每个单词的 $C\left(W_{i}\right)$ 拼接，上接隐层，然后接 softmax 去预测后面应该后续接哪个单词。

这个 $C\left(W_{i}\right)$ 是什么？这其实就是单词对应的 Word Embedding 值，那个矩阵 $Q$ 包含 $V$ 行，$V$ 代表词典大小，每一行内容代表对应单词的 Word embedding值。只不过 $Q$ 的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵 $Q$，当这个网络训练好之后，矩阵 $Q$ 的内容被正确赋值，每一行代表一个单词对应的 Word embedding值。

所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵 $Q$，这就是单词的 Word Embedding是被如何学会的。



# 相关

- [从 Word Embedding到 Bert 模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)
