---
title: 1.10 如何评价个性化推荐系统的效果
toc: true
date: 2019-09-04
---

### 18.6.10 如何评价个性化推荐系统的效果？

- **准确率与召回率（Precision & Recall）**

准确率和召回率是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。其中精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率；召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率。

<center>

![](http://images.iterate.site/blog/image/20190722/6z9ArYXaLU02.jpg?imageslim){ width=55% }

</center>


一般来说，Precision就是检索出来的条目（比如：文档、网页等）有多少是准确的，Recall就是所有准确的条目有多少被检索出来了。

正确率、召回率和 F 值是在鱼龙混杂的环境中，选出目标的重要评价指标。不妨看看这些指标的定义先：

	正确率 = 提取出的正确信息条数 /  提取出的信息条数

	召回率 = 提取出的正确信息条数 /  样本中的信息条数

两者取值在 0 和 1 之间，数值越接近 1，查准率或查全率就越高。

	F值  = 正确率 * 召回率 * 2 / (正确率 + 召回率) （F 值即为正确率和召回率的调和平均值）

不妨举这样一个例子：某池塘有 1400 条鲤鱼，300只虾，300只鳖。现在以捕鲤鱼为目的。撒一大网，逮着了 700 条鲤鱼，200只虾，100只鳖。那么，这些指标分别如下：

	正确率 = 700 / (700 + 200 + 100) = 70%

	召回率 = 700 / 1400 = 50%

	F值 = 70% * 50% * 2 / (70% + 50%) = 58.3%

不妨看看如果把池子里的所有的鲤鱼、虾和鳖都一网打尽，这些指标又有何变化：

	正确率 = 1400 / (1400 + 300 + 300) = 70%

	召回率 = 1400 / 1400 = 100%

	F值 = 70% * 100% * 2 / (70% + 100%) = 82.35%

由此可见，正确率是评估捕获的成果中目标成果所占得比例；召回率，顾名思义，就是从关注领域中，召回目标类别的比例；而 F 值，则是综合这二者指标的评估指标，用于综合反映整体的指标。

当然希望检索结果 Precision 越高越好，同时 Recall 也越高越好，但事实上这两者在某些情况下有矛盾的。比如极端情况下，我们只搜索出了一个结果，且是准确的，那么 Precision 就是 100%，但是 Recall 就很低；而如果我们把所有结果都返回，那么比如 Recall 是 100%，但是 Precision 就会很低。因此在不同的场合中需要自己判断希望 Precision 比较高或是 Recall 比较高。如果是做实验研究，可以绘制 Precision-Recall曲线来帮助分析。

**注意：准确率和召回率是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下准确率高、召回率就低，召回率低、准确率高，当然如果两者都低，那是什么地方出问题了**。一般情况，用不同的阀值，统计出一组不同阀值下的精确率和召回率，如下图：

<center>

![](http://images.iterate.site/blog/image/20190722/oSa8gAhJQ0rD.jpg?imageslim){ width=55% }

</center>


**如果是做搜索，那就是保证召回的情况下提升准确率；如果做疾病监测、反垃圾，则是保准确率的条件下，提升召回。**

所以，在两者都要求高的情况下，可以用 F1 值来衡量。

<center>

![](http://images.iterate.site/blog/image/20190722/y71yHxkWBb9l.png?imageslim){ width=55% }

</center>


公式基本上就是这样，但是如何算图中的 A、B、C、D呢？**这需要人工标注，人工标注数据需要较多时间且枯燥，如果仅仅是做实验可以用用现成的语料。当然，还有一个办法，找个一个比较成熟的算法作为基准，用该算法的结果作为样本来进行比照**，这个方法也有点问题，如果有现成的很好的算法，就不用再研究了。

- **综合评价指标（F-Measure）**

P和 R 指标有时候会出现的矛盾的情况，这样就需要综合考虑他们，最常见的方法就是 F-Measure（又称为 F-Score）。

F-Measure是 Precision 和 Recall 加权调和平均：

<center>

![](http://images.iterate.site/blog/image/20190722/zJfxd0DYu8wc.png?imageslim){ width=55% }

</center>


当参数α=1时，就是最常见的 F1，也即

<center>

![](http://images.iterate.site/blog/image/20190722/qYEa954D8mkz.png?imageslim){ width=55% }

</center>


可知 F1 综合了 P 和 R 的结果，当 F1 较高时则能说明试验方法比较有效。

- **E值**

E值表示查准率 P 和查全率 R 的加权平均值，当其中一个为 0 时，E值为 1，其计算公式：

<center>

![](http://images.iterate.site/blog/image/20190722/RYOxcWu88Gbp.png?imageslim){ width=55% }

</center>


b越大，表示查准率的权重越大。

- **平均正确率（Average Precision）**

  平均正确率表示不同查全率的点上的正确率的平均。

- **AP和 mAP(mean Average Precision)**

  ​	mAP是为解决 P（准确率），R（召回率），F-measure的单点值局限性的。为了得到 一个能够反映全局性能的指标，可以看考察下图，其中两条曲线(方块点与圆点)分布对应了两个检索系统的准确率-召回率曲线 。

<center>

![](http://images.iterate.site/blog/image/20190722/gqOzacX40MEf.jpg?imageslim){ width=55% }

</center>


  ​	可以看出，虽然两个系统的性能曲线有所交叠但是以圆点标示的系统的性能在绝大多数情况下要远好于用方块标示的系统。

  ​	从中我们可以 发现一点，如果一个系统的性能较好，其曲线应当尽可能的向上突出。

  ​	更加具体的，曲线与坐标轴之间的面积应当越大。

  ​	最理想的系统， 其包含的面积应当是 1，而所有系统的包含的面积都应当大于 0。这就是用以评价信息检索系统的最常用性能指标，平均准确率 mAP 其规范的定义如下:

<center>

![](http://images.iterate.site/blog/image/20190722/ccFhssIdmMFp.jpg?imageslim){ width=55% }

</center>


- **ROC和 AUC**

  ​	ROC和 AUC 是评价分类器的指标，上面第一个图的 ABCD 仍然使用，只是需要稍微变换。

<center>

![](http://images.iterate.site/blog/image/20190722/ekW5toARnQvk.jpg?imageslim){ width=55% }

</center>


  ROC关注两个指标

  ```
   True  Positive Rate( TPR ) = TP / [ TP + FN] ，TPR代表能将正例分对的概率
   False Positive Rate( FPR ) = FP / [ FP + TN] ，FPR代表将负例错分为正例的概率
  ```

  ​	在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，这也就描绘了分类器在 TP（真正的正例）和 FP（错误的正例）间的 trade-off。ROC的主要分析工具是一个画在 ROC 空间的曲线——ROC curve。我们知道，对于二值分类问题，实例的值往往是连续值，我们通过设定一个阈值，将实例分类到正类或者负类（比如大于阈值划分为正类）。因此我们可以变化阈值，根据不同的阈值进行分类，根据分类结果计算得到 ROC 空间中相应的点，连接这些点就形成 ROC curve。ROC curve经过（0,0）（1,1），实际上(0, 0)和(1, 1)连线形成的 ROC curve实际上代表的是一个随机分类器。一般情况下，这个曲线都应该处于(0, 0)和(1, 1)连线的上方。如图所示。

<center>

![](http://images.iterate.site/blog/image/20190722/LMigBq7uENeu.jpg?imageslim){ width=55% }

</center>


  ​	用 ROC curve来表示分类器的 performance 很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。

  ​	于是**Area Under roc Curve(AUC)**就出现了。顾名思义，AUC的值就是处于 ROC curve下方的那部分面积的大小。通常，AUC的值介于 0.5到 1.0之间，较大的 AUC 代表了较好的 Performance。

  ​	AUC计算工具：<http://mark.goadrich.com/programs/AUC/>

  ​	**P/R和 ROC 是两个不同的评价指标和计算方式，一般情况下，检索用前者，分类、识别等用后者。**





# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
