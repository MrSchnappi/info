---
title: 21 泰勒展开
toc: true
date: 2019-08-31
---
# 可以补充进来的

- 泰勒级数的矢量展开没懂


# 泰勒级数与极值

泰勒级数：

- 用多项式逼近的方式描述高阶导数。前提是 $f(x)$ 是一个无限次可导的函数。

因此泰勒/迈克劳林级数实质上就是 多项式逼近。

是用一个多项式函数来近似代替一个可导函数。

泰勒公式是一个用函数在某点的信息描述其附近取值的公式。如果函数足够平滑的话，在已知函数在某一点的各阶导数值的情况之下，泰勒公式可以用这些导数值做系数构建一个多项式来近似函数在这一点的邻域中的值。


Taylor公式：在任何一点 $x_0$ 附近对 $f(x)$ 做多项式逼近：

$$
f\left(x_{0}+\Delta_{x}\right)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right) \Delta_{x}+\frac{f^{\prime \prime}\left(x_{0}\right)}{2 !} \Delta_{x}^{2}+\cdots+\frac{f^{(n)}\left(x_{0}\right)}{n !} \Delta_{x}^{n}+o\left(\Delta_{x}^{n}\right)
$$

Maclaurin公式：在 $0$ 附近对 $f(x)$ 做多项式逼近：即将上面的 $x_0$ 变成 $0$：

$$
f(x)=f(0)+f^{\prime}(0) x+\frac{f^{\prime \prime}(0)}{2 !} x^{2}+\cdots+\frac{f^{(n)}(0)}{n !} x^{n}+o\left(x^{n}\right)
$$

注：在 ai 中我们不关注对于尾巴上的余项 $o\left(\Delta_{x}^{n}\right)$ 的大小估计 。

只要是初等函数，就可以展开成多项式累加的形式，因为 $f(x)$ 的唯一要求就是无限次可导。这个是 Taylor 公式的一个很重要的应用。

用一张动图模拟这个过程：

<center>

![img](https://pic4.zhimg.com/v2-9dd69ab2c20ca721bc0979d7ebaa0253_b.webp)


</center>


<span style="color:red;">对泰勒级数感觉理解不深，到底应用在什么情况下？最好将 Taylor 公式的所有应用都总结一下。</span>

<span style="color:red;">牛顿法的推导用的就是泰勒展开式？是的，牛顿法用的是二阶的泰勒级数来逼近，因为是二阶，所以可以很容易找到 $x_0$ 附近的使 $f(x)$ 最小的 $x$，然后再新建一个二阶泰勒级数来逼近，再求一个新的 $x$，以此类推。关于牛顿法怎么用 Taylor 公式的要补充在这里。</span>牛顿法，除非你知道目标函数非常好才采用牛顿法。

这个地方视频里引申了一下，求 $e^x$ 的时候，虽然可以用泰勒公式展开来求，但是实际的计算机中并不是用这种方式来求的。不作为重点，因此没有写。



## 泰勒级数展开 标量

输入为标量的泰勒级数展开：

$$
f\left(x_{k}+\delta\right) \approx f\left(x_{k}\right)+f^{\prime}\left(x_{k}\right) \delta+\frac{1}{2} f^{\prime \prime}\left(x_{k}\right) \delta^{2}
$$

称满足 $f^{\prime}\left(x_{k}\right)=0$ 的点为平稳点（候选点），此时如果还有：

- $f^{\prime \prime}\left(x_{k}\right)>0$， $x_{k}$ 为一严格局部极小点（反之，严格局部最大点）（充分条件）
- 如果 $f^{\prime \prime}\left(x_{k}\right)=0$，有可能是一个鞍点（saddle point），why?

思考实际使用中的局限？


## 泰勒级数展开 矢量

<span style="color:red;">没懂</span>

和标量情况对比

输入为矢量的泰勒级数展开：

$$
f\left(\mathbf{x}_{k}+\boldsymbol{\delta}\right) \approx f\left(\mathbf{x}_{k}\right)+\nabla^{T} f\left(\mathbf{x}_{k}\right) \boldsymbol{\delta}+\frac{1}{2} \boldsymbol{\delta}^{T} \nabla^{2} f\left(\mathbf{x}_{k}\right) \boldsymbol{\delta}
$$

称满足 $\nabla^{T} f\left(\mathbf{x}_{k}\right)=0$ 的点为平稳点（候选点），此时如果还有：

- $\nabla^{2} f\left(\mathbf{x}_{k}\right) \succ 0$， $x_{k}$ 为一严格局部极小点（反之，严格局部最大点）
- 如果 $\nabla^{2} f\left(\mathbf{x}_{k}\right)$ 不定矩阵，是一个鞍点（saddle point）
- 思考 $\nabla^{2} f\left(\mathbf{x}_{k}\right) \succeq 0$

梯度方向？梯度下降法从哪里来？


## 使用：

察基尼指数的图像、熵、分类误差率三者之间的关系：

将 $f(x)=-\ln x$ 在 $x=1$ 处一阶展开，忽略高阶无穷小，得到 $\mathrm{f}(\mathrm{x}) \approx 1-\mathrm{x}$：

$$
\begin{aligned}
H(X)&=-\sum_{k=1}^{K} p_{k} \ln p_{k}\\&\approx \sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)
\end{aligned}
$$

<center>

![](http://images.iterate.site/blog/image/20190518/I502X840nHLW.png?imageslim){ width=55% }

</center>


<span style="color:red;">这个地方视频里没有怎么讲，说在讲决策树的时候会讨论，是一个非常重要的基尼指数，因此，看到决策树章节的时候回来补充下。</span>




# 相关

- 七月在线
