---
title: 25 范数
toc: true
date: 2019-05-06
---
# 可以补充进来的


# 范数

有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用称为范数(norm)的函数来衡量向量大小。形式上，$L^{p}$ 范数定义如下：

$$
\|x\|_{p}=\left(\sum_{i}\left|x_{i}\right|^{p}\right)^{\frac{1}{p}}\tag{2.30}
$$


其中 $p \in \mathbb{R}$ ，$p \geq 1$。
范数(包括 $L^{p}$ 范数)是将向量映射到非负值的函数。直观上来说，向量 $\boldsymbol{x}$ 的范数衡量从原点到点 $\boldsymbol{x}$ 的距离。更严格地说，范数是满足下列性质的任意函数：

- $f(\boldsymbol{x})=0 \Rightarrow \boldsymbol{x}=\mathbf{0}$
- $f(\boldsymbol{x}+\boldsymbol{y}) \leq f(\boldsymbol{x})+f(\boldsymbol{y})$ (三角不等式(triangle inequality));
- $\forall \alpha \in \mathbb{R}, f(\alpha \boldsymbol{x})=|\alpha| f(\boldsymbol{x})$


当 $p=2$ 时，$L^{2}$ 范数称为欧几里得范数(Euclidean norm)。它表示从原点出发到向量 $x$ 确定的点的欧几里得距离。<span style="color:red;">嗯，欧几里得距离。</span>$L^{2}$ 范数在机器学习中出现得十分频繁，经常简化表示为 $\|x\|$，略去了下标 $2$。平方 $L^{2}$ 范数也经常用来衡量向量的大小，可以简单地通过点积 $\boldsymbol{x}^{\top} \boldsymbol{x}$ 计算。<span style="color:red;">嗯。</span>


平方 $L^{2}$ 范数在数学和计算上都比 $L^{2}$ 范数本身更方便。例如，平方 $L^{2}$ 范数对 $x$ 中每个元素的导数只取决于对应的元素，而 $L^{2}$ 范数对每个元素的导数和整个向量相关。但是在很多情况下，平方 $L^{2}$ 范数也可能不受欢迎，因为它在原点附近增长得十分缓慢。<span style="color:red;">为什么平方范数在数学和计算上都比范数本身更方便？这个地方没有特别清楚。</span>

在某些机器学习应用中，区分恰好是零的元素和非零但值很小的元素是很重要的。<span style="color:red;">什么时候是很重要的？场景要总结到这里。嗯，在后面的阅读过程中注意。</span>在这些情况下，我们转而使用在各个位置斜率相同，同时保持简单的数学形式的函数：$L^{1}$ 范数。$L^{1}$ 范数可以简化如下：

$$
\|\boldsymbol{x}\|_{1}=\sum_{i}\left|x_{i}\right|\tag{2.31}
$$

当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用 $L^{1}$ 范数。每当 $x$ 中某个元素从 $0$ 增加 $\epsilon$，对应的 $L^{1}$ 范数也会增加 $\epsilon$。

有时候我们会统计向量中非零元素的个数来衡量向量的大小。<span style="color:red;">嗯，什么场景下会这么统计？</span>有些作者将这种函数称为 “$L^{0}$ 范数”，但是这个术语在数学意义上是不对的。向量的非零元素的数目不是范数，因为对向量缩放 $\alpha$ 倍不会改变该向量非零元素的数目。<span style="color:red;">嗯。</span>因此，$L^{1}$ 范数经常作为表示非零元素数目的替代函数。<span style="color:red;">嗯，还是想知道 $L^{0}$ 范数的应用场景，以及这种场景下使用 $L^{0}$ 来代替后的效果是什么？</span>

另外一个经常在机器学习中出现的范数是 $L^{\infty}$ 范数，也被称为最大范数 (max norm)。这个范数表示向量中具有最大幅值的元素的绝对值：<span style="color:red;">哇塞，厉害呀，嗯，表示的是向量中具有最大幅值的元素的绝对值。这个范数一般什么时候使用？而且，为什么这个 $\infty$ 是写在右下角的不是右上角吗？</span>


$$
\|x\|_{\infty}=\max _{i}\left|x_{i}\right|\tag{2.32}
$$

有时候我们可能也希望衡量矩阵的大小。<span style="color:red;">什么时候我们会要衡量矩阵的大小？</span>在深度学习中，最常见的做法是使用 Frobenius 范数 (Frobenius norm)，即：<span style="color:red;"> Frobenius 范数是什么？什么时候用的？</span>

$$
\|\boldsymbol{A}\|_{F}=\sqrt{\sum_{i, j} A_{i, j}^{2}}\tag{2.33}
$$

其类似于向量的 $L^2$ 范数。

两个向量的点积可以用范数来表示，具体如下：

$$
\boldsymbol{x}^{\top} \boldsymbol{y}=\|\boldsymbol{x}\|_{2}\|\boldsymbol{y}\|_{2} \cos \theta\tag{2.34}
$$

其中 $\theta$ 表示 $\boldsymbol{x}$ 和 $\boldsymbol{y}$ 之间的夹角。


## 常见范数

常用的向量的范数如下。

- $L^0$ 范数：$\|x\|_{0}$ 为 $x$ 向量各个非零元素的个数。

- $L^1$ 范数：$\|x\|_{1}$ 为 $x$ 向量各个元素绝对值之和，也叫“稀疏规则算子”（Lasso Regularization）。<span style="color:red;">稀疏规则算子？这个之前有看到过吗？</span>

- $L^2$ 范数：$\|x\|_{2}$ 为 $x$ 向量各个元素平方和的 $1/2$ 次方，$L^2$ 范数又称 Euclidean 范数或者 Frobenius 范数。在回归里面，有人把有它的回归叫 “岭回归”（Ridge Regression），有人也叫它 “权值衰减（Weight Decay）”。<span style="color:red;">是这样吗？这里面提到的 Euclidean 范数 和 Frobenius 范数  和 岭回归 和 权值衰减真的指的是 $L^2$ 范数吗？确认下，总结下。</span>

- $L^p$ 范数：$\|x\|_{p}$ 为 $x$ 向量各个元素绝对值 $p$ 次方和的 $1/p$ 次方。

- $L^\infty$ 范数：$\|x\|_{\infty}$ 为 $x$ 向量各个元素绝对值最大那个元素的绝对值。<span style="color:red;">嗯，是的。</span>



# 相关

- 《深度学习》花书
- 《深度学习框架 Pytorch 快速开发与实战》
