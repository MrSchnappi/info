---
title: 26 特殊类型的矩阵和向量
toc: true
date: 2019-05-07
---
# 特殊类型的矩阵和向量

有些特殊类型的矩阵和向量是特别有用的。

## 对角矩阵 diagonal matrix

对角矩阵(diagonal matrix)只在主对角线上含有非零元素，其他位置都是零。形式上，矩阵 $\boldsymbol{D}$ 是对角矩阵，当且仅当对于所有的 $i \neq j, \quad D_{i, j}=0$。

我们已经看到过一个对角矩阵：单位矩阵，其对角元素全部是 $1$。

我们用 $diag(\boldsymbol{v})$ 表示对角元素由向量 $\boldsymbol{v}$ 中元素给定的一个对角方阵。<span style="color:red;">哦，还可以这样写，不错不错。</span>

对角矩阵受到关注的部分原因是对角矩阵的乘法计算很高效。计算乘法 $\operatorname{diag}(\boldsymbol{v}) \boldsymbol{x}$，我们只需要将 $\boldsymbol{x}$ 中的每个元素 $x_i$ 放大 $v_i$ 倍。<span style="color:red;">嗯。</span>换言之，$\operatorname{diag}(\boldsymbol{v}) \boldsymbol{x}=\boldsymbol{v} \odot \boldsymbol{x}$。 <span style="color:red;">嗯，是的。</span>

计算对角方阵的逆矩阵也很高效。对角方阵的逆矩阵存在，当且仅当对角元素都是非零值，在这种情况下，$\operatorname{diag}(\boldsymbol{v})^{-1}=\operatorname{diag}\left(\left[1 / v_{1}, \ldots, 1 / v_{n}\right]^{\top}\right)$。<span style="color:red;">嗯，好的。</span>

在很多情况下，我们可以根据任意矩阵导出一些通用的机器学习算法，但通过将一些矩阵限制为对角矩阵，我们可以得到计算代价较低的(并且简明扼要的)算法。<span style="color:red;">嗯，但是怎么将矩阵限制为对角矩阵呢？</span>

并非所有的对角矩阵都是方阵。长方形的矩阵也有可能是对角矩阵。非方阵的对角矩阵没有逆矩阵，但我们仍然可以高效地计算它们的乘法。对于一个长方形对角矩阵 $\boldsymbol{D}$ 而言，乘法 $\boldsymbol{D} \boldsymbol{x}$ 会涉及 $\boldsymbol{x}$ 中每个元素的缩放，如果 $\boldsymbol{D}$ 是瘦长型矩阵，那么在缩放后的末尾添加一些零；如果 $\boldsymbol{D}$ 是胖宽型矩阵，那么在缩放后去掉最后一些元素。<span style="color:red;">哦，这个之前没有注意过，不知道，嗯，现在知道了</span>

## 对称矩阵

对称(symmetric)矩阵是转置和自己相等的矩阵，即：

$$
\boldsymbol{A}=\boldsymbol{A}^{\top}\tag{2.36}
$$

当某些不依赖参数顺序的双参数函数生成元素时，对称矩阵经常会出现。<span style="color:red;">什么是不依赖参数顺序的双参数函数？这个函数生成什么元素？</span>例如，如果 $\boldsymbol{A}$ 是一个距离度量矩阵，$\boldsymbol{A}_{i, j}$ 表示点 $i$ 到点 $j$ 的距离，那么 $A_{i, j}=A_{j, i}$ ，因为距离函数是对称的。<span style="color:red;">哦，这个的确，嗯。不过一般这样的对称矩阵生成之后用来做什么呢？</span>


## 正交与标准正交


OK，我们先介绍下单位向量：

单位向量(unit vector)是具有单位范数(unit norm)的向量，即：

$$
\|x\|_{2}=1
$$

<span style="color:red;">嗯，这个之前没有注意过。</span>

##

如果 $\boldsymbol{x}^{\top} \boldsymbol{y}=0$ ，那么向量 $\boldsymbol{x}$ 和向量 $\boldsymbol{y}$ 互相正交(orthogonal)。如果两个向量都有非零范数，那么这两个向量之间的夹角是 90 度。

在 $\mathbb{R}^{n}$ 中，至多有 $n$ 个范数非零向量互相正交。如果这些向量不但互相正交，而且范数都为 $1$，那么我们称它们是标准正交(orthonormal)。<span style="color:red;">嗯，是的。</span>


正交矩阵 (orthogonal matrix) 指行向量和列向量是分别标准正交的方阵，即：

$$
\boldsymbol{A}^{\top} \boldsymbol{A}=\boldsymbol{A} \boldsymbol{A}^{\top}=\boldsymbol{I}\tag{2.38}
$$

<span style="color:red;">嗯嗯，是的，一环套一环，上面这一小段写的很清楚，嗯，从单位矩阵的意思到正交，再到标准正交，最后再到正交矩阵。嗯。</span>


这意味着：

$$
\boldsymbol{A}^{-1}=\boldsymbol{A}^{\top}
$$

<span style="color:red;">是的是的。</span>

正交矩阵受到关注是因为求逆计算代价小。<span style="color:red;">是的，的确求逆运算的代价小，但是我们什么时候会使用到正交矩阵呢？一般是怎么使用的呢？</span>

我们需要注意正交矩阵的定义。违反直觉的是，正交矩阵的行向量不仅是正交的，还是标准正交的。对于行向量或列向量互相正交但不是标准正交的矩阵，没有对应的专有术语。<span style="color:red;">嗯，好吧，的确有点特殊。</span>





# 相关

- 《深度学习》花书
