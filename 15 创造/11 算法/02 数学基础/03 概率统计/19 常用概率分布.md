---
title: 19 常用概率分布
toc: true
date: 2019-05-18
---
# 可以补充进来的

- 感觉讲的这些有些只是提了下，但是既然提了肯定是有用到的，要多补充下。

# 常用概率分布


许多简单的概率分布在机器学习的众多领域中都是有用的。

## Bernoulli 分布


Bernoulli 分布 (Bernoulli distribution) 是单个二值随机变量的分布。它由单个参数 $\phi \in[0,1]$ 控制， $\phi$ 给出了随机变量等于 $1$ 的概率。

<span style="color:red;">一般的使用场景也补充下。</span>


它具有如下的一些性质。


$$
P(\mathrm{x}=1)=\phi\tag{3.16}
$$

$$
P(\mathrm{x}=0)=1-\phi\tag{3.17}
$$


$$
P(\mathrm{x}=x)=\phi^{x}(1-\phi)^{1-x}\tag{3.18}
$$

<span style="color:red;">上面这个没怎么明白。</span>

$$
\mathbb{E}_{\mathbf{x}}[\mathrm{x}]=\phi\tag{3.19}
$$


$$
\operatorname{Var}_{\mathrm{x}}(\mathrm{x})=\phi(1-\phi)\tag{3.20}
$$



## Multinoulli 分布

Multinoulli 分布 (multinoulli distribution) 或者范畴分布 (categorical distribution) 是指在具有 $k$ 个不同状态的单个离散型随机变量上的分布，其中 $k$ 是一个有限值。Multinoulli 分布由向量 $p \in[0,1]^{k-1}$ 参数化，其中每一个分量 $p_{i}$ 表示第 $i$ 个状态的概率。<span style="color:red;">嗯，是的。</span>最后的第 $k$ 个状态的概率可以通过 $1-\mathbf{1}^{\top} \boldsymbol{p}$ 给出。<span style="color:red;">这个是为啥？</span>注意我们必须限制 $\mathbf{1}^{\top} \boldsymbol{p} \leq 1$ 。<span style="color:red;">没怎么明白前面这个式子。</span> Multinoulli 分布经常用来表示对象分类的分布，所以我们很少假设状态 $1$ 具有数值 $1$ 之类的。因此，我们通常不需要去计算 Multinoulli 分布的随机变量的期望和方差。<span style="color:red;">嗯。</span>


Bernoulli 分布和 Multinoulli 分布足够用来描述在它们领域内的任意分布。它们能够描述这些分布，不是因为它们特别强大，而是因为它们的领域很简单。它们可以对那些能够将所有的状态进行枚举的离散型随机变量进行建模。当处理的是连续型随机变量时，会有不可数无限多的状态，所以任何通过少量参数描述的概率分布都必须在分布上加以严格的限制。<span style="color:red;">没有很明白。</span>

## 高斯分布

实数上最常用的分布就是正态分布 (normal distribution)，也称为高斯分布 (Gaussian distribution)：


$$
\mathcal{N}\left(x ; \mu, \sigma^{2}\right)=\sqrt{\frac{1}{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right)\tag{3.21}
$$



图 3.1画出了正态分布的概率密度函数：

<center>

![](http://images.iterate.site/blog/image/20190518/EIpT5taNOzDl.png?imageslim){ width=55% }

</center>

图 3.1　正态分布。正态分布 $\mathcal{N}\left(x ; \mu, \sigma^{2}\right)$ 呈现经典的“钟形曲线”的形状，其中中心峰的 $x$ 坐标由 $\mu$ 给出，峰的宽度受 $\sigma$ 控制。在这个示例中，我们展示的是标准正态分布(standard normal distribution)，其中 $\mu=0$， $\sigma=1$。



正态分布由两个参数控制，$\mu \in \mathbb{R}$ 和 $\sigma \in(0, \infty)$ 。参数 $\mu$ 给出了中心峰值的坐标，这也是分布的均值：$\mathbb{E}[\mathrm{x}]=\mu$ 。分布的标准差用 $\sigma$ 表示，方差用 $\sigma^{2}$ 表示。

当我们要对概率密度函数求值时，需要对 $\sigma$ 平方并且取倒数。当我们需要经常对不同参数下的概率密度函数求值时，一种更高效的参数化分布的方式是使用参数 $\beta \in(0, \infty)$ 来控制分布的精度(precision)(或方差的倒数)：<span style="color:red;">什么意思？</span>

$$
\mathcal{N}\left(x ; \mu, \beta^{-1}\right)=\sqrt{\frac{\beta}{2 \pi}} \exp \left(-\frac{1}{2} \beta(x-\mu)^{2}\right)\tag{3.22}
$$

采用正态分布在很多应用中都是一个明智的选择。当我们由于缺乏关于某个实数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选择，其中有两个原因：



- 第一，我们想要建模的很多分布的真实情况是比较接近正态分布的。中心极限定理(central limit theorem)说明很多独立随机变量的和近似服从正态分布。这意味着在实际中，很多复杂系统都可以被成功地建模成正态分布的噪声，即使系统可以被分解成一些更结构化的部分。<span style="color:red;">嗯，许多随机变量的和近似服从正太分布。嗯，很多复杂系统都可以二笔成功的建模成正态分布的噪声。</span>

- 第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。充分利用和证明这个想法需要更多的数学工具，我们推迟到第 19.4.2 节进行讲解。<span style="color:red;">为什么说正态分布在实数上有最大的不确定性？</span>

正态分布可以推广到 $\mathbb{R}^{n}$ 空间，这种情况下被称为多维正态分布(multivariate normal distribution)。它的参数是一个正定对称矩阵 $\Sigma$ ：

$$
\mathcal{N}(x ; \boldsymbol{\mu}, \mathbf{\Sigma})=\sqrt{\frac{1}{(2 \pi)^{n} \operatorname{det}(\boldsymbol{\Sigma})}} \exp \left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right)\tag{3.23}
$$

参数 $\boldsymbol{\mu}$ 仍然表示分布的均值，只不过现在是向量值。参数 $\mathbf{\Sigma}$ 给出了分布的协方差矩阵。和单变量的情况类似，当我们希望对很多不同参数下的概率密度函数多次求值时，协方差矩阵并不是一个很高效的参数化分布的方式，因为对概率密度函数求值时需要对 $\mathbf{\Sigma}$ 求逆。我们可以使用一个精度矩阵(precision matrix) $\beta$ 进行替代：

$$
\mathcal{N}\left(x ; \boldsymbol{\mu}, \boldsymbol{\beta}^{-1}\right)=\sqrt{\frac{\operatorname{det}(\boldsymbol{\beta})}{(2 \pi)^{n}}} \exp \left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\beta}(\boldsymbol{x}-\boldsymbol{\mu})\right)\tag{3.24}
$$


我们常常把协方差矩阵固定成一个对角阵。一个更简单的版本是各向同性(isotropic)高斯分布，它的协方差矩阵是一个标量乘以单位阵。<span style="color:red;">怎么固定成一个对角阵？什么是各向同性高斯分布？一般在什么时候使用？</span>


## 指数分布和 Laplace 分布

在深度学习中，我们经常会需要一个在 $x=0$ 点处取得边界点(sharp point)的分布。为了实现这一目的，我们可以使用指数分布(exponential distribution)：

$$
p(x ; \lambda)=\lambda \mathbf{1}_{x \geq 0} \exp (-\lambda x)
$$

<span style="color:red;">为什么是这么写的？</span>

指数分布用指示函数(indicator function) $\mathbf{1}_{x \geq 0}$ 来使得当 $x$ 取负值时的概率为零。<span style="color:red;">嗯，好的。</span>

一个联系紧密的概率分布是 Laplace 分布(Laplace distribution)，它允许我们在任意一点μ处设置概率质量的峰值：

$$
\operatorname{Laplace}(x ; \mu, \gamma)=\frac{1}{2 \gamma} \exp \left(-\frac{|x-\mu|}{\gamma}\right)\tag{3.26}
$$

<span style="color:red;">没有很清楚这两个分布一般什么时候使用？效果怎么样？Laplace 分布到底是什么？怎么理解呢？</span>

## Dirac 分布和经验分布

在一些情况下，我们希望概率分布中的所有质量都集中在一个点上。这可以通过 Dirac delta 函数(Dirac delta function) $\delta(x)$ 定义概率密度函数来实现：

$$
p(x)=\delta(x-\mu)\tag{3.27}
$$



Dirac delta 函数被定义成在除了 $0$ 以外的所有点的值都为 $0$ ，但是积分为 $1$。Dirac delta 函数不像普通函数一样对 $x$ 的每一个值都有一个实数值的输出，它是一种不同类型的数学对象，被称为广义函数 (generalized function)，广义函数是依据积分性质定义的数学对象。我们可以把 Dirac delta 函数想成一系列函数的极限点，这一系列函数把除 $0$ 以外的所有点的概率密度越变越小。<span style="color:red;">嗯，有点理解，但是还是有点模糊。</span>

通过把 $p(x)$ 定义成 $\delta$ 函数左移 $-\mu$ 个单位，我们得到了一个在 $x=\mu$ 处具有无限窄也无限高的峰值的概率质量。

Dirac 分布经常作为经验分布(empirical distribution) 的一个组成部分出现：

$$
\hat{p}(\boldsymbol{x})=\frac{1}{m} \sum_{i=1}^{m} \delta\left(\boldsymbol{x}-\boldsymbol{x}^{(i)}\right)\tag{3.28}
$$



经验分布将概率密度 $\frac{1}{m}$ 赋给 $m$ 个点 $\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)}$ 中的每一个，这些点是给定的数据集或者采样的集合。只有在定义连续型随机变量的经验分布时，Dirac delta函数才是必要的。对于离散型随机变量，情况更加简单：经验分布可以被定义成一个 Multinoulli 分布，对于每一个可能的输入，其概率可以简单地设为在训练集上那个输入值的经验频率(empirical frequency)。<span style="color:red;">嗯。有点理解。但是 对于连续型随机变量的经验分布才是必要的。这个一般来说用在哪里？要怎么用呢？</span>

当我们在训练集上训练模型时，可以认为从这个训练集上得到的经验分布指明了采样来源的分布。关于经验分布另外一种重要的观点是，它是训练数据的似然最大的那个概率密度函数(见第 5.5节)。<span style="color:red;">嗯，对于似然最大 的概念还是要补充下的。不然看到这句总是感觉不是很踏实。</span>



## 分布的混合

### 混合模型

通过组合一些简单的概率分布来定义新的概率分布也是很常见的。一种通用的组合方法是构造混合分布 (mixture distribution)。混合分布由一些组件 (component) 分布构成。每次实验，样本是由哪个组件分布产生的取决于从一个 Multinoulli 分布中采样的结果：<span style="color:red;">是的。</span>


$$
P(\mathrm{x})=\sum_{i} P(\mathrm{c}=i) P(\mathrm{x} | \mathrm{c}=i)\tag{3.29}
$$


这里 $P(\mathrm{c})$ 是对各组件的一个 Multinoulli 分布。

我们已经看过一个混合分布的例子了：实值变量的经验分布对于每一个训练实例来说，就是以 Dirac 分布为组件的混合分布。<span style="color:red;">哎呦喂，真的是这样哎。</span>

混合模型是组合简单概率分布来生成更丰富的分布的一种简单策略。在第 16 章中，我们更加详细地探讨从简单概率分布构建复杂模型的技术。<span style="color:red;">嗯，想知道。看到后补充到这里下。</span>

混合模型使我们能够一瞥以后会用到的一个非常重要的概念-潜变量(latent variable)。潜变量是我们不能直接观测到的随机变量。混合模型的组件标识变量 $\mathrm{c}$ 就是其中一个例子。潜变量在联合分布中可能和  $\mathrm{x}$  有关，在这种情况下，$P(\mathrm{x}, \mathrm{c})=P(\mathrm{x} | \mathrm{c}) P(\mathrm{c})$ 。潜变量的分布 $P(\mathrm{c})$ 以及关联潜变量和观测变量的条件分布 $P(\mathrm{x} | \mathrm{c})$ ，共同决定了分布 $P(\mathrm{x})$ 的形状，尽管描述 $P(\mathrm{x})$ 时可能并不需要潜变量。<span style="color:red;">是的是的，潜变量是非常重要的。而且，从这个地方切入感觉非常 nice。</span>

潜变量将在第 16.5节中深入讨论。<span style="color:red;">嗯，看到后补充下这里。</span>

### 高斯混合模型

一个非常强大且常见的混合模型是高斯混合模型 (Gaussian Mixture Model)，它的组件 $p(\mathbf{x} | \mathbf{c}=i)$ 是高斯分布。每个组件都有各自的参数，均值 $\mu^{(i)}$ 和协方差矩阵 $\Sigma^{(i)}$ 。有一些混合可以有更多的限制。例如，协方差矩阵可以通过 $\boldsymbol{\Sigma}^{(i)}=\boldsymbol{\Sigma}, \forall i$ 的形式在组件之间共享参数。和单个高斯分布一样，高斯混合模型有时会限制每个组件的协方差矩阵为对角的或者各向同性的(标量乘以单位矩阵)。<span style="color:red;">嗯，之前一直有看到过高斯混合模型，现在才比较清楚。那么高斯混合模型一般使用在什么场景下呢？各组件什么时候需要共享参数？各组件的参数要怎么更新和设定？</span>

除了均值和协方差以外，高斯混合模型的参数指明了给每个组件 $i$ 的先验概率(prior probability) $\alpha_{i}=P(\mathrm{c}=i)$ 。“先验”一词表明了在观测到 $\mathbf{x}$ 之前传递给模型关于 $\mathrm{c}$ 的信念。<span style="color:red;">嗯，先验。不过这个地方有个问题，怎么指明各个组件的先验概率的？</span>作为对比，$P(\mathrm{c} | \boldsymbol{x})$ 是后验概率(posterior probability)，因为它是在观测到 $\mathrm{x}$ 之后进行计算的。<span style="color:red;">是的。</span>高斯混合模型是概率密度的万能近似器(universal approximator)，在这种意义下，任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度来逼近。<span style="color:red;">怎么这么厉害呢！看到这句话，想起了双层神经网络可以用来逼近任何函数。嗯，是不是，确认下。</span>


图 3.2　展示了某个高斯混合模型生成的样本：<span style="color:red;">嗯，之前看到这个图还有点奇怪，现在看嗯，的确是这样。</span>

<center>

![](http://images.iterate.site/blog/image/20190518/pylDDJQJrzn5.png?imageslim){ width=55% }

</center>

图 3.2 来自高斯混合模型的样本。在这个示例中，有 $3$ 个组件。从左到右：

- 第 $1$ 个组件具有各向同性的协方差矩阵，这意味着它在每个方向上具有相同的方差。
- 第 $2$ 个组件具有对角的协方差矩阵，这意味着它可以沿着每个轴的对齐方向单独控制方差。该示例中，沿着 $x_2$ 轴的方差要比沿着 $x_1$ 轴的方差大。
- 第 $3$ 个组件具有满秩的协方差矩阵，使它能够沿着任意基的方向单独地控制方差。<span style="color:red;">满秩的概念要补充下。</span>



# 相关

- 《深度学习》花书
