---
title: 61 最大似然估计
toc: true
date: 2019-05-23
---
# 最大似然估计

之前，我们已经看过常用估计的定义，并分析了它们的性质。但是这些估计是从哪里来的呢？

我们希望有些准则可以让我们从不同模型中得到特定函数作为好的估计，而不是猜测某些函数可能是好的估计，然后分析其偏差和方差。<span style="color:red;">嗯，是的。</span>

最常用的准则是最大似然估计。

考虑一组含有 $m$ 个样本的数据集 $\mathbb{X}=\left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)}\right\}$，独立地由未知的真实数据生成分布 $p_{\text { data }}(\mathbf{x})$ 生成。

令 $p_{\text { model }}(\mathbf{x} ; \boldsymbol{\theta})$ 是一族由 $\boldsymbol{\theta}$ 确定在相同空间上的概率分布。换言之，$p_{\text { model }}(x ; \boldsymbol{\theta})$ 将任意输入 $\boldsymbol{x}$ 映射到实数来估计真实概率 $p_{\text { data }}(\boldsymbol{x})$。

对 $\boldsymbol{\theta}$ 的最大似然估计被定义为：

$$
\begin{aligned}
\boldsymbol{\theta}_{\mathrm{ML}}&=\underset{\boldsymbol{\theta}}{\arg \max } p_{\text { model }}(\mathbb{X} ; \boldsymbol{\theta})\\&=\underset{\boldsymbol{\theta}}{\arg \max } \prod_{i=1}^{m} p_{\text { model }}\left(\boldsymbol{x}^{(i)} ; \boldsymbol{\theta}\right)
\end{aligned}
$$



多个概率的乘积会因很多原因不便于计算。例如，计算中很可能会出现数值下溢。为了得到一个便于计算的等价优化问题，我们观察到似然对数不会改变其 $\arg \max$，但是将乘积转化成了便于计算的求和形式：

$$
\boldsymbol{\theta}_{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } \sum_{i=1}^{m} \log p_{\text { model }}\left(\boldsymbol{x}^{(i)} ; \boldsymbol{\theta}\right)\tag{5.58}
$$

因为当重新缩放代价函数时 $\arg \max$ 不会改变，我们可以除以 $m$ 得到和训练数据经验分布 $\hat{p}_{\text { data }}$ 相关的期望作为准则：

$$
\boldsymbol{\theta}_{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } \mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text { data }}} \log p_{\text { model }}(\boldsymbol{x} ; \boldsymbol{\theta})\tag{5.59}
$$

一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布 $\hat{p}_{\text { data }}$ 和模型分布之间的差异，两者之间的差异程度可以通过 KL 散度度量。KL 散度被定义为：

$$
D_{\mathrm{KL}}\left(\hat{p}_{\text { data }} \| p_{\text { model }}\right)=\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text { data }}}\left[\log \hat{p}_{\text { data }}(\boldsymbol{x})-\log p_{\text { model }}(\boldsymbol{x})\right]\tag{5.60}
$$

左边一项仅涉及数据生成过程，和模型无关。这意味着当训练模型最小化 KL 散度时，我们只需要最小化

$$
-\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text { data }}}\left[\log p_{\text { model }}(\boldsymbol{x})\right]\tag{5.61}
$$

当然，这和式(5.59)中最大化是相同的。

最小化 KL 散度其实就是在最小化分布之间的交叉熵。许多作者使用术语 “交叉熵” 特定表示伯努利或 softmax 分布的负对数似然，但那是用词不当的。任何一个由负对数似然组成的损失都是定义在训练集上的经验分布和定义在模型上的概率分布之间的交叉熵。例如，均方误差是经验分布和高斯模型之间的交叉熵。<span style="color:red;">任何一个由负对数似然组成的损失都是定义在训练集上的经验分布和定义在模型上的概率分布之间的交叉熵。嗯有点不是特别透彻。</span>

我们可以将最大似然看作使模型分布尽可能地和经验分布 $\hat{p}_{\text { data }}$ 相匹配的尝试。理想情况下，我们希望匹配真实的数据生成分布 $p_{\text { data }}$，但我们无法直接知道这个分布。

虽然最优 $\boldsymbol{\theta}$ 在最大化似然或是最小化 KL 散度时是相同的，但目标函数值是不一样的。在软件中，我们通常将两者都称为最小化代价函数。因此最大化似然变成了最小化负对数似然(NLL)，或者等价的是最小化交叉熵。将最大化似然看作最小化 KL 散度的视角在这个情况下是有帮助的，因为已知 KL 散度最小值是零。当 $\boldsymbol{x}$ 取实数时，负对数似然是负值。

## 条件对数似然和均方误差

最大似然估计很容易扩展到估计条件概率 $P(\mathbf{y} | \mathbf{x} ; \boldsymbol{\theta})$ ，从而给定 $\mathbf{x}$ 预测 $\mathbf{y}$ 。实际上这是最常见的情况，因为这构成了大多数监督学习的基础。

如果 $\mathbf{X}$ 表示所有的输入，$\mathbf{Y}$ 表示我们观测到的目标，那么条件最大似然估计是：

$$
\boldsymbol{\theta}_{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } P(\boldsymbol{Y} | \boldsymbol{X} ; \boldsymbol{\theta})\tag{5.62}
$$

如果假设样本是独立同分布的，那么式(5.62)可以分解成：


$$
\boldsymbol{\theta}_{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } \sum_{i=1}^{m} \log P\left(\boldsymbol{y}^{(i)} | \boldsymbol{x}^{(i)} ; \boldsymbol{\theta}\right)\tag{5.63}
$$

### 示例：线性回归作为最大似然

第 5.1.4节介绍的线性回归，可以被看作最大似然过程。之前，我们将线性回归作为学习从输入 $\boldsymbol{x}$ 映射到输出 $\hat{y}$ 的算法。从 $\boldsymbol{x}$ 到 $\hat{y}$ 的映射选自最小化均方误差(我们或多或少介绍的一个标准)。现在，我们以最大似然估计的角度重新审视线性回归。我们现在希望模型能够得到条件概率 $p(y | \boldsymbol{x})$，而不只是得到一个单独的预测 $\hat{y}$ 。想象有一个无限大的训练集，我们可能会观测到几个训练样本有相同的输入 $\boldsymbol{x}$ 但是不同的 $\hat{y}$ 。现在学习算法的目标是拟合分布 $p(y | \boldsymbol{x})$ 到和 $\boldsymbol{x}$ 相匹配的不同的 $\hat{y}$。为了得到我们之前推导出的相同的线性回归算法，我们定义。函数预测高斯的均值。在这个例子中，我们假设方差是用户固定的某个常量 $\sigma^{2}$。这种函数形式 $p(y | \boldsymbol{x})$ 会使得最大似然估计得出和之前相同的学习算法。由于假设样本是独立同分布的，条件对数似然( 式(5.63))如下

$$
\sum_{i=1}^{m} \log p\left(y^{(i)} | \boldsymbol{x}^{(i)} ; \boldsymbol{\theta}\right)\tag{5.64}
$$

$$
=-m \log \sigma-\frac{m}{2} \log (2 \pi)-\sum_{i=1}^{m} \frac{\left\|\hat{y}^{(i)}-y^{(i)}\right\|^{2}}{2 \sigma^{2}}\tag{5.65}
$$

其中 $\hat{y}^{(i)}$ 是线性回归在第 $i$ 个输入 $\boldsymbol{x}^{(i)}$ 上的输出，$m$ 是训练样本的数目。对比均方误差和对数似然，

$$
\mathrm{MSE}_{\text { train }}=\frac{1}{m} \sum_{i=1}^{m}\left\|\hat{y}^{(i)}-y^{(i)}\right\|^{2}\tag{5.66}
$$


我们立刻可以看出，最大化关于 $\boldsymbol{w}$ 的对数似然和最小化均方误差会得到相同的参数估计 $\boldsymbol{w}$ 。但是对于相同的最优 $\boldsymbol{w}$ ，这两个准则有着不同的值。这验证了 MSE 可以用于最大似然估计。<span style="color:red;">嗯，厉害呀。</span>

正如我们将看到的，最大似然估计有几个理想的性质。

## 最大似然的性质

最大似然估计最吸引人的地方在于，它被证明当样本数目 $m \rightarrow \infty$ 时，就收敛率而言是最好的渐近估计。

在合适的条件下，最大似然估计具有一致性(参考第 5.4.5节)，意味着训练样本数目趋向于无穷大时，参数的最大似然估计会收敛到参数的真实值。这些条件是：

- 真实分布 $p \mathrm{data}$ 必须在模型族 $p_{\text { model }}(\cdot ; \boldsymbol{\theta})$ 中。否则，没有估计可以还原 $p \mathrm{data}$ 。<span style="color:red;">嗯，不过，啥时候真实分布不在模型族里面？还是说这个真实分布就是随机的？模型族里面有哪些模型？怎么知道是不是在模型族里面？</span>

- 真实分布 $p \mathrm{data}$ 必须刚好对应一个 $\boldsymbol{\theta}$ 值。否则，最大似然估计恢复出真实分布 $p \mathrm{data}$ 后，也不能决定数据生成过程使用哪个 $\boldsymbol{\theta}$ 。<span style="color:red;">是的是的，不过，会有对应多个 $\boldsymbol{\theta}$ 的时候吗？这样任取一个 $\boldsymbol{\theta}$ 不行吗？好还是说这个 $\boldsymbol{\theta}$ 是不固定的？</span>

除了最大似然估计，还有其他的归纳准则，其中许多共享一致估计的性质。然而，一致估计的统计效率 (statistic efficiency) 可能区别很大。某些一致估计可能会在固定数目的样本上获得一个较低的泛化误差，或者等价地，可能只需要较少的样本就能达到一个固定程度的泛化误差。<span style="color:red;">嗯。还有什么归纳规则，都总结到这里吧。而且，共享一致估计的性质是什么样的。什么是一致估计的统计效率？</span>

统计效率通常用于有参情况(parametric case)的研究中(例如线性回归)。在有参情况中，我们的目标是估计参数值(假设有可能确定真实参数)，而不是函数值。一种度量和真实参数相差多少的方法是计算均方误差的期望，即计算 $m$ 个从数据生成分布中出来的训练样本上的估计参数和真实参数之间差值的平方。有参均方误差估计随着 $m$ 的增加而减少，当 $m$ 较大时，Cramér-Rao 下界表明不存在均方误差低于最大似然估计的一致估计。<span style="color:red;">没有很明白，补充下。</span>

因为这些原因(一致性和统计效率)，最大似然通常是机器学习中的首选估计方法。当样本数目小到会发生过拟合时，正则化策略如权重衰减可用于获得训练数据有限时方差较小的最大似然有偏版本。<span style="color:red;">补充下。</span>



# 相关

- 《深度学习》花书
