---
title: 大纲
toc: true
date: 2019-09-29
---
# 大纲

应用统计学专业的所有课程都要合并进来。

拥有优秀的统计知识；

熟悉常用统计分析方法，例如：聚类分析、判别分析、主成分分析、回归分析。


## 可以补充进来的

现代统计学：

“学习统计学的最佳方法是什么？”答案：阅读《An Introduction to Statistical Learning》。然后，如果你读完了，想要了解更多，阅读《The Elements of Statistical Learning》。这两本书是由斯坦福大学、华盛顿大学和南加州大学的统计学教授共同撰写的，是用现代技术进行数据统计的最直观、最好的数据。蒂布利亚尼是这两本书的共同作者。可以免费下载。

- 《An Introduction to Statistical Learning》

下载链接1：<http://www-bcf.usc.edu/~gareth/ISL/>

下载链接2（含中文版）：<http://download.csdn.net/download/majinlei121/9658748>

- 《The Elements of Statistical Learning》 这本书就是三个作者给数据挖掘，机器学习这个宫殿搭的一个脚手架。处处可见作者思路的轨迹，是怎么想出那么多方法的。

下载链接：<https://web.stanford.edu/~hastie/ElemStatLearn/>

中文版学习资料：<http://www.loyhome.com/elements_of_statistical_learining_lecture_notes/>



《The Elements of Statistical Learning》 大概介绍：

作者出发点是“维数灾难(the curse of dimensionality)”。

如果用“少量的训练样本在高维特征空间中分布变得稀疏，导致算法缺乏泛化能力”这样的话来解释，我想任何人第一次听说的时候绝对不会明白。我即使现在明白了，也还是觉得这种表达太装13了，太绕。那就换一种角度来说。

解方程的时候，理论上方程个数N只要等于未知数个数p，那么就可以了。但是实际中考虑的误差之类的，方程数要比未知数多，才能保证求出的解是“好的”。总是希望最好是“所有可能的样本”拿来都能套用这个解。也就是说是全局最优解。

“维灾难”是说，随着未知数个数增加，需要的方程个数增加得大得多。具体到机器学习上1个训练样本就是1个方程，那么维数p一高，你的训练样本N再多也是“不够用”的，即使N>>p。

解决方法就俩字——想办法“降维”。降低描述每个样本的特征向量的维数p，也就是降未知数的个数。这样方程数量不变的情况下，未知数少了。

除了维灾难，还有一种情况，就是未知数反过来比方程多的时候，而且是p>>N。比如DNA微阵列分析、语义分析这些。以DNA来说，对某种癌症，上万的基因的可能致病作用是未知数（如果考虑不同基因组合作用，就更多了），可病人样本的总是有限的，有时就是要在几千个，几百个的样本上进行分析，希望能“挖掘”到放之四海而皆准的原理来。这时候想把`p`降到`<N`，实在不现实。

而方程太少，参数太多，肯定是无穷多种解啊，可是致病机理一定是确定的。想得到唯一解怎么办呢？嗯，加约束条件。

我概括作者研究的总方向就是一个：线性模型+罚约束。

作者在第3章开始讲线性回归，然后第4章线性分类。从最基础的最小二乘解开始，然后是对解加2范数约束得到Ridge Regression岭回归；再到加1范数的约束，于是有了Lasso，LAR；然后是1 2 范数的组合Elastic Net等等等……。一开始读过，觉得这些没什么特别之处啊，说一千道一万，不就是个线性方程组的解么，能有什么意思？就像《地道战》里的牛娃说的：“鼓捣这玩意，能把黑风口的炮楼挖掉么？”Amazon也有读者觉得前几章有点浪费篇幅。除了因为觉得线性回归、分类太简单，还有挤占别的章节篇幅的嫌疑，后面再说。

但是越往后读，就越要经常往回翻第3章。

与其说这是作者的写作方式，倒不如说作者的研究路径就是这种不断“回溯”的过程（借用软件工程里的话）。具体地说：作者就是用这种不断应用“线性模型+罚约束”的思想，串起来一系列的方法：

第5章从样条到加罚平滑样条和小波，这就到了kernel的思想：经过非线性的核/基函数作用，非线性问题又变成了线性问题。

这就到了第9章的广义加法模型：树和多元加法样条。

有了广义加法模型，第10章作者用“逐次向加法模型添加非线性基函数，以使指数形式的损失函数变小”的角度分析了Adaboost，boosting，以及提出了MART。

然后到了第15章Random Forest。这章很短，但是之前一路看过来之后，觉得就是这么简单而已。

16章的Ensemble Learning也是如此，开头的ECOC虚晃一枪。后面仍然使用boosting 、Random Forest的思路，用线性组合+罚来集成多分类器。

最后的18章，推广到 `p>>N` 的情况下的特征选择问题。作者展示了在DNA微阵列分析问题上，把之前各种方法随便组合起来一下（比如加罚的logistic 回归，线性回归+PCA = 监督PCA），都有不俗表现。

通读此书之后让人觉得是某位大侠，从最朴实无华的招式起手，初觉平淡无奇，但是后面不断深入、不断变化，到最后的眼花缭乱，屡出奇招。而每次变招都是从原来那最简单的招式开始（比如太极拳，就是揽雀尾啊，那几招）。这几章构成了一个连贯的思路，作者们30多年来的成果主要也集中上面这几章。

其他的章节要么是为了挂靠在这个思路下面(4线性分类)，要么是为了讲这个思路不得不提到(6核平滑、7模型选择、13原型方法和近邻、14无监督学习)。其中的7、14也是非常重要的章节。

剩下的就比较鸡肋了，应该是作者不擅长或者不care，但是因为写书还是放进来的(8 贝叶斯推断、11神经网络、12支持向量机、17图模型)。仅以这些方法本身论，都是能单独写一本书的，也应该算是“必知必会”的知识点了，但是本书中篇幅和内容上都比较简略了。
