---
title: 4.04 常用分类方法优缺点
toc: true
date: 2019-09-03
---
# 常用分类算法优缺点


| 算法                        | 优点                                                                                                                                                                                                                                                 | 缺点                                                                                                                                                 |
|:--------------------------- |:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |:---------------------------------------------------------------------------------------------------------------------------------------------------- |
| Bayes 贝叶斯分类法  <span style="color:red;">没有很清楚。</span>        | 1）所需估计的参数少，对于缺失数据不敏感。<br />2）有着坚实的数学基础，以及稳定的分类效率。                                                                                                                                                           | 1）需要假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。<br />2）需要知道先验概率。<br />3）分类决策存在错误率。      |
| Decision Tree决策树         | 1）不需要任何领域知识或参数假设。<br />2）适合高维数据。<br />3）简单易于理解。<br />4）短时间内处理大量数据，得到可行且效果较好的结果。<br />5）能够同时处理数据型和常规性属性。                                                                    | 1）对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。<span style="color:red;">是这样吗？</span><br />2）易于过拟合。<br />3）忽略属性之间的相关性。<span style="color:red;">嗯，这个倒是，忽略了属性之间的相关性。</span><br />4）不支持在线学习。 <span style="color:red;">嗯，的确不支持在线学习。</span>|
| SVM支持向量机               | 1）可以解决小样本下机器学习的问题。<br />2）提高泛化性能。<br />3）可以解决高维、非线性问题。超高维文本分类仍受欢迎。<span style="color:red;">超高维文本分类还会使用 SVM 吗？</span><br />4）避免神经网络结构选择和局部极小的问题。 <span style="color:red;">这个是什么意思？避免神经网络结构选择核局部极小的问题？</span>                                                                                | 1）对缺失数据敏感。<br />2）内存消耗大，难以解释。<span style="color:red;">内存消耗大吗？为啥？训练的时候吗？</span><br />3）运行和调参略烦人。<span style="color:red;">到现在还是不清楚怎么才能把 SVM 调好。</span>                                                                        |
| KNN K近邻                   | 1）思想简单，理论成熟，既可以用来做分类也可以用来做回归；<span style="color:red;">K 近邻用于回归是怎么使用的？</span> <br />2）可用于非线性分类；<br /> 3）训练时间复杂度为 O(n)； <br />4）准确度高，对数据没有假设，对 outlier 不敏感；                                                                           | 1）计算量太大。<br />2）对于样本分类不均衡的问题，会产生误判。<span style="color:red;">不均衡的样本会产生误判吗？</span><br />3）需要大量的内存。<br />4）输出的可解释性不强。 <span style="color:red;">可解释性不强吗？</span>                                |
| Logistic Regression逻辑回归 | 1）速度快。<br />2）简单易于理解，直接看到各个特征的权重。<br />3）能容易地更新模型吸收新的数据。<br />4）如果想要一个概率框架，动态调整分类阀值。                                                                                                   | 特征处理复杂。需要归一化和较多的特征工程。                                                                                                           |
| Neural Network 神经网络     | 1）分类准确率高。<br />2）并行处理能力强。<br />3）分布式存储和学习能力强。<br />4）鲁棒性较强，不易受噪声影响。                                                                                                                                     | 1）需要大量参数（网络拓扑、阀值、阈值）。<br />2）结果难以解释。<br />3）训练时间过长。                                                              |
| Adaboosting                 | 1）adaboost是一种有很高精度的分类器。<br />2）可以使用各种方法构建子分类器，Adaboost算法提供的是框架。<br />3）当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。<br />4）简单，不用做特征筛选。<br />5）不用担心 overfitting。 | 对 outlier 比较敏感  <span style="color:red;">adaboost 对 outlier 比较敏感吗？</span>                                                                                                                                   |
