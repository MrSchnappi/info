---
title: 1.02 一些基本的符号
toc: true
date: 2018-07-30 10:57:57
---


# 计算学习理论


## 举个例子

给定样例集 $D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$，$\boldsymbol{x}_{i} \in \mathcal{X}$ ，对于二分类问题，$y_{i} \in \mathcal{Y}=\{-1,+1\}$ 。

假设 $\mathcal{X}$ 中的所有样本服从一个隐含未知的分布 $\mathcal{D}$ ， $D$ 中所有样本都是独立地从这个分布上采样而得，即独立同分布(independent and identically distributed，简称 i.i.d. ) 样本。

令 $h$ 为从 $\mathcal{X}$ 到 $\mathcal{Y}$ 的一个映射，其泛化误差为：

$$
E(h ; \mathcal{D})=P_{\boldsymbol{x} \sim \mathcal{D}}(h(\boldsymbol{x}) \neq y)
$$

$h$ 在 $D$ 上的经验误差为：

$$
\widehat{E}(h ; D)=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(h\left(\boldsymbol{x}_{i}\right) \neq y_{i}\right)
$$

由于 $D$ 是 $\mathcal{D}$ 的独立同分布采样，因此的经验误差的期望等于其泛化误差。

在上下文明确时，我们将 $E(h ; \mathcal{D})$ 和 $\widehat{E}(h ; D)$ 分别简记为 $E(h)$ 和 $\widehat{E}(h)$ 。令 $\epsilon$ 为 $E(h)$ 的上限，即 $E(h) \leqslant \epsilon$ ；我们通常用 $\epsilon$ 表示预先设定的学得模型所应满足的误差要求，亦称“误差参数”。

后面我们会研究经验误差与泛化误差之间的逼近程度。<span style="color:red;">后面有讲吗？补充下。</span>若 $h$ 在数据集 $D$ 上的经验误差为 $0$，则称 $h$ 与 $D$ 一致，否则称其与 $D$ 不一致。对任意两个映 射 $h_{1}, h_{2} \in \mathcal{X} \rightarrow \mathcal{Y}$，可通过其“不合”(disagreement)来度量它们之间的差别：

$$
d\left(h_{1}, h_{2}\right)=P_{\boldsymbol{x} \sim \mathcal{D}}\left(h_{1}(\boldsymbol{x}) \neq h_{2}(\boldsymbol{x})\right)
$$





# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
