---
title: 56 稳定性
toc: true
date: 2018-07-30 11:04:41
---
# 可以补充进来的

# 稳定性

无论是基于 VC 维还是 Rademacher 复杂度来推导泛化误差界，所得到的结果均与具体学习算法无关，对所有学习算法都适用。这使得人们能够脱离具体学习算法的设计来考虑学习问题本身的性质。

但在另一方面，若希望获得与算法有关的分析结果，则需另辟蹊径。稳定性 ability分析是这方面一个值 得关注的方向。

顾名思义，算法的“稳定性”考察的是算法在输入发生变化时，输出是否 会随之发生较大的变化。学习算法的输入是训练集，因此下面我们先定义训练集的两种变化。

给定 $D=\left\{\boldsymbol{z}_{1}=\left(\boldsymbol{x}_{1}, y_{1}\right), \boldsymbol{z}_{2}=\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots, \boldsymbol{z}_{m}=\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$，$\boldsymbol{x}_{i} \in \mathcal{X}$ 是来 自分布 $\mathcal{D}$ 的独立同分布示例， $y_{i}=\{-1,+1\}$ 。对假设空间 $\mathcal{H} : \mathcal{X} \rightarrow\{-1,+1\}$ 和学习算法 $\mathfrak{L}$ ，令 $\mathfrak{L}_{D} \in \mathcal{H}$ 表示基于训练集 $D$ 从假设空间 $\mathcal{H}$ 中学得的假设。考虑 $D$ 的以下变化：

- $D^{\backslash i}$ 表示移除 $D$ 中第 $i$ 个样例得到的集合

$$
D^{\backslash i}=\left\{\boldsymbol{z}_{1}, \boldsymbol{z}_{2}, \ldots, \boldsymbol{z}_{i-1}, \boldsymbol{z}_{i+1}, \ldots, \boldsymbol{z}_{m}\right\}
$$

- $D^{i}$ 表示替换 $D$ 中第 $i$ 个样例得到的集合

$$
D^{i}=\left\{\boldsymbol{z}_{1}, \boldsymbol{z}_{2}, \ldots, \boldsymbol{z}_{i-1}, \boldsymbol{z}_{i}^{\prime}, \boldsymbol{z}_{i+1}, \ldots, \boldsymbol{z}_{m}\right\}
$$

其中 $\boldsymbol{z}_{i}^{\prime}=\left(\boldsymbol{x}_{i}^{\prime}, y_{i}^{\prime}\right)$ , $\boldsymbol{x}_{i}^{\prime}$ 服从分布 $\mathcal{D}$ 并独立于 $D$。

损失函数 $\ell\left(\mathfrak{L}_{D}(\boldsymbol{x}), y\right) : \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}^{+}$ 刻画了假设 $\mathfrak{L}_{D}$ 的预测标记 $\mathfrak{L}_{D}(\boldsymbol{x})$ 与
真实标记 $y$ 之间的差别，简记为 $\ell\left(\mathfrak{L}_{D}, \boldsymbol{z}\right)$。下面定义关于假设 $\mathfrak{L}_{D}$ 的几种损失。

- 泛化损失

$$
\ell(\mathfrak{L}, \mathcal{D})=\mathbb{E}_{\boldsymbol{x} \in \mathcal{X}, \boldsymbol{z}=(\boldsymbol{x}, y)}\left[\ell\left(\mathfrak{L}_{D}, \boldsymbol{z}\right)\right]\tag{12.54}
$$

- 经验损失

$$
\widehat{\ell}(\mathfrak{L}, D)=\frac{1}{m} \sum_{i=1}^{m} \ell\left(\mathfrak{L}_{D}, \boldsymbol{z}_{i}\right)\tag{12.55}
$$

- 留一 (leave-one - out)损失

$$
\ell_{l o o}(\mathfrak{L}, D)=\frac{1}{m} \sum_{i=1}^{m} \ell\left(\mathfrak{L}_{D^{ \backslash i}}, \boldsymbol{z}_{i}\right)\tag{12.56}
$$

下面定义算法的均匀稳定性(uniform stability):

## 算法的均匀稳定性(uniform stability)

对任何 $\boldsymbol{x} \in \mathcal{X}$，$\boldsymbol{z}=(\boldsymbol{x}, y)$   若学习算法 $\mathfrak{L}$ 满足

$$
\left|\ell\left(\mathfrak{L}_{D}, \boldsymbol{z}\right)-\ell\left(\mathfrak{L}_{D^{\backslash i}}, \boldsymbol{z}\right)\right| \leqslant \beta, \quad i=1,2, \ldots, m\tag{12.57}
$$

则称 $\mathfrak{L}$ 关于损失函数 $\ell$ 满足 $\beta$-均匀稳定性。

显然，若算法 $\mathfrak{L}$ 关于损失函数 $\ell$ 满足 $\beta$-均匀稳定性，则有

$$
\begin{aligned} &\left|\ell\left(\mathfrak{L}_{D}, \boldsymbol{z}\right)-\ell\left(\mathfrak{L}_{D^{i}}, \boldsymbol{z}\right)\right| \\ \leqslant &\left|\ell\left(\mathfrak{L}_{D}, \boldsymbol{z}\right)-\ell\left(\mathfrak{L}_{D ^{\backslash i}}, \boldsymbol{z}\right)\right|+\left|\ell\left(\mathfrak{L}_{D^{i}}, \boldsymbol{z}\right)-\ell\left(\mathfrak{L}_{D^{\backslash i}}, \boldsymbol{z}\right)\right| \\ \leqslant & 2 \beta \end{aligned}
$$


也就是说，移除示例的稳定性包含替换示例的稳定性。

若损失函数 $\ell$ 有界，即对所有 $D$ 和 $\boldsymbol{z}=(\boldsymbol{x}, y)$ 有 $0 \leqslant l\left(\mathfrak{L}_{D}, \boldsymbol{z}\right) \leqslant M$ ，则有：

## 基于稳定性分析推导出的学习算法 $\mathfrak{L}$ 学得假设的泛化误差界

给定从分布 $\mathcal{D}$ 上独立同分布采样得到的大小为 $m$ 的示例集 $D$，若学习算法 $\mathfrak{L}$ 满足关于损失函数 $\ell$ 的 $\beta$-均匀稳定性，且损失函数 $\ell$ 的上界为 $M$，$0<\delta<1$，则对任意 $m \geqslant 1$，以至少 $1-\delta$ 的概率有

$$
\ell(\mathfrak{L}, \mathcal{D}) \leqslant \widehat{\ell}(\mathfrak{L}, D)+2 \beta+(4 m \beta+M) \sqrt{\frac{\ln (1 / \delta)}{2 m}}\tag{12.58}
$$
$$
\ell(\mathfrak{L}, \mathcal{D}) \leqslant \ell_{l o o}(\mathfrak{L}, D)+\beta+(4 m \beta+M) \sqrt{\frac{\ln (1 / \delta)}{2 m}}\tag{12.59}
$$

> $$
> l(\varepsilon, D) \leq l_{l o o}(\overline{\varepsilon}, D)+\beta+(4 m \beta+M) \sqrt{ \frac{\ln (1 / \delta)}{2 m}}
> $$
> [解析]：取 $ \varepsilon=\beta+(4 m \beta+M) \sqrt\frac{\ln (1 / \delta)}{2 m}$ 时，可以得到：
>
> $l(\varepsilon, D)-l_{l o o}(\varepsilon, D) \leq \varepsilon$ 以至少 $1-\frac{\delta} 2 $ 的概率成立，$K$ 折交叉验证，当 $K=m$ 时，就成了留一法，这时候会有很不错的泛化能力，但是有前提条件，对于损失函数 $l$ 满足 $ \beta$ 均匀稳定性，且 $ \beta$ 应该是 $O(\frac{1}m)$ 这个量级，仅拿出一个样本，可以保证很小的 $ \beta$，随着 $K$ 的减小，训练的样本会减少，$\beta$ 会逐渐增大，当 $\beta$ 量级小于 $O(\frac{1}m)$ 时，交叉验证就会不合理了

上述给出了基于稳定性分析推导出的学习算法 $\mathfrak{L}$ 学得假设的泛化误差界。从式(12.58)可看出，经验损失与泛化损失之间差别的收敛率为 $\beta \sqrt{m}$ ，若 $\beta=O\left(\frac{1}{m}\right)$，则可保证收敛率为 $O\left(\frac{1}{\sqrt{m}}\right)$ 。与定理 12.3和定理 12.6比较可知，这与基于 VC 维和 Rademacher 复杂度得到的收敛率一致。

需注意，学习算法的稳定性分析所关注的是 $|\widehat{\ell}(\mathfrak{L}, D)-\ell(\mathfrak{L}, \mathcal{D})|$ ，而假设空 间复杂度分析所关注的是 $\sup _{h \in \mathcal{H}}|\widehat{E}(h)-E(h)|$ ；也就是说，稳定性分析不必考 虑假设空间中所有可能的假设，只需根据算法自身的特性(稳定性)来讨论输出 假设 $\mathfrak{L}_{D}$ 的泛化误差界。那么，稳定性与可学习性之间有什么关系呢？

首先，必须假设 $\beta \sqrt{m} \rightarrow 0$ ，这样才能保证稳定的学习算法 $\mathfrak{L}$ 具有一定的泛 化能力，即经验损失收敛于泛化损失，否则可学习性无从谈起。为便于计算，我 们假定 $\beta=\frac{1}{m}$ ，代入式(12.58)可得

$$
\ell(\mathfrak{L}, \mathcal{D}) \leqslant \widehat{\ell}(\mathfrak{L}, D)+\frac{2}{m}+(4+M) \sqrt{\frac{\ln (1 / \delta)}{2 m}}\tag{12.60}
$$

> 最小化经验误差和最小化经验损失有时并不相同, 这是由于存在某些病态的 损失函数 $\ell$ 使得最小化经验损失并不是最小化经验误差。为简化讨论，本章假定最小化经验损失的同时会最小化经验误差。


对损失函数 $\ell$，若学习算法 $\mathfrak{L}$ 所输出的假设满足经验损失最小化，则称算法 $\mathfrak{L}$ 满足经验风险最小化(Empirical Risk Minimization)原则，简称算法是 ERM 的。关于学习算法的稳定性和可学习性，有如下定理：

**定理 12.9** 若学习算法 $\mathfrak{L}$ 是 ERM 且稳定的，则假设空间 $\mathcal{H}$ 可学习。

**证明** 令 $g$ 表示 $\mathcal{H}$ 中具有最小泛化损失的假设，即

$$
\ell(g, \mathcal{D})=\min _{h \in \mathcal{H}} \ell(h, \mathcal{D})
$$

再令

$$
\epsilon^{\prime}=\frac{\epsilon}{2}
$$

$$
\frac{\delta}{2}=2 \exp \left(-2 m\left(\epsilon^{\prime}\right)^{2}\right)
$$

由 Hoeffding 不等式(12.6)可知，当 $m \geqslant \frac{2}{\epsilon^{2}} \ln \frac{4}{\delta}$ 时，

$$
|\ell(g, \mathcal{D})-\widehat{\ell}(g, D)| \leqslant \frac{\epsilon}{2}
$$

以至少 $1-\delta / 2$ 的概率成立。令式(12.60)中

$$
\frac{2}{m}+(4+M) \sqrt{\frac{\ln (2 / \delta)}{2 m}}=\frac{\epsilon}{2}
$$


解得 $m=O\left(\frac{1}{\epsilon^{2}} \ln \frac{1}{\delta}\right)$ 使

$$
\ell(\mathfrak{L}, \mathcal{D}) \leqslant \widehat{\ell}(\mathfrak{L}, D)+\frac{\epsilon}{2}
$$

以至少 $1-\delta / 2$ 的概率成立。从而可得

$$
\begin{aligned} \ell(\mathfrak{L}, \mathcal{D})-\ell(g, \mathcal{D}) & \leqslant \widehat{\ell}(\mathfrak{L}, D)+\frac{\epsilon}{2}-\left(\widehat{\ell}(g, D)-\frac{\epsilon}{2}\right) \\ & \leqslant \widehat{\ell}(\mathfrak{L}, D)-\widehat{\ell}(g, D)+\epsilon \\ & \leqslant \epsilon \end{aligned}
$$

以至少 $1-\delta$ 的概率成立。定理 12.9得证。

对上面这个定理读者也许会纳闷，为什么学习算法的稳定性能导出假设空 间的可学习性？学习算法和假设空间是两码事呀。事实上，要注意到稳定性与 假设空间并非无关，由稳定性的定义可知两者通过损失函数 $\ell$ 联系起来。




# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
