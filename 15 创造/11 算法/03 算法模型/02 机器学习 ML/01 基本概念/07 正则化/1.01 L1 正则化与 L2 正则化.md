---
title: 1.01 L1 正则化与 L2 正则化
toc: true
date: 2019-08-27
---
# 线性回归的损失函数

给定数据集 $D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$ ，其中 $\boldsymbol{x} \in \mathbb{R}^{d}$ ，$y \in \mathbb{R}$ 。我们考虑最简单的线性回归模型，以平方误差为损失函数，则优化目标为

$$
\min _{\boldsymbol{w}} \sum_{i=1}^{m}\left(y_{i}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}\right)^{2}
$$





## 引入 $\mathrm{L}_{2}$ 正则化

为了缓解过拟合问题，对线性回归的损失函数引入正则化项。若使用 $\mathrm{L}_{2}$ 范数正则化，则有

$$
\min _{\boldsymbol{w}} \sum_{i=1}^{m}\left(y_{i}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}\right)^{2}+\lambda\|\boldsymbol{w}\|_{2}^{2}
$$


其中正则化参数 $\lambda>0$ 。

这样的 L2 正则化后的线性回归也被称为“岭回归” (ridge regression) 。

通过引入 $\mathrm{L}_{2}$ 范数正则化，确能显著降低过拟合的风险。


## 引入 $\mathrm{L}_{1}$ 正则化

能否将正则化项中的 $\mathrm{L}_{2}$ 范数替换为 $\mathrm{L}_{p}$ 范数呢？

答案是肯定的。若令 $p=1$ ，即采用 $\mathrm{L}_{1}$ 范数，则有

$$
\min _{\boldsymbol{w}} \sum_{i=1}^{m}\left(y_{i}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}\right)^{2}+\lambda\|\boldsymbol{w}\|_{1}
$$

其中正则化参数  $\lambda>0$ 。


这样的 L1 正则化后的线性回归也被称为 LASSO 回归 (Least Absolute Shrinkage and Selection Operator) 。



$\mathrm{L}_{1}$ 范数和 $\mathrm{L}_{2}$ 范数正则化都有助于降低过拟合风险，但前者还会带来一个额外的好处：它比后者更易于获得“稀疏”(sparse)解，即它求得的 $\boldsymbol{w}$ 会有更少的非零分量。






# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
