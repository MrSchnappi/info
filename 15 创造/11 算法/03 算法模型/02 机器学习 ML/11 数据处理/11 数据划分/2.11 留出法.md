---
title: 2.11 留出法
toc: true
date: 2019-09-20
---

# 留出法 Holdout

Holdout 检验是最简单也是最直接的验证方法，它将原始的样本集合随机划分成训练集和验证集两部分。

比如：

- 对于一个点击率预测模型，我们把样本按照 70%~30% 的比例分成两部分，70% 的样本用于模型训练；30% 的样本用于模型验证，包括绘制 ROC 曲线、计算精确率和召回率等指标来评估模型性能。


留出法实际上是最常用的，但是，这中间有个问题，我们在划分数据集的时候，是需要尽可能的保持数据分布的一致性的，不然的话会引入额外的偏差，这个偏差会对最终结果产生影响。

## 如何在划分的时候保持数据分布的一致性

那么怎么在划分的时候保持数据分布的一致性呢？

首先，什么是保持数据分布的一致性？其实就是在分类任务中至少要保持样本的类别比例相似。<span style="color:red;">这里只提了分类任务，如果是回归任务，怎么分层采样？如果是聚类任务呢？</span>

我们常用的划分方式就是**分层采样** (stratified sampling)。

OK，这就是我们使用分层采样的原因，因为它正是一种保留类别比例的采样方式。

比如说：我们想对 $D$ 进行 7/3 分 ，若 $D$ 包含 500 个正例、500 个反例，那么分层采样得到的 $S$ 应包含 350 个正例、350 个反例，而 $T$ 则包含 150 个正例和 150 个反例。

## 评估后取平均值

但是，实际上，即便在给定 7/3 分的比例之后，我们仍然是有很多种划分方式来对 $D$ 进行分割的。

比如上面的例子中，可以把 $D$ 中的样本排序，然后把前 350 个正例放到 $S$ 中，也可以把后 350 个正例放到 $S$ 中，等等，这些不同的划分还是会将导致不同的训练集/测试集，相应的，模型评估的结果也还是会有差别。<span style="color:red;">是的。</span>

可见，单次使用留出法得到的估计结果往往还是不够稳定可靠的。

因此，我们在使用留出法时，**一般要采用若干次随机划分、然后重复进行实验评估后取平均值作为留出法的评估结果。**

例如我们可以进行 100 次随机划分，每次产生一个训练集/测试集用于实验评估，那么 100 次后就得到 100 个结果，而留出法返回的则是这 100 个结果的平均。（同时可得估计结果的标准差，<span style="color:red;">这个标准差有用吗？能衡量什么吗？</span>）

## Holdout 检验的缺点：

- 在验证集上计算出来的最后评估指标与原始分组有很大关系。<span style="color:red;">是的。好像现在平时用的就是这个。</span>
- 如果我令 $S$ 包含绝大多数样本，那么训练出的模型可以更接近用全部的 $D$ 训练出的模型，但是，这时候，由于 $T$ 比较小，因此评估结果可能不够稳定准确。
- 而如果我们令 $T$ 多包含一些样本，那么 $S$ 与 $D$ 的差别就更大了，也就是说，根据 $S$ 生成的模型与用 $D$ 训练出的模型是有可能有较大差别的，这就而降低了评估结果的保真性 (fidelity)。<span style="color:red;">是的，但是什么是保真性？</span>


为了消除随机性，研究者们引入了“交叉检验”的思想。


## 样本划分的比率


在划分上，可以分两种情况：

1、在样本量有限的情况下，有时候会把验证集和测试集合并。实际中，若划分为三类，那么训练集：验证集：测试集=6:2:2；若是两类，则训练集：验证集=7:3。这里需要主要在数据量不够多的情况，验证集和测试集需要占的数据比例比较多，以充分了解模型的泛化性。
2、在海量样本的情况下，这种情况在目前深度学习中会比较常见。此时由于数据量巨大，我们不需要将过多的数据用于验证和测试集。例如拥有 1 百万样本时，我们按训练集：验证集：测试集=98:1:1的比例划分，1%的验证和 1%的测试集都已经拥有了 1 万个样本。这已足够验证模型性能了。

此外，三个数据集的划分不是一次就可以的，若调试过程中发现，三者得到的性能评价差异很大时，可以重新划分以确定是数据集划分的问题导致还是由模型本身导致的。其次，若评价指标发生变化，而导致模型性能差异在三者上很大时，同样可重新划分确认排除数据问题，以方便进一步的优化。


# 相关

- 《机器学习》
- 《百面机器学习》
