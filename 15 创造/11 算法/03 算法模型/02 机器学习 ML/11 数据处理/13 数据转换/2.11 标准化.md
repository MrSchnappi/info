---
title: 2.11 标准化
toc: true
date: 2019-09-20
---
# 可以补充进来的

- 不对。


# 标准化



数据标准化，是为了：

- 消除指标之间的量纲影响，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。


数据标准化处理主要包括两个方面：

- 数据同趋化处理
- 无量纲化处理

介绍如下：

- 数据同趋化处理主要解决不同性质数据问题，对不同性质指标直接加总不能正确反映不同作用力的综合结果，须先考虑改变逆指标数据性质，使所有指标对测评方案的作用力同趋化，再加总才能得出正确结果。
- 数据无量纲化处理主要解决数据的可比性。经过上述标准化处理，原始数据均转换为无量纲化指标测评值，即各指标值都处于同一个数量级别上，可以进行综合测评分析。


目前数据标准化方法有多种，归结起来可以分为：

- 直线型方法(如极值法、标准差法)
- 折线型方法(如三折线法)
- 曲线型方法(如半正态性分布)。

不同的标准化方法，对系统的评价结果会产生不同的影响，然而不幸的是，在数据标准化方法的选择上，还没有通用的法则可以遵循。

其中最典型的就是数据的归一化处理，即将数据统一映射到 $[0,1]$ 区间上。







## Min-Max 标准化

Min-Max Normalization

离差标准化

Min-Max 标准化是对原始数据的线性变换，使结果值映射到 $[0, 1]$ 之间。转换函数如下：

$$
X^{*}=\frac{X-\min }{\max -\min }
$$

其中 max 为样本数据的最大值，min 为样本数据的最小值。

缺点：

- 这种方法有个缺陷就是当有新数据加入时，可能导致 max 和 min 的变化，需要重新定义。<span style="color:red;">要怎么解决新加入的数据问题？</span>

## Z-score 标准化方法

这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。经过处理的数据符合标准正态分布，即均值为 $0$，标准差为 $1$，转化函数为：


$$
X^{*}=\frac{X-\mu}{\sigma}
$$

注意，一般来说 z-score 不是归一化，而是标准化，归一化只是标准化的一种。

其中 $\mu$ 为所有样本数据的均值，$\sigma$ 为所有样本数据的标准差。


使用前提：

- 该种标准化方式要求原始数据的分布可以近似为高斯分布，否则效果会变得很糟糕。

适用场景：

- z-score 标准化方法适用于属性 A 的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。


```py
def z_score(x, axis):
​    x = np.array(x).astype(float)
​    xr = np.rollaxis(x, axis=axis)
​    xr -= np.mean(x, axis=axis)
​    xr /= np.std(x, axis=axis)
​    # print(x)
​    return x
```

## 使用场景

- 在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法。比如图像处理中，将 RGB 图像转换为灰度图像后将其值限定在[0 255]的范围。
- 在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用 PCA 技术进行降维的时候，第二种方法(Z-score standardization)表现更好。


原因是：

- 使用线性变换后，其协方差产生了倍数值的缩放，因此这种方式无法消除量纲对方差、协方差的影响，对 PCA 分析影响巨大；同时，由于量纲的存在，使用不同的量纲、距离的计算结果会不同。

- 而在 Z-score 标准化方法中，新的数据由于对方差进行了归一化，这时候每个维度的量纲其实已经等价了，每个维度都服从均值为 0、方差 1 的正态分布，在计算距离的时候，每个维度都是去量纲化的，避免了不同量纲的选取对距离计算产生的巨大影响。








## 1. scale 零均值单位方差


```
from sklearn import preprocessing
import numpy as np
#raw_data
X = np.array([[1., -1., 2.], [2., 0., 0.], [0., 1., -1.]])
X_scaled = preprocessing.scale(X)
#output
X_scaled = [[ 0.         -1.22474487  1.33630621]
             [ 1.22474487  0.         -0.26726124]
             [-1.22474487  1.22474487 -1.06904497]]
＃scaled之后的数据零均值，单位方差
X_scaled.mean(axis=0)  # column mean: array([ 0.,  0.,  0.])
X_scaled.std(axis=0)  #column standard deviation: array([ 1.,  1.,  1.])
```


　

## 2. StandardScaler 计算训练集的平均值和标准差，以便测试数据集使用相同的变换


```py
scaler = preprocessing.StandardScaler().fit(X)
#out:
 StandardScaler(copy=True, with_mean=True, with_std=True)
scaler.mean_
#out:
array([ 1.,  0. ,  0.33333333])
scaler.std_
#out:
 array([ 0.81649658,  0.81649658,  1.24721913])
#测试将该 scaler 用于输入数据，变换之后得到的结果同上
scaler.transform(X)
 #out:
array([[ 0., -1.22474487,  1.33630621],  [ 1.22474487, 0. , -0.26726124],  [-1.22474487,1.22474487, -1.06904497]])
scaler.transform([[-1., 1., 0.]])
#scale the new data, out:
array([[-2.44948974,  1.22474487, -0.26726124]])
```

注：

1. 若设置 with_mean=False 或者 with_std=False，则不做 centering 或者 scaling 处理。
2. scale和 StandardScaler 可以用于回归模型中的目标值处理。



# 相关

- [机器学习基础与实践（二）----数据转换](https://www.cnblogs.com/charlotte77/p/5622325.html)
- 《深度学习框架 Pytorch 快速开发与实战》
- [数据标准化/归一化 normalization](https://blog.csdn.net/pipisorry/article/details/52247379)
- 《百面机器学习》
