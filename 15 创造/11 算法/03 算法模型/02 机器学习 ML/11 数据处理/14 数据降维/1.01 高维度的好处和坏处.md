---
title: 1.01 高维度的好处和坏处
toc: true
date: 2019-09-03
---
# 可以补充进来的

- 感觉说的很多，但是没在点子上。

# 高维有什么好处

增加维度，可以使样本分布更加稀疏，可以更容易找到一个超平面将训练样本分开。

## 高维有什么坏处

### 没有足够的样本覆盖所有的特征空间，就会出现过拟合问题

尽管在高维特征空间时训练样本线性可分，但是映射到低维空间后，结果正好相反。事实上，**增加特征数量使得高维空间线性可分，相当于在低维空间内训练一个复杂的非线性分类器。不过，这个非线性分类器太过“聪明”，仅仅学到了一些特例。如果将其用来辨别那些未曾出现在训练样本中的测试样本时，通常结果不太理想，会造成过拟合问题**。

在维度很高的时候，没有足够的样本覆盖所有的特征空间，就会出现过拟合问题。

**换句话说，通过减少特征数量，可以避免出现过拟合问题，从而避免“维数灾难”。**


### 高维样本的稀疏性不是均匀分布的

维数灾难的另一个影响是训练样本的稀疏性并不是均匀分布的。处于中心位置的训练样本比四周的训练样本更加稀疏。<span style="color:red;">处于中心位置的训练样本比四周的训练样本更加稀疏是什么意思？</span>

<center>

![](http://images.iterate.site/blog/image/20190722/D7jPmasICH2A.png?imageslim){ width=35% }

</center>


假设有一个二维特征空间，如上图所示的矩形，在矩形内部有一个内切的圆形。由于越接近圆心的样本越稀疏，因此，相比于圆形内的样本，那些位于矩形四角的样本更加难以分类。当维数变大时，特征超空间的容量不变，但单位圆的容量会趋于 0，在高维空间中，大多数训练数据驻留在特征超空间的角落。散落在角落的数据要比处于中心的数据难于分类。




# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions) 原文
