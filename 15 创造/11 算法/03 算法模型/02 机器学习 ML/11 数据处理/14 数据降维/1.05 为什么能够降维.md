---
title: 1.05 为什么能够降维
toc: true
date: 2019-08-27
---
# 可以补充进来的

- MDS 不清楚。

# 为什么能进行降维


## 低维嵌入

为什么能进行降维？这是因为在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维 “嵌入” (embedding)。

下图给出了一个直观的例子。原始高维空间中的样本点，在这个低维嵌入子空间中更容易进行学习。<span style="color:red;">没看懂这个图，什么是低维嵌入？</span>


<center>

![](http://images.iterate.site/blog/image/180629/5L0l17ka9h.png?imageslim){ width=55% }


</center>

## 多维缩放 MDS

若要求原始空间中样本之间的距离在低维空间中得以保持，即得到“多维缩放”(Multiple Dimensional Scaling，简称 MDS) 这样一种经典的降维方法。

下面做一个简单的介绍。

假定 $m$ 个样本在原始空间的距离矩阵为 $\mathbf{D} \in \mathbb{R}^{m \times m}$ , 其第 $i$ 行 $j$ 列的元素 $dist_{ij}$ 为样本 $\boldsymbol{x}_{i}$ 到 $\boldsymbol{x}_{j}$ 的距离。我们的目标是获得样本在 $d'$ 维空间的表示 $\mathbf{Z} \in \mathbb{R}^{d^{\prime} \times m}$ ,$d'\leq d$ ，且任意两个样本在 $d'$ 维空间中的欧氏距离等于原始空间中的距离，即  $\left\|\boldsymbol{z}_{i}-\boldsymbol{z}_{j}\right\|=\operatorname{dist} _{i j}$ 。<span style="color:red;">嗯，这个倒是。</span>


令 $\mathbf{B}=\mathbf{Z}^{\mathrm{T}} \mathbf{Z} \in \mathbb{R}^{m \times m}$ ，其中 $\mathbf{B}$ 为降维后样本的内积矩阵，$b_{i j}=\boldsymbol{z}_{i}^{\mathrm{T}} \boldsymbol{z}_{j}$ ，有

$$
\begin{aligned} \operatorname{dist}_{i j}^{2} &=\left\|\boldsymbol{z}_{i}\right\|^{2}+\left\|\boldsymbol{z}_{j}\right\|^{2}-2 \boldsymbol{z}_{i}^{\mathrm{T}} \boldsymbol{z}_{j} \\ &=b_{i i}+b_{j j}-2 b_{i j} \end{aligned}
$$


为便于讨论，令降维后的样本 $\mathbf{Z}$ 被中心化，即 $\sum_{i=1}^{m} \boldsymbol{z}_{i}=\mathbf{0}$ 。显然，矩阵 $\mathbf{B}$ 的行与列之和均为零，即 $\sum_{i=1}^{m} b_{i j}=\sum_{j=1}^{m} b_{i j}=0$ 。易知

$$
\sum_{i=1}^{m} \operatorname{dist}_{i j}^{2}=\operatorname{tr}(\mathbf{B})+m b_{j j}\tag{10.4}
$$
$$
\sum_{j=1}^{m} \operatorname{dist}_{i j}^{2}=\operatorname{tr}(\mathbf{B})+m b_{i i}\tag{10.5}
$$
$$
\sum_{i=1}^{m} \sum_{j=1}^{m} d i s t_{i j}^{2}=2 m \operatorname{tr}(\mathbf{B})\tag{10.6}
$$


> $$\sum^m_{i=1}dist^2_{ij}=tr(\boldsymbol B)+mb_{jj}$$
> [推导]：
> $$\begin{aligned}
> \sum^m_{i=1}dist^2_{ij}&= \sum^m_{i=1}b_{ii}+\sum^m_{i=1}b_{jj}-2\sum^m_{i=1}b_{ij}\\
> &=tr(B)+mb_{jj}
> \end{aligned}​$$

其中 $tr(\cdot )$ 表示矩阵的迹(trace)，$\operatorname{tr}(\mathbf{B})=\sum_{i=1}^{m}\left\|\boldsymbol{z}_{i}\right\|^{2}$ 。令

$$
\operatorname{dist}_{i\cdot}^{2}=\frac{1}{m} \sum_{j=1}^{m} \operatorname{dist}_{i j}^{2}\tag{10.7}
$$
$$
\operatorname{dist}_{\cdot j}^{2}=\frac{1}{m} \sum_{i=1}^{m} d i s t_{i j}^{2}\tag{10.8}
$$
$$
\operatorname{dist}_{\cdot\cdot}^{2}=\frac{1}{m^{2}} \sum_{i=1}^{m} \sum_{j=1}^{m} d i s t_{i j}^{2}\tag{10.9}
$$


由式(10.3)和式(10.4)~(10.9)可得

$$
b_{i j}=-\frac{1}{2}\left(\operatorname{dist} _{i j}^{2}-\operatorname{dist}_{i\cdot}^{2}-\text {dist}_{\cdot j}^{2}+\text {dist}_{\cdot\cdot}^{2}\right)\tag{10.10}
$$


> $$b_{ij}=-\frac{1}{2}(dist^2_{ij}-dist^2_{i\cdot}-dist^2_{\cdot j}+dist^2_{\cdot\cdot})$$
> [推导]：由公式（10.3）可得，
> $$b_{ij}=-\frac{1}{2}(dist^2_{ij}-b_{ii}-b_{jj})$$
> 由公式（10.6）和（10.9）可得，
> $$\begin{aligned}
> tr(\boldsymbol B)&=\frac{1}{2m}\sum^m_{i=1}\sum^m_{j=1}dist^2_{ij}\\
> &=\frac{m}{2}dist^2_{\cdot\cdot}
> \end{aligned}$$
> 由公式（10.4）和（10.8）可得，
> $$\begin{aligned}
> b_{jj}&=\frac{1}{m}\sum^m_{i=1}dist^2_{ij}-\frac{1}{m}tr(\boldsymbol B)\\
> &=dist^2_{\cdot j}-\frac{1}{2}dist^2_{\cdot\cdot}
> \end{aligned}$$
> 由公式（10.5）和（10.7）可得，
> $$\begin{aligned}
> b_{ii}&=\frac{1}{m}\sum^m_{j=1}dist^2_{ij}-\frac{1}{m}tr(\boldsymbol B)\\
> &=dist^2_{i\cdot}-\frac{1}{2}dist^2_{\cdot\cdot}
> \end{aligned}$$
> 综合可得，
> $$\begin{aligned}
> b_{ij}&=-\frac{1}{2}(dist^2_{ij}-b_{ii}-b_{jj})\\
> &=-\frac{1}{2}(dist^2_{ij}-dist^2_{i\cdot}+\frac{1}{2}dist^2_{\cdot\cdot}-dist^2_{\cdot j}+\frac{1}{2}dist^2_{\cdot\cdot})\\
> &=-\frac{1}{2}(dist^2_{ij}-dist^2_{i\cdot}-dist^2_{\cdot j}+dist^2_{\cdot\cdot})
> \end{aligned}$$

由此即可通过降维前后保持不变的距离矩阵 $\mathbf{D}$ 求取内积矩阵 $\mathbf{B}$ 。

对矩阵 $\mathbf{B}$ 做特征值分解(eigenvalue decomposition)，$\mathbf{B}=\mathbf{V} \mathbf{\Lambda} \mathbf{V}^{\mathrm{T}}$ ，其中 $\mathbf{\Lambda}=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \dots, \lambda_{d}\right)$ 为特征值构成的对角矩阵，$\lambda_{1} \geqslant \lambda_{2} \geqslant \ldots \geqslant \lambda_{d}$ ，$\mathbf{V}$ 为特征向量矩阵。假定其中有 $d^{*}$ 个非零特征值，它们构成对角矩阵  $\mathbf{\Lambda}_{*}=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{d^{*}}\right)$，令 $\mathbf{V}_{*}$ 表示相应的特征向量矩阵，则 $\mathbf{Z}$ 可表达为

$$
\mathbf{Z}=\mathbf{\Lambda}_{*}^{1 / 2} \mathbf{V}_{*}^{\mathrm{T}} \in \mathbb{R}^{d^{*} \times m}\tag{10.11}
$$

在现实应用中为了有效降维，往往仅需降维后的距离与原始空间中的距离尽可能接近，而不必严格相等。此时可取 $d^{\prime} \ll d$ 个最大特征值构成对角矩阵 $\tilde{\mathbf{\Lambda}}=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \dots, \lambda_{d^{\prime}}\right)$，令 $\tilde{\mathbf{V}}$ 表示相应的特征向量矩阵，则 $\mathbf{Z}$ 可表达为

$$
\mathbf{Z}=\tilde{\mathbf{\Lambda}}^{1 / 2} \tilde{\mathbf{V}}^{\mathrm{T}} \in \mathbb{R}^{d^{\prime} \times m}
$$

图 10.3给出了 MDS算法的描述。

<center>

![](http://images.iterate.site/blog/image/180629/2LB3C5BjCj.png?imageslim){ width=55% }


</center>


一般来说，欲获得低维子空间，最简单的是对原始高维空间进行线性变换。 给定 $d$ 维空间中的样本 $\mathbf{X}=\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right) \in \mathbb{R}^{d \times m}$ ，变换之后得到 $d^{\prime} \leqslant d$ 维空间中的样本，

$$
\mathbf{Z}=\mathbf{W}^{\mathrm{T}} \mathbf{X}
$$

其中 $\mathbf{W} \in \mathbb{R}^{d \times d^{\prime}}$ 是变换矩阵， $\mathbf{Z} \in \mathbb{R}^{d^{\prime} \times m}$ 是样本在新空间中的表达。

变换矩阵 $\mathbf{W}$ 可视为 $d'$ 个 d 维基向量，$\boldsymbol{z}_{i}=\mathbf{W}^{\mathbf{T}} \boldsymbol{x}_{i}$ 是第 $i$ 个样本与 $d'$ 个基向量分别做内积而得到的 $d'$ 维属性向量。换言之，$\boldsymbol{z}_i$ 是原属性向量 $\boldsymbol{x}_{i}$ 在新 坐标系 $\left\{\boldsymbol{w}_{1}, \boldsymbol{w}_{2}, \cdots, \boldsymbol{w}_{d^{\prime}}\right\}$ 中的坐标向量。若 $\boldsymbol{w}_{i}$ 与 $\boldsymbol{w}_{j}(i \neq j)$ 正交，则新坐标系是一个正交坐标系，此时 $\mathbf{W}$ 为正交变换。显然，新空间中的属性是原空间中 属性的线性组合。

基于线性变换来进行降维的方法称为线性降维方法，它们都符合式(10.13)的基本形式，不同之处是对低维子空间的性质有不同的要求，相当于对 $\mathbf{W}$ 施加了不同的约束。在下一节我们将会看到，若要求低维子空间对样本具有最大可分性，则将得到一种极为常用的线性降维方法。

对降维效果的评估，通常是比较降维前后学习器的性能，若性能有所提高 则认为降维起到了作用。若将维数降至二维或三维，则可通过可视化技术来直观地判断降维效果。<span style="color:red;">嗯。</span>








# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
