---
title: 特征选择
toc: true
date: 2019-10-18
---
#### **Feature Selection**

总的来说，我们应该**生成尽量多的 Feature，相信 Model 能够挑出最有用的 Feature**。但有时先做一遍 Feature Selection 也能带来一些好处：

- Feature 越少，训练越快。
- 有些 Feature 之间可能存在线性关系，影响 Model 的性能。
- **通过挑选出最重要的 Feature，可以将它们之间进行各种运算和操作的结果作为新的 Feature，可能带来意外的提高。**

Feature Selection 最实用的方法也就是看 Random Forest 训练完以后得到的 **Feature Importance** 了。其他有一些更复杂的算法在理论上更加 Robust，但是缺乏实用高效的实现，比如这个。从原理上来讲，增加 Random Forest 中树的数量可以在一定程度上加强其对于 Noisy Data 的 Robustness。

看 Feature Importance 对于某些数据经过**脱敏**处理的比赛尤其重要。这可以免得你浪费大把时间在琢磨一个不重要的变量的意义上。
