---
title: 2.14 Log-Cosh损失
toc: true
date: 2019-09-03
---
# Log-Cosh损失


Log-cosh是另一种应用于回归问题中的，且比 L2 更平滑的的损失函数。它的计算方式是预测误差的双曲余弦的对数。

$$
L\left(y, y^{p}\right)=\sum_{i=1}^{n} \log \left(\cosh \left(y_{i}^{p}-y_{i}\right)\right)
$$


图示：


<center>

![mark](http://images.iterate.site/blog/image/20190902/unS5PiUEo4Km.png?imageslim)

</center>


## Log-cosh 的优点

优点：对于较小的 $x$，$log(cosh(x))$ 近似等于 $(x^2)/2$，对于较大的 $x$，近似等于 $abs(x)-log(2)$。这意味着‘logcosh’基本类似于均方误差，但不易受到异常点的影响。

它具有 Huber 损失所有的优点，但不同于 Huber 损失的是，Log-cosh二阶处处可微。

那么为什么需要二阶导数？

许多机器学习模型如 XGBoost，就是采用牛顿法来寻找最优点。而牛顿法就需要求解二阶导数（Hessian）。因此对于诸如 XGBoost 这类机器学习框架，损失函数的二阶可微是很有必要的。<span style="color:red;">是的。</span>


XGBoost 里面的目标函数：

<center>

$$
\operatorname{obj}^{(t)} = \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)+\text { constant }
$$

</center>

式子中的 $g_{i}$ 和 $h_{i}$ 为：

$$
g_{i}=\partial_{\hat{y}_{i}^{(j-1)}} l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)
$$


$$
h_{i}=\partial_{\hat{y}_{i}^{(j-1)}}^{2} l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)
$$

## Log-cosh 的缺点

但 Log-cosh损失也并非完美，其仍存在某些问题。

比如误差很大的话，一阶梯度和 Hessian 会变成定值，这就导致 XGBoost 出现缺少分裂点的情况。

# 相关

- [分位数回归的代码](https://github.com/groverpr/Machine-Learning/blob/master/notebooks/09_Quantile_Regression.ipynb)
