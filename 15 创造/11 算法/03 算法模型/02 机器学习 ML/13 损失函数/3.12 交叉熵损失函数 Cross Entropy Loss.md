---
title: 3.12 交叉熵损失函数 Cross Entropy Loss
toc: true
date: 2019-09-03
---
# 可以补充进来的

- 来由没写。与平方误差损失函数的对比没写。


# 交叉熵代价函数 cross-entropy

应用场景：

- 二分类。


Cross Entropy Loss 是非常重要的损失函数，也是应用最多的损失函数之一。

二分类问题的交叉熵 Loss 主要有两种形式，下面分别详细介绍：


## 输出标签 label 的表示方式为 $\{0,1\}$

第一种形式是基于输出标签 label 的表示方式为 $\{0,1\}$，也最为常见。

它的 Loss 表达式为：

$$
L=-[y \log \hat{y}+(1-y) \log (1-\hat{y})]
$$

这个公式是如何推导的呢？很简单，从极大似然性的角度出发，预测类别的概率可以写成：

$$
P(y | x)=\hat{y}^{y} \cdot(1-\hat{y})^{(1-y)}
$$

我们可以这么来看，当真实样本标签 $y = 1$ 时，上面式子第二项就为 $1$，概率等式转化为：

$$
P(y=1 | x)=\hat{y}
$$

当真实样本标签 $y = 0$ 时，上面式子第一项就为 $1$，概率等式转化为：

$$
P(y=0 | x)=1-\hat{y}
$$

我们希望的是概率 $P(y|x)$ 越大越好。首先，我们对 $P(y|x)$ 引入 $log$ 函数，因为 $log$ 运算并不会影响函数本身的单调性。则有：

$$
\log P(y | x)=\log \left(\hat{y}^{y} \cdot(1-\hat{y})^{(1-y)}\right)=y \log \hat{y}+(1-y) \log (1-\hat{y})
$$

我们希望 $log P(y|x)$ 越大越好，反过来，只要 $log P(y|x)$ 的负值 $-log P(y|x)$ 越小就行了。那我们就可以引入损失函数，且令 $Loss = -log P(y|x)$ 即可。则得到损失函数为：


$$
L=-[y \log \hat{y}+(1-y) \log (1-\hat{y})]
$$

我们来看，当 $y = 1$ 时：

$$
L=-\log \hat{y}
$$

因为，

$$
\hat{y}=\frac{1}{1+e^{-s}}
$$

所以，代入到 $L$ 中，得：

$$
L=\log \left(1+e^{-s}\right)
$$

这时候，Loss 的曲线如下图所示：


![mark](http://images.iterate.site/blog/image/20190902/o0cFuzOCt6sX.png?imageslim)

从图中明显能够看出，$s$ 越大于零，$L$ 越小，函数的变化趋势也完全符合实际需要的情况。

当 $y = 0$ 时：

$$
L=-\log (1-\hat{y})
$$

对上式进行整理，同样能得到：

$$
L=\log \left(1+e^{s}\right)
$$

这时候，Loss 的曲线如下图所示：

![mark](http://images.iterate.site/blog/image/20190902/1vT7sXQ8LjzA.png?imageslim)

从图中明显能够看出，$s$ 越小于零，$L$ 越小，函数的变化趋势也完全符合实际需要的情况。

## 输出标签 label 的表示方式为 $\{-1,+1\}$

第二种形式是基于输出标签 label 的表示方式为 $\{-1,+1\}$，也比较常见。它的 Loss 表达式为：

$$
L=\log \left(1+e^{-y s}\right)
$$

下面对上式做个简单的推导，我们在 $0$ 小节说过，$ys$ 的符号反映了预测的准确性。除此之外，$ys$ 的数值大小也反映了预测的置信程度。所以，从概率角度来看，预测类别的概率可以写成：

$$
P(y | x)=g(y s)
$$

分别令 $y = +1$ 和 $y = -1$ 就能很容易理解上面的式子。

接下来，同样引入 $log$ 函数，要让概率最大，反过来，只要其负数最小即可。那么就可以定义相应的损失函数为：

$$
L=-\log g(y s)=-\log \frac{1}{1+e^{-y s}}=\log \left(1+e^{-y s}\right)
$$

这时候，我们以 $ys$ 为横坐标，可以绘制 Loss 的曲线如下图所示：

<center>

![mark](http://images.iterate.site/blog/image/20190902/KN5eco4EL9Yk.png?imageslim)

</center>

其实上面介绍的两种形式的交叉熵 Loss 是一样的，只不过由于标签 label 的表示方式不同，公式稍有变化。标签用 $\{-1,+1\}$ 表示的好处是可以把 $ys$ 整合在一起，作为横坐标，容易作图且具有实际的物理意义。


## 交叉熵 Loss 的优点

总结一下，交叉熵 Loss 的优点是在整个实数域内，Loss 近似线性变化。尤其是当 $ys << 0$ 的时候，Loss 更近似线性。这样，模型受异常点的干扰就较小。 而且，交叉熵 Loss 连续可导，便于求导计算，是使用最广泛的损失函数之一。


交叉熵损失会重重惩罚那些置信度高但是错误的预测值。

**二次代价函数适合输出神经元是线性的情况，交叉熵代价函数适合输出神经元是 S 型函数的情况。**


## 梯度推导

权值 $w$ 和偏置 $b​$ 的梯度推导如下：

$$
\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)\;，
\frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)
$$

当误差越大时，梯度就越大，权值 $w$ 和偏置 $b$ 调整就越快，训练的速度也就越快。



# 相关

- [确定不收藏？机器学习必备的分类损失函数速查手册](https://redstonewill.com/1584/)
