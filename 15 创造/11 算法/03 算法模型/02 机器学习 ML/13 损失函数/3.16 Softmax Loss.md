---
title: 3.16 Softmax Loss
toc: true
date: 2019-09-03
---
# Softmax Loss

对于多分类问题，也可以使用 Softmax Loss。

机器学习模型的 Softmax 层，正确类别对于的输出是：

$$
S=\frac{e^{s}}{\sum_{j=1}^{C} e^{s_{j}}}
$$

其中，$C$ 为类别个数，小写字母 $s$ 是正确类别对应的 Softmax 输入，大写字母 $S$ 是正确类别对应的 Softmax 输出。

由于 $log$ 运算符不会影响函数的单调性，我们对 $S$ 进行 $log$ 操作。另外，我们希望 $log(S)$ 越大越好，即正确类别对应的相对概率越大越好，那么就可以对 $log(S)$ 前面加个负号，来表示损失函数：

$$
L=-\log \frac{e^{s}}{\sum_{j=1}^{C} e^{s_{j}}}=-s+\log \sum_{j=1}^{C} e^{s_{j}}
$$

Softmax Loss 的曲线如下图所示：

<center>

![mark](http://images.iterate.site/blog/image/20190902/dLOBpxhxzHmE.png?imageslim)

</center>

上图中，当 $s<<0$ 时，Softmax 近似线性；当 $s>>0$ 时，Softmax 趋向于零。Softmax 同样受异常点的干扰较小，多用于神经网络多分类问题中。



# 相关

- [确定不收藏？机器学习必备的分类损失函数速查手册](https://redstonewill.com/1584/)
