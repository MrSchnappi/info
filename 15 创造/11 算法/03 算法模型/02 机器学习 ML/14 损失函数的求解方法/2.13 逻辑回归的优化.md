---
title: 2.13 逻辑回归的优化
toc: true
date: 2019-08-27
---

# 一个常用的机器学习模型，逻辑回归

一个常用的机器学习模型，逻辑回归，对应的优化问题就是凸优化问题。具体来说，对于二分类问题，$Y=\{1,-1\}$，假设模型参数为 $\theta$ ，则逻辑回归的优化问题为：

$$
\min _{\theta} L(\theta)=\sum_{i=1}^{n} \log \left(1+\exp \left(-y_{i} \theta^{\mathrm{T}} x_{i}\right)\right)\tag{7.9}
$$

<span style="color:red;">？上面这个是逻辑回归的损失函数吗？</span>

可以通过计算目标函数的二阶 Hessian 矩阵来验证凸性。令：<span style="color:red;">二阶 Hessian 矩阵怎么算的来着？</span>

$$
L_{i}(\theta)=\log \left(1+\exp \left(-y_{i} \theta^{\mathrm{T}} x_{i}\right)\right)\tag{7.10}
$$

对该函数求一阶导，得到：

$$
\begin{aligned} \nabla L_{i}(\theta) &=\frac{1}{1+\exp \left(-y_{i} \theta^{\mathrm{T}} x_{i}\right)} \exp \left(-y_{i} \theta^{\mathrm{T}} x_{i}\right) \cdot\left(-y_{i} x_{i}\right) \\ &=\frac{-y_{i} x_{i}}{1+\exp \left(y_{i} \theta^{\mathrm{T}} x_{i}\right)} \end{aligned}\tag{7.11}
$$

继续求导，得到函数的 Hessian 矩阵：

$$
\begin{aligned}
\nabla^{2} L_{i}(\theta)&=\frac{y_{i} x_{i} \cdot \exp \left(y_{i} \theta^{\mathrm{T}} x_{i}\right) \cdot y_{i} x_{i}^{T}}{\left(1+\exp \left(y_{i} \theta^{\mathrm{T}} x_{i}\right)\right)^{2}}\\
&=\frac{\exp \left(y_{i} \theta^{\mathrm{T}} x_{i}\right)}{\left(1+\exp \left(y_{i} \theta^{\mathrm{T}} x_{i}\right)\right)^{2}} x_{i} x_{i}^{\mathrm{T}}\end{aligned}\tag{7.12}
$$

该矩阵满足半正定的性质 $\nabla^{2} L_{i}(\theta) \succeq 0$，因此 $\nabla^{2} L(\theta)=\sum_{i=1}^{n} \nabla^{2} L_{i}(\theta) \succeq 0$，函数 $L(\cdot)$ 为凸函数。

对于凸优化问题，所有的局部极小值都是全局极小值，因此这类问题一般认为是比较容易求解的问题。







# 相关

- 《百面机器学习》
