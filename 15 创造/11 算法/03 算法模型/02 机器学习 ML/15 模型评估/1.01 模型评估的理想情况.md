---
title: 1.01 模型评估的理想情况
toc: true
date: 2018-07-30 12:10:59
---

# 模型的评估与选择

# 模型选择的问题

在现实任务中，我们往往有多种学习算法可供选择，甚至对同一个学习算法，当使用不同的参数配置时，也会产生不同的模型。

那么，我们该选用哪一个学习算法、使用哪一种参数配置呢？<span style="color:red;">是呀</span>

这就是机器学习中的 “模型选择” (model selection) 问题。

当然，理想的解决方案肯定是直接对候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型。然而，通过之前的学习我们知道，泛化误差是无法直接获得的，而如果把训练误差作为标准，训练误差又有过拟合现象，不适合作为标准。<span style="color:red;">是的。</span>

既然泛化误差无法获得，训练误差又不能作为参考依据，那么，在现实中，我们如何进行模型评估与选择呢？


注意：在现实任务中往往还会考虑时间开销、存储开销、可解释性等方面的因素，这里暂且只考虑泛化误差。<span style="color:red;">时间开销、存储开销、可解释性这些也要总结一下，因为实际的问题中，这些也都是必须要考虑的。</span>






## 对模型进行评价实际上取决于任务需求

不同的任务需求对应的评判指标也不同。而，对于不同的模型来说，使用不同的评判指标往往会得到不同的评判结果。这就意味着，模型的好坏是相对的，什么样的模型是好的，不仅取决于我们的算法和数据，还取决于任务需求。<span style="color:red;">是的，比如有的模型 AUC 会好一些，F1就差一些，因此到底要按照神马来比较是很重要的。</span>

## 不管需求是什么，肯定要涉及到 $f(x)$ 与 $y_i$ 的比较

在预测任务中，给定样本集 $D=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_m,y_m)\}$ ，其中 $y_i$ 是示例 $x_i$ 的真实标记。要评估学习器的性能，我们就肯定要把学习器预测结果 $f(x)$ 与真实标记 $y_i$ 进行比较。<span style="color:red;">那么非预测任务中呢？有没有非预测任务？</span>



# 相关

- 《机器学习》 西瓜书
