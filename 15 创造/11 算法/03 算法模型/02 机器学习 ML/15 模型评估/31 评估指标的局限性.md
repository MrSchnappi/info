---
title: 31 评估指标的局限性
toc: true
date: 2019-03-23
---
# 评估指标的局限性

在模型评估过程中，分类问题、排序问题、回归问题往往需要使用不同的指标进行评估。

在诸多的评估指标中，大部分指标只能片面地反映模型的一部分性能。

如果不能合理地运用评估指标，不仅不能发现模型本身的问题，而且会得出错误的结论。

下面假想几个模型评估场景，看看大家能否触类旁通，发现模型评估指标的局限性。

准确率（Accuracy），精确率（Precision），召回率（Recall），均方根误差（Root Mean Square Error，RMSE）

## 准确率的局限性。

Hulu 的奢侈品广告主们希望把广告定向投放给奢侈品用户。Hulu通过第三方的数据管理平台（Data Management Platform，DMP）拿到了一部分奢侈品用户的数据，并以此为训练集和测试集，训练和测试奢侈品用户的分类模型。该模型的分类准确率超过了 95％，但在实际广告投放过程中，该模型还是把大部分广告投给了非奢侈品用户，这可能是什么原因造成的？

在解答该问题之前，我们先回顾一下分类准确率的定义。准确率是指分类正确的样本占总样本个数的比例，即，

$$Accuracy=\frac{n_{correct}}{n_{total}}$$

其中 $n_{correct}$ 为被正确分类的样本个数，$n_{total}$ 为总样本的个数。


准确率是分类问题中最简单也是最直观的评价指标，但存在明显的缺陷。

比如，当负样本占 99％时，分类器把所有样本都预测为负样本也可以获得 99％的准确率。

所以，当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。<span style="color:red;">是的，这个的确是这样。嗯，看来每次当某个指标很高的时候，都要考虑下是不是真的就是 OK 的。</span>

明确了这一点，这个问题也就迎刃而解了。显然，奢侈品用户只占 Hulu 全体用户的一小部分，虽然模型的整体分类准确率高，但是不代表对奢侈品用户的分类准确率也很高。在线上投放过程中，我们只会对模型判定的“奢侈品用户”进行投放，因此，对“奢侈品用户”判定的准确率不够高的问题就被放大了。

为了解决这个问题，可以使用更为有效的平均准确率（每个类别下的样本准确率的算术平均）作为模型评估的指标。<span style="color:red;">是的，这个平均准确率怎么从 keras 或者 tensorflow 中拿到？确认下，或者是要手动写代码实现的？</span>

事实上，这是一道比较开放的问题，面试者可以根据遇到的问题一步步地排查原因。标准答案其实也不限于指标的选择，即使评估指标选择对了，仍会存在模型过拟合或欠拟合、测试集和训练集划分不合理、线下评估与线上测试的样本分布存在差异等一系列问题，但评估指标的选择是最容易被发现，也是最可能影响评估结果的因素。<span style="color:red;">嗯，这段提到的几个问题方向都是在排查的时候要用到的。</span>

## 精确率与召回率的权衡

Hulu提供视频的模糊搜索功能，搜索排序模型返回的 Top 5的精确率非常高，但在实际使用过程中，用户还是经常找不到想要的视频，特别是一些比较冷门的剧集，这可能是哪个环节出了问题呢？

要回答这个问题，首先要明确两个概念，精确率和召回率。精确率是指分类正确的正样本个数占分类器判定为正样本的样本个数的比例。召回率是指分类正确的正样本个数占真正的正样本个数的比例。

在排序问题中，通常没有一个确定的阈值把得到的结果直接判定为正样本或负样本，而是采用 Top N返回结果的 Precision 值和 Recall 值来衡量排序模型的性能，即认为模型返回的 Top N的结果就是模型判定的正样本，然后计算前 N 个位置上的准确率 Precision@N和前 N 个位置上的召回率 Recall@N。

Precision 值和 Recall 值是既矛盾又统一的两个指标，为了提高 Precision 值，分类器需要尽量在“更有把握”时才把样本预测为正样本，但此时往往会因为过于保守而漏掉很多“没有把握”的正样本，导致 Recall 值降低。<span style="color:red;">是的，这个的确会导致冷门的资源搜索不出来。</span>

回到问题中来，模型返回的 Precision@5的结果非常好，也就是说排序模型 Top 5的返回值的质量是很高的。但在实际应用过程中，用户为了找一些冷门的视频，往往会寻找排在较靠后位置的结果，甚至翻页去查找目标视频。但根据题目描述，用户经常找不到想要的视频，这说明模型没有把相关的视频都找出来呈现给用户。<span style="color:red;">是的。</span>

显然，问题出在召回率上。如果相关结果有 100 个，即使 Precision@5达到了 100％，Recall@5也仅仅是 5％。在模型评估时，我们是否应该同时关注 Precision 值和 Recall 值？进一步而言，是否应该选取不同的 Top N的结果进行观察呢？是否应该选取更高阶的评估指标来更全面地反映模型在 Precision 值和 Recall 值两方面的表现？<span style="color:red;">嗯。</span>

答案都是肯定的，为了综合评估一个排序模型的好坏，不仅要看模型在不同 Top N下的 Precision@N 和 Recall@N，而且最好绘制出模型的 P-R（Precision-Recall）曲线。这里简单介绍一下 P-R曲线的绘制方法。<span style="color:red;">嗯，现在才知道为什么要画这个 PR 曲线。</span>


P-R曲线的横轴是召回率，纵轴是精确率。对于一个排序模型来说，其 P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。整条 P-R曲线是通过将阈值从高到低移动而生成的。图 2.1是 P-R曲线样例图，其中实线代表模型 A 的 P-R曲线，虚线代表模型 B 的 P-R曲线。原点附近代表当阈值最大时模型的精确率和召回率。

![](http://images.iterate.site/blog/image/20190323/NYBGnqfuUu98.png?imageslim){ width=55% }

由图可见，当召回率接近于 0 时，模型 A 的精确率为 0.9，模型 B 的精确率是 1，这说明模型 B 得分前几位的样本全部是真正的正样本，而模型 A 即使得分最高的几个样本也存在预测错误的情况。并且，随着召回率的增加，精确率整体呈下降趋势。但是，当召回率为 1 时，模型 A 的精确率反而超过了模型 B。这充分说明，只用某个点对应的精确率和召回率是不能全面地衡量模型的性能，只有通过 P-R曲线的整体表现，才能够对模型进行更为全面的评估。<span style="color:red;">是的。但是上面这两个模型我们要使用哪个呢？还是说对于不同的场景有不同的选择？要具体明确下的。</span>

除此之外，F1 score和 ROC 曲线也能综合地反映一个排序模型的性能。F1 score是精准率和召回率的调和平均值，它定义为

$$F1=\frac{2\times precision \times recall}{precision+recall}$$

ROC曲线会在后面的小节中单独讨论，这里不再赘述。

<span style="color:red;">那么这个 P-R 曲线 和 F1 score 和 ROC 曲线我们用的时候看重哪个呢？</span>


## 平方根误差的“意外”

Hulu作为一家流媒体公司，拥有众多的美剧资源，预测每部美剧的流量趋势对于广告投放、用户增长都非常重要。<span style="color:red;">是呀。</span>

我们希望构建一个回归模型来预测某部美剧的流量趋势，但无论采用哪种回归模型，得到的 RMSE 指标都非常高。然而事实是，模型在 95％的时间区间内的预测误差都小于 1％，取得了相当不错的预测结果。那么，造成 RMSE 指标居高不下的最可能的原因是什么？<span style="color:red;">嗯？这个是为什么？</span>
分析与解答


RMSE经常被用来衡量回归模型的好坏，但按照题目的叙述，RMSE这个指标却失效了。先看一下 RMSE 的计算公式为，

$$RMSE=\sqrt{\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{n}}$$

其中，$y_i$ 是第 $i$ 个样本点的真实值，是第 $i$ 个样本点的预测值，$n$ 是样本点的个数。

一般情况下，RMSE 能够很好地反映回归模型预测值与真实值的偏离程度。<span style="color:red;">是呀。</span>

但在实际问题中，如果存在个别偏离程度非常大的离群点（Outlier）时，即使离群点数量非常少，也会让 RMSE 指标变得很差。<span style="color:red;">好吧。</span>


回到问题中来，模型在 95％的时间区间内的预测误差都小于 1％，这说明，在大部分时间区间内，模型的预测效果都是非常优秀的。然而，RMSE却一直很差，这很可能是由于在其他的 5％时间区间内存在非常严重的离群点。事实上，在流量预估这个问题中，噪声点确实是很容易产生的，比如流量特别小的美剧、刚上映的美剧或者刚获奖的美剧，甚至一些相关社交媒体突发事件带来的流量，都可能会造成离群点。<span style="color:red;">嗯，是的，好。</span>


针对这个问题，有什么解决方案呢？可以从三个角度来思考。

- 第一，如果我们认定这些离群点是“噪声点”的话，就需要在数据预处理的阶段把这些噪声点过滤掉。
- 第二，如果不认为这些离群点是“噪声点”的话，就需要进一步提高模型的预测能力，将离群点产生的机制建模进去（这是一个宏大的话题，这里就不展开讨论了）。<span style="color:red;">嗯？要怎么将离群点产生的机制建模进去？</span>
- 第三，可以找一个更合适的指标来评估该模型。关于评估指标，其实是存在比 RMSE 的鲁棒性更好的指标，比如平均绝对百分比误差（Mean Absolute Percent Error，MAPE），它定义为：$MAPE=\sum_{i=1}^n\left |\frac{y_i-\hat{y_i}}{y_i}\right |\times\frac{100}{n}$ 。相比 RMSE，MAPE相当于把每个点的误差进行了归一化，降低了个别离群点带来的绝对误差的影响。<span style="color:red;">有些厉害！</span>



本小节基于三个假想的 Hulu 应用场景和对应的问题，说明了选择合适的评估指标的重要性。每个评估指标都有其价值，但如果只从单一的评估指标出发去评估模型，往往会得出片面甚至错误的结论；只有通过一组互补的指标去评估模型，才能更好地发现并解决模型存在的问题，从而更好地解决实际业务场景中遇到的问题。<span style="color:red;">是的，厉害呀。</span>









# 相关

- 《百面机器学习》
