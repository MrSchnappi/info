---
title: 2.03 只有一个特征的时候
toc: true
date: 2019-08-27
---

## 我们看看只有一个特征的时候

### 只有一个特征的时候

当只有一个特征的时候，即 $w$ 只是一个数值，那么此时的线性回归试图学得 $f(x_i)=wx_i+b$ 使得 $f(x_i)\simeq y_i$。

OK，现在我们想求得这个 $w$ 和 $b$，使得 $f(x_i)\simeq y_i$。

### 现在需要对 $f(x)$ 与 $y_i$ 之间的近似进行衡量

但是呢，在这之前，我们必须首先知道，到底怎么去衡量 $f(x_i)$ 与 $y_i$ 的近似情况。不然的话，我们也不知道我们得到的 $w$ 和 $b$ 是不是足够好，是不是有更好的使 $f(x_i)$ 更近似于 $y_i$。<span style="color:red;">是呀</span>

也就是说：我们要知道对 $f(x_i)$ 与 $y_i$ 的近似性的度量方法

怎么衡量呢？

其实我们可以把 $f(x_i)$ 与 $y_i$ 的近似情况看成是 $f(x_i)$ 与 $y_i$ 的距离，而越近似，就说明距离越小。

那么这二者之间的距离怎么求呢？OK，这里我们想到了欧式距离 (Euclidean distance)。是可以的。<span style="color:red;">差点忘了，这个地方怎么就自然的想到了欧式距离呢？这么多种距离的计算方式，别的都不考虑了吗？</span>

写成如下：

$$
\begin{aligned}\left(w^{*}, b^{*}\right) &=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(f\left(x_{i}\right)-y_{i}\right)^{2} \\ &=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2} \end{aligned}\tag{3.4}
$$

<span style="color:red;">上面这个式子左边为什么是 $(w^*,b^*)$ ？</span>

实际呢，这个欧氏距离其实就是对应的均方误差，而这其实是回归任务中最常用的性能度量。<span style="color:red;">是的，我看到这个式子的时候就想到了均方误差</span>

### 现在我们来求一套 $w$ 和 $b$ 使得这个距离最小

实际上，基于均方误差最小化来进行模型求解的方法被称为 "最小二乘法"(least squre method)。<span style="color:red;">是这样吗？为什么叫做最小二乘法？

</span>而求解 $w$ 和 $b$ 使 $E_{(w, b)}=\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}$ 最小化的过程，称为线性回归模型的最小二乘 "参数估计" (prameter estimation)。<span style="color:red;">对于最小二乘还是要仔细总结下的。</span>

OK，要求最小化，我们首先想到求导数：

将 $E_{(w,b)}$ 分别对 $w$ 和 $b$ 求导，得到：

$$
\frac{\partial E_{(w, b)}}{\partial w}=2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)\tag{3.5}
$$
$$
\frac{\partial E_{(w, b)}}{\partial b}=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)\tag{3.6}
$$

令两个导数为 0 得到：<span style="color:red;">这个式子自己要推一遍的</span>

$$
w=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\overline{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}\tag{3.7}
$$
$$
b=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\tag{3.8}
$$


> $$ w=\cfrac{\sum_{i=1}^{m}y_i(x_i-\bar{x})}{\sum_{i=1}^{m}x_i^2-\cfrac{1}{m}(\sum_{i=1}^{m}x_i)^2} $$
>
> [推导]：令式（3.5）等于 0：
> $$ 0 = w\sum_{i=1}^{m}x_i^2-\sum_{i=1}^{m}(y_i-b)x_i $$
> $$ w\sum_{i=1}^{m}x_i^2 = \sum_{i=1}^{m}y_ix_i-\sum_{i=1}^{m}bx_i $$
> 由于令式（3.6）等于 0 可得 $b=\cfrac{1}{m}\sum_{i=1}^{m}(y_i-wx_i)$，又 $\cfrac{1}{m}\sum_{i=1}^{m}y_i=\bar{y}$，$\cfrac{1}{m}\sum_{i=1}^{m}x_i=\bar{x}$，则 $b=\bar{y}-w\bar{x}$，代入上式可得：
> $$
> \begin{aligned}
>     w\sum_{i=1}^{m}x_i^2 & = \sum_{i=1}^{m}y_ix_i-\sum_{i=1}^{m}(\bar{y}-w\bar{x})x_i \\
>     w\sum_{i=1}^{m}x_i^2 & = \sum_{i=1}^{m}y_ix_i-\bar{y}\sum_{i=1}^{m}x_i+w\bar{x}\sum_{i=1}^{m}x_i \\
>     w(\sum_{i=1}^{m}x_i^2-\bar{x}\sum_{i=1}^{m}x_i) & = \sum_{i=1}^{m}y_ix_i-\bar{y}\sum_{i=1}^{m}x_i \\
>     w & = \cfrac{\sum_{i=1}^{m}y_ix_i-\bar{y}\sum_{i=1}^{m}x_i}{\sum_{i=1}^{m}x_i^2-\bar{x}\sum_{i=1}^{m}x_i}
> \end{aligned}
> $$
> 又 $\bar{y}\sum_{i=1}^{m}x_i=\cfrac{1}{m}\sum_{i=1}^{m}y_i\sum_{i=1}^{m}x_i=\bar{x}\sum_{i=1}^{m}y_i$，$\bar{x}\sum_{i=1}^{m}x_i=\cfrac{1}{m}\sum_{i=1}^{m}x_i\sum_{i=1}^{m}x_i=\cfrac{1}{m}(\sum_{i=1}^{m}x_i)^2$，代入上式即可得式（3.7）：
> $$w=\cfrac{\sum_{i=1}^{m}y_i(x_i-\bar{x})}{\sum_{i=1}^{m}x_i^2-\cfrac{1}{m}(\sum_{i=1}^{m}x_i)^2} $$
>
> 【注】：式（3.7）还可以进一步化简为能用向量表达的形式，将 $\cfrac{1}{m}(\sum_{i=1}^{m}x_i)^2=\bar{x}\sum_{i=1}^{m}x_i$ 代入分母可得：
> $$
> \begin{aligned}
>      w & = \cfrac{\sum_{i=1}^{m}y_i(x_i-\bar{x})}{\sum_{i=1}^{m}x_i^2-\bar{x}\sum_{i=1}^{m}x_i} \\
>      & = \cfrac{\sum_{i=1}^{m}(y_ix_i-y_i\bar{x})}{\sum_{i=1}^{m}(x_i^2-x_i\bar{x})}
> \end{aligned}
> $$
> 又因为 $\bar{y}\sum_{i=1}^{m}x_i=\bar{x}\sum_{i=1}^{m}y_i=\sum_{i=1}^{m}\bar{y}x_i=\sum_{i=1}^{m}\bar{x}y_i=m\bar{x}\bar{y}=\sum_{i=1}^{m}\bar{x}\bar{y}$，$\sum_{i=1}^{m}x_i\bar{x}=\bar{x}\sum_{i=1}^{m}x_i=\bar{x}\cdot m \cdot\frac{1}{m}\cdot\sum_{i=1}^{m}x_i=m\bar{x}^2=\sum_{i=1}^{m}\bar{x}^2$，则上式可化为：
> $$
> \begin{aligned}
>     w & = \cfrac{\sum_{i=1}^{m}(y_ix_i-y_i\bar{x}-x_i\bar{y}+\bar{x}\bar{y})}{\sum_{i=1}^{m}(x_i^2-x_i\bar{x}-x_i\bar{x}+\bar{x}^2)} \\
>     & = \cfrac{\sum_{i=1}^{m}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{m}(x_i-\bar{x})^2}
> \end{aligned}
> $$
> 若令 $\boldsymbol{x}=(x_1,x_2,...,x_m)^T$，$\boldsymbol{x}_{d}=(x_1-\bar{x},x_2-\bar{x},...,x_m-\bar{x})^T$ 为去均值后的 $\boldsymbol{x}$，$\boldsymbol{y}=(y_1,y_2,...,y_m)^T$，$\boldsymbol{y}_{d}=(y_1-\bar{y},y_2-\bar{y},...,y_m-\bar{y})^T$ 为去均值后的 $\boldsymbol{y}$，其中 $\boldsymbol{x}$、$\boldsymbol{x}_{d}$、$\boldsymbol{y}$、$\boldsymbol{y}_{d}$ 均为 m 行 1 列的列向量，代入上式可得：
> $$w=\cfrac{\boldsymbol{x}_{d}^T\boldsymbol{y}_{d}}{\boldsymbol{x}_d^T\boldsymbol{x}_{d}}$$


而这两个是 $w$ 和 $b$ 的最优解的闭式 (closed-form) 解 。<span style="color:red;">什么是闭式解？</span>

其中 $\overline{x}=\frac{1}{m} \sum_{i=1}^{m} x_{i}$ 为 $x$ 的均值。

OK，实际上上面，我们已经求出了，只有一个特征的时候，为了使 $f(x_i)$ 最近似于 $y_i$ ，此时的 $w$ 和 $b$ 的值。

下面，我们把它扩展到 $d$ 个属性的时候：



# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
