---
title: 3.04 逻辑回归求解
toc: true
date: 2019-08-27
---
# 可以补充进来的


- 为什么 logistic 回归用交叉熵而不用欧氏距离损失，是因为前者是凸函数，后者不是

# 逻辑回归求解



## 求解 $w$ 和 $b$

<span style="color:red;">这次看比上次又清楚了一些。</span>

### 怎么确定式子中的 $w$ 和 $b$ 呢？

$$y=\frac{1}{1+e^{-(w^Tx+b)} }$$

下面我们来看看如何确定式子中的 $w$ 和 $b$ 的。 若将上式中的 $y$ 视为类后验概率估计 $p(y=1|x)$ ，则 $ln\frac{y}{1-y}=w^Tx+b$ 可重写为： <span style="color:red;">为什么可以将 $y$ 视为类后验概率估计 $p(y=1|x)$ ，没明白？</span>

$$
\ln \frac{p(y=1 | \boldsymbol{x})}{p(y=0 | \boldsymbol{x})}=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\tag{3.22}
$$


显然有：<span style="color:red;">是的，把 $p(y=1|x)$ 代入 $ln\frac{y}{1-y}=w^Tx+b$ 中的 $y$ 就可以算出。</span>

$$
p(y=1 | \boldsymbol{x})=\frac{e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}\tag{3.23}
$$
$$
p(y=0 | \boldsymbol{x})=\frac{1}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}\tag{3.24}
$$

<span style="color:blue;">这个式子写出来之后后续有用到吗？有用到，代入对数似然中，把概率用 $\beta$ 的表达式替换了，这样式子中就只有 $\beta$ 这个参数了。</span>

### 使用极大似然法来估计 $w$ 和 $b$

于是，我们可通过"极大似然法" (maximum likelihood method) 来估计 $w$ 和 $b$ 。给定数据集 $\left\{\left(\boldsymbol{x}_{i}, y_{i}\right)\right\}_{i=1}^{m}$ ，"对数似然" (log likelihood) 为：<span style="color:red;">为什么可以用极大似然法来估计？原因还是要补充在这里的。</span>

$$
\ell(\boldsymbol{w}, b)=\sum_{i=1}^{m} \ln p\left(y_{i} | \boldsymbol{x}_{i} ; \boldsymbol{w}, b\right)\tag{3.25}
$$


最大化"对数似然" ，即令每个样本属于其真实标记的概率越大越好。为便于讨论，令 $\boldsymbol{\beta}=(\boldsymbol{w} ; b)$，$\hat{\boldsymbol{x}}=(\boldsymbol{x} ; 1)$， 则 $\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$ 可简写为 $\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}$，再令 $p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})=p(y=1 | \hat{\boldsymbol{x}} ; \boldsymbol{\beta})$ ，$p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})=p(y=0 | \hat{\boldsymbol{x}} ; \boldsymbol{\beta})=1-p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})$，则上式中的似然项可重写为 ：

$$
p\left(y_{i} | \boldsymbol{x}_{i} ; \boldsymbol{w}, b\right)=y_{i} p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)+\left(1-y_{i}\right) p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\tag{3.26}
$$

<span style="color:red;">是的，这里还是看得懂的，可以这么写。</span>

带入对数似然函数，并根据上面的 $p(y=1\mid x)$ 和 $p(y=0\mid x)$，得到：<span style="color:red;">这一步略了计算过程，要补上，每一步都要清楚的算过。</span>

$$
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(-y_{i} \boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}+\ln \left(1+e^{\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}}\right)\right)\tag{3.27}
$$


> $$ \ell(\boldsymbol{\beta})=\sum_{i=1}^{m}(-y_i\boldsymbol{\beta}^T\hat{\boldsymbol x}_i+\ln(1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i})) $$
>
> [推导]：将式（3.26）代入式（3.25）可得：
> $$ \ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\ln\left(y_ip_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})+(1-y_i)p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})\right) $$
> 其中 $p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})=\cfrac{e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}}{1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}},p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})=\cfrac{1}{1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}}$，代入上式可得：
> $$\begin{aligned}
> \ell(\boldsymbol{\beta})&=\sum_{i=1}^{m}\ln\left(\cfrac{y_ie^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}+1-y_i}{1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}}\right) \\
> &=\sum_{i=1}^{m}\left(\ln(y_ie^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}+1-y_i)-\ln(1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i})\right)
> \end{aligned}$$
> 由于 $y_i$=0或 1，则：
> $$ \ell(\boldsymbol{\beta}) =
> \begin{cases}
> \sum_{i=1}^{m}(-\ln(1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i})),  & y_i=0 \\
> \sum_{i=1}^{m}(\boldsymbol{\beta}^T\hat{\boldsymbol x}_i-\ln(1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i})), & y_i=1
> \end{cases} $$
> 两式综合可得：
> $$ \ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(y_i\boldsymbol{\beta}^T\hat{\boldsymbol x}_i-\ln(1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i})\right) $$
> 由于此式仍为极大似然估计的似然函数，所以最大化似然函数等价于最小化似然函数的相反数，也即在似然函数前添加负号即可得式（3.27）。
>
> 【注】：若式（3.26）中的似然项改写方式为 $p(y_i|\boldsymbol x_i;\boldsymbol w,b)=[p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})]^{y_i}[p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})]^{1-y_i}$，再将其代入式（3.25）可得：
> $$\begin{aligned}
>  \ell(\boldsymbol{\beta})&=\sum_{i=1}^{m}\ln\left([p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})]^{y_i}[p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})]^{1-y_i}\right) \\
> &=\sum_{i=1}^{m}\left[y_i\ln\left(p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})\right)+(1-y_i)\ln\left(p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})\right)\right] \\
> &=\sum_{i=1}^{m} \left \{ y_i\left[\ln\left(p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})\right)-\ln\left(p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})\right)\right]+\ln\left(p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})\right)\right\} \\
> &=\sum_{i=1}^{m}\left[y_i\ln\left(\cfrac{p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})}{p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})}\right)+\ln\left(p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})\right)\right] \\
> &=\sum_{i=1}^{m}\left[y_i\ln\left(e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}\right)+\ln\left(\cfrac{1}{1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i}}\right)\right] \\
> &=\sum_{i=1}^{m}\left(y_i\boldsymbol{\beta}^T\hat{\boldsymbol x}_i-\ln(1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol x}_i})\right)
> \end{aligned}$$
> 显然，此种方式更易推导出式（3.27）


现在，这个对数似然函数已经变成这样了，我们要求的就是这个对数似然函数最大的时候的 $\beta$ 值。

### 使用牛顿法求解 $\beta$

<span style="color:red;">自己求解一下</span>

这个式子是关于 $\beta$ 的高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如梯度下降法 (gradient descent method) 、牛顿法 (Newton method) 等都可求得其最优解，于是就得到 ：<span style="color:red;">使用梯度下降和牛顿法求解的过程这里要说一下。</span>

$$
\boldsymbol{\beta}^{*}=\underset{\boldsymbol{\beta}}{\arg \min } \ell(\boldsymbol{\beta})\tag{3.28}
$$

<span style="color:red;">不是要最大化对数似然吗？这里是不是抄错了？核对一下 pdf 。</span>

以牛顿法为例，其第 $t+1$ 轮迭代解的更新公式为


$$
\boldsymbol{\beta}^{t+1}=\boldsymbol{\beta}^{t}-\left(\frac{\partial^{2} \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^{\mathrm{T}}}\right)^{-1} \frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}\tag{3.29}
$$


其中关于 $\boldsymbol{\beta}$  的一阶、 二阶导数分别为

$$
\frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}=-\sum_{i=1}^{m} \hat{\boldsymbol{x}}_{i}\left(y_{i}-p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\tag{3.30}
$$
$$
\frac{\partial^{2} \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^{\mathrm{T}}}=\sum_{i=1}^{m} \hat{\boldsymbol{x}}_{i} \hat{\boldsymbol{x}}_{i}^{\mathrm{T}} p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\left(1-p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\tag{3.31}
$$


> $$\frac{\partial l(\beta)}{\partial \beta}=-\sum_{i=1}^{m}\hat{\boldsymbol x}_i(y_i-p_1(\hat{\boldsymbol x}_i;\beta))$$
>
> [解析]：此式可以进行向量化，令 $p_1(\hat{\boldsymbol x}_i;\beta)=\hat{y}_i$，代入上式得：
> $$\begin{aligned}
> 	\frac{\partial l(\beta)}{\partial \beta} &= -\sum_{i=1}^{m}\hat{\boldsymbol x}_i(y_i-\hat{y}_i) \\
> 	& =\sum_{i=1}^{m}\hat{\boldsymbol x}_i(\hat{y}_i-y_i) \\
> 	& ={\boldsymbol X^T}(\hat{\boldsymbol y}-\boldsymbol{y}) \\
> 	& ={\boldsymbol X^T}(p_1(\boldsymbol X;\beta)-\boldsymbol{y}) \\
> \end{aligned}$$


<span style="color:red;">好吧，关键的步骤都写出来了，要自己把它补全。</span>






# 相关

- 《机器学习》 周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
