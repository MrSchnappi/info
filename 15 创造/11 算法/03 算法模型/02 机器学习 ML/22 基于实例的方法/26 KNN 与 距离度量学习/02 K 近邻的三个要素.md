---
title: 02 K 近邻的三个要素
toc: true
date: 2019-08-28
---

OK，从上面的分类过程可以看出，这其中的：

* K值的选择 ：到底选几个？
* 距离度量：什么叫最近？
* 分类决策规则：知道它周边的 k 个样本了，怎么来投票决定这个测试样本的类别？


这三点其实就决定了一个 KNN 算法的效果，它们也是 KNN 算法的三个基本要素。


# 再仔细看一下这三要素


* k 值的选择
  * k 值的选择会对 k 近邻算法的结果产生重大的影响。
  * 如果选择较小的 k 值，就相当于用较小的邻域中的训练实例进行预测，“学习” 的近似误差（approximation error）会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是 “学习” 的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k 值的减小就意味着整体模型变得复杂，容易发生过拟合。<span style="color:red;">什么是近似误差和估计误差</span>
  * 如果选择较大的 k 值，就相当于用较大的邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。 k 值的增大就意味着整体的模型变得简单。

* 距离度量
  * 特征空间中两个实例点的距离是两个实例点相似程度的反映。
  * k 近邻模型的特征空间一般是 n 维实数向量空间 \(R^n\)，使用的距离是欧氏距离，但也可以是其他距离，如更一般的 \(L_p\) 距离，或者 Minkowski 距离。

* 分类决策规则
  * k 近邻算法中的分类决策规则往往是多数表决，即由输入实例的 k 个邻近的训练实例中的多数类决定输入实例的类。**除了多数表决的方法还有什么方法吗？**





# 相关

- [第 2 章 k-近邻算法](http://ml.apachecn.org/mlia/knn/)
