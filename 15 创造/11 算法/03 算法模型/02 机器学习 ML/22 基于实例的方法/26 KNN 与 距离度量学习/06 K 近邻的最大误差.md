---
title: 06 K 近邻的最大误差
toc: true
date: 2019-08-28
---
作为一个非参数学习算法，k-近邻能达到非常高的容量。例如，假设我们有一个用 0-1误差度量性能的多分类任务。在此设定中，当训练样本数目趋向于无穷大时，1-最近邻收敛到两倍贝叶斯误差。超出贝叶斯误差的原因是它会随机从等距离的临近点中随机挑一个。而存在无限的训练数据时，所有测试点 $\boldsymbol{x}$ 周围距离为零的邻近点有无限多个。如果我们使用所有这些临近点投票的决策方式，而不是随机挑选一个，那么该过程将会收敛到贝叶斯错误率。<span style="color:red;">有些厉害。</span>
