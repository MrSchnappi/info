---
title: 13 设置目标
toc: true
date: 2019-08-27
---

## 设置一个目标

对 $\mathbf{M}$ 进行学习当然要设置一个目标。假定我们是希望提高近邻分类器的性能，则可将 $\mathbf{M}$ 直接嵌入到近邻分类器的评价指标中去，通过优化该性能指标相应地求得 $\mathbf{M}$ 。

下面我们以近邻成分分析(Neighbourhood Component Analysis，简称 NCA) 为例进行讨论。

近邻分类器在进行判别时通常使用多数投票法，邻域中的每个样本投 1 票, 邻域外的样本投 0 票。不妨将其替换为概率投票法。对于任意样本 $\boldsymbol{x}_{j}$ ，它对 $\boldsymbol{x}_{i}$ 分类结果影响的概率为

$$
p_{i j}=\frac{\exp \left(-\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{\mathbf{M}}^{2}\right)}{\sum_{l} \exp \left(-\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{l}\right\|_{\mathbf{M}}^{2}\right)}
$$



当 $i=j$ 时，$p_{ij}$ 最大。显然，$\boldsymbol{x}_{j}$ 对 $\boldsymbol{x}_{i}$ 的影响随着它们之间距离的增大而减小。 若以留一法 (LOO) 正确率的最大化为目标，则可计算 $\boldsymbol{x}_{i}$ 的留一法正确率，即它被自身之外的所有样本正确分类的概率为

$$
p_{i}=\sum_{j \in \Omega_{i}} p_{i j}
$$

其中 $\Omega_i$ 表示与 $\boldsymbol{x}_{i}$ 属于相同类别的样本的下标集合。于是，整个样本集上的留一法正确率为

$$
\sum_{i=1}^{m} p_{i}=\sum_{i=1}^{m} \sum_{j \in \Omega_{i}} p_{i j}
$$


将式(10.35)代入(10.37)，再考虑到 $\mathbf{M}=\mathbf{P} \mathbf{P}^{\mathrm{T}}$，则 NCA 的优化目标为

$$
\min _{\mathbf{P}} 1-\sum_{i=1}^{m} \sum_{j \in \Omega_{i}} \frac{\exp \left(-\left\|\mathbf{P}^{\mathrm{T}} \boldsymbol{x}_{i}-\mathbf{P}^{\mathrm{T}} \boldsymbol{x}_{j}\right\|_{2}^{2}\right)}{\sum_{l} \exp \left(-\left\|\mathbf{P}^{\mathrm{T}} \boldsymbol{x}_{i}-\mathbf{P}^{\mathrm{T}} \boldsymbol{x}_{l}\right\|_{2}^{2}\right)}
$$

求解式(10.38)即可得到最大化近邻分类器 LOO 正确率的距离度量矩阵 $\mathbf{M}$。









# 相关

- 《机器学习》周志华
