---
title: 31 决策树中连续值的处理
toc: true
date: 2019-08-27
---

## 连续值处理


到目前为止我们仅仅讨论了基于离散属性来生成决策树的方法，实际上，在现实学习任务中我们常常会遇到连续属性，因此，我们有必要讨论一下如何在决策树学习中使用连续属性。<span style="color:red;">嗯这个我一直想知道。</span>

由于连续属性的可取值数目不再有限，因此，我们不能直接根据连续属性的可取值来对结点进行划分。此时，连续属性离散化技术就可以派上用场了，最简单的策略是采用二分法 (bi-partition) 对连续属性进行处理，实际上，这正是 C4.5 决策树算法中采用的机制。<span style="color:red;">什么是连续属性离散化技术？</span>


### 怎么用二分法对连续属性进行处理呢？


OK，那么怎么用二分法对连续属性进行处理呢？

我们给定样本集 D 和连续属性 a ，假定 a 在 D 上出现了 n 个不同的取值，然后我将这些值从小到大进行排序，记为 $\left\{a^{1}, a^{2}, \ldots, a^{n}\right\}$ 。OK，这个时候，基于划分点 t 我们就可以将 D 分为子集 $D_t^-$ 和 子集 $D_t^+$ ，其中：


* $D_t^-$ 包含那些在属性 a 上取值不大于 t 的样本
* $D_t^+$ 包含那些在属性 a 上取值大于 t 的样本


嗯，有一点很明显，对于相邻的属性取值 $a^i$ 与 $a^{i+1}$ 来说，t 在区间 $[a^i,a^{i+1})$ 中取任意一个值所产生的划分结果都是相同的，这里，我们就取这两个值的中间值 $\frac{a^i+a^{i+1} }{2}$ 作为划分点。（可将划分点设为该属性在训练集中出现的不大于中位点的最大值，从而使得最终决策树使用的划分点都在训练集中出现过。<span style="color:red;">这句话什么意思？不大于哪个中位点？</span>）

OK，那么对连续属性 a 的 n 个属性值，我们就可以得到 n -1 个候选的划分值，每个都可以对整个的元素进行划分：


$$
T_{a}=\left\{\frac{a^{i}+a^{i+1}}{2} | 1 \leqslant i \leqslant n-1\right\}\tag{4.7}
$$


OK，这个时候，我们就可以像对待离散属性值一样来考察这些划分点了。

例如，对于信息增益公式：


$$
\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)\tag{4.2}
$$


我们可以写成：


$$
\begin{aligned} \operatorname{Gain}(D, a) &=\max _{t \in T_{a}} \operatorname{Gain}(D, a, t) \\ &=\max _{t \in T_{a}} \operatorname{Ent}(D)-\sum_{\lambda \in\{-,+\}} \frac{\left|D_{t}^{\lambda}\right|}{|D|} \operatorname{Ent}\left(D_{t}^{\lambda}\right) \end{aligned}\tag{4.8}
$$


> [解析]：样本集 $D$ 中的**连续特征** $a$，假设特征 $a$ 有 $n$ 个不同的取值，对其进行大小排序，记为 $\lbrace{a^1,a^2,...,a^n}\rbrace$，根据特征 $a$ 可得到 $n-1$ 个划分点 $t$，划分点 $t$ 的集合为
> $$
> T_a=\lbrace{\frac{a^i+a^{i+1}}{2}|1\leq{i}\leq{n-1}}\rbrace \tag {4.7}
> $$
> 对于取值集合 $T_a$  中的每个 $t$  值计算将特征 $a$  离散为一个特征值只有两个值，分别是 $\lbrace{a} >t\rbrace$ 和 $\lbrace{a} \leq{t}\rbrace$  的特征，计算新特征的信息增益，找到信息增益最大的 $t$ 值即为该特征的最优划分点。
> $$
> \begin{aligned}
> Gain(D,a) &= \max\limits_{t \in T_a} \ Gain(D,a) \\
> &= \max\limits_{t \in T_a} \ Ent(D)-\sum_{\lambda \in \{-,+\}} \frac{\left | D_t^{\lambda } \right |}{\left |D  \right |}Ent(D_t^{\lambda }) \end{aligned} \tag{4.8}
> $$


其中 $Gain(D,a,t)$ 是样本集 D 基于划分点 t 二分后的信息增益，于是，我们就可以选择使 $Gain(D,a,t)$ 最大化的划分点。

作为一个例子，我们在表 4.1的西瓜数据集 2.0 上増加两个连续属性“密度”和“含糖率”，得到表 4.3所示的西瓜数据集 3.0.下面我们用这个数据集 来生成一棵决策树.

<center>

![](http://images.iterate.site/blog/image/180626/BKke3850Fd.png?imageslim){ width=55% }


</center>


对属性“密度”，在决策树学习开始时，根结点包含的 17 个训练 样本在该属性上取值均不同。根据式（4.7），该属性的候选划分点集合 包含 16 个候选值： T_{密度}={0.244, 0.294, 0.351，0.381, 0.420, 0.459, 0.518, 0.574, 0.600, 0.621，0.636, 0.648, 0.661，0.681, 0.708, 0.746} 。由式（4.8）可计算出属性“密度”的信息增益为 0.262，对应于划分点 0.381.

对属性“含糖率”，其候选划分点集合也包含 16 个候选值 T_{含糖率}= {0.049, 0.074, 0.095, 0.101,0.126, 0.155, 0.179, 0.204, 0.213, 0.226, 0.250, 0.265, 0.292, 0.344, 0.373, 0.418} 。类似的，根据式（4.8）可计算出其信息増益为 0.349, 对应于划分点 0.126.

再由 4.2.1节可知，表 4.3的数据上各属性的信息增益为：

- Gam（D，色泽）=0.109; Gain（P，根蒂）=0.143;
- Gain（P，敲声）=0.141; Gain（D?纹理）=0.381;
- Gain（P，脐部）=0.289; Gain（D，触感）=0.006;
- Gain（D，密度）=0.262; Gain（D5 含糖率）=0.349.

于是，“纹理”被选作根结点划分属性，此后结点划分过程递归进行，最终生成如图 4.8 所示的决策树.

<center>

![](http://images.iterate.site/blog/image/180626/F02jEbhl8m.png?imageslim){ width=55% }


</center>

需注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性.
