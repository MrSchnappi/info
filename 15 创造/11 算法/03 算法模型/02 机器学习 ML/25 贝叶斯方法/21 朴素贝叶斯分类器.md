---
title: 21 朴素贝叶斯分类器
toc: true
date: 2018-06-28 08:52:37
---
# 可以补充进来的

- 7.3 节例子计算有笔误，第 152 页的第 9 个等式应为 $P_{(凹陷|\boldsymbol 是)}=\cfrac{5}{8}$。截止到目前（第 28 次印刷），西瓜书官方勘误修订仅在第 8 次印刷时修正了第 3 个等式 $P_{(蜷缩|\boldsymbol 是)}$，但第 9 个等式仍未修正（若修正第 84 页的西瓜数据集 3.0也可，但亦未修正）。<span style="color:red;">要修正进来。</span>


# 朴素贝叶斯分类器


不难发现，基于贝叶斯公式 (7.8) 来估计后验概率 $P(c | \boldsymbol{x})$ 的主要困难在于: 类条件概率 $P(c | \boldsymbol{x})$ 是所有属性上的联合概率，难以从有限的训练样本直接 估计而得。

为避开这个障碍，朴素贝叶斯分类器(Naive Bayes classifier)采用了 “属性条件独立性假设” (attribute conditional independence assumption)：对已知类别，假设所有属性相互独立。换言之，假设每个属性独立地对分类结果发 生影响.

基于属性条件独立性假设，式(7.8)可重写为

$$
P(c | \boldsymbol{x})=\frac{P(c) P(\boldsymbol{x} | c)}{P(\boldsymbol{x})}=\frac{P(c)}{P(\boldsymbol{x})} \prod_{i=1}^{d} P\left(x_{i} | c\right)\tag{7.14}
$$


其中 $d$ 为属性数目，$x_i$  为 $\boldsymbol{x}$ 在第 $i$ 个属性上的取值.

由于对所有类别来说 $P(\boldsymbol{x})$ 相同，因此基于式(7.6)的贝叶斯判定准则有

$$
h_{n b}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } P(c) \prod_{i=1}^{d} P\left(x_{i} | c\right)\tag{7.15}
$$


这就是朴素贝叶斯分类器的表达式。

显然，朴素贝叶斯分类器的训练过程就是基于训练集 $D$ 来估计类先验概率 $P(c)$ ，并为每个属性估计条件概率 $P(x_i|c)$ .

令 $D_c$ 表示训练集 $D$ 中第 $c$ 类样本组成的集合，若有充足的独立同分布样本，则可容易地估计出类先验概率：

$$
P(c)=\frac{\left|D_{c}\right|}{|D|}\tag{7.16}
$$

对离散属性而言，令 $D_{c, x_{i}}$ 表示 $D_c$ 在第 i 个属性上取值为 $x_i$ 的样本组成的集合，则条件概率 $P(x_i|c)$ 可估计为

$$
P\left(x_{i} | c\right)=\frac{\left|D_{c, x_{i}}\right|}{\left|D_{c}\right|}\tag{7.17}
$$


对连续属性可考虑概率密度函数，假定 $p\left(x_{i} | c\right) \sim \mathcal{N}\left(\mu_{c, i}, \sigma_{c, i}^{2}\right)$ ，其中 $\mu_{c,i}$ 和 $\sigma_{c, i}^{2}$ 分别是第 $c$ 类样本在第 $i$ 个属性上取值的均值和方差，则有

$$
p\left(x_{i} | c\right)=\frac{1}{\sqrt{2 \pi} \sigma_{c, i}} \exp \left(-\frac{\left(x_{i}-\mu_{c, i}\right)^{2}}{2 \sigma_{c, i}^{2}}\right)\tag{7.18}
$$

> $$P_{(\boldsymbol x_{i}|c)}\in[0,1]$$
> $$p_{(\boldsymbol x_{i}| c)}$$
> [解析]：式(7.17)所得 $P_{(\boldsymbol x_{i}|c)}\in[0,1]$ 为条件概率，但式(7.18)所得 $p_{(\boldsymbol x_{i}| c)}$ 为条件概率密度而非概率，其值并不在局限于区间 $[0,1]$ 之内。

下面我们用西瓜数据集 3.0训练一个朴素贝叶斯分类器，对测试例“测 1”进行分类：

<center>

![](http://images.iterate.site/blog/image/180628/2b2cIJad8H.png?imageslim){ width=55% }


</center>


首先估计类先验概率 $P(c)$ ，显然有

<center>

![](http://images.iterate.site/blog/image/180628/FDCB31gb56.png?imageslim){ width=55% }

</center>

然后，为每个属性估计条件概率 $P(x_i|c)$:

<center>

![](http://images.iterate.site/blog/image/180628/9fj6dL1mBd.png?imageslim){ width=55% }

</center>
<center>

![](http://images.iterate.site/blog/image/180628/cdi0IbJhag.png?imageslim){ width=55% }


</center>

于是，有(实践中常通过取对数的方式来将 “连乘” 转化为 “连加” 以避免数值下溢)

<center>

![](http://images.iterate.site/blog/image/180628/6bLm6J5m9c.png?imageslim){ width=55% }

</center>


由于 $0.038 > 6.80 \times 10^{-5}$ ，因此，朴素贝叶斯分类器将测试样本 “测 1” 判别为 “好瓜”.

需注意，若某个属性值在训练集中没有与某个类同时出现过，则直接基于 式(7.17)进行概率估计，再根据式(7.15)进行判别将出现问题。例如，在使用西 瓜数据集 3.0 训练朴素贝叶斯分类器时，对一个“敲声=清脆”的测试例，有

<center>

![](http://images.iterate.site/blog/image/180628/KhfIKIE0b5.png?imageslim){ width=55% }

</center>

由于式(7.15)的连乘式计算出的概率值为零，因此，无论该样本的其他属性是什 么，哪怕在其他属性上明显像好瓜，分类的结果都将是 “好瓜=否”，这显然不太合理.

为了避免其他属性携带的信息被训练集中未出现的属性值 “抹去”， 在估计概率值时通常要进行“平滑”(smoothing)，常用 “拉普拉斯修正” (Laplacian correction)。具体来说，令 $N$ 表示训练集 D 中可能的类别 数，$N_i$ 表示第 i 个属性可能的取值数，则式(7.16)和(7.17)分别修正为

$$
\hat{P}(c)=\frac{\left|D_{c}\right|+1}{|D|+N}\tag{7.19}
$$
$$
\hat{P}\left(x_{i} | c\right)=\frac{\left|D_{c, x_{i}}\right|+1}{\left|D_{c}\right|+N_{i}}\tag{7.20}
$$


例如，在本节的例子中，类先验概率可估计为

<center>

![](http://images.iterate.site/blog/image/180628/1H2A4bjKd3.png?imageslim){ width=55% }


</center>

类似地，$P_{青绿|是}$ 是和 $P_{青绿|否}$ 可估计为

<center>

![](http://images.iterate.site/blog/image/180628/5ALhHfDCEb.png?imageslim){ width=55% }


</center>
<center>

![](http://images.iterate.site/blog/image/180628/9HDihi0Gga.png?imageslim){ width=55% }

</center>

同时，上文提到的概率 $P_{清脆|是}$ 可估计为

<center>

![](http://images.iterate.site/blog/image/180628/1e5B5eGllg.png?imageslim){ width=55% }


</center>


显然，拉普拉斯修正避免了因训练集样本不充分而导致概率估值为零的问题，并且在训练集变大时，修正过程所引入的先验(prior)的影响也会逐渐变得可忽 略，使得估值渐趋向于实际概率值.

> 拉普拉斯修正实质上假设了属性值与类别均匀分布，这是在朴素贝叶斯学习过程中额外引入的关于数据的先验。<span style="color:red;">嗯。</span>

在现实任务中朴素贝叶斯分类器有多种使用方式。例如，若任务对预测速 度要求较高，则对给定训练集，可将朴素贝叶斯分类器涉及的所有概率估值事 先计算好存储起来，这样在进行预测时只需 “查表” 即可进行判别；若任务数据更替频繁，则可采用 “懒惰学习”(lazy learning)方式，先不进行任何训练， 待收到预测请求时再根据当前数据集进行概率估值；若数据不断増加，则可在 现有估值基础上，仅对新増样本的属性值所涉及的概率估值进行计数修正即可实现增量学习.<span style="color:red;">什么是懒惰学习？怎么实现增量学习？</span>



# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
