---
title: 5.11 EM 算法推导
toc: true
date: 2019-09-03
---

### 2.20.2 EM算法推导

<span style="color:red;">没看懂。</span>

对于 $m$ 个样本观察数据 $x=(x^{1},x^{2},...,x^{m})$，现在想找出样本的模型参数 $\theta$，其极大化模型分布的对数似然函数为：

$$
\theta = \mathop{\arg\max}_\theta\sum\limits_{i=1}^m logP(x^{(i)};\theta)
$$

<span style="color:red;">为什么是这个？</span>

如果得到的观察数据有未观察到的隐含数据 $z=(z^{(1)},z^{(2)},...z^{(m)})$，极大化模型分布的对数似然函数则为：

$$
\theta =\mathop{\arg\max}_\theta\sum\limits_{i=1}^m logP(x^{(i)};\theta) = \mathop{\arg\max}_\theta\sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)};\theta)  \tag{a}
$$

由于上式不能直接求出 $\theta$，采用缩放技巧：

$$
\begin{align} \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)};\theta)   & = \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} \\ & \geqslant  \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} \end{align}   \tag{1}
$$

<span style="color:red;">这两步是怎么算的？</span>

上式用到了 Jensen 不等式：

$$
log\sum\limits_j\lambda_jy_j \geqslant \sum\limits_j\lambda_jlogy_j\;\;,  \lambda_j \geqslant 0, \sum\limits_j\lambda_j =1
$$

并且引入了一个未知的新分布 $Q_i(z^{(i)})$。

此时，如果需要满足 Jensen 不等式中的等号，所以有：

$$
\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} =c, c为常数
$$

由于 $Q_i(z^{(i)})$ 是一个分布，所以满足

$$
\sum\limits_{z}Q_i(z^{(i)}) =1
$$

综上，可得：<span style="color:red;">怎么得到的？</span>

$$
Q_i(z^{(i)})  = \frac{P(x^{(i)}， z^{(i)};\theta)}{\sum\limits_{z}P(x^{(i)}, z^{(i)};\theta)} =  \frac{P(x^{(i)}, z^{(i)};\theta)}{P(x^{(i)};\theta)} = P( z^{(i)}|x^{(i)};\theta)
$$

如果 $Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)};\theta)$ ，则第(1)式是我们的包含隐藏数据的对数似然的一个下界。如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要最大化下式：

$$
\mathop{\arg\max}_\theta \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}， z^{(i)};\theta)}{Q_i(z^{(i)})}
$$

简化得：

$$
\mathop{\arg\max}_\theta \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}, z^{(i)};\theta)}
$$

以上即为 EM 算法的 M 步，$\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}, z^{(i)};\theta)}​$ 可理解为 $logP(x^{(i)}, z^{(i)};\theta)$ 基于条件概率分布 $Q_i(z^{(i)})$ 的期望。以上即为 EM 算法中 E 步和 M 步的具体数学含义。

<span style="color:red;">没懂。</span>





# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions) 原文
