---
title: 2.16 K-means 聚类方法的改进
toc: true
date: 2019-08-28
---

## 针对 K 均值算法的缺点，有哪些改进的模型？

K均值算法的主要缺点如下：

- 需要人工预先确定初始 K 值，且该值和真实的数据分布未必吻合。
- K均值只能收敛到局部最优，效果受到初始值很大。<span style="color:red;">这个缺点怎么解决？</span>
- 易受到噪点的影响。
- 样本点只能被划分到单一的类中。<span style="color:red;">是呀，这个缺点怎么解决？</span>


**K-means++ 算法**

K均值的改进算法中，对初始值选择的改进是很重要的一部分。而这类算法中，最具影响力的当属 K-means++ 算法。

原始 K 均值算法最开始随机选取数据集中 K 个点作为聚类中心，而 K-means++ 按照如下的思想选取 K 个聚类中心。

假设已经选取了 n 个初始聚类中心（$0<n<K$），则在选取第 n＋1 个聚类中心时，距离当前 n 个聚类中心越远的点会有更高的概率被选为第 n＋1 个聚类中心。在选取第一个聚类中心（n=1）时同样通过随机的方法。可以说这也符合我们的直觉，聚类中心当然是互相离得越远越好。

当选择完初始点后，K-means++ 后续的执行和经典 K 均值算法相同，这也是对初始值选择进行改进的方法等共同点。<span style="color:red;">嗯，我们平时用的 K-means 就是这个吗？还是说是原始的 K-means？</span>

<span style="color:red;">上面这个 K-means++ 方法对于 K-means 上面的几个缺点有什么确定的帮助吗？</span>

**ISODATA算法**

当 K 值的大小不确定时，可以使用 ISODATA 算法。ISODATA的全称是迭代自组织数据分析法。

在 K 均值算法中，聚类个数 K 的值需要预先人为地确定，并且在整个算法过程中无法更改。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出 K 的大小。

ISODATA算法就是针对这个问题进行了改进，它的思想也很直观：

- 当属于某个类别的样本数过少时，把该类别去除
- 当属于某个类别的样本数过多、分散程度较大时，把该类别分为两个子类别。

<span style="color:red;">嗯，有些不错。</span>

ISODATA 算法在 K 均值算法的基础之上增加了两个操作：

- 一是分裂操作，对应着增加聚类中心数
- 二是合并操作，对应着减少聚类中心数

ISODATA 算法是一个比较常见的算法，其缺点是需要指定的参数比较多，不仅仅需要一个参考的聚类数量 $K_o$，还需要指定 3 个阈值。

下面介绍 ISODATA 算法的各个输入参数：

- 预期的聚类中心数目 $K_o$。在 ISODATA 运行过程中聚类中心数可以变化， $K_o$ 是一个用户指定的参考值，该算法的聚类中心数目变动范围也由其决定。具体地，最终输出的聚类中心数目常见范围是从 $K_o$ 的一半，到两倍 $K_o$。
- 每个类所要求的最少样本数目 $N_{min}$。如果分裂后会导致某个子类别所包含样本数目小于该阈值，就不会对该类别进行分裂操作。
- 最大方差 $Sigma$。用于控制某个类别中样本的分散程度。当样本的分散程度超过这个阈值时，且分裂后满足 $K_o$ 的范围，则进行分裂操作。
- 两个聚类中心之间所允许最小距离 $D_{min}$。如果两个类靠得非常近（即这两个类别对应聚类中心之间的距离非常小），小于该阈值时，则对这两个类进行合并操作。


如果希望样本不划分到单一的类中，可以使用模糊 $C$ 均值或者高斯混合模型，高斯混合模型会在下一节中详细讲述。




# 原文及引用

- 《百面机器学习》
