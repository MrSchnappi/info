---
title: 2.12 LDA 推导
toc: true
date: 2019-09-03
---
二类 LDA 算法原理

输入：数据集 $D=\{(\boldsymbol x_1,\boldsymbol y_1),(\boldsymbol x_2,\boldsymbol y_2),...,(\boldsymbol x_m,\boldsymbol y_m)\}​$，其中样本 $\boldsymbol x_i ​$ 是 $n$ 维向量，$\boldsymbol{y}_{i} \in\{0,1\}$，降维后的目标维度 $d​$。定义

- $N_j(j=0,1)$ 为第 $j$ 类样本个数；
- $X_j(j=0,1)$ 为第 $j$ 类样本的集合；
- $u_j(j=0,1)​$ 为第 $j​$ 类样本的均值向量；
- $\sum_j(j=0,1)$ 为第 $j$ 类样本的协方差矩阵。

​其中

$$
u_j = \frac{1}{N_j} \sum_{\boldsymbol x\in X_j}\boldsymbol x(j=0,1)，
\sum_j = \sum_{\boldsymbol x\in X_j}(\boldsymbol x-u_j)(\boldsymbol x-u_j)^T(j=0,1)
$$

​假设投影直线是向量 $\boldsymbol w$，对任意样本 $\boldsymbol x_i$，它在直线 $w$ 上的投影为 $\boldsymbol w^Tx_i$，两个类别的中心点 $u_0$, $u_1$ 在直线 $w$ 的投影分别为 $\boldsymbol w^Tu_0$ 、$\boldsymbol w^Tu_1$。

LDA的目标是让两类别的数据中心间的距离 $\| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 \|^2_2$ 尽量大，与此同时，希望同类样本投影点的协方差 $\boldsymbol w^T \sum_0 \boldsymbol w$、$\boldsymbol w^T \sum_1 \boldsymbol w$ 尽量小，最小化 $\boldsymbol w^T \sum_0 \boldsymbol w + \boldsymbol w^T \sum_1 \boldsymbol w​$ 。<span style="color:red;">嗯。</span>

​OK，我们定义类内散度矩阵：

$$
S_w = \sum_0 + \sum_1 =
\sum_{\boldsymbol x\in X_0}(\boldsymbol x-u_0)(\boldsymbol x-u_0)^T +
\sum_{\boldsymbol x\in X_1}(\boldsymbol x-u_1)(\boldsymbol x-u_1)^T
$$
​
类间散度矩阵：

$$
S_b = (u_0 - u_1)(u_0 - u_1)^T
$$

​据上分析，优化目标为

$$
\mathop{\arg\max}_\boldsymbol w J(\boldsymbol w) = \frac{\| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 \|^2_2}{\boldsymbol w^T \sum_0\boldsymbol w + \boldsymbol w^T \sum_1\boldsymbol w} =
\frac{\boldsymbol w^T(u_0-u_1)(u_0-u_1)^T\boldsymbol w}{\boldsymbol w^T(\sum_0 + \sum_1)\boldsymbol w} =
\frac{\boldsymbol w^TS_b\boldsymbol w}{\boldsymbol w^TS_w\boldsymbol w}
$$

​根据广义瑞利商的性质，矩阵 $S^{-1}_{w} S_b$ 的最大特征值为 $J(\boldsymbol w)$ 的最大值，即矩阵 $S^{-1}_{w} S_b$ 的最大特征值对应的特征向量即为 $\boldsymbol w$。<span style="color:red;">什么是广义瑞利商？</span>





# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions) 原文
