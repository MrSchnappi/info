---
title: 02 集成学习什么情况下效果好
toc: true
date: 2018-06-28 12:29:30
---
# 可以补充进来的

- ensemble 读音似 “昂桑宝” 而非 “因桑宝”.


# 为什么集成学习效果会好？


在一般经验中，如果把好坏不等的东西掺到一起，那么通常结果会是比最坏的要好一些，比最好的要坏一些。集成学习把多个学习器结合起来，如何能获得比最好的单一学习器更好的性能呢？

<center>

![](http://images.iterate.site/blog/image/180628/JmF1K90fIJ.png?imageslim){ width=55% }


</center>

考虑一个简单的例子：在二分类任务中，假定三个分类器在三个测试样本 上的表现如图 8.2所示，其中 $\surd$ 表示分类正确，$\times$ 表示分类错误，集成学习的结 果通过投票法(voting)产生，即“少数服从多数”

- 在图 8.2(a)中，每个分类器 都只有 66.6%的精度，但集成学习却达到了 100%;
- 在图 8.2(b)中，三个分类器没有差别，集成之后性能没有提高；
- 在图 8.2(c)中，每个分类器的精度都只有 33.3%，集成学习的结果变得更糟

这个简单的例子显示出：要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器习器至少不差于不能太坏，并且要有“多样性”(diversity)，即学习器间具有差异.


我们来做个简单的分析。考虑二分类问题 $y\in\{-1,+1\}$ 和真实函数 $f$ ，假定基分类器的错误率为 $\epsilon$ ，即对每个基分类器 $h_i$ 有：

$$
P\left(h_{i}(\boldsymbol{x}) \neq f(\boldsymbol{x})\right)=\epsilon\tag{8.1}
$$

假设集成通过简单投票法结合 $T$ 个基分类器，若有超过半数的基分类器正确, 则集成分类就正确：

$$
H(\boldsymbol{x})=\operatorname{sign}\left(\sum_{i=1}^{T} h_{i}(\boldsymbol{x})\right)\tag{8.2}
$$


假设基分类器的错误率相互独立，则由 Hoeffding 不等式可知，集成的错误率为：

$$
\begin{aligned} P(H(\boldsymbol{x}) \neq f(\boldsymbol{x})) &=\sum_{k=0}^{\lfloor T / 2\rfloor}\left(\begin{array}{l}{T} \\ {k}\end{array}\right)(1-\epsilon)^{k} \epsilon^{T-k} \\ & \leqslant \exp \left(-\frac{1}{2} T(1-2 \epsilon)^{2}\right) \end{aligned}\tag{8.3}
$$

> $$
> \begin{aligned} P(H(\boldsymbol{x}) \neq f(\boldsymbol{x})) &=\sum_{k=0}^{\lfloor T / 2\rfloor} \left( \begin{array}{c}{T} \\ {k}\end{array}\right)(1-\epsilon)^{k} \epsilon^{T-k} \\ & \leqslant \exp \left(-\frac{1}{2} T(1-2 \epsilon)^{2}\right) \end{aligned}
> $$
> [推导]:由基分类器相互独立，设 X 为 T 个基分类器分类正确的次数，因此 $\mathrm{X} \sim \mathrm{B}(\mathrm{T}, 1-\mathrm{\epsilon})$
> $$
> \begin{aligned} P(H(x) \neq f(x))=& P(X \leq\lfloor T / 2\rfloor) \\ & \leqslant P(X \leq T / 2)
> \\ & =P\left[X-(1-\varepsilon) T \leqslant \frac{T}{2}-(1-\varepsilon) T\right]
> \\ & =P\left[X-
> (1-\varepsilon) T \leqslant -\frac{T}{2}\left(1-2\varepsilon\right)]\right]
>  \end{aligned}
> $$
> 根据 Hoeffding 不等式 $P(X-(1-\epsilon)T\leqslant -kT) \leq \exp (-2k^2T)$
> 令 $k=\frac {(1-2\epsilon)}{2}$ 得
> $$
> \begin{aligned} P(H(\boldsymbol{x}) \neq f(\boldsymbol{x})) &=\sum_{k=0}^{\lfloor T / 2\rfloor} \left( \begin{array}{c}{T} \\ {k}\end{array}\right)(1-\epsilon)^{k} \epsilon^{T-k} \\ & \leqslant \exp \left(-\frac{1}{2} T(1-2 \epsilon)^{2}\right) \end{aligned}
> $$


上式显示出，随着集成中个体分类器数目 $T$ 的增大，集成的错误率将指数级下降，最终趋向于零.

然而我们必须注意到，上面的分析有一个关键假设：基学习器的误差相互独立。在现实任务中，个体学习器是为解决同一个问题训练出来的，它们显然不 可能相互独立！事实上，个体学习器的 “准确性” 和 “多样性”本身就存在冲突。一般的，准确性很高之后，要增加多样性就需牺牲准确性。事实上，如何产 生并结合“好而不同”的个体学习器，恰是集成学习研究的核心.<span style="color:red;">嗯</span>




# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
