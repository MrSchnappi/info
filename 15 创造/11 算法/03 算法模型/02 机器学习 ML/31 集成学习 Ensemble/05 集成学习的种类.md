---
title: 05 集成学习的种类
toc: true
date: 2019-04-21
---
# 可以补充进来的



# 集成学习的种类


根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即

- 个体学习器间存在强依赖关系、必须串行生成的序列化方法，
- 以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；

前者的代表是 Boosting，后者的代表是 Bagging 和 “随机森林” (Random Forest).


集成学习是一大类模型融合策略和方法的统称，其中包含多种集成学习的思想。本题希望考察的是读者对于各主要集成学习方法的基本了解程度。

## 集成学习分哪几种？他们有何异同？

### Boosting

Boosting 方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。

它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。<span style="color:red;">嗯，是的。</span>

Boosting的过程很类似于人类学习的过程（见图 12.1），我们学习新知识的过程往往是迭代式的：

- 第一遍学习的时候，我们会记住一部分知识，但往往也会犯一些错误，对于这些错误，我们的印象会很深。
- 第二遍学习的时候，就会针对犯过错误的知识加强学习，以减少类似的错误发生。
- 不断循环往复，直到犯错误的次数减少到很低的程度。

Boosting 主要思想：迭代式学习

<center>

![](http://images.iterate.site/blog/image/20190420/jXtGi7zlT57C.png?imageslim){ width=55% }

</center>

### Bagging

Bagging 与 Boosting 的串行训练方式不同，Bagging 方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。<span style="color:red;">是的，挺好的。</span>

其中很著名的算法之一是基于决策树基分类器的随机森林（Random Forest）。

为了让基分类器之间互相独立，将训练集分为若干子集（当训练样本数量较少时，子集之间可能有交叠）。

Bagging 方法更像是一个集体决策的过程，每个个体都进行单独学习，学习的内容可以相同，也可以不同，也可以部分重叠。但由于个体之间存在差异性，最终做出的判断不会完全一致。在最终做决策时，每个个体单独作出判断，再通过投票的方式做出最后的集体决策（见图 12.2）。<span style="color:red;">是的。</span>


Bagging 主要思想：集体投票决策：

<center>

![](http://images.iterate.site/blog/image/20190420/0Bsqq5T93mm5.png?imageslim){ width=55% }

</center>

我们再从消除基分类器的偏差和方差的角度来理解 Boosting 和 Bagging 方法的差异。

基分类器，有时又被称为弱分类器，因为基分类器的错误率要大于集成分类器。

基分类器的错误，是偏差和方差两种错误之和：

- 偏差主要是由于分类器的表达能力有限导致的系统性错误，表现在训练误差不收敛。
- 方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。

<span style="color:red;">是的。有些厉害。</span>

- Boosting 方法是通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。
- Bagging 方法则是采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合，来减小集成分类器的方差。

假设所有基分类器出错的概率是独立的，在某个测试样本上，用简单多数投票方法来集成结果，超过半数基分类器出错的概率会随着基分类器的数量增加而下降。<span style="color:red;">是的。</span>

<center>

![](http://images.iterate.site/blog/image/20190420/fgVUKDbOIbS8.png?imageslim){ width=55% }

</center>

图 12.3 是 Bagging 算法的示意图，Model 1、Model 2、Model 3 都是用训练集的一个子集训练出来的，单独来看，它们的决策边界都很曲折，有过拟合的倾向。集成之后的模型（红线所示）的决策边界就比各个独立的模型平滑了，这是由于集成的加权投票方法，减小了方差。<span style="color:red;">是的。</span>


## 逸闻趣事 里奥·布雷曼


里奥·布雷曼（Leo Breiman）是 20 世纪著名的统计学家，他是加州大学伯克利分校教授和美国科学院院士。他是 CART 的发明者之一，Bagging 方法和随机森林的提出者。<span style="color:red;">CART 什么？</span>

虽然已经于 2005 年仙逝，但他每年的论文被引用次数仍在稳步增长，仅仅 2017 年就有一万一千余次引用，可以说他一直活在统计、机器学习研究者们心中。<span style="color:red;">厉害呀！</span>

布雷曼有着传奇的一生：

布雷曼的本科是在以高门槛著称的加州理工物理系度过的，大一他的成绩很好，也拿到了奖学金。然而大二开始，他对课程丧失了兴趣，一直到大四，成绩一直在及格线挣扎。学物理不成，他凭借着数学天赋，1954 年在加州大学伯克利分校获得了数学博士学位。然而，他接下来却并没有从事学术，而且跑去参军了。后来他去加州大学洛杉矶分校担任教职，直到 1980 年又回到伯克利担任教授，此时离他从伯克利毕业，已经过去了 25 年。<span style="color:red;">真的厉害！</span>

最后，引用两段布雷曼荣退后，1994年时在伯克利统计系毕业时的讲话：

“试问其他学科诸如物理学、数学和工程学的学生，25 年后会和现在有何区别，答案很简单，没啥区别呗。毕竟两千多年前，阿基米德就在研究物理、微积分和工程，25 年的变化又算什么呢。但迅速发展的统计学不一样，谁也不知道 25 年后会是什么样子。”<span style="color:red;">厉害</span>

“事实上，我并没有学过任何统计学的课程。我的朋友、同事，曾任斯坦福大学统计系主任的杰里·弗莱曼也没有，他原本是一位实验物理学家。约翰·图基曾经是研究纯数学的。乔治·博克斯曾经是一位化学家。许多其他杰出的统计学家，也是因缘际会，来到统计学这条大船上的。”<span style="color:red;">有些厉害。</span>


一晃又快过去 25 年了，如今机器学习领域正如文中当年的统计学一样，迅速发展、不问出身。如果你对登上这条新船还有所犹豫的话，看看布雷曼他们的例子，或许会有共鸣。<span style="color:red;">是呀。</span>




# 相关

- 《百面机器学习》
