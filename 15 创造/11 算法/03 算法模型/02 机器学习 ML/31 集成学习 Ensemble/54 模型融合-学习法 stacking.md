---
title: 54 模型融合-学习法 stacking
toc: true
date: 2018-08-12 20:01:50
---




# 结合策略-学习法

Stacking 本身是一种著名的集成学习方法，且有 不少集成学习算法可视为 其变体或特例。它也可看作一种特殊的结合策略，因此本书在此介绍。

当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。stacking 是学习法的典型代表。这里我们把个体学习器称为初级学习器，用于结合的学习器称为 次级学习器或元学习器(meta-learner)。


Stacking 先从初始数据集训练出初级学习器，然后 “生成” 一个新数据集 用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。

Stacking 的算法描述如图 8.9 所示，这里我们假定初级学习器使用不同学习算法产生，即初级集成是异质的。

<center>

![](http://images.iterate.site/blog/image/180628/JkghECbEbb.png?imageslim){ width=55% }

</center>


在训练阶段，次级训练集是利用初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险会比较大。因此，一般是通过使用交叉验证或留一法这样的方式，用训练初级学习器未使用的样本来生次级学习器的训练样本。以 $k$ 折交叉验证为例，初始训练集 D 被隨机划分为 $k$ 个大小相 似的集合 $D_{1}, D_{2}, \dots, D_{k}$ 。令 $D_j$ 和 $\overline{D}_{j}=D \backslash D_{j}$ 分别表示第 $j$ 折的测试集和训练集。给定 $T$ 个初级学习算法，初级学习器 $h_{t}^{(j)}$ 通过在 $\overline{D}_{j}$ 上使用第 $t$ 个学习算法而得。对于 $D_j$ 中每个样本 $\boldsymbol{x}_{i}$ ，令 $z_{i t}=h_{t}^{(j)}\left(\boldsymbol{x}_{i}\right)$ ，则由 $\boldsymbol{x}_{i}$ 所产生的次级训练样例的示例部分为 $\boldsymbol{z}_{i}=\left(z_{i 1} ; z_{i 2} ; \ldots ; z_{i T}\right)$ ，标记部分为 $y_i$ 。于是，在整个交叉验证过程结束后，从这 $T$ 个初级学习器产生的次级训练集是 $D^{\prime}=\left\{\left(\boldsymbol{z}_{i}, y_{i}\right)\right\}_{i=1}^{m}$ ，然后 $D^{\prime}$ 将用于训练次级学习器。


次级学习器的输入属性表示和次级学习算法对 Stacking 集成的泛化性能有很大影响。有研究表明，将初级学习器的输出类概率作为次级学习器的输入属性，用多响应线性回归(Multi-response Linear Regression，简称 MLR)作为次级学习算法效果较好［Ting and Witten, 1999］，在 MLR 中使用不同的属性集更佳［Seewald，2002］。

贝叶斯模型平均(Bayes Model Averaging，简称 BMA)基于后验概率来为 不同模型赋予权重，可视为加权平均法的一种特殊实现。[Clarke，2003]对 Stacking 和 BMA 进行了比较。理论上来说，若数据生成模型恰在当前考虑的 模型中，且数据噪声很少，则 BMA 不差于 Stacking；然而，在现实应用中无法确保数据生成模型一定在当前考虑的模型中，甚至可能难以用当前考虑的模型来进行近似，因此，Stacking通常优于 BMA，因为其鲁棒性比 BMA 更好，而且 BMA 对模型近似误差非常敏感。


# 相关

- 《机器学习》周志华
