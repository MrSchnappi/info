---
title: 61 误差-分歧分解
toc: true
date: 2019-08-27
---

## 误差-分歧分解

8.1节提到，欲构建泛化能力强的集成，个体学习器应“好而不同”。现在 我们来做一个简单的理论分析。


假定我们用个体学习器 $h_{1}, h_{2}, \ldots, h_{T}$ 通过加权平均法(8.23)结合产生的集成来完成回归学习任务 $f : \mathbb{R}^{d} \mapsto \mathbb{R}$ 。对示例 $\boldsymbol{x}$ ，定义学习器 $h_i$ 的 “分歧” (ambiguity)为

$$
A\left(h_{i} | \boldsymbol{x}\right)=\left(h_{i}(\boldsymbol{x})-H(\boldsymbol{x})\right)^{2}
$$

则集成的“分歧”是

$$
\begin{aligned} \overline{A}(h | \boldsymbol{x}) &=\sum_{i=1}^{T} w_{i} A\left(h_{i} | \boldsymbol{x}\right) \\ &=\sum_{i=1}^{T} w_{i}\left(h_{i}(\boldsymbol{x})-H(\boldsymbol{x})\right)^{2} \end{aligned}
$$

显然，这里的 “分歧” 项表征了个体学习器在样本 $\boldsymbol{x}$ 上的不一致性，即在 一定程度上反映了个体学习器的多样性。个体学习器 $h_i$ 和集成 $H$ 的平方误差分别为

$$
E\left(h_{i} | \boldsymbol{x}\right)=\left(f(\boldsymbol{x})-h_{i}(\boldsymbol{x})\right)^{2}
$$
$$
E(H | \boldsymbol{x})=(f(\boldsymbol{x})-H(\boldsymbol{x}))^{2}
$$


令 $\overline{E}(h | \boldsymbol{x})=\sum_{i=1}^{T} w_{i} \cdot E\left(h_{i} | \boldsymbol{x}\right)$ 表示个体学习器误差的加权均值，有

$$
\begin{aligned} \overline{A}(h | \boldsymbol{x}) &=\sum_{i=1}^{T} w_{i} E\left(h_{i} | \boldsymbol{x}\right)-E(H | \boldsymbol{x}) \\ &=\overline{E}(h | \boldsymbol{x})-E(H | \boldsymbol{x}) \end{aligned}
$$

式(8.31)对所有样本 $\boldsymbol{x}$ 均成立，令 $p(\boldsymbol{x})$ 表示样本的概率密度，则在全样本
上有

$$
\sum_{i=1}^{T} w_{i} \int A\left(h_{i} | \boldsymbol{x}\right) p(\boldsymbol{x}) d \boldsymbol{x}=\sum_{i=1}^{T} w_{i} \int E\left(h_{i} | \boldsymbol{x}\right) p(\boldsymbol{x}) d \boldsymbol{x}-\int E(H | \boldsymbol{x}) p(\boldsymbol{x}) d \boldsymbol{x}
$$

类似的，个体学习器 $h_i$ 在全样本上的泛化误差和分歧项分别为：

$$
E_{i}=\int E\left(h_{i} | \boldsymbol{x}\right) p(\boldsymbol{x}) d \boldsymbol{x}
$$
$$
A_{i}=\int A\left(h_{i} | \boldsymbol{x}\right) p(\boldsymbol{x}) d \boldsymbol{x}
$$

集成的泛化误差为

$$
E=\int E(H | \boldsymbol{x}) p(\boldsymbol{x}) d \boldsymbol{x}
$$


将式(8.33)~(8.35)代入式(8.32)，再令 $\overline{E}=\sum_{i=1}^{T} w_{i} E_{i}$ 表示个体学习器泛化误差的加权均值，$\overline{A}=\sum_{i=1}^{T} w_{i} A_{i}$ 表示个体学习器的加权分歧值，有

$$
E=\overline{E}-\overline{A}
$$

式(8.36)这个漂亮的式子明确提示出：个体学习器准确性越高、多样性越大，则集成越好。上面这个分析首先由［Krogh and Vedelsby, 1995］给出，称为 “误差-分歧分解” (error-ambiguity decomposition)。


至此，读者可能很高兴：我们直接把 $\overline{E}-\overline{A}$ 作为优化目标来求解，不就能得到最优的集成了？遗憾的是，在现实任务中很难直接对 $\overline{E}-\overline{A}$ 进行优化，不 仅由于它们是定义在整个样本空间上，还由于 $\overline{A}$ 不是一个可直接操作的多样性度量，它仅在集成构造好之后才能进行估计。此外需注意的是，上面的推导过程 只适用于回归学习，难以直接推广到分类学习任务上去。



# 相关

- 《机器学习》周志华
