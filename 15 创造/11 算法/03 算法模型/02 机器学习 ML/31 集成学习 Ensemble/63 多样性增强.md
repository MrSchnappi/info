---
title: 63 多样性增强
toc: true
date: 2019-08-27
---

## 多样性增强

在集成学习中需有效地生成多样性大的个体学习器。与简单地直接用初始 数据训练出个体学习器相比，如何增强多样性呢？

一般思路是在学习过程中引入随机性，常见做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。

### 数据样本扰动

给定初始数据集，可从中产生出不同的数据子集，再利用不同的数据子集训练出不同的个体学习器。数据样本扰动通常是基于采样法，例如在 Bagging 中使用自助采样，在 AdaBoost 中使用序列采样。此类做法简单高效，使用最广。

对很多常见的基学习器，例如决策树、神经网络等，训练样本稍加变化就会 导致学习器有显著变动，数据样本扰动法对这样的“不稳定基学习器”很有效; 然而，有一些基学习器对数据样本的扰动不敏感，例如线性学习器、支持向量机、朴素贝叶斯、近邻学习器等，这样的基学习器称为稳定基学习器(stable base learner)，对此类基学习器进行集成往往需使用输入属性扰动等其他机制。

### 输入属性扰动

子空间一般指从初始的高维属性空间投影产生的低维属性空间，描述低维空间的属性是通过初始属性投影变换而得，未必是初始属性。参见第 10 章。

训练样本通常由一组属性描述，不同的“子空间” (subspace，即属性子 集)提供了观察数据的不同视角。显然，从不同子空间训练出的个体学习器必然 有所不同。著名的隨机子空间(random subspace)算法就依赖于输入属性扰动，该算法从初始属性集中抽取出若干个属性子集，再基于每个属性子集训练一个基学习器，算法描述如图 8。11 所示。

对包含大量冗余属性的数据， 在子空间中训练个体学习器不仅能产生多样性大的个体，还会因属性数的减少而大幅节省时间开销，同时，由于冗余属性多，减少一些属性后训练出的个体学习器也不至于太差。若数据只包含少量属性，或者冗余属性很少，则不宜使用输入属性扰动法。

<center>

![](http://images.iterate.site/blog/image/180628/3aHcCkKK4m.png?imageslim){ width=55% }


</center>


### 输出表示扰动

此类做法的基本思路是对输出表示进行操级以增强多样性。可对训练样本 的类标记稍作变动，如“翻转法”(Flipping Output) 随机改变 一些训练样本的标记；也可对输出表示进行转化，如 “输出调制法”(Output Smearing) 将分类输出转化为回归输出后构建个体学习器; 还可将原任务拆解为多个可同时求解的子任务，如 ECOC 法利用纠错输出码将多分类任务拆解为一系列二分类任务来训练基学习器。

### 算法参数扰动

基学习算法一般都有参数需进行设置，例如神经网络的隐层神经元数、初始连接权值等，通过随机设置不同的参数，往往可产生差别较大的个体学习器。

例如 “负相关法”(Negative Correlation) 显式地通过正则 化项来强制个体神经网络使用不同的参数。对参数较少的算法，可通过将其学 习过程中某些环节用其他类似方式代替，从而达到扰动的目的，

例如可将决策树使用的属性选择机制替换成其他的属性选择机制。值得指出的是，使用单一学习器时通常需使用交叉验证等方法来确定参数值，这事实上已使用了不同参数训练出多个学习器，只不过最终仅选择其中一个学习器进行使用，而集成学习则相当于把这些学习器都利用起来；由此也可看出，集成学习技术的实际计算开销并不比使用单一学习器大很多。

不同的多样性增强机制可同时使用，例如 8。3.2 节介绍的随机森林中同时使用了数据样本扰动和输入属性扰动，有些方法甚至同时使用了更多机制。



# 相关

- 《机器学习》周志华
