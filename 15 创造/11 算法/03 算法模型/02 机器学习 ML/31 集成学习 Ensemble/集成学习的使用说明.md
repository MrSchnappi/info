---
title: 集成学习的使用说明
toc: true
date: 2019-10-18
---
## **Ensemble Generation**

Ensemble Learning 是指将多个不同的 Base Model 组合成一个 Ensemble Model 的方法。它可以**同时降低最终模型的 Bias 和 Variance**（证明可以参考这篇论文，我最近在研究类似的理论，可能之后会写新文章详述)，**从而在提高分数的同时又降低 Overfitting 的风险**。在现在的 Kaggle 比赛中要不用 Ensemble 就拿到奖金几乎是不可能的。

常见的 Ensemble 方法有这么几种：

- Bagging：使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。
- Boosting：迭代地训练 Base Model，每次根据上一个迭代中预测错误的情况修改训练样本的权重。也即 Gradient Boosting 的原理。比 Bagging 效果好，但更容易 Overfit。
- Blending：用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。
- Stacking：接下来会详细介绍。

从理论上讲，Ensemble 要成功，有两个要素：

- **Base Model 之间的相关性要尽可能的小。**这就是为什么非 Tree-based Model 往往表现不是最好但还是要将它们包括在 Ensemble 里面的原因。Ensemble 的 Diversity 越大，最终 Model 的 Bias 就越低。

- **Base Model 之间的性能表现不能差距太大。**这其实是一个 **Trade-off**，在实际中很有可能表现相近的 Model 只有寥寥几个而且它们之间相关性还不低。但是实践告诉我们即使在这种情况下 Ensemble 还是能大幅提高成绩。


**Stacking**

相比 Blending，Stacking 能更好地利用训练数据。以 5-Fold Stacking 为例，它的基本原理如图所示：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/bicdMLzImlibQl5MrGufWibskZ7UmGdZPmnpG5Ml40JAFoOeDD0YlAxcwtjHP7gz0eBZTHdGNeOkpDIsQcpur49eA/640.jpeg?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

整个过程很像 Cross Validation。首先将训练数据分为 5 份，接下来一共 5 个迭代，每次迭代时，将 4 份数据作为 Training Set 对每个 Base Model 进行训练，然后在剩下一份 Hold-out Set 上进行预测。**同时也要将其在测试数据上的预测保存下来。**这样，每个 Base Model 在每次迭代时会对训练数据的其中 1 份做出预测，对测试数据的全部做出预测。5 个迭代都完成以后我们就获得了一个 `#训练数据行数 x #Base Model 数量` 的矩阵，这个矩阵接下来就作为第二层的 Model 的训练数据。当第二层的 Model 训练完以后，将之前保存的 Base Model 对测试数据的预测（**因为每个 Base Model 被训练了 5 次，对测试数据的全体做了 5 次预测，所以对这 5 次求一个平均值，从而得到一个形状与第二层训练数据相同的矩阵**）拿出来让它进行预测，就得到最后的输出。

这里给出我的实现代码：



```py
class Ensemble(object):

   def __init__(self, n_folds, stacker, base_models):
       self.n_folds = n_folds
       self.stacker = stacker
       self.base_models = base_models

   def fit_predict(self, X, y, T):
       X = np.array(X)
       y = np.array(y)
       T = np.array(T)

       folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))
       S_train = np.zeros((X.shape[0], len(self.base_models)))
       S_test = np.zeros((T.shape[0], len(self.base_models)))

       for i, clf in enumerate(self.base_models):
           S_test_i = np.zeros((T.shape[0], len(folds)))
           for j, (train_idx, test_idx) in enumerate(folds):
               X_train = X[train_idx]
               y_train = y[train_idx]
               X_holdout = X[test_idx]
               # y_holdout = y[test_idx]
               clf.fit(X_train, y_train)
               y_pred = clf.predict(X_holdout)[:]
               S_train[test_idx, i] = y_pred
               S_test_i[:, j] = clf.predict(T)[:]
           S_test[:, i] = S_test_i.mean(1)
       self.stacker.fit(S_train, y)
       y_pred = self.stacker.predict(S_test)[:]
       return y_pred
```



获奖选手往往会使用比这复杂得多的 Ensemble，会出现三层、四层甚至五层，不同的层数之间有各种交互，还有将经过不同的 Preprocessing 和不同的 Feature Engineering 的数据用 Ensemble 组合起来的做法。但对于新手来说，稳稳当当地实现一个正确的 5-Fold Stacking 已经足够了。


# 相关
