---
title: 09 最大熵模型的原理与概率图表示
toc: true
date: 2019-04-05
---
# 可以补充进来的

# 解释最大熵模型的原理，并给出概率图模型表示

<span style="color:red;">这个地方看的不是很懂，有点不清楚，要弄明白。</span>

信息是指人们对事物理解的不确定性的降低或消除，而熵就是不确定性的度量，熵越大，不确定性也就越大。

最大熵原理是概率模型学习的一个准则，指导思想是在满足约束条件的模型集合中选取熵最大的模型，即不确定性最大的模型。

在平时生活中，我们也会有意无意地使用最大熵的准则，例如人们常说的鸡蛋不能放在一个篮子里，就是指在事情具有不确定性的时候，我们倾向于尝试它的多种可能性，从而降低结果的风险。同时，在摸清了事情背后的某种规律之后，可以加入一个约束，将不符合规律约束的情况排除，在剩下的可能性中去寻找使得熵最大的决策。<span style="color:red;">嗯，是的。</span>

假设离散随机变量 $x$ 的分布是 $P(x)$，则关于分布 P 的熵定义为：<span style="color:red;">一直想知道为什么要这样定义？</span>

$$
H(P)=-\sum_{x} P(x) \log P(x)\tag{6.13}
$$

可以看出当 $x$ 服从均匀分布时对应的熵最大，也就是不确定性最高。

给定离散随机变量 $x$ 和 $y$ 上的条件概率分布 $P(y|x)$，定义在条件概率分布上的条件熵为：

$$
H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)
$$


其中 $\tilde{P}_{(x)}$ 为样本在训练数据集上的经验分布，即 x 的各个取值在样本中出现的频率统计。


最大熵模型就是要学习到合适的分布 $P(y|x)$ ，使得条件熵 $H(P)$ 的取值最大。

在对训练数据集一无所知的情况下，最大熵模型认为 $P(y | x)$ 是符合均匀分布的。

那么当我们有了训练数据集之后呢？我们希望从中找到一些规律，从而消除一些不确定性，这时就需要用到特征函数 $f(x, y)$ 。特征函数 $f$ 描述了输入 $x$ 和输出 $y$ 之间的一个规律，例如当 $x=y$ 时， $f(x, y)$ 等于一个比较大的正数。为了使学习到的模型 $P(y | x)$ 能够正确捕捉训练数据集中的这一规律（特征），我们加入一个约束，使得特征函数 $f(x, y)$ 关于经验分布 $\tilde{P}_{(x, y)}$ 的期望值与关于模型 $P(y | x)$ 和经验分布 $\tilde{P}_{(x)}$ 的期望值相等，即：

$$E_{\tilde{P}}(f)=E_{P}(f)\tag{6.15}$$


其中，特征函数 $f(x, y)$ 关于经验分布 $\tilde{P}_{(x, y)}$ 的期望值计算公式为：

$$E_{\tilde{P}}(f)=\sum_{x, y} \tilde{P}(x, y) f(x, y)\tag{6.16}$$

$f(x, y)$ 关于模型 $P(y | x)$ 和经验分布 $\tilde{P}_{(x)}$ 的期望值计算公式为：

 $$E_{p}(f)=\sum_{x, y} \tilde{P}(x) P(y | x) f(x, y)\tag{6.17}$$

综上，给定训练数据集 $T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots,\left(x_{N}, y_{N}\right)\right\}$ ，以及 M 个特征函数 $\left\{f_{i}(x, y), i=1,2, \ldots, M\right\}$，最大熵模型的学习等价于约束最优化问题：

$$
\begin{array}{l}{\max _{P} H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)} \\ {\text {s.t.}, E_{\tilde{P}}\left(f_{i}\right)=E_{P}\left(f_{i}\right), \forall i=1,2, \ldots, M} \\ {\sum_{y} P(y | x)=1}\end{array}\tag{6.18}
$$

求解之后可以得到最大熵模型的表达形式为：

$$
P_{w}(y | x)=\frac{1}{Z} \exp \left(\sum_{i=1}^{M} w_{i} f_{i}(x, y)\right)\tag{6.19}
$$

最终，最大熵模型归结为学习最佳的参数 $w$，使得 $P_{w}(y | x)$ 最大化。从概率图模型的角度理解，我们可以看到 $P_{w}(y | x)$ 的表达形式非常类似于势函数为指数函数的马尔可夫网络，其中变量 $x$ 和 $y$ 构成了一个最大团。<span style="color:red;">没看懂</span>

如图 6.3所示最大熵模型的概率图模型。

![](http://images.iterate.site/blog/image/20190406/VMk1FkptlMvt.png?imageslim){ width=55% }



# 相关

- 《百面机器学习》
