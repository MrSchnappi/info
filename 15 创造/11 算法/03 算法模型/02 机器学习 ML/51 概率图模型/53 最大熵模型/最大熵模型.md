---
title: 最大熵模型
toc: true
date: 2018-08-12 20:09:22
---
# 最大熵模型


对最大熵模型进行总结


# 学习的目的


目的，以另外一种角度来理解 Logistic 回归。而且，最大熵模型这个熵这个概念本身在后面都会用到。其实今天的内容很多都是信息论里面的内容。<span style="color:red;">看来信息论要仔细掌握下。</span>


# 学习目标


- 理解并掌握熵 Entropy 的定义
    - 理解 “Huffman编码是所有编码中总编码长度最短的” 熵含义
- 理解联合熵 H(X,Y)、相对熵 D(X||Y) 、条件熵 H(X|Y)、互信息 I(X,Y)的定义和含义，并了解如下公式：
    - \(H(X\mid Y) = H(X,Y) - H(Y)=H(X) - I(X,Y)\)
    - \(H(Y\mid X) = H(X,Y) - H(X)=H(Y) – I(X,Y)\)
    - \(I(X,Y) = H(X) - H(X\mid Y) = H(X) + H(Y) - H(X,Y) \geq 0\)
- 掌握最大熵模型 Maxent
-  Maximum Entropy Models
- 了解最大熵在自然语言处理 NLP 中的应用
- 最大熵模型和极大似然估计 MLE 的关系
- 副产品：了解数据分析、函数作图的一般步骤


# 需要具备的基础知识


<span style="color:red;">这个在后面的证明中会用到，因此写在这里，实际上可以写在后面，挪到后面去。</span>

预备定理


![](http://images.iterate.site/blog/image/180728/F34f9GgC6g.png?imageslim){ width=55% }

# 一个例子作为引子


对给定的某个骰子，经过 N 次投掷后发现，点数的均值为 5.5，请问：再投一次出现点 5 的概率有多大？


## 下面是求解过程：


令 6 个面朝上的概率是 $(p_1,p_2,\codts p_6)$，用向量 p 表示。

那么目标函数是：\(H(\overrightarrow{p})=-\sum_{i=1}^{6}p_ilnp_i\) <span style="color:red;">为什么目标函数是这么一个目标函数？</span>

约束条件是：\(\sum_{i=1}^{6}p_i=1\)。\(\sum_{i=1}^{6}i\cdot p_i=5.5\)

也就是说，这是一个在约束条件下求极值的问题，因此可以直接使用 Lagrange 乘子法，写出 Lagrange 函数：

\[L(\overrightarrow{p},\lambda_1,\lambda_2)=-\sum_{i=1}^{6}p_ilnp_i+\lambda_i(1-\sum_{i=1}^{6}p_i)+\lambda_2(5.5-\sum_{i=1}^{6}i\cdot p_i)\]

直接对\(p_i\)进行求导：

\[\begin{align*}\frac{\partial L}{\partial p_i}&=lnp_i+1-\lambda_1-i\cdot \lambda_2==0\\&\Rightarrow p_i=e^{1-\lambda_1-i\cdot \lambda_2}\end{align*}\]

就求出了 \(p_i\) ，然后将这个 \(p_i\) 带入到两个约束条件中，就求出了对应的\(\lambda\)

\[\lambda_1=5.932,\lambda_2=-1.087\]

将测试的 \(p_i\) 画出来如下：


##  \(p_i\) 画出来如下：




![](http://images.iterate.site/blog/image/180728/jhhalEdh5f.png?imageslim){ width=55% }




## 结论与问题：


可见出现 6 的概率是很高的。而且上面这个目标函数求的结果与我们的直观想象还是很接近的，但是这个目标函数 \(H(\overrightarrow{p})=-\sum_{i=1}^{6}p_ilnp_i\) 是什么呢？那里来的呢？

好了，下面正式开始：






# 一个称假币的问题


假设有 5 个硬币：1,2,3,4,5，其中一个是假的，且比真币轻。有一架没有砝码的天平，天平每次能比较两堆硬币，得出的结果可能是以下三种之一：


- 左边比右边轻
- 右边比左边轻
- 两边同样重

问：至少要几次称量才能确保找到假币？

## 问题解答：

称量方法如下图：

![](http://images.iterate.site/blog/image/180728/fhKKFDmme6.png?imageslim){ width=55% }

因此，答案为：至少称 2 次就可以找出假币。

但是，现在就有一个问题了，为什么一定是至少称量 2 次才能 **确保** 找到假币呢？怎么确定没有更少的次数呢？是呀？

OK，下面就是讲怎么知道 2 是称量来确保找到假币的下界的：


## 确保找到假币 的理论次数下界 是多少


我们先假设 \(x\) 就是假硬币的序号，那么 \(x\) 满足于 \(x\in X=\{1,2,3,4,5\}\)。

然后呢，我们可以用 \(y_i\) 来表示第 \(i\) 次使用天平得到的结果，同样的， \(y_i\) 满足于 \(y\in Y=\{1,2,3\}\) 其中的 1 表示”左轻“，2表示“平衡”，3表示“右轻”。当我们用天平进行称量的时候，如果称了 n 次，那么得到的结果序列就是 \(y_1y_2\cdots y_n\)。

由于 \(y\in Y=\{1,2,3\}\)  ，也就是说 \(y_i\) 每次只有三种选择 ，因此 \(y_1y_2\cdots y_n\) 的所有可能组合数目为\(3^n\)。

下面就是一个比较难理解的东西了：

之前的题目的意思可以概括称：通过一些称量来得到假币的序号。而现在我已经一系列的称量： \(y_1y_2\cdots y_n\) ，也有了假币的序号： \(x\) 。

虽然我不知道这些序号都是些什么，但是我可以理解为：当我能够通过称量得到假币的序号的时候，这个时候的 \(y_1y_2\cdots y_n\) 就确定了一个 \(x\) 。虽然我这个时候仍然不知道 \(x\) 具体是什么， \(y_1y_2\cdots y_n\) 具体是什么。

而这种 \(y_1y_2\cdots y_n\) 就确定了一个 \(x\) 就可以写成一个映射：\(map(y_1y_2\cdots y_n)=x\)。

那么这个时候就需要注意了：我的这个映射的前提是什么？是这个 \(x\) 能够被确定下来。而假如说我的 y 序列只有一步 \(y_1\) ，那么我最多只能映射出 \(3^1\) 种的 \(x\)，少于 5。也就是说，我只称量一次的时候，这个时候的称量序列不足以确定所有的 \(x\) ，因此，要想确认出所有的 \(x\) ，我的称量序列的长度至少也需要是 2，即：\(3^2\geq 5\) 。

也就是说：\(y_1y_2\cdots y_n\) 的变化数目要大于等于 x的变化数目。即要：\(3^n\geq 5\)。


## OK，我们把它梳理到一般意义上


我们把它扩展到一般意义上 ：**为什么可以这样扩展？需要什么限制条件吗？**

\[\mid Y\mid ^n\geq\mid X\mid\]

即：

\[\mid Y\mid^n\geq \mid X\mid\Rightarrow nlog\mid Y\mid \geq log\mid X\mid \Rightarrow n\geq frac{log\mid X\mid}{log \mid Y\mid}\]

也就是说，我们可以这样认为：

我们想用 \(y_1y_2\cdots y_n\)  来表达\(x\)。即相当于将 x 的信息通过编码来实现： \(x:y_1y_2\cdots y_n\)  **（注意，这种编码的思想在 HMM 中仍然会进行考察，到时候回来补充一下）**。

所以，可以这么看：x 是不确定程度的一个度量，y 是一次编码的表达能力，那么：

  * X的总不确定度是：\(H(x)=log\mid X\mid=log 5\)
  * Y的表达能力是：\(H(Y)=log\mid Y\mid=log3\)


那么至少需要多少个 Y 才能准确的表示 X？

\[\frac{H(X)}{H(Y)}=\frac{log5}{log3}=1.46\]

** 厉害啊，这种编码的思想，利害，要再理解下。 **


OK，到这里，我们就大概知道了这种基本的称假币问题，所引起的编码的最小长度的问题。嗯还是很厉害的。


下面，我们针对这个问题进行变化：






# 我们还知道了硬币可能是假币的概率


假设有 5 个硬币：1,2,3,4,5，其中一个是假的，比其他的硬币轻。已知第 1、2个硬币是假硬币的概率都是三分之一；第 2、3、4个是假硬币的概率都是九分之一。有一架没有砝码的天平，假设使用天平 n 次能够找到假币。问 n 的期望值至少是多少？

与上面的问题相比，我们不仅知道有一个假币，还知道了这几个硬币可能是假币的概率。

那么怎么求解呢？


## 解答


1/3 概率的硬币有 2 个，1/9 概率的硬币有 3 个，也就是说：一个天平的表达能力是 log3,1/3概率的时候 是 -log(1/3) 也就是 log3。从而可以算出期望是 4/3。

那么：

\[(\frac{1}{3}+\frac{1}{3})\times \frac{log3}{log3}+3\frac{1}{9}\times \frac{log9}{log3}=\frac{4}{3}\]

**为什么可以这么算？没明白？为什么提到了期望？**

思考一下：\(log_2p\)是什么？**为什么是以 2 为底数？**




# Huffman编码


还记得 Huffman 编码吗？刚才的 4/3其实本质上就是 Huffman 编码至少的平均长度。**什么是 Huffman 编码？那里讲到的？为什么本质上是它的至少的平均长度？**


## 解释 Huffman 编码

![](http://images.iterate.site/blog/image/180728/l67ebbBLFA.png?imageslim){ width=55% }

![](http://images.iterate.site/blog/image/180728/53gcC6H9EL.png?imageslim){ width=55% }

先把至少的拿出来，再把其次的拿出来，因为这里面是天平，所以是三叉树，选三个最小的拿出来，得到结果之后再选三个最小的拿出来就得到了。<span style="color:red;">没明白？什么是 Huffman 编码？与这个地方有什么关系吗？</span>


## 代码如下


<span style="color:red;">需要自己实现一遍</span>


![](http://images.iterate.site/blog/image/180728/fGK8GebH8G.png?imageslim){ width=55% }


熵的那部分我拿出来了。

下面开始解释最大熵模型：


# 最大熵模型


Maximum Entropy


## 最大熵模型的原则

- 承认已知事物(知识)
- 对未知事物不做任何假设，没有任何偏见

不进行任何的假设就相当于熵最大。


## 举个例子



![](http://images.iterate.site/blog/image/180728/hmI7aAihKK.png?imageslim){ width=55% }


![](http://images.iterate.site/blog/image/180728/14DLHmHIk2.png?imageslim){ width=55% }


![](http://images.iterate.site/blog/image/180728/Kl29cCIF61.png?imageslim){ width=55% }

怎么解呢？

首先，我们知道：概率平均分布等价于熵最大。

因此问题转化为：计算 X 和 Y 的分布，使 H(Y|X) 达到最大值，并且满足如下条件：


![](http://images.iterate.site/blog/image/180728/Be77lfh906.png?imageslim){ width=55% }

即可以写成：

![](http://images.iterate.site/blog/image/180728/3Cbh4dEiF2.png?imageslim){ width=55% }

OK，这样就可以使用 Lagrange 方法。


## 而 Maxent 的一般式

![](http://images.iterate.site/blog/image/180728/e0haI09mI4.png?imageslim){ width=55% }

其中得特征(Feature)和样本(Sample)：

特征：(x,y)

* y:这个特征中需要确定的信息
* x:这个特征中的上下文信息


样本：关于某个特征(x,y)的样本，特征所描述的语法现象在标准集合里的分布：


* (xi,yi)对
* yi 是 y 的一个实例
* xi 是 yi 的上下文
* (x1,y1)  (x2,y2)  (x3,y3) ……


特征函数


![](http://images.iterate.site/blog/image/180728/JDA7Fig1BG.png?imageslim){ width=55% }

条件 Constraints


![](http://images.iterate.site/blog/image/180728/a7aE0lhbJE.png?imageslim){ width=55% }

那么我们想在这个模型熵取期望的话：

特征 $f$ 在模型中的期望值：


![](http://images.iterate.site/blog/image/180728/KGb0C38b4b.png?imageslim){ width=55% }

其中 $p(xi)$ 替换为实验中给定的。


## 最大熵模型：最大条件熵


![](http://images.iterate.site/blog/image/180728/cb5DJmfL69.png?imageslim){ width=55% }

最大熵模型在 NLP 中的完整提法


![](http://images.iterate.site/blog/image/180728/0A5IJkI8GH.png?imageslim){ width=55% }


## 最大熵模型总结

![](http://images.iterate.site/blog/image/180728/f5hBLLlGKa.png?imageslim){ width=55% }

OK，这里做一些解释：为什么要求 maxH(Y|X)，不求 maxH(x,y) 因为我们给定了样本，给定了 x 去求 y 。

特征函数是什么？为额什么会有这些特征函数？

![](http://images.iterate.site/blog/image/180728/5a7A3e5A07.png?imageslim){ width=55% }

是样本上某个(x,y)的频率，可以近似于概率。

![](http://images.iterate.site/blog/image/180728/GgbDCH4hHK.png?imageslim)这个是实际的这个，想让它等于样本上的\(\widetilde{E}(f_i)\){ width=55% } 。



因此可以列出 Lagrange 函数：


![](http://images.iterate.site/blog/image/180728/9gCFaAimdd.png?imageslim){ width=55% }

最大熵模型 MaxEnt 的目标拉格朗日函数 L


![](http://images.iterate.site/blog/image/180728/BHCdDhCAIi.png?imageslim){ width=55% }

上面求偏导的过程比较麻烦，因为是一个函数对一个函数求导。

因此这个地方补充一下为什么是可以求导的：

“泛函求导”** 泛函求导这部分可以单独拿出来。**


![](http://images.iterate.site/blog/image/180728/4BimLeg40I.png?imageslim){ width=55% }

泛函求导——“类比”


![](http://images.iterate.site/blog/image/180728/h3LGdmmCK8.png?imageslim){ width=55% }

嗯，补充完了，这个就说明了 L 的第一个括号里面求导之后的结果为什么是那样的。

我们再重新强调一下求出的结果：

最优解形式 Exponential：求偏导，等于 0


![](http://images.iterate.site/blog/image/180728/fCbHAcL0eg.png?imageslim){ width=55% }

这个地方解释一下：把\(exp(1-\lambda_0)\) 写成 \(Z_{\lambda}(x)\)，然后把得到的 \(p(y|x)\) 带回到约束条件\(\sum_{y\in Y}^{} p(y|x)=1\) 里面，让它加和等于去，这样的话就是：

![](http://images.iterate.site/blog/image/180728/hBBCHHkb7J.png?imageslim){ width=55% }

由于是关于 y 的求和，而  \(Z_{\lambda}(x)\) 与 y 无关，因此，可以提出来，即：


![](http://images.iterate.site/blog/image/180728/H4Fld0ki4m.png?imageslim){ width=55% }



那么我们之前的 Logistic 回归的模型是什么样子呢？


# 最大熵模型与 Logistic/Softmax 回归


![](http://images.iterate.site/blog/image/180728/Jeh7Hf8A3F.png?imageslim){ width=55% }

Logistic回归的这个式子不管 c 等于 0 还是 1，都可以写成：

而我们求的最大熵模型也是 e 的某某线性加和。

如果从 Logistic 回归变成 softmax 回归，即从两点分布变成 k 点分布：

直接推广可以得到：


![](http://images.iterate.site/blog/image/180728/D2L3jCm32f.png?imageslim){ width=55% }

底下是规划因子，上面就是这个 e 的某某，所以 Logistic 回归它给出的结论也是在那种情况之下的熵最大的情况，求得那个分界面，分割线，就是在那种情况下熵最大的一种分类。

可见，这二者是完全统一的。**再看下。**

之前我们在讲极大似然估计的时候：说最大似然估计是：


![](http://images.iterate.site/blog/image/180728/4BAlIKcfEh.png?imageslim){ width=55% }



![](http://images.iterate.site/blog/image/180728/lLg38GABJe.png?imageslim){ width=55% }



![](http://images.iterate.site/blog/image/180728/bmbJC9DHd2.png?imageslim){ width=55% }

上面第二个式子是二元情况。第二项是常数（因为凡是加一个横杠的指的是我们的样本空间），可以忽略，第一项就是一开始从其他方法求出的条件熵的定义式。

因此我们算极大似然估计取极大值和我们算熵取极大值，本质上是一个东西。

也就是说：MLE与条件熵：


![](http://images.iterate.site/blog/image/180728/Bgi8Jl4Iik.png?imageslim){ width=55% }

求 L 的对偶函数


![](http://images.iterate.site/blog/image/180728/mE90I196jg.png?imageslim){ width=55% }



![](http://images.iterate.site/blog/image/180728/D1iaBk1gGl.png?imageslim){ width=55% }

最终得出的结论：


![](http://images.iterate.site/blog/image/180728/AIAkEGfA86.png?imageslim){ width=55% }

可以看出，熵和似然是通过两个思路看待同一个问题。

关于\(\lambda\)怎么求解呢？


## λ的求解

![](http://images.iterate.site/blog/image/180728/Lj7jLGBmKF.png?imageslim){ width=55% }

IIS要了解下。

另外 softmax目标函数是这个样子：


![](http://images.iterate.site/blog/image/180728/jHgKjKAHmb.png?imageslim){ width=55% }

# 再次强调一下：




![](http://images.iterate.site/blog/image/180728/3KIHl56jJC.png?imageslim){ width=55% }



# 总结：


最大熵模型出了与 softmax 是一直的，而且在 NLP 上还是用的很多的。

![](http://images.iterate.site/blog/image/180728/2cj4mglG7d.png?imageslim){ width=55% }





# 附：IIS算法公式推导 这个没讲




## IIS的思想


假设最大熵模型当前的参数向量是λ，希望找到新的参数向量λ+δ，使得模型的对数似然函数值 L 增加。重复这一过程，直至找到对数似然函数的最大值。


![](http://images.iterate.site/blog/image/180728/dK0hadBjI2.png?imageslim){ width=55% }

![](http://images.iterate.site/blog/image/180728/CgDb6G4IEH.png?imageslim){ width=55% }

![](http://images.iterate.site/blog/image/180728/bEJfJck514.png?imageslim){ width=55% }

![](http://images.iterate.site/blog/image/180728/978dLh0839.png?imageslim){ width=55% }

![](http://images.iterate.site/blog/image/180728/jJH71HbegL.png?imageslim){ width=55% }

![](http://images.iterate.site/blog/image/180728/3K0KGGjD1k.png?imageslim){ width=55% }

![](http://images.iterate.site/blog/image/180728/edhGBD2HBj.png?imageslim){ width=55% }




# COMMENT：


参考文献也都要看下。




  * Elements of Information Theory (Cover & Thomas)


  * http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92


  * A maximum entropy approach to natural language processing (Adam Berger)


  * A Brief MaxEnt Tutorial (Adam Berger)


  * Learning to parse natural language with maximum entropy models (Adwait Ratnaparkhi)


  * A simple Introduction to Maximum Entropy Models for Natural Language Processing (Adwait Ratnaparkhi)


  * 统计学习方法，李航著，清华大学出版社，2012年


要结合 李航的《统计学习方法》那本书再看一下这个地方。



**还是由很多地方不明白的。**




# 相关

1. 七月在线 机器学习
