---
title: 16 最大熵马尔可夫模型为什么会产生标注偏置问题
toc: true
date: 2019-04-06
---
# 马尔可夫模型


马尔可夫模型，隐马尔可夫模型


## 最大熵马尔可夫模型为什么会产生标注偏置问题？如何解决？

以隐马尔可夫模型为例介绍标注偏置问题（Label Bias Problem）。

隐马尔可夫模型等用于解决序列标注问题的模型中，常常对标注进行了独立性假设。

也就是说，将隐马尔可夫应用于标注问题中，实际上是有两个前提的：

- 前提 1：假设隐状态（即序列标注问题中的标注） $x_i$ 的状态满足马尔可夫过程，$t$ 时刻的状态 $x_t$ 的条件分布，仅仅与其前一个状态 $x_{t-1}$ 有关，即 $P\left(x_{t} | x_{1}, x_{2}, \ldots \ldots, x_{t-1}\right)=P\left(x_{t} | x_{t-1}\right)$
- 前提 2：假设观测序列中各个状态仅仅取决于它对应的隐状态 $P\left(y_{t} | x_{1}, x_{2}, \ldots \ldots, x_{n}, y_{i}, y_{2}, \ldots \ldots, y_{t-1},y_{t+1}, \ldots \ldots \right)=P\left(y_{t} | x_{t}\right)$ <span style="color:red;">好吧，这个假设前提之前好像没有明确说过，是隐马尔可夫模型应用的通用前提吗？</span>
。

隐马尔可夫模型建模时考虑了隐状态间的转移概率和隐状态到观测状态的输出概率。

实际上，在序列标注问题中，隐状态（标注）不仅和单个观测状态相关，还和观察序列的长度、上下文等信息相关。例如词性标注问题中，一个词被标注为动词还是名词，不仅与它本身以及它前一个词的标注有关，还依赖于上下文中的其他词。

于是引出了最大熵马尔可夫模型（Maximum Entropy Markov Model，MEMM），如图所示：

![](http://images.iterate.site/blog/image/20190406/apEzLFeJiHhY.png?imageslim){ width=55% }

与隐马尔可夫模型不同在于：

- 最大熵马尔可夫模型在建模时，去除了隐马尔可夫模型中观测状态相互独立的假设，考虑了整个观测序列，因此获得了更强的表达能力。
- 同时，隐马尔可夫模型是一种对隐状态序列和观测状态序列的联合概率 $P(x, y)$ 进行建模的生成式模型，而最大熵马尔可夫模型是直接对标注的后验概率 $P(y | x)$ 进行建模的判别式模型。<span style="color:red;">似懂非懂，就是没懂。而且，是怎么实现的？想知道来龙去脉。</span>

最大熵马尔可夫模型建模如下，


$$
p\left(x_{1 \cdots n} | y_{1 \cdots n}\right)=\prod_{i=1}^{n} p\left(x_{i} | x_{i-1}, y_{1 \cdots n}\right)\tag{6.22}
$$

其中 $p\left(x_{i} | x_{i-1}, y_{1 \cdots n}\right)$ 会在局部进行归一化，即枚举 $x_i$ 的全部取值进行求和之后计算概率，计算公式为：

$$
p\left(x_{i} | x_{i-1}, y_{1 \cdots n}\right)=\frac{\exp \left(F\left(x_{i}, x_{i-1}, y_{1 \cdots n}\right)\right)}{Z\left(x_{i-1}, y_{1-n}\right)}\tag{6.23}
$$

其中 Z 为归一化因子：

$$
Z\left(x_{i-1}, y_{1 \cdots n}\right)=\sum_{x_{i}} \exp \left(F\left(x_{i}, x_{i-1}, y_{1 \cdots n}\right)\right)\tag{6.24}
$$

其中 $F\left(x_{i}, x_{i-1}, y_{1 \cdots n}\right)$ 为 $x_{i}, x_{i-1}, y_{1 \cdots n}$ 所有特征的线性叠加。

最大熵马尔可夫模型存在标注偏置问题，如图 6.7所示：

![](http://images.iterate.site/blog/image/20190406/VbNw1KwRHYUL.png?imageslim){ width=55% }

可以发现，状态 1 倾向于转移到状态 2，状态 2 倾向于转移到状态 2 本身。

但是实际计算得到的最大概率路径是 1->1->1->1，状态 1 并没有转移到状态 2，如图 6.8所示：

![](http://images.iterate.site/blog/image/20190406/m8XNTof7F3jf.png?imageslim){ width=55% }

这是因为，从状态 2 转移出去可能的状态包括 1、2、3、4、5，概率在可能的状态上分散了，而状态 1 转移出去的可能状态仅仅为状态 1 和 2，概率更加集中。

由于局部归一化的影响，隐状态会倾向于转移到那些后续状态可能更少的状态上，以提高整体的后验概率。这就是标注偏置问题。<span style="color:red;">似懂非懂。好像是这样？还是要再理解下。对于概率的基本概念还是要扎实掌握，比如后验概率什么的。</span>

条件随机场（Conditional Random Field，CRF）在最大熵马尔可夫模型的基础上，进行了全局归一化，如图 6.9所示：

![](http://images.iterate.site/blog/image/20190406/EeUhzxWg8MH8.png?imageslim){ width=55% }

条件随机场建模如下：

$$
p\left(x_{1 \cdots n} | y_{1 \cdots n}\right)=\frac{1}{Z\left(y_{1} \ldots_{n}\right)} \prod_{i=1}^{n} \exp \left(F\left(x_{i}, x_{i-1}, y_{1 \cdots n}\right)\right)\tag{6.25}
$$

其中归一化因子 $Z\left(y_{1} \ldots_{n}\right)$ 是在全局范围进行归一化，枚举了整个隐状态序列 $y_{1} \ldots_{n}$ 的全部可能，从而解决了局部归一化带来的标注偏置问题。<span style="color:red;">嗯，看到这个地方对于上面有所了解了。不过还是要完全消化才行。</span>







# 相关

- 《百面机器学习》
