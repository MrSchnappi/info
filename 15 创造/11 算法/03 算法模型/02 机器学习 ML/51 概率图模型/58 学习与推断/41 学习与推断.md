---
title: 41 学习与推断
toc: true
date: 2019-08-27
---


基于概率图模型定义的联合概率分布，我们能对目标变量的边际分布(marginal distribution)或以某些可观测变量为条件的条件分布进行推断。

条件分布我们已经接触过很多，例如在隐马尔可夫模型中要估算观测序列 $\mathbf{x}$ 在给定参数 $\lambda$ 下的条件概率分布。

边际分布则是指对无关变量求和或积分后得到结果，例如在马尔可夫网中，变量的联合分布被表示成极大团的势函数乘积，于是，给定参数 $\Theta$ 求解某个变量 $x$ 的分布，就变成对联合分布中其他无关变量进行积分的过程，这称为“边际化”(marginalization).


对概率图模型，还需确定具体分布的参数，这称为参数估计或参数学习问 题，通常使用极大似然估计或最大后验概率估计求解。但若将参数视为待推测的变量，则参数估计过程和推断十分相似，可以 “吸收” 到推断问题中。因此，下面我们只讨论概率图模型的推断方法.

具体来说，假设图模型所对应的变量集 $\mathbf{x}=\left\{x_{1}, x_{2}, \ldots, x_{N}\right\}$ 能分为 $\mathbf{x}_{E}$ 和 $\mathbf{x}_{F}$ 两个不相交的变量集，推断问题的目标就是计算边际概率 $P\left(\mathbf{x}_{F}\right)$ 或条件概率 $P\left(\mathbf{x}_{F} | \mathbf{x}_{E}\right)$ 。由条件概率定义有

$$
P\left(\mathbf{x}_{F} | \mathbf{x}_{E}\right)=\frac{P\left(\mathbf{x}_{E}, \mathbf{x}_{F}\right)}{P\left(\mathbf{x}_{E}\right)}=\frac{P\left(\mathbf{x}_{E}, \mathbf{x}_{F}\right)}{\sum_{\mathbf{x}_{F}} P\left(\mathbf{x}_{E}, \mathbf{x}_{F}\right)}
$$


其中联合概率 $P\left(\mathbf{x}_{E}, \mathbf{x}_{F}\right)$ 可基于概率图模型获得，因此，推断问题的关键就是 如何高效地计算边际分布，即

$$
P\left(\mathbf{x}_{E}\right)=\sum_{\mathbf{x}_{F}} P\left(\mathbf{x}_{E}, \mathbf{x}_{F}\right)
$$

概率图模型的推断方法大致可分为两类。第一类是精确推断方法，希望能计算出目标变量的边际分布或条件分布的精确值；遗憾的是，一般情形下，此类 算法的计算复杂度随着极大团规模的増长呈指数増长，适用范围有限。第二类 是近似推断方法，希望在较低的时间复杂度下获得原问题的近似解；此类方法 在现实任务中更常用.






精确推断方法通常需要很大的计算开销，因此在现实应用中近似推断方法 更为常用。

近似推断方法大致可分为两大类：

- 第一类是采样(sampling)，通过使 用随机化方法完成近似；
- 第二类是使用确定性近似完成近似推断，典型代表为 变分推断(variational inference)。
