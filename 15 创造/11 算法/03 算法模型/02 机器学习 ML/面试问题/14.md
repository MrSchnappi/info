---
title: 14
toc: true
date: 2018-07-30
---
本文及其它机器学习、深度学习算法的全面系统讲解可以阅读《机器学习与应用》，清华大学出版社，雷明著，由 SIGAI 公众号作者倾力打造，自 2019 年 1 月出版以来已重印 3 次。

- [书的购买链接](https://link.zhihu.com/?target=https%3A//item.jd.com/12504554.html)
- [书的勘误，优化，源代码资源](https://link.zhihu.com/?target=http%3A//www.tensorinfinity.com/paper_78.html)

> SIGAI飞跃计划第一期已经进行 4 周了，在这 4 周的学习中，同学们提出了不少好问题。在这里，我们将每周直播答疑的问题进行筛选和整理，写成今天的公众号文章，供大家参考。相信会对大家的学习和实践有所帮助！



> **问题 1：线性回归的损失函数是凸函数的证明**

假设有 l 个训练样本，特征向量为 x*i*，标签值为 y*i*，这里使用均方误差（MSE），线性回归训练时优化的目标为：

![img](https://pic4.zhimg.com/80/v2-4801589a0b38a820ba2f07a8b933647f_hd.jpg)

损失函数对权重向量 w 的一阶偏导数为：

![img](https://pic4.zhimg.com/80/v2-5646f156b7d234155eb8d397cb3423bb_hd.jpg)

损失函数对权重向量 w 的二阶偏导数为：

![img](https://pic4.zhimg.com/80/v2-e5f4a9cce2c8b1a83766cb64870beed3_hd.jpg)

因此目标函数的 Hessian 矩阵为：

![img](https://pic4.zhimg.com/80/v2-596a70dc170cedd758997fba0c7b4baf_hd.jpg)

写成矩阵形式为：

![img](https://pic2.zhimg.com/80/v2-6e45e59a4a2d4e6cfc3bc8f8b9a84fb1_hd.jpg)

其中 X 是所有样本的特征向量按照列构成的矩阵。对于任意不为 0 的向量 x，有：

![img](https://pic4.zhimg.com/80/v2-fc4afaabfb02713475e0dd19cf2fd66b_hd.jpg)

因此 Hessian 矩阵半正定，目标函数是凸函数。

> 问题 2：L1和 L2 正则化的选定标准？

这个问题没有理论上的定论。在神经网络中我们一般选择 L2 正则化。以线性回归为例，使用 L2 正则化的岭回归和和使用 L1 正则化的 LASSO 回归都有应用。如果使用 L2 正则化，则正则化项的梯度值为 w；如果是 L1 正则化，则正则化项的梯度值为 sgn(w)。一般认为，L1正则化的结果更为稀疏。可以证明，两种正则化项都是凸函数。

> 问题 3：什么时候用朴素贝叶斯，什么时候用正态贝叶斯？

一般我们都用朴素贝叶斯，因为它计算简单。除非特征向量维数不高、特征分量之间存在严重的相关性我们才用正态贝叶斯，如果特征向量是 n 维的，正态贝叶斯在训练时需要计算 n 阶矩阵的逆矩阵和行列式，这非常耗时。

> 问题 4：可否请雷老师讲解一下 discriminative classifier 和 generative classifier的异同?

判别模型直接得到预测函数 f(x)，或者直接计算概率值 p(y|x)，比如 SVM 和 logistic 回归，softmax回归。SVM直接得到分类超平面的方程，logistic回归和 softmax 回归，以及最后一层是 softmax 层的神经网络，直接根据输入向量 x 得到它属于每一类的概率值 p(y|x)。判别模型只关心决策面，而不管样本的概率分布。生成模型计算 p(x, y)或者 p(x|y) ，通俗来说，生成模型假设每个类的样本服从某种概率分布，对这个概率分布进行建模。

![img](https://pic2.zhimg.com/80/v2-e4e4d7df24f38831c2d7833cc4fee475_hd.jpg)

> 问题 5 雷老师下回可以分享一下自己的学习方法吗？机器学习的内容又多又难，涉及理论与实践，很容易碰到问题卡壳的情况。

首先要确定：卡壳在什么地方？数学公式不理解？算法的思想和原理不理解？还是算法的实现细节不清楚？
如果是数学知识欠缺，或者不能理解，需要先去补数学。如果是对机器学习算法本身使用的思想，思路不理解，则重点去推敲算法的思路。如果是觉得算法太抽象，则把算法形象化，用生动的例子来理解，或者看直观的实验结果。配合实验，实践，能更清楚的理解算法的效果，实现，细节问题。

> 问题 6 流形学习，拉普拉斯特征映射，证明拉普拉斯矩阵半正定。

假设 L 是图的拉普拉斯矩阵，D是加权度对角矩阵，W是邻接矩阵。对于任意不为 0 的向量 f，有：

![img](https://pic1.zhimg.com/80/v2-0b98deb3e1596d113249a50348345d28_hd.jpg)

因此拉普拉斯矩阵半正定。这里矩阵 D 的对角线元素是矩阵 W 的每一行元素的和。



> 问题 7：线性判别分析：优化目标有冗余，这个冗余怎么理解呢？

线性判别分析优化的目标函数为：

![img](https://pic3.zhimg.com/80/v2-b7cc075480c96ba3b0f8a80d1f405b36_hd.jpg)

如果向量 w 是最优解，则将其乘以不为 0 的系数 k 之后，向量 kw 仍然是最优解，证明如下：

![img](https://pic1.zhimg.com/80/v2-a9b885830035b08836aa8c80ebb9be10_hd.jpg)

从几何上看，w可 kw 这两个向量表示的是一个方向，如果 w 是最佳投影方向，则 kw 还是这个方向：

![img](https://pic4.zhimg.com/80/v2-0f173ac3fe9d0151f4b49a31ccb2b56b_hd.jpg)



> 问题 8：决策树，如果是回归树，在寻找最佳分裂时的标准

对于回归树，寻找最佳分裂的标准是分裂之后的回归误差最小化。这等价于让分裂之前的回归误差减去分裂之后的回归误差最大化：

![img](https://pic4.zhimg.com/80/v2-f8de5fd0d8f4c28edf2cbceac724b80b_hd.jpg)

展开之后为：

![img](https://pic2.zhimg.com/80/v2-16fde3ba24f57c595bc0ae5583ec6bd1_hd.jpg)


由于前面的都是常数，因此这等价于将下面的值最大化：

![img](https://pic2.zhimg.com/80/v2-25566b26ebd1655d96281fc92dfa1af9_hd.jpg)

具体细节可以参考[SIGAI](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485762%26idx%3D2%26sn%3Dcb477d9ce96931c56a0130471d63c04f%26chksm%3Dfdb694d5cac11dc329ae2b3253a5029947af5edfa1786a72a2e33a04a2766449e2ea305b979e%26scene%3D21%23wechat_redirect)之前的公众号文章“[理解决策树](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484827%26idx%3D1%26sn%3D043d7d0159baaddfbf92ed78ee5b1124%26chksm%3Dfdb6980ccac1111a9faeae7f517fee46a1dfab19612f76ccfe5417487b3f090ab8fc702d18b8%26scene%3D21%23wechat_redirect)”。



> 问题 9 抽样误差是怎么判定的？能否消除抽样误差？

只要抽样的样本不是整个样本空间，理论上就会有抽样误差，只是是否严重而已。对于一个一般性的数据集，无法从理论上消除抽样误差。在机器学习中，我们无法得到所有可能的训练样本，只能从中抽取一部分，一般要让样本尽量有代表性、全面。

![img](https://pic2.zhimg.com/80/v2-718189ff532e766f5c86e9211777bbe9_hd.jpg)

> 问题 10：卷积神经网络中的 w 到底是怎么更新的，我知道利用梯度下降法和误差函数可以更新 w 值，但是对具体更新的过程还不是很理解。比如每次怎么调整，是一层一层调整还是整体调整，调整的结果是遵循最小化误差函数，但是过程中怎么能体现出来？

反向传播时对每一层计算出参数梯度值之后立即更新；所有层都计算出梯度值之后一起更新，这两种方式都是可以的。所有层的参数都按照梯度下降法更新完一轮，才算一次梯度下降法迭代。

![img](https://pic2.zhimg.com/80/v2-8c882996618e8ac291369ce2ca2358f1_hd.jpg)

> 问题 11：对于凸优化问题的理解，我自己感觉这个很难实现，首先实际问题中有许多问题是不知道约束问题和目标函数的，不知道是不是我做的图像识别的问题，我之前对于目标函数的认识就是使用 softmax 的交叉损失函数，这里可能是我自己的理解不够吧，还需要老师给点提示。

所有机器学习算法的优化目标函数都是确定的，如果带有约束条件，约束条件也是确定的，不会存在不知道目标函数和约束条件的算法

> 问题 12：如何选择机器学习算法是映射函数 f(x)？

映射函数的选取没有一个严格的理论。神经网络，决策树可以拟合任意目标函数，但决策树在高维空间容易过拟合，即遇到维数灾难问题。神经网络的结构和激活函数确定之后，通过调节权重和偏置项可以得到不同的函数。决策树也是如此，不同的树结构代表不同的函数，而在训练开始的时候我们并不知道函数具体是什么样子的。其他的算法，函数都是确定的，如 logistic 回归，SVM，我们能调节的只有它们的参数。每类问题我们都要考虑精度，速度来选择适合它的函数。

> 问题 13：梯度下降法的总结

1.为什么需要学习率？保证泰勒展开在 x 的邻域内进行，从而可以忽略高次项。
2.只要没有到达驻点，每次迭代函数值一定能下降，前提是学习率设置合理。
3.迭代终止的判定规则。达到最大迭代次数，或者梯度充分接近于 0。
4.只能保证找到梯度为 0 的点，不能保证找到极小值点，更不能保证找到全局极小值点。

梯度下降法的改进型，本质上都只用了梯度即一阶导数信息，区别在于构造更新项的公式不同。

> 问题 14：牛顿法的总结

1.不能保证每次迭代函数值下降。
2.不能保证收敛。
3.学习率的设定-直线搜索。
4.迭代终止的判定规则。达到最大迭代次数，或者梯度充分接近于 0。
5.只能保证找到梯度为 0 的点，不能保证找到极小值点，更不能保证找到全局极小值点。

> 问题 15：为什么不能用斜率截距式的方程？

无法表达斜率为正无穷的情况-垂直的直线。直线方程两边同乘以一个不为 0 的数，还是同一条直线。

![img](https://pic3.zhimg.com/80/v2-13dc7edac8ffbd49fa91d84774a94aa6_hd.jpg)

> 问题 16：神经网络的正则化项和动量项的比较。

正则化项的作用：缓解过拟合，迫使参数尽可能小。以 L2 正则化为例：

![img](https://pic2.zhimg.com/80/v2-e5ddfcaaa30cea890a4a5f5c3ab3d2ed_hd.jpg)

动量项的作用：加速收敛，减少震荡。计算公式为：

![img](https://pic2.zhimg.com/80/v2-f5b4d5815714e854a5330a03d211b1d5_hd.jpg)


这相当于累积了之前的梯度信息，并且呈指数级衰减。实现时，先加正则化项，计算动量项。



# 相关

- [机器学习和深度学习中值得弄清楚的一些问题](https://zhuanlan.zhihu.com/p/41219906)
