---
title: 5
toc: true
date: 2018-07-30
---
#### SGD中 S（Stochastic）代表什么

答: 随机选取一个样本来学习。
知识点：梯度下降的优化框架有三种：批量梯度下降（全），随机梯度下降（一），小批量梯度下降（mini）。它们不同之处在于每次学习（更新模型参数）使用的样本个数，每次更新使用不同的样本会导致每次学习的准确性和学习时间不同。
批量梯度下降其缺点在于每次学习时间过长，并且如果训练集很大以至于需要消耗大量的内存，并且全量梯度下降不能进行在线模型参数更新。随机梯度下降最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动)，不过从另一个方面来看，随机梯度下降所带来的波动有个好处就是，对于类似盆地区域（即很多局部极小值点）那么这个波动的特点可能会使得优化的方向从当前的局部极小值点跳到另一个更好的局部极小值点，这样便可能对于非凸函数，最终收敛于一个较好的局部极值点，甚至全局极值点。相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于全量梯度下降，其提高了每次学习的速度。Dauphin指出更严重的问题不是局部极值点，而是鞍点。（目标函数在此点上的梯度（一阶导数）值为 0，但从该点出发的一个方向是函数的极大值点，而在另一个方向是函数的极小值点。）

- Momentum：要是当前时刻的梯度与历史时刻梯度方向相似，这种趋势在当前时刻则会加强；要是不同，则当前时刻的梯度方向减弱）
- Adagrad：也是一种基于梯度的优化算法，它能够对每个参数自适应不同的学习速率，对稀疏特征，得到大的学习更新，对非稀疏特征，得到较小的学习更新，因此该优化算法适合处理稀疏特征数据。在 GloVe 中便使用 Adagrad 来训练得到词向量(Word Embeddings), 频繁出现的单词赋予较小的更新，不经常出现的单词则赋予较大的更新。
- Adam：也是一种不同参数自适应不同学习速率方法，与 Adadelta 与 RMSprop 区别在于，它计算历史梯度衰减方式不同，不使用历史平方衰减，其衰减方式类似动量。

总结：如果你的数据特征是稀疏的，那么你最好使用自适应学习速率 SGD 优化方法(Adagrad、Adadelta、RMSprop与 Adam)，因为你不需要在迭代过程中对学习速率进行人工调整。

#### 监督学习／迁移学习／半监督学习／弱监督学习／非监督学习？

答: 监督学习，数据有 label；迁移学习，迁移参数等，希望模型能够适应其他数据集；半监督学习，部分有标签部分没有；弱监督学习，部分无标签，标签粗粒度，错标签；非监督，无标签。
知识点：

- 半监督学习：其训练数据的一部分是有标签的，另一部分没有标签，而没标签数据的数量常常极大于有标签数据数量（这也是符合现实情况的）。隐藏在半监督学习下的基本规律在于：数据的分布必然不是完全随机的，通过一些有标签数据的局部特征，以及更多没标签数据的整体分布，就可以得到可以接受甚至是非常好的分类结果。（可以说是弱监督的子集）
- 无监督学习：AutoEncoding，PCA，RF，K-means，GAN等。
- 强化学习：强化学习是一个多次决策的过程，可以形成一个决策链；监督学习只是一个一次决策的过程。
- 迁移学习：迁移学习(Transfer learning) 顾名思义就是就是把已学训练好的模型参数迁移到新的模型来帮助新模型训练。考虑到大部分数据或任务是存在相关性的，所以通过迁移学习我们可以将已经学到的模型参数（也可理解为模型学到的知识）通过某种方式来分享给新模型从而加快并优化模型的学习效率不用像大多数网络那样从零学习。
- 多任务学习：把多个相关（related）的任务（task）放在一起学习。多个任务之间共享一些因素，它们可以在学习过程中，共享它们所学到的信息。相关联的多任务学习比单任务学习能去的更好的泛化（generalization）效果。
- 弱监督学习：弱监督通常分为三种类型。第一种是不完全监督，即只有训练数据集的一个（通常很小的）子集有标签，其它数据则没有标签。在很多任务中都存在这种情况。例如，在图像分类中，真值标签是人工标注的；从互联网上获得大量的图片很容易，然而由于人工标注的费用，只能标注其中一个小子集的图像。第二种是不确切监督，即只有粗粒度的标签。又以图像分类任务为例。我们希望图片中的每个物体都被标注；然而我们只有图片级的标签而没有物体级的标签。第三种是不准确监督，即给定的标签并不总是真值。出现这种情况的原因有，标注者粗心或疲倦，或者一些图像本身就难以分类。

#### Softmax Loss？

答：Softmax Loss就是 Softmax 上正确的标签的概率的负对数，这样的话，Loss越小，表示模型效果也越好。而且 softmax loss的求导特别方便，只需要将计算的概率向量对应真正结果的那一维减一就好了。具体推导可以看[卷积神经网络系列之 softmax loss对输入的求导推导](https://blog.csdn.net/u014380165/article/details/79632950)，其实基本是简单的求导，学过高数的就可以求。为什么要取指数，第一个原因是要模拟 max 的行为，所以要让大的更大。第二个原因是需要一个可导的函数。
知识点：现在还是 a 和 b，a>b，如果我们取按照 softmax 来计算取 a 和 b 的概率，那 a 的 softmax 值大于 b 的，所以 a 会经常取到，而 b 也会偶尔取到，概率跟它们本来的大小有关。所以说不是 max，而是 softmax。根据公式:Si=eVi∑jeVjSi=eVi∑jeVj，也就是该元素的指数和所有元素的指数和的比值。交叉熵是 J(δ)=−1m∑mi=1y(i)log(hθ(x(i)))+(1−y(i))log(1−hθ(x(i)))J(δ)=−1m∑i=1my(i)log(hθ(x(i)))+(1−y(i))log(1−hθ(x(i)))，它的推导也主要是高数部分的推导，具体可以看[交叉熵代价函数(损失函数)及其求导推导](https://blog.csdn.net/jasonzzj/article/details/52017438)

#### CNN最成功的应用是在 CV，那为什么 NLP 和 Speech 的很多问题也可以用 CNN 解出来？为什么 AlphaGo 里也用了 CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？

答：以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个 Filter 只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如果每一个点的处理使用相同的 Filter，则为全卷积，如果使用不同的 Filter，则为 Local-Conv。DeepFace在做人脸检测的时候先进行了两次全卷积＋一次池化，提取了低层次的边缘／纹理等特征。后接了 3 个 Local-Conv层，这里是用 Local-Conv的原因是，人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。

#### 什么样的资料集不适合用深度学习?

答：数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。
数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。

#### 对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法

答：优化算法时，针对具体问题进行分析，是算法优化的核心所在。没有免费午餐原理。

#### 用贝叶斯机率说明 Dropout 的原理

答：回想一下使用 Bagging 学习，我们定义 k 个不同的模型，从训练集有替换采样构造 k 个不同的数据集，然后在训练集上训练模型。Dropout的目标是在指数级数量的神经网络上近似这个过程。Dropout训练与 Bagging 训练不太一样。在 Bagging 的情况下，所有模型是独立的。在 Dropout 的情况下，模型是共享参数的，其中每个模型继承的父神经网络参数的不同子集。参数共享使得在有限可用的内存下代表指数数量的模型变得可能。在 Bagging 的情况下，每一个模型在其相应训练集上训练到收敛。在 Dropout 的情况下，通常大部分模型都没有显式地被训练，通常该模型很大，以致到宇宙毁灭都不能采样所有可能的子网络。取而代之的是，可能的子网络的一小部分训练单个步骤，参数共享导致剩余的子网络能有好的参数设定。
知识点：Dropout为什么会起作用。

- 由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此不能保证每 2 个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。
- 可以将 dropout 看作是模型平均的一种。对于每次输入到网络中的样本（可能是一个样本，也可能是一个 batch 的样本），其对应的网络结构都是不同的，但所有的这些不同的网络结构又同时 share 隐含节点的权值。这样不同的样本就对应不同的模型，是 bagging 的一种极端情况。个人感觉这个解释稍微靠谱些，和 bagging，boosting理论有点像，但又不完全相同。
- native bayes是 dropout 的一个特例。Native bayes有个错误的前提，即假设各个特征之间相互独立，这样在训练样本比较少的情况下，单独对每个特征进行学习，测试时将所有的特征都相乘，且在实际应用时效果还不错。而 Droput 每次不是训练一个特征，而是一部分隐含层特征。
- 还有一个比较有意思的解释是，Dropout类似于性别在生物进化中的角色，物种为了使适应不断变化的环境，性别的出现有效的阻止了过拟合，即避免环境改变时物种可能面临的灭亡。

#### 何为共线性, 跟过拟合有啥关联?

答：共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。共线性会造成冗余，导致过拟合。解决方法：排除变量的相关性／加入权重正则。

#### 广义线性模型是怎被应用在深度学习中?

答：深度学习从统计学角度，可以看做递归的广义线性模型。广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数 g(.)，形式变为：y=g−1(wx+b)。

#### 什麽造成梯度消失问题? 推导一下

答：神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用 BP 算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为 0。造成学习停止。

#### Weights Initialization. 不同的方式，造成的后果。为什么会造成这样的结果？

答：几种主要的权值初始化方法：lecun_uniform / glorot_normal / he_normal / batch_normal，主要是 scale 的地方不同。具体的可以看链接：[keras文档](https://keras-cn.readthedocs.io/en/latest/other/initializations/)

#### 为什么网络够深(Neurons 足够多)的时候，总是可以避开较差 Local Optima?

答：The training and testing error become increasingly decorrelated as the network size increases. This provides further indication that attempting to find the absolute possible minimum is of limited use with regards to generalization performance. Critical points of large networks exhibit the layered structure where high-quality low-index critical points lie close to the global minimum. 我的理解，网络层数越多，它能探索到的范围就越大，它的极值点就越靠近全局最优点。

#### Loss. 有哪些定义方式（基于什么？）， 有哪些优化方式，怎么优化，各自的好处，以及解释？

答：Cross-Entropy / MSE / K-L散度(相对熵)。前面两个挺简单的，说一下 KL 散度
知识点:D(p||q)=∑ni=1p(x)logp(x)q(x)D(p||q)=∑i=1np(x)log⁡p(x)q(x)。MSE的不好的地方：1. 其偏导值在输出概率值接近 0 或者接近 1 的时候非常小，这可能会造成模型刚开始训练时，偏导值几乎消失。2. 通过 Softmax 输出的曲线是波动的，有很多局部的极值点。 即，非凸优化问题 (non-convex)

#### Dropout？如何理解

答：你可以把它理解成一个多模型的 ensemble，随机关闭了一些神经元，相当于多个网络的合并，因此效果会提升。在 dropout 之后，需要对神经元进行 release，乘以 1/(1-p)，从而达到恢复原来的激活值点的程度。

#### Activation Function

答：1.Step. 阶跃函数，无法应用于 BP 神经网络，因为导数为 0，无法回传。2.Identity. 线性函数，当存在非线性，就无法使用。3.ReLU. 简单高效，小于 0 无导数。4.Sigmoid，Tanh函数，logistic regression里经常用，容易计算。但是只在接近 0 的时候导数比较大。5.Leaky ReLU. 小于 0 的部分不为 0，允许梯度学习。6.PReLU，小于 0 的斜率是在学习的时候学习到的。参考[26种神经网络激活函数可视化](https://www.jiqizhixin.com/articles/2017-10-10-3)

#### 怎么降低过拟合？

答：引入正则化（L1，L2），Dropout，提前终止训练（Cross Validation），增加样本量

#### BatchNormalization

先来思考一个问题：我们知道在神经网络训练开始前，都要对输入数据做一个归一化处理，那么具体为什么需要归一化呢？归一化后有什么好处呢？原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。
对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。
我们知道网络一旦 train 起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和 input 计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal Covariate Shift”。Paper所提出的算法，就是要解决在训练过程中，中间层数据分布发生改变的情况，于是就有了 Batch Normalization.
在每次 mini-batch反向传播之后重新对参数进行 0 均值 1 方差标准化。这样可以使用更大的学习速率，以及花费更少的精力在参数初始化点上。Batch normalization充当着正则化、减少甚至消除掉 Dropout 的必要性。

#### Gradient noise

即在每次迭代计算梯度中加上一个高斯分布 N(0,σ2t)的随机误差。对梯度增加随机误差会增加模型的鲁棒性，即使初始参数值选择地不好，并适合对特别深层次的负责的网络进行训练。其原因在于增加随机噪声会有更多的可能性跳过局部极值点并去寻找一个更好的局部极值点，这种可能性在深层次的网络中更常见。

#### 奇异值(SVD)分解

矩阵的奇异值是一个数学意义上的概念，一般是由奇异值分解（Singular Value Decomposition，简称 SVD 分解）得到。如果要问奇异值表示什么物理意义，那么就必须考虑在不同的实际工程应用中奇异值所对应的含义。奇异值往往对应着矩阵中隐含的重要信息，且重要性和奇异值大小正相关。每个矩阵 A 都可以表示为一系列秩为 1 的“小矩阵”之和，而奇异值则衡量了这些“小矩阵”对于 A 的权重。
在图像处理领域，奇异值不仅可以应用在数据压缩上，还可以对图像去噪。如果一副图像包含噪声，我们有理由相信那些较小的奇异值就是由于噪声引起的。当我们强行令这些较小的奇异值为 0 时，就可以去除图片中的噪声。
也可以用在推荐系统里，用协同过滤的算法。

#### Reference

1. [深度学习岗位面试问题整理笔记](https://zhuanlan.zhihu.com/p/25005808)
2. [深度学习相关的职位面试时一般会问什么？会问一些传统的机器学习算法吗？](https://www.zhihu.com/question/54308150)
3. [Batch Normalization 学习笔记](https://blog.csdn.net/hjimce/article/details/50866313)
4. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)
5. [奇异值的物理意义是什么？](https://www.zhihu.com/question/22237507)
6. [SVD在推荐系统中的应用](https://yanyiwu.com/work/2012/09/10/SVD-application-in-recsys.html)

#### 其他建议

1. 动态规划、贪心、树、链表都是常考题。可以在 leetcode 上刷题，至少 100 道左右，基本就能应对面试中的算法 coding 了。把答案用一个函数的形式写出来。
2. 面试中遇到的机器学习算法主要有线性回归、朴素贝叶斯、决策树、GDBT、随机森林、Adaboost、逻辑回归（和 Softmax）、SVM、神经网络和卷积神经网络，写逻辑回归的极大似然估计的推导，SVM会问思想，神经网络会问随机梯度下降和反向传播。损失函数、过拟合、算法的优缺点是经常问到的点。机器学习算法中哪些是回归算法哪些是分类的。
3. 传统机器学习方面问题。lr，svm，gmm，kmeans，HMM，NB，Decision Tree，Information Gain，Gini Index，ensemble model（bagging，stacking，boosting，结合 RF，Adaboost，GBDT），Feature Selection，PCA，LDA，bias vs variance
4. 深度学习方面问题。手推 BP，梯度消失/爆炸原因，解决方法，Loss Function，BN原理，防止过拟合方法，SGD，momentum，RMSProp，Adam，牛顿法，拉格朗日乘子法，对偶问题，KKT条件。
5. Coding题型：排序、双指针、dp、贪心、分治、递归、回溯、字符串、树、链表、trie、bfs、dfs等等



- [深度学习面试笔记](http://lindongding.com/2018/05/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AE%B0/)
