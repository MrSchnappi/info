---
title: Batch Size
toc: true
date: 2019-09-03
---

## 3.5 Batch_Size

### 3.5.1 为什么需要 Batch_Size？

Batch 的选择，首先决定的是下降的方向。

如果数据集比较小，可采用全数据集的形式，好处是：

1. 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。
2. 由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用 Rprop 只基于梯度符号并且针对性单独更新各权值。<span style="color:red;">什么意思？Rprop 是什么？没明白这一段。</span>

对于更大的数据集，假如采用全数据集的形式，坏处是：

1. 随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。
2. 以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 RMSProp 的妥协方案。<span style="color:red;">什么意思？Rprop 的梯度修正值是什么？RMSProp 是什么？</span>

### 3.5.2 Batch_Size 值的选择

假如每次只训练一个样本，即 Batch_Size = 1。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。此时，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。<span style="color:red;">什么意思？</span>

既然 Batch_Size 为全数据集或者 Batch_Size = 1 都有各自缺点，可不可以选择一个适中的 Batch_Size 值呢？

此时，可采用批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。

### 3.5.3 在合理范围内，增大 Batch_Size 有何好处？

1. 内存利用率提高了，大矩阵乘法的并行化效率提高。
2. 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。
3. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。<span style="color:red;">嗯。</span>

### 3.5.4 盲目增大 Batch_Size 有何坏处？

1. 内存利用率提高了，但是内存容量可能撑不住了。
2. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。<span style="color:red;">？想要达到相同的精度，花费的时间会变长吗？对参数的修正液显得更加缓慢吗？</span>
3. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。<span style="color:red;">嗯，是的。</span>

### 3.5.5 调节 Batch_Size 对训练效果影响到底如何？

1. Batch_Size 太小，模型表现效果极其糟糕(error飙升)。
2. 随着 Batch_Size 增大，处理相同数据量的速度越快。
3. 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。<span style="color:red;">嗯，这个倒是。</span>
4. 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。<span style="color:red;">是的是的。</span>
5. 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。
