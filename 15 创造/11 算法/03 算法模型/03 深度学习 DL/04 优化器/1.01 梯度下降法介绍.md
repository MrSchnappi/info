---
title: 1.01 梯度下降法介绍
toc: true
date: 2019-08-27
---
![mark](http://images.iterate.site/blog/image/20190818/2RBGgEC2pjVP.png?imageslim)




## 梯度下降法介绍

梯度下降是机器学习中常见优化算法之一，梯度下降法有以下几个作用：

梯度下降是迭代法的一种，可以用于求解最小二乘问题。<span style="color:red;">什么是最小二乘问题？</span>

在求解机器学习算法的模型参数，即无约束优化问题时，主要有梯度下降法（Gradient Descent）和最小二乘法。

## 梯度概念


- 梯度是一个向量，即有方向有大小。
- 梯度的方向是导数最大的方向。
- 梯度的值是最大方向导数的值。

## 梯度下降法要解决的问题

在线性归回中，需要解决下面的最优化问题：

$$\theta^∗= \underset{ \theta }{\operatorname{arg\ min}}  L(\theta) \tag1$$
- $L$ :lossfunction（损失函数）
- $\theta$ :parameters（参数）

这里的 parameters 是复数，即 $\theta$ 指代一堆参数，比如上篇说到的 $w$ 和 $b$ 。

我们要找一组参数 $\theta$ ，让损失函数越小越好，这个问题可以用梯度下降法解决：

假设 $\theta$ 有里面有两个参数 $\theta_1, \theta_2$
随机选取初始值

$$
\theta^0 = \begin{bmatrix}
\theta_1^0 \\
\theta_2^0
\end{bmatrix} \tag2
$$

那么：

$$
\left[\begin{array}{l}{\theta_{1}^{1}} \\ {\theta_{2}^{1}}\end{array}\right]=\left[\begin{array}{l}{\theta_{1}^{0}} \\ {\theta_{2}^{0}}\end{array}\right]-\eta\left[\begin{array}{l}{\partial L\left(\theta_{1}^{0}\right) / \partial \theta_{1}} \\ {\partial L\left(\theta_{2}^{0}\right) / \partial \theta_{2}}\end{array}\right]
$$

我们对其中的差值进行表示：

$$
\nabla L(\theta)=\left[\begin{array}{l}{\partial L\left(\theta_{1}\right) / \partial \theta_{1}} \\ {\partial L\left(\theta_{2}\right) / \partial \theta_{2}}\end{array}\right]
$$

得到：

$$
\theta^{1}=\theta^{0}-\eta \nabla L\left(\theta^{0}\right)
$$


然后分别计算初始点处，两个参数对 $L$ 的偏微分，然后 $\theta^0$ 减掉 $\eta$ 乘上偏微分的值，得到一组新的参数。同理反复进行这样的计算。

$\triangledown L(\theta)$ 即为梯度。
 > $\eta$ 叫做 Learning rates（学习速率）

![mark](http://images.iterate.site/blog/image/20190818/KSJHOe1JAVjO.png?imageslim)

上图举例将梯度下降法的计算过程进行可视化。







# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)
