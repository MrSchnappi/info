---
title: 1.05 各种批量梯度下降法对比
toc: true
date: 2019-09-03
---

# 各种梯度下降法性能比较

​	下表简单对比随机梯度下降（SGD）、批量梯度下降（BGD）、小批量梯度下降（Mini-batch GD）、和 Online GD的区别：

|                |    BGD     |   SGD    | Mini-batch GD |   Online GD    |
|:--------------:|:----------:|:--------:|:-------------:|:--------------:|
|     训练集     |    固定    |   固定   |     固定      |    实时更新    |
| 单次迭代样本数 | 整个训练集 | 单个样本 | 训练集的子集  | 根据具体算法定 |
|   算法复杂度   |     高     |    低    |     一般      |       低       |
|     时效性     |     低     |   一般   |     一般      |       高       |
|     收敛性     |    稳定    |  不稳定  |    较稳定     |     不稳定     |

<span style="color:red;">为什么 Online GD 不稳定？</span>

BGD、SGD、Mini-batch GD，前面均已讨论过，这里介绍一下 Online GD。

​Online GD 于 Mini-batch GD/SGD 的区别在于，所有训练数据只用一次，然后丢弃。这样做的优点在于可预测最终模型的变化趋势。

​Online GD 在互联网领域用的较多，比如搜索广告的点击率（CTR）预估模型，网民的点击行为会随着时间改变。用普通的 BGD 算法（每天更新一次）一方面耗时较长（需要对所有历史数据重新训练）；另一方面，无法及时反馈用户的点击行为迁移。而 Online GD 算法可以实时的依据网民的点击行为进行迁移。<span style="color:red;">是的。</span>
