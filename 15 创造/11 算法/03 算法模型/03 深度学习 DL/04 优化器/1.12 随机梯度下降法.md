---
title: 1.12 随机梯度下降法
toc: true
date: 2019-09-03
---


# 随机梯度下降法

经典的优化方法，如梯度下降法，在每次迭代时需要使用所有的训练数据，这给求解大规模数据的优化问题带来了挑战。

如何克服这个挑战，对于掌握机器学习，尤其是深度学习至关重要。



在机器学习中，优化问题的目标函数通常可以表示成：

$$
L(\theta)=\mathbb{E}_{(x, y) \sim P_{\text { data }}} L(f(x, \theta), y)\tag{7.37}
$$


其中：

- $\theta$ 是待优化的模型参数
- $x$ 是模型输入
- $f(x, \theta)$ 是模型的实际输出
- $y$ 是模型的目标输出
- 函数 $L$ 刻画了模型在数据 $(x y)$ 上的损失
- $P_{\mathrm{data}}$ 表示数据的分布
- $E$ 表示期望

因此，$L(\theta)$ 刻画了当参数为 $\theta$ 时，模型在所有数据上的平均损失。

我们希望能够找到平均损失最小的模型参数，也就是求解优化问题：

$$
\theta^{*}=\arg \min L(\theta)\tag{7.38}
$$

经典的梯度下降法采用所有训练数据的平均损失来近似目标函数，即：

$$
L(\theta)=\frac{1}{M} \sum_{i=1}^{M} L\left(f\left(x_{i}, \theta\right), y_{i}\right)\tag{7.39}
$$

$$
\nabla L(\theta)=\frac{1}{M} \sum_{i=1}^{M} \nabla L\left(f\left(x_{i}, \theta\right), y_{i}\right)\tag{7.40}
$$


其中 $M$ 是训练样本的个数。模型参数的更新公式为：

$$
\theta_{t+1}=\theta_{t}-\alpha \nabla L\left(\theta_{t}\right)\tag{7.41}
$$

因此，经典的梯度下降法在每次对模型参数进行更新时，需要遍历所有的训练数据。

当 $M$ 很大时，这需要很大的计算量，耗费很长的计算时间，在实际应用中基本不可行。

为了解决该问题，随机梯度下降法（Stochastic Gradient Descent，SGD）用单个训练样本的损失来近似平均损失，即：

$$
L\left(\theta ; x_{i}, y_{i}\right)=L\left(f\left(x_{i}, \theta\right), y_{i}\right)\tag{7.42}
$$

$$
\nabla L\left(\theta ; x_{i}, y_{i}\right)=\nabla L\left(f\left(x_{i}, \theta\right), y_{i}\right)\tag{7.43}
$$

因此，随机梯度下降法用单个训练数据即可对模型参数进行一次更新，大大加快了收敛速率。该方法也非常适用于数据源源不断到来的在线更新场景。<span style="color:red;">是的。</span>






# 随机梯度和批量梯度区别

​随机梯度下降（SDG）和批量梯度下降（BDG）是两种主要梯度下降法，其目的是增加某些限制来加速运算求解。

下面通过介绍两种梯度下降法的求解思路，对其进行比较。

假设函数为：

$$
h_\theta (x_0,x_1,...,x_3) = \theta_0 x_0 + \theta_1 x_1 + \ldots+ \theta_n x_n
$$

损失函数为：

$$
J(\theta_0, \theta_1, ... , \theta_n) =
\frac{1}{2m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
,x^{j}_1,...,x^{j}_n)-y^j)^2
$$

其中，$m​$ 为样本个数，$j​$ 为参数个数。

1、 **批量梯度下降的求解思路如下：**

a) 得到每个 $\theta​$ 对应的梯度：

$$
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{m}\sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i
$$

b) 由于是求最小化风险函数，所以按每个参数 $\theta​$ 的梯度负方向更新 $\theta_i​$ ：

$$
\theta_i=\theta_i - \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i
$$

c) 从上式可以注意到，它得到的虽然是一个全局最优解，但每迭代一步，都要用到训练集所有的数据，如果样本数据很大，这种方法迭代速度就很慢。

相比而言，随机梯度下降可避免这种问题。

2、**随机梯度下降的求解思路如下：**

a) 相比批量梯度下降对应所有的训练样本，随机梯度下降法中损失函数对应的是训练集中每个样本的粒度。

损失函数可以写成如下这种形式，

$$
J(\theta_0, \theta_1, ... , \theta_n) =
\frac{1}{m} \sum^{m}_{j=0}(y^j - h_\theta (x^{j}_0
,x^{j}_1,...,x^{j}_n))^2 =
\frac{1}{m} \sum^{m}_{j=0} cost(\theta,(x^j,y^j))
$$

b）对每个参数 $\theta​$ 按梯度方向更新 $\theta​$：

$$
\theta_i = \theta_i + (y^j - h_\theta (x^{j}_0, x^{j}_1, ... ,x^{j}_n))
$$

c) 随机梯度下降是通过每个样本来迭代更新一次。

随机梯度下降伴随的一个问题是噪音较批量梯度下降要多，使得随机梯度下降并不是每次迭代都向着整体最优化方向。

**小结：**

随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：

|     方法     | 特点                                                                                                                                                                                                                                              |
|:------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 批量梯度下降 | a）采用所有数据来梯度下降。<br/>b）批量梯度下降法在样本量很大的时候，训练速度慢。                                                                                                                                                                 |
| 随机梯度下降 | a）随机梯度下降用一个样本来梯度下降。<br/>b）训练速度很快。<br />c）随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。<br />d）收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。 |



批量梯度下降法的每一步都把整个训练集载入进来进行计算，时间花费和内存开销都非常大，无法应用于大数据集、大模型的场景。

相反，随机梯度下降法则放弃了对梯度准确性的追求，每步仅仅随机采样一个（或少量）样本来估计当前梯度，计算速度快，内存开销小。

但由于每步接受的信息量有限，随机梯度下降法对梯度的估计常常出现偏差，造成目标函数曲线收敛得很不稳定，伴有剧烈波动，有时甚至出现不收敛的情况。<span style="color:red;">会不收敛吗？是什么情况？为什么出现？</span>




图 7.4展示了两种方法在优化过程中的参数轨迹，可以看出，批量梯度下降法稳定地逼近最低点，而随机梯度下降法的参数轨迹曲曲折折简直是“黄河十八弯”。
图 7.4 参数优化轨迹：

![](http://images.iterate.site/blog/image/20190407/jxYi8QP5MFOo.png?imageslim){ width=55% }






# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions) 原文
