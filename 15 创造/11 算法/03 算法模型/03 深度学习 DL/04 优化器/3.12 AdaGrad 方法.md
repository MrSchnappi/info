---
title: 3.12 AdaGrad 方法
toc: true
date: 2019-09-03
---

### AdaGrad 方法

惯性的获得是基于历史信息的，那么，除了从过去的步伐中获得一股子向前冲的劲儿，还能获得什么呢？我们还期待获得对周围环境的感知，即使蒙上双眼，依靠前几次迈步的感觉，也应该能判断出一些信息，比如这个方向总是坑坑洼洼的，那个方向可能很平坦。

随机梯度下降法对环境的感知是指在参数空间中，根据不同参数的一些经验性判断，自适应地确定参数的学习速率，不同参数的更新步幅是不同的。

例如，在文本处理中训练词嵌入模型的参数时，有的词或词组频繁出现，有的词或词组则极少出现。数据的稀疏性导致相应参数的梯度的稀疏性，不频繁出现的词或词组的参数的梯度在大多数情况下为零，从而这些参数被更新的频率很低。在应用中，我们希望更新频率低的参数可以拥有较大的更新步幅，而更新频率高的参数的步幅可以减小。<span style="color:red;">是呀，感觉也很在理。</span>

AdaGrad方法采用 “历史梯度平方和” 来衡量不同参数的梯度的稀疏性，取值越小表明越稀疏，具体的更新公式表示为：

$$
\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{\sum_{k=0}^{t} g_{k, i}^{2}+\epsilon}} g_{t, i}\tag{7.51}
$$

其中 $\theta_{t+1,i}$ 表示 $(t+1)$ 时刻的参数向量 $\theta_{t＋1}$ 的第 $i$ 个参数， $g_{k,i}$ 表示 $k$ 时刻的梯度向量 $g_k$ 的第 $i$ 个维度（方向）。

另外，分母中求和的形式实现了退火过程，这是很多优化技术中常见的策略，意味着随着时间推移，学习速率越来越小，从而保证了算法的最终收敛。<span style="color:red;">什么是实现了退火过程？</span>





### Adagrad 算法
#### Adagrad 是什么？
每个参数的学习率都把它除上之前微分的均方根。解释：

普通的梯度下降为：

$$w^{t+1} \leftarrow  w^t -η^tg^t \tag3$$
$$\eta^t =\frac{\eta^t}{\sqrt{t+1}} \tag4$$

- $w$ 是一个参数

Adagrad 可以做的更好：
$$w^{t+1} \leftarrow  w^t -\frac{η^t}{\sigma}g^t \tag5$$
$$g^t =\frac{\partial L(\theta^t)}{\partial w} \tag6$$
- $\sigma^t$ :之前参数的所有微分的均方根，对于每个参数都是不一样的。

#### Adagrad举例
下图是一个参数的更新过程

![mark](http://images.iterate.site/blog/image/20190818/2C9Q6XnkcMAb.png?imageslim)

将 Adagrad 的式子进行化简：
![mark](http://images.iterate.site/blog/image/20190818/XT2GCbDOklzD.png?imageslim)


#### Adagrad 存在的矛盾？
![mark](http://images.iterate.site/blog/image/20190818/1yie8Qg7dmdK.png?imageslim)

在 Adagrad 中，当梯度越大的时候，步伐应该越大，但下面分母又导致当梯度越大的时候，步伐会越小。

下图是一个直观的解释：

![mark](http://images.iterate.site/blog/image/20190818/RtUcGMuf05mT.png?imageslim)

下面给一个正式的解释：

![mark](http://images.iterate.site/blog/image/20190818/OpcDBS5SHB5t.png?imageslim)

比如初始点在 $x_0$，最低点为 $−\frac{b}{2a}$，最佳的步伐就是 $x0$ 到最低点之间的距离 $\left | x_0+\frac{b}{2a} \right |$，也可以写成 $\left | \frac{2ax_0+b}{2a} \right |$。而刚好 $|2ax_0+b|$ 就是方程绝对值在 $x_0$ 这一点的微分。

这样可以认为如果算出来的微分越大，则距离最低点越远。而且最好的步伐和微分的大小成正比。所以如果踏出去的步伐和微分成正比，它可能是比较好的。

结论 1-1：梯度越大，就跟最低点的距离越远。

这个结论在多个参数的时候就不一定成立了。

#### 多参数下结论不一定成立
对比不同的参数

![mark](http://images.iterate.site/blog/image/20190818/Pk9ldCMOcOJO.png?imageslim)

上图左边是两个参数的损失函数，颜色代表损失函数的值。如果只考虑参数 $w_1$，就像图中蓝色的线，得到右边上图结果；如果只考虑参数 $w_2$，就像图中绿色的线，得到右边下图的结果。确实对于 $a$ 和 $b$，结论 1-1是成立的，同理 $c$ 和 $b$ 也成立。但是如果对比 $a$ 和 $c$，就不成立了，$c$ 比 $a$ 大，但 $c$ 距离最低点是比较近的。

所以结论 1-1是在没有考虑跨参数对比的情况下，才能成立的。所以还不完善。

之前说到的最佳距离 $\left | \frac{2ax_0+b}{2a} \right |$，还有个分母 $2a$ 。对 function 进行二次微分刚好可以得到：
$$\frac{\partial ^2y}{\partial x^2} = 2a \tag7$$
所以最好的步伐应该是：
$$\frac{一次微分}{二次微分}$$
即不止和一次微分成正比，还和二次微分成反比。最好的 step 应该考虑到二次微分：

![mark](http://images.iterate.site/blog/image/20190818/bXSO8gVJD3Rr.png?imageslim)

#### Adagrad 进一步的解释


再回到之前的 Adagrad

![mark](http://images.iterate.site/blog/image/20190818/R169TW4soetk.png?imageslim)

对于 $\sqrt{\sum_{i=0}^t(g^i)^2}$ 就是希望再尽可能不增加过多运算的情况下模拟二次微分。（如果计算二次微分，在实际情况中可能会增加很多的时间消耗）







# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)
- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions) 原文
