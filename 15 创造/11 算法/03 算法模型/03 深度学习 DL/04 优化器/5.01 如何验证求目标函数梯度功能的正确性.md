---
title: 5.01 如何验证求目标函数梯度功能的正确性
toc: true
date: 2019-04-07
---
# 梯度验证

在用梯度下降法求解优化问题时，最重要的操作就是计算目标函数的梯度。对于一些比较复杂的机器学习模型，如深度神经网络，目标函数的梯度公式也非常复杂，很容易写错。


因此，在实际应用中，写出计算梯度的代码之后，通常需要验证自己写的代码是否正确。<span style="color:red;">好吧，有些厉害。</span>


微积分，线性代数

## 如何验证求目标函数梯度功能的正确性？

给定优化问题 $\min _{\theta \in \mathbb{R}^{n}} L(\theta)$ ，假设已经用代码实现了求目标函数值和求目标函数梯度的功能。请问，如何利用求目标函数值的功能来验证求目标函数梯度的功能是否正确？


根据梯度的定义，目标函数的梯度为：

$$
\nabla L(\theta)=\left[\frac{\partial L(\theta)}{\partial \theta_{1}}, \cdots, \frac{\partial L(\theta)}{\partial \theta_{n}}\right]^{T}\tag{7.27}
$$

其中对任意的 $i=1,2, \ldots, n$。梯度的第 $i$ 个元素的定义为：

$$
\frac{\partial L(\theta)}{\partial \theta_{i}}=\lim _{h \rightarrow 0} \frac{L\left(\theta+h e_{i}\right)-L\left(\theta-h e_{i}\right)}{2 h}\tag{7.28}
$$

其中 $e_{i}$ 是单位向量，维度与 $\theta$ 相同，仅在第 $i$ 个位置取值为 $1$，其余位置取值为 $0$。<span style="color:red;">嗯。</span>因此，可以取 $h$ 为一个比较小的数（例如 $10^{−7}$），则有：

$$
\frac{\partial L(\theta)}{\partial \theta_{i}} \approx \frac{L\left(\theta+h e_{i}\right)-L\left(\theta-h e_{i}\right)}{2 h}\tag{7.29}
$$

式（7.29）的左边为目标函数梯度的第 $i$ 个分量，右边仅和目标函数值有关，二者应近似相等。

下面利用泰勒展开来计算该近似误差。令单变量函数：

$$
\tilde{L}(x)=L\left(\theta+x e_{i}\right)\tag{7.30}
$$


根据泰勒展开及拉格朗日余项公式，式（7.30）可写为：

$$
L\left(\theta+h e_{i}\right)=\tilde{L}(h)=\tilde{L}(0)+\tilde{L}^{\prime}(0) h+\frac{1}{2} \tilde{L^{\prime \prime}}(0) h^{2}+\frac{1}{6} \tilde{L}^{(3)}\left(p_{i}\right) h^{3}\tag{7.31}
$$

其中 $p_{i} \in(0, h)$ 。类似地：

$$
L\left(\theta-h e_{i}\right)=\tilde{L}(-h)=\tilde{L}(0)-\tilde{L}^{\prime}(0) h+\frac{1}{2} \tilde{L}^{\prime\prime}(0) h^{2}-\frac{1}{6} \tilde{L}^{(3)}\left(q_{i}\right) h^{3}\tag{7.32}
$$

其中 $q_{i} \in(-h, 0)$ 。两个式子相减，等号两边同时除以 $2h$，并由于


$$
\tilde{L}^{\prime}(0)=\frac{\partial L(\theta)}{\partial \theta_{i}}\tag{7.33}
$$

根据式（7.31）～式（7.33）可得：

$$
\frac{L\left(\theta+h e_{i}\right)-L\left(\theta-h e_{i}\right)}{2 h}=\frac{\partial L(\theta)}{\partial \theta_{i}}+\frac{1}{12}\left(\tilde{L}^{(3)}\left(p_{i}\right)+\tilde{L}^{(3)}\left(q_{i}\right)\right) h^{2}\tag{7.34}
$$


当 $h$ 充分小时，$p_i$ 和 $q_i$ 都很接近 $0$，可以近似认为 $h^2$ 项前面的系数是常数 $M$，因此近似式的误差为：

$$
\left|\frac{L\left(\theta+h e_{i}\right)-L\left(\theta-h e_{i}\right)}{2 h}-\frac{\partial L(\theta)}{\partial \theta_{i}} \right|\approx M h^{2}\tag{7.35}
$$


由此可知，当 $h$ 较小时，$h$ 每减小为原来的 $10^{−1}$，近似误差约减小为原来的 $10^{−2}$，即近似误差是 $h$ 的高阶无穷小。<span style="color:red;">嗯，是的。</span>

在实际应用中，我们随机初始化 $\theta$ ，取 $h$ 为较小的数（例如 $10^{−7}$），并对 $i=1,2, \ldots \ldots, n$ 依次验证：

$$
\left|\frac{L\left(\theta+h e_{i}\right)-L\left(\theta-h e_{i}\right)}{2 h}-\frac{\partial L(\theta)}{\partial \theta_{i}} \right| \leqslant h
$$

是否成立。<span style="color:red;">是呀，厉害。</span>

如果对于某个下标 $i$ ，该不等式不成立，则有以下两种可能。

1. 该下标对应的 $M$ 过大。
2. 该梯度分量计算不正确。

此时可以：

固定 $\theta$ ，减小 $h$ 为原来的 $10^{−1}$，并再次计算下标 $i$ 对应的近似误差：

- 若近似误差约减小为原来的 $10^{−2}$，则对应于第一种可能，我们应该采用更小的 $h$ 重新做一次梯度验证；
- 否则对应于第二种可能，我们应该检查求梯度的代码是否有错误。<span style="color:red;">是的，厉害 nice</span>

# 相关

- 《百面机器学习》
