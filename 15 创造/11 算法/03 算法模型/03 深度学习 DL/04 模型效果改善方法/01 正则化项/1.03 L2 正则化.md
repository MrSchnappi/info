---
title: 1.03 L2 正则化
toc: true
date: 2019-08-31
---

## 1. $L^2$ 参数正则化

最简单而又最常见的参数范数惩罚，即 $L^2$ 参数范数惩罚，通常被称为权重衰减。

这个正则化策略通过向目标函数添加一个正则项：

$$
\Omega(\boldsymbol \theta) = \frac{1}{2} \|{\boldsymbol w}\|_2^2
$$

使权重更加接近原点(注：更一般地，我们可以将参数正则化为接近空间中的任意特定点，令人惊讶的是这样也仍有正则化效果，但是特定点越接近真实值结果越好。当我们不知道正确的值应该是正还是负时，零是有意义的默认值。由于模型参数正则化为零的情况更为常见，我们将只探讨这种特殊情况。<span style="color:red;">为什么模型参数正则化为 0 的情况更加常见？</span>)。

在其他学术圈，$L^2$ 也被称为岭回归。<span style="color:red;">哦，原来这个就是岭回归，为什么叫做岭回归呢？</span>

下面，我们可以通过研究正则化后目标函数的梯度，洞察一些权重衰减的正则化表现：

为了简单起见，我们假定其中没有偏置参数，因此 $\boldsymbol \theta$ 就是 $\boldsymbol w$。

这样一个模型具有以下总的目标函数：


$$\begin{aligned}
\tilde{J}(\boldsymbol w;\boldsymbol X, \boldsymbol y) =\frac{\alpha}{2} \boldsymbol w^\top \boldsymbol w +  J(\boldsymbol w;\boldsymbol X, \boldsymbol y),
\end{aligned}$$

与之对应的梯度为

$$\begin{aligned}
\nabla_{\boldsymbol w} \tilde{J}(\boldsymbol w;\boldsymbol X,\boldsymbol y) =\alpha \boldsymbol w +  \nabla_{\boldsymbol w} J(\boldsymbol w;\boldsymbol X, \boldsymbol y).
\end{aligned}$$

<span style="color:red;">嗯，很清楚。</span>

使用单步梯度下降更新权重，即执行以下更新：

$$\begin{aligned}
\boldsymbol w \leftarrow \boldsymbol w - \epsilon(\alpha \boldsymbol w + \nabla_{\boldsymbol w} J(\boldsymbol w;\boldsymbol X, \boldsymbol y)).
\end{aligned}$$

换种写法就是：

$$\begin{aligned}
\boldsymbol w \leftarrow (1-\epsilon \alpha)\boldsymbol w - \epsilon \nabla_{\boldsymbol w} J(\boldsymbol w;\boldsymbol X, \boldsymbol y).\tag{7.5}
\end{aligned}$$

我们可以看到，加入权重衰减后会引起学习规则的修改，即在每步执行通常的梯度更新之前先收缩权重向量（将权重向量乘以一个常数因子）。嗯，这是单个步骤发生的变化。

那么，在训练的整个过程会发生什么呢？



我们进一步简化分析，令 $\boldsymbol w^*$ 为未正则化的目标函数取得最小训练误差时的权重向量，即 $\boldsymbol w^* = \arg \min _{\boldsymbol w} J(\boldsymbol w)$， 并在 $\boldsymbol w^*$ 的邻域对目标函数做二次近似。<span style="color:red;">什么是做二次近似？</span>如果目标函数确实是二次的(如以均方误差拟合线性回归模型的情况)，则该近似是完美的。近似的 $\hat J(\boldsymbol \theta)$ 如下：<span style="color:red;">为什么是 $\boldsymbol \theta$？</span>


$$\begin{aligned}
\hat J(\boldsymbol \theta) = J(\boldsymbol w^*) + \frac{1}{2}(\boldsymbol w - \boldsymbol w^*)^\top \boldsymbol H (\boldsymbol w - \boldsymbol w^*),\tag{7.6}
\end{aligned}$$

其中 $\boldsymbol H$ 是 $J$ 在 $\boldsymbol w^*$ 处计算的 Hessian 矩阵(关于 $\boldsymbol w$)。<span style="color:red;">为什么是这个式子？</span>因为 $\boldsymbol w^*$ 被定义为最优，即梯度消失为 $0$，所以该二次近似中没有一阶项。同样地，因为 $\boldsymbol w^*$ 是 $J$ 的一个最优点，我们可以得出 $\boldsymbol H$ 是半正定的结论。<span style="color:red;">怎么得到这个结论的？虽然没看懂，但是感觉还是有道理的。</span>

当 $\hat J$ 取得最小时，其梯度

$$\begin{aligned}
\nabla_{\boldsymbol w} \hat{J}(\boldsymbol w) = \boldsymbol H (\boldsymbol w - \boldsymbol w^*)\tag{7.7}
\end{aligned}$$


为 $0$。

为了研究权重衰减带来的影响，我们在式 7.7 中添加权重衰减的梯度。现在我们探讨最小化正则化后的 $\hat J$。我们使用变量 $\tilde{\boldsymbol w}$ 表示此时的最优点:

$$
\alpha \tilde{\boldsymbol{w}}+\boldsymbol{H}\left(\tilde{\boldsymbol{w}}-\boldsymbol{w}^{*}\right)=0\tag{7.8}
$$
$$
(\boldsymbol{H}+\alpha \boldsymbol{I}) \tilde{\boldsymbol{w}}=\boldsymbol{H} \boldsymbol{w}^{*}\tag{7.9}
$$
$$
\tilde{\boldsymbol{w}}=(\boldsymbol{H}+\alpha \boldsymbol{I})^{-1} \boldsymbol{H} \boldsymbol{w}^{*}\tag{7.10}
$$


当 $\alpha$ 趋向于 $0$ 时，正则化的解 $\tilde{\boldsymbol w}$ 会趋向 $\boldsymbol w^*$。那么当 $\alpha$ 增加时会发生什么呢？因为 $\boldsymbol H$ 是实对称的，所以我们可以将其分解为一个对角矩阵 $\boldsymbol \Lambda$ 和一组特征向量的标准正交基 $\boldsymbol Q$，并且有 $\boldsymbol H = \boldsymbol Q \boldsymbol \Lambda \boldsymbol Q^\top$。将其应用于式 7.10，可得：


$$\begin{aligned}
\tilde \boldsymbol w &= ( \boldsymbol Q \boldsymbol \Lambda \boldsymbol Q^\top + \alpha \boldsymbol I)^{-1} \boldsymbol Q \boldsymbol \Lambda \boldsymbol Q^\top \boldsymbol w^* \\
&=  [ \boldsymbol Q( \boldsymbol \Lambda+ \alpha \boldsymbol I)  \boldsymbol Q^\top ]^{-1} \boldsymbol Q \boldsymbol \Lambda \boldsymbol Q^\top \boldsymbol w^* \tag{7.11~7.13}\\
\end{aligned}$$


我们可以看到权重衰减的效果是沿着由 $\boldsymbol H$ 的特征向量所定义的轴缩放 $\boldsymbol w^*$。具体来说，我们会根据 $\frac{\lambda_i}{\lambda_i + \alpha}$ 因子缩放与 $\boldsymbol H$ 第 $i$ 个特征向量对齐的 $\boldsymbol w^*$ 的分量。（不妨查看图 2.3 回顾这种缩放的原理，详见特征分解章节。）。

<center>

![](http://images.iterate.site/blog/image/20190721/mUVsTJNE2NTH.png?imageslim){ width=55% }

</center>

> 图 2.3　特征向量和特征值的作用效果。特征向量和特征值的作用效果的一个实例。在这里，矩阵 $\boldsymbol{A}$ 有两个标准正交的特征向量，对应特征值为 $\lambda_{1}$ 的 $\boldsymbol{v}^{(1)}$ 以及对应特征值为 $\lambda_{2}$ 的 $\boldsymbol{v}^{(2)}$。(左)我们画出了所有单位向量 $\boldsymbol{u} \in \mathbb{R}^{2}$ 的集合，构成一个单位圆。(右)我们画出了所有 $\boldsymbol{A} \boldsymbol{u}$ 点的集合。通过观察 $\boldsymbol{A}$ 拉伸单位圆的方式，我们可以看到它将 $\boldsymbol{v}^{(2)}$ 方向的空间拉伸了 $\lambda_{i}$ 倍。<span style="color:red;">是的是的，很清晰，很形象。</span>

沿着 $\boldsymbol H$ 特征值较大的方向(如 $\lambda_i \gg \alpha$)正则化的影响较小。而 $\lambda_i \ll \alpha$ 的分量将会收缩到几乎为零。这种效应如图 7.1 所示。


<center>

![](http://images.iterate.site/blog/image/20190713/jiUcnJ82NwwQ.png?imageslim){ width=55% }

</center>

> 7.1 $L^2$（或权重衰减）正则化对最佳 $\boldsymbol w$ 值的影响。实线椭圆表示没有正则化目标的等值线。虚线圆圈表示 $L^2$ 正则化项的等值线。在 $\tilde{\boldsymbol w}$ 点，这两个竞争目标达到平衡。目标函数 $J$ 的 Hessian 的第一维特征值很小。当从 $\boldsymbol w^*$ 水平移动时，目标函数不会增加得太多。因为目标函数对这个方向没有强烈的偏好，所以正则化项对该轴具有强烈的影响。正则化项将 $w_1$ 拉向零。而目标函数对沿着第二维远离 $\boldsymbol w^*$ 的移动非常敏感。对应的特征值较大，表示高曲率。因此，权重衰减对 $w_2$ 的位置影响相对较小。


只有在显著减小目标函数方向上的参数会保留得相对完好。在无助于目标函数减小的方向（对应 Hessian 矩阵较小的特征值）上改变参数不会显著增加梯度。这种不重要方向对应的分量会在训练过程中因正则化而衰减掉。


目前为止，我们讨论了权重衰减对优化一个抽象通用的二次代价函数的影响。这些影响具体是怎么和机器学习关联的呢？我们可以研究线性回归，它的真实代价函数是二次的，因此我们可以使用相同的方法分析。再次应用分析，我们会在这种情况下得到相同的结果，但这次我们使用训练数据的术语表述。线性回归的代价函数是平方误差之和：

$$
\begin{aligned}
(\boldsymbol X \boldsymbol w - \boldsymbol y)^\top (\boldsymbol X \boldsymbol w - \boldsymbol y).
\end{aligned}\tag{7.14}
$$

我们添加 $L^2$ 正则项后，目标函数变为

$$
\begin{aligned}
(\boldsymbol X \boldsymbol w - \boldsymbol y)^\top (\boldsymbol X \boldsymbol w - \boldsymbol y) + \frac{1}{2}\alpha \boldsymbol w^\top \boldsymbol w.
\end{aligned}\tag{7.15}
$$

这将普通方程的解从

$$
\begin{aligned}
\boldsymbol w = (\boldsymbol X^\top \boldsymbol X)^{-1} \boldsymbol X^\top \boldsymbol y
\end{aligned}\tag{7.16}
$$

变为

$$
\begin{aligned}
\boldsymbol w = (\boldsymbol X^\top \boldsymbol X + \alpha \boldsymbol I)^{-1} \boldsymbol X^\top \boldsymbol y .
\end{aligned}\tag{7.17}
$$

式 7.16 中的矩阵 $\boldsymbol X^\top\boldsymbol X$ 与协方差矩阵 $\frac{1}{m}\boldsymbol X^\top\boldsymbol X$ 成正比。$L^2$ 正则项将这个矩阵替换为式 7.17 中的 $(\boldsymbol X^\top \boldsymbol X + \alpha \boldsymbol I)^{-1}$ 这个新矩阵与原来的是一样的，不同的仅仅是在对角加了 $\alpha$。这个矩阵的对角项对应每个输入特征的方差。<span style="color:red;">这个矩阵的对角项对应每个输入特征的方差 是什么意思？</span>我们可以看到，$L^2$ 正则化能让学习算法"感知"到具有较高方差的输入 $\boldsymbol x$，因此与输出目标的协方差较小（相对增加方差）的特征的权重将会收缩。<span style="color:red;">什么是协方差较小的特征的权重？</span>


# 相关

- 《深度学习》花书
