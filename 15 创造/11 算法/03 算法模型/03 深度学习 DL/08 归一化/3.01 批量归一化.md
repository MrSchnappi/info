---
title: 3.01 批量归一化
toc: true
date: 2019-08-31
---
# 批量归一化

要明确相关的东西，比如到底有什么好。



- 学习率
- 权重衰减系数 <span style="color:red;">这个是什么？是指 ReLU 的 $a$ 吗，确认下。</span>
- Dropout比例
- 等

这些参数的选择会显著影响模型最终的训练效果。

批量归一化（Batch Normalization，BN）方法有效规避了这些复杂参数对网络训练产生的影响，在加速训练收敛的同时也提升了网络的泛化能力。<span style="color:red;">哇塞，对于 BN 有了新的认识。嗯，的确。</span>







三、批量正则化部分

No11 批量正则化论文：
这个没的说，必修课，不懂的化，会被鄙视成渣渣！
论文《Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift》，该论文网址是：
Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
No12 实例归一化论文：
时代不同了，批量归一化也升级了，赶紧学学新的归一化吧。
在对抗神经网络模型、风格转换这类生成式任务中，常用实例归一化取代批量归一化。因为，生成式任务的本质是——将生成样本的特征分布与目标样本的特征分布进行匹配。生成式任务中的每个样本都有独立的风格，不应该与批次中其他的样本产生太多联系。所以，实例归一化适用于解决这种基于个体的样本分布问题。详细说明见以下链接：
https://arxiv.org/abs/1607.08022
No13 ReNorm算法论文：
ReNorm算法与 BatchNorm 算法一样，注重对全局数据的归一化，即对输入数据的形状中的 N 维度、H维度、W维度做归一化处理。不同的是，ReNorm算法在 BatchNorm 算法上做了一些改进，使得模型在小批次场景中也有良好的效果。具体论文见以下链接：
https://arxiv.org/pdf/1702.03275.pdf
No14 GroupNorm算法论文：
GroupNorm算法是介于 LayerNorm 算法和 InstanceNorm 算法之间的算法。它首先将通道分为许多组（group），再对每一组做归一化处理。
GroupNorm算法与 ReNorm 算法的作用类似，都是为了解决 BatchNorm 算法对批次大小的依赖。具体论文见下方链接：
Group Normalization
No15 SwitchableNorm算法论文：
我们国人做产品都喜欢这么干！all in one ，好吧。既然那么多批量归一化的方法。来，来，来，我们来个 all in one吧。不服来辩，我这啥都有！
SwitchableNorm算法是将 BN 算法、LN算法、IN算法结合起来使用，并为每个算法都赋予权重，让网络自己去学习归一化层应该使用什么方法。具体论文见下方链接：
https://arxiv.org/abs/1806.10779
