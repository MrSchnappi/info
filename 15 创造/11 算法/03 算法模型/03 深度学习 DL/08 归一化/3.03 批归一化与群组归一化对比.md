---
title: 3.03 批归一化与群组归一化对比
toc: true
date: 2019-09-03
---

### 3.6.11 批归一化和群组归一化比较

| 名称                                           | 特点                                                                                                                                                                                                                                                                                                                              |
| ---------------------------------------------- |:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 批量归一化（Batch Normalization，以下简称 BN） | 可让各种网络并行训练。但是，批量维度进行归一化会带来一些问题——批量统计估算不准确导致批量变小时，BN 的误差会迅速增加。在训练大型网络和将特征转移到计算机视觉任务中（包括检测、分割和视频），内存消耗限制了只能使用小批量的 BN。<span style="color:red;">是呀，的确是一个问题，如果小的 batch 那么估算不准确，BN 的误差会增大，但是计算机视觉里面又只能用小 batch，那么还要使用 BN 吗？</span>                                                                                                    |
| 群组归一化 Group Normalization (简称 GN)       | GN 将通道分成组，并在每组内计算归一化的均值和方差。GN 的计算与批量大小无关，并且其准确度在各种批量大小下都很稳定。<span style="color:red;">这个 GN 是什么？原理是什么？怎么将通道分组的？为什么这样就与批量大小无关了？</span>                                                                                                                                                                                                                |
| 比较                                           | 在 ImageNet 上训练的 ResNet-50上，GN 使用批量大小为 2 时的错误率比 BN 的错误率低 10.6％ ；当使用典型的批量时，GN 与 BN 相当，并且优于其他标归一化变体。而且，GN 可以自然地从预训练迁移到微调。在进行 COCO 中的目标检测和分割以及 Kinetics 中的视频分类比赛中，GN 可以胜过其竞争对手，表明 GN 可以在各种任务中有效地取代强大的 BN。 |

<span style="color:red;">这么厉害的 GN 为什么之前没有听说过？要补充下，群组归一化的应用场景是什么？有什么缺点吗？为什么效果会比 BN 好？</span>
