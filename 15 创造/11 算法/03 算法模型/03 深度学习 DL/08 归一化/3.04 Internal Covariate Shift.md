---
title: 3.04 Internal Covariate Shift
toc: true
date: 2019-09-03
---

# 如何理解 Internal Covariate Shift？

深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。

Google 将这一现象总结为 Internal Covariate Shift，简称 ICS。 什么是 ICS 呢？

大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同。<span style="color:red;">条件概率一致，边缘概率不同，是什么情况有什么图像吗？</span>

大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了 covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。<span style="color:red;">没明白。</span>

**那么 ICS 会导致什么问题？**

简而言之，每个神经元的输入数据不再是“独立同分布”。

1. 上层参数需要不断适应新的输入数据分布，降低学习速度。
1. 下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。
1. 每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。

<span style="color:red;"> ICS 之前没有听说过。</span>



# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
