---
title: 4.02 Normalization 的通用框架和思想
toc: true
date: 2019-09-03
---
# Normalization 的通用框架与基本思想



我们以神经网络中的一个普通神经元为例。神经元接收一组输入向量 $\mathbf{x}=\left(x_{1}, x_{2}, \cdots, x_{d}\right)$ 通过某种运算后，输出一个标量值：

$$
y=f(\mathbf{x})
$$

由于 ICS 问题的存在， $\mathbf{x}$ 的分布可能相差很大。要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。



因此，以 BN 为代表的 Normalization 方法退而求其次，进行了简化的白化操作。基本思想是：在将 $\mathbf{x}$ 送给神经元之前，先对其做**平移和伸缩变换**， 将 $\mathbf{x}$ 的分布规范化成在固定区间范围的标准分布。



通用变换框架就如下所示：

$$
h=f\left(\mathbf{g} \cdot \frac{\mathbf{x}-\mu}{\sigma}+\mathbf{b}\right)
$$
我们来看看这个公式中的各个参数。



（1）$\mu$ 是**平移参数**（shift parameter）， $\sigma$ 是**缩放参数**（scale parameter）。通过这两个参数进行 shift 和 scale 变换：

$$
\hat{\mathbf{x}}=\frac{\mathbf{x}-\mu}{\sigma}
$$

得到的数据符合均值为 0、方差为 1 的标准分布。



（2） $\mathbf{b}$ 是**再平移参数**（re-shift parameter）， $\mathbf{g}$ 是**再缩放参数**（re-scale parameter）。将 上一步得到的 $\hat{\mathbf{x}}$ 进一步变换为：

$$
\mathbf{y}=\mathbf{g} \cdot \hat{\mathbf{x}}+\mathbf{b}
$$

最终得到的数据符合均值为 $\mathbf{b}$ 、方差为 $\mathbf{g}^{2}$ 的分布。



奇不奇怪？奇不奇怪？

说好的处理 ICS，第一步都已经得到了标准分布，第二步怎么又给变走了？



答案是——**为了保证模型的表达能力不因为规范化而下降**。

我们可以看到，第一步的变换将输入数据限制到了一个全局统一的确定范围（均值为 0、方差为 1）。下层神经元可能很努力地在学习，但不论其如何变化，其输出的结果在交给上层神经元进行处理之前，将被粗暴地重新调整到这一固定范围。

沮不沮丧？沮不沮丧？

难道我们底层神经元人民就在做无用功吗？


所以，为了尊重底层神经网络的学习结果，我们将规范化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是针对该神经元量身定制的一个确定范围（均值为 $\mathbf{b}$ 、方差为 $\mathbf{g}^{2}$ ）。rescale 和 reshift 的参数都是可学习的，这就使得 Normalization 层可以学习如何去尊重底层的学习结果。

除了充分利用底层学习的能力，另一方面的重要意义在于保证获得非线性的表达能力。Sigmoid 等激活函数在神经网络中有着重要作用，通过区分饱和区和非饱和区，使得神经网络的数据变换具有了非线性计算能力。而第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力。

那么问题又来了——

**经过这么的变回来再变过去，会不会跟没变一样？**



不会。因为，再变换引入的两个新参数 g 和 b，可以表示旧参数作为输入的同一族函数，但是新参数有不同的学习动态。在旧参数中， $\mathbf{x}$ 的均值取决于下层神经网络的复杂关联；但在新参数中， $\mathbf{y}=\mathbf{g} \cdot \hat{\mathbf{x}}+\mathbf{b}$ 仅由 $\mathbf{b}$ 来确定，去除了与下层计算的密切耦合。新参数很容易通过梯度下降来学习，简化了神经网络的训练。


那么还有一个问题——

**这样的 Normalization 离标准的白化还有多远？**

标准白化操作的目的是“独立同分布”。独立就不说了，暂不考虑。变换为均值为 $\mathbf{b}$ 、方差为 $\mathbf{g}^{2}$ 的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已。（所以，这个坑还有得研究呢！）







# 相关

- [详解深度学习中的 Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
