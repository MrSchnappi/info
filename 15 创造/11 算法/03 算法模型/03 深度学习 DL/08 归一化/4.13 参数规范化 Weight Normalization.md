---
title: 4.13 参数规范化 Weight Normalization
toc: true
date: 2019-09-03
---
# 参数规范化 Weight Normalization



前面我们讲的模型框架

$$
h=f\left(\mathbf{g} \cdot \frac{\mathbf{x}-\mu}{\sigma}+\mathbf{b}\right)
$$

中，经过规范化之后的 $\mathbf{y}$ 作为输入送到下一个神经元，应用以 $\mathbf{w}$ 为参数的 $f_{\mathrm{w}}(\cdot)$ 函数定义的变换。最普遍的变换是线性变换，即 $f_{\mathbf{w}}(\mathbf{x})=\mathbf{w} \cdot \mathbf{x}$。



BN 和 LN 均将规范化应用于输入的特征数据 $\mathbf{x}$ ，而 WN 则另辟蹊径，将规范化应用于线性变换函数的权重 $\mathbf{w}$ ，这就是 WN 名称的来源。


<center>

![mark](http://images.iterate.site/blog/image/20190902/XfCBvDAwVVIb.png?imageslim)


</center>

具体而言，WN 提出的方案是，**将权重向量 $\mathbf{w}$ 分解为向量方向 $\hat{\mathbf{v}}$ 和向量模 $g$ 两部分**：

$$
\mathbf{w}=g \cdot \hat{\mathbf{v}}=g \cdot \frac{\mathbf{v}}{\|\mathbf{v}\|}
$$


其中 $\mathbf{v}$ 是与 $\mathbf{w}$ 同维度的向量， $\|\mathbf{v}\|_{}$ 是欧氏范数，因此 $\hat{\mathbf{v}}$ 是单位向量，决定了 $\mathbf{w}$ 的方向；$g$ 是标量，决定了 $\mathbf{w}$ 的长度。由于 $\|\mathbf{w}\| \equiv|g|$ ，因此这一权重分解的方式将权重向量的欧氏范数进行了固定，从而实现了正则化的效果。



**乍一看，这一方法似乎脱离了我们前文所讲的通用框架？**



并没有。其实从最终实现的效果来看，异曲同工。我们来推导一下看。

$$
\begin{array}{c}{f_{\mathrm{w}}(W N(\mathbf{x}))=\mathbf{w} \cdot W N(\mathbf{x})=g \cdot \frac{\mathbf{v}}{\|\mathbf{v}\|} \cdot \mathbf{x}} \\ {=\mathbf{v} \cdot g \cdot \frac{\mathbf{x}}{\|\mathbf{v}\|}=f_{\mathrm{v}}\left(g \cdot \frac{\mathbf{x}}{\|\mathbf{v}\|}\right)}\end{array}
$$

对照一下前述框架：

$$
h=f\left(\mathbf{g} \cdot \frac{\mathbf{x}-\mu}{\sigma}+\mathbf{b}\right)
$$

我们只需令：

$$
\sigma=\|\mathbf{v}\|, \quad \mu=0, \quad \mathbf{b}=0
$$

就完美地对号入座了！



回忆一下，BN 和 LN 是用输入的特征数据的方差对输入数据进行 scale，而 WN 则是用 神经元的权重的欧氏范式对输入数据进行 scale。**虽然在原始方法中分别进行的是特征数据规范化和参数的规范化，但本质上都实现了对数据的规范化，只是用于 scale 的参数来源不同。**



另外，我们看到这里的规范化只是对数据进行了 scale，而没有进行 shift，因为我们简单地令 $\mu=0$. 但事实上，这里留下了与 BN 或者 LN 相结合的余地——那就是利用 BN 或者 LN 的方法来计算输入数据的均值 $\mu$ 。



WN 的规范化不直接使用输入数据的统计量，因此避免了 BN 过于依赖 mini-batch 的不足，以及 LN 每层唯一转换器的限制，同时也可以用于动态网络结构。







# 相关

- [详解深度学习中的 Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
