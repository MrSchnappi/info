---
title: DeepMind 通过删除神经元来理解深度学习
toc: true
date: 2019-11-17
---
编者按：深度学习算法近年来取得了长足的进展，也给整个人工智能领域送上了风口。但深度学习系统中分类器和特征模块都是自学习的，神经网络的可解释性成为困扰研究者的一个问题，人们常常将其称为黑箱。但理解深度神经网络的工作原理，对于解释其决策方式，并建立更强大的系统至关重要。



近日，DeepMind 发布了其关于神经网络可解释性的最新研究成果，他们通过删除网络中的某些神经元组，从而判定其对于整个网络是否重要。核心发现有如下两点：



- 可解释的神经元（例如“猫神经元”）并不比难以解释的神经元更重要。
- 泛化性良好的网络对于删除神经元的操作更具适应性。





**以下内容来自 DeepMind，人工智能头条翻译。**



深度神经网络由许多独立的神经元组成，这些神经元以复杂且反直觉的方式结合起来，进而解决各种具有挑战性的任务。这种复杂性赋予了神经网络强大的功能，但也使其成为一个令人困惑且不透明的黑箱。



理解深度神经网络的工作原理，对于解释其决策、建立更强大的系统至关重要。想象一下，在不了解各个齿轮如何装配的情况下，制造一块钟表的难度会有多大。在神经科学和深度学习领域中，理解神经网络的一种方法是研究单个神经元的作用，特别是那些容易解释的神经元。



我们即将在第六届国际学习表征会议（ICLR）上发表关于单一方向泛化重要性的研究，它采用了一种受实验神经科学启发的方法——探索损伤的影响——来确定深层神经网络中的小组神经元的重要性，以及更容易解释的神经元的重要性是否更高。



通过删除单个神经元和神经元组，我们测量了破坏网络对性能的影响。在实验中，我们有两个惊人的发现：



- 之前的许多研究都试图去理解容易解释的个体神经元（例如“猫神经元”，或者说深层网络中只有对猫的图像有反应的神经元），但是我们发现这些可解释的神经元并不比难以解释的神经元更重要。



- 与只能对已经见过的图像进行分类的网络相比，对未见过的图像也能正确分类的网络对神经元缺失有着更好的弹性。换句话说，泛化性良好的网络比泛化性差的网络对单方向的依赖性要小很多。



### **▌**“猫神经元”或许更容易解释，但是它们的重要性并不会更高



在神经科学和深度学习中，容易解释的神经元（“选择性”神经元）已经被广泛分析，它们只对单一输入类别的图像（例如狗）有积极回应。在深度学习中，这导致了研究者对猫神经元（cat neurons）、情感神经元（sentiment neurons）和括号神经元（parentheses neurons）的重视。然而，与大多数具有低选择性、更令人费解、难以解释的活性的神经元相比，这些为数不多的具有高选择性神经元的相对重要性仍然未知。





![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjwQNNSqw3NqzzD9Xt9SQMG3VGic2AjFLicchWMGticp27I4zHV9Z8P14ibhyrib5icFetEtfnsa39o7hP8g/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**\*与那些对看似随机的图像集作出积极或消极回应的令人困惑的神经元相比，具有清晰回应模式（比如只对“狗”积极回应，对其他一切类别消极回应）的神经元更容易解释。***



为了评估神经元的重要性，我们测量了删除神经元后，神经网络在图像分类任务中的性能变化。如果一个神经元是非常重要的，删除它应该会产生严重的后果，而且网络性能会大大降低，而删除一个不重要的神经元应该没有什么影响。神经科学家通常进行类似的实验，尽管他们不能达到这些实验所必需的细粒度精度，但是在人工神经网络中则很容易实现。

![img](https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwQNNSqw3NqzzD9Xt9SQMG3kQ2aibPoGTyzTscZ9cMStXeEk1zfLhHK57fo3gnm0NlLUIrcl0hOw7w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)删除神经元对简单神经网络影响的概念图。颜色越深，代表神经元越活跃。你可以在原网页上尝试单击隐藏层神经元删除它们，并查看输出神经元的活动变化。请注意，仅删除一个或两个神经元对输出的影响很小，而删除大多数神经元的影响很大，并且某些神经元比其他神经元更重要！



令人惊讶的是，我们发现选择性和重要性之间没有什么关系。换句话说，“猫神经元”并不比令人困惑的神经元更重要。这一发现与神经科学最近的研究成果相呼应，后者已经证明，令人困惑的的神经元实际上可以提供相当多的信息。为了理解深度神经网络，我们的研究不能只局限于最容易解释的神经元。

![img](https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjwQNNSqw3NqzzD9Xt9SQMG3CIc4LWFy0libMhRlgsKUssRQyicl2pOjibkzicVh5dp36rJlpoANKezBgw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**\*虽然“猫神经元”可能更容易记解释，但它们并不比令人困惑且没有明显偏好的神经元更加重要。***



### **▌**泛化能力更强的网络更不容易崩溃



虽然我们希望创建智能系统，但是只有当这个系统能够泛化到新的场景时，我们才能称之为智能系统。例如，如果一个图像分类网络只能对它见过的特定的狗的图像进行分类，却认不出同一只狗的最新图像时，这个网络就是无用的。这些系统只有在对新的实例进行智能分类时，才算是有作用的。



伯克利、Google Brain、DeepMind 最近合作发表的一篇论文在 ICLR 2017 上获得了最佳论文。该论文表明，深度网络可以简单地记住每一幅图像，而不是以更人性化的方式学习（例如，理解“狗”的抽象概念）。



然而，关于神经网络是否学习到了决定泛化能力的解，我们一直没有明确的答案。通过逐步删除越来越大的神经元群，我们发现，相比简单地记忆先前在训练中看到的图像的网络，泛化良好的网络对删除神经元的鲁棒性强得多。换句话说，泛化能力更强的网络更不容易崩溃（尽管这种情况可能发生）。





**\*随着越来越多的神经元群被删除，泛化良好的网络的性能下降速度明显更慢。***



通过这种方式测量神经网络的鲁棒性，我们可以评估这个网络是否在利用我们不希望的记忆能力在“作弊”。理解网络在记忆时如何是变化的，将有助于我们建立泛化能力更好的新网络。



### **▌**受神经科学启发的分析方法



这些发现证明了，使用实验神经科学启发的技术可以帮助我们理解神经网络的能力。使用这些方法，我们发现高选择性的独立神经元并不比非选择性神经元更重要，并且泛化良好的网络比简单地记忆训练数据的网络对独立神经元的依赖性更小。这些结果暗示，独立神经元的重要性可能远不如我们认为的那么重要。



通过解释所有神经元的作用，而不仅仅是那些容易理解的神经元，我们希望更好地理解神经网络的内部工作原理，并且利用这种理解来构建更智能和更通用的系统。



阅读完整论文：https://arxiv.org/abs/1803.06959




# 相关

- [DeepMind新成果：通过删除神经元来理解深度学习](https://mp.weixin.qq.com/s?__biz=MzAwNDI4ODcxNA==&mid=2652246793&idx=1&sn=09e5370aa24c8708d7a405d1813b3a96&scene=21#wechat_redirect)
