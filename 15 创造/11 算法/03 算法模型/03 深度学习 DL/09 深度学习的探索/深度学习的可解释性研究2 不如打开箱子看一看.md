---
title: 深度学习的可解释性研究2 不如打开箱子看一看
toc: true
date: 2019-11-17
---
# 深度学习的可解释性研究2 不如打开箱子看一看

在上一节中我们介绍了深度学习可解释性的三种方法：1. 隐层分析法，2. 敏感性分析法 3. 代理/替代模型法。在这一节中我们主要介绍第一种方法：隐层分析法。

## **黑箱真的是黑箱吗？——深度学习的物质组成视角**

通过上一节的介绍我们也了解到，深度学习的黑箱性主要来源于其高度非线性性质，每个神经元都是由上一层的线性组合再加上一个非线性函数的得到，我们无法像理解线性回归的参数那样通过非常solid的统计学基础假设来理解神经网络中的参数含义及其重要程度、波动范围。但实际上我们是知道这些参数的具体值以及整个训练过程的，所以**神经网络模型本身其实并不是一个黑箱，其黑箱性在于我们没办法用人类可以理解的方式理解模型的具体含义和行为，**而神经网络的一个非常好的性质在于神经元的分层组合形式，这让我们可以用物质组成的视角来理解神经网络的运作方式。比如如下图所示，人体的组成过程是从分子-细胞-组织-器官-系统-人体：

![img](https://pic4.zhimg.com/80/v2-870d78f6a7e7097185d2c373544c8f8b_hd.jpg)人体的组成结构示意

而通过一些对神经网络隐层的可视化我们也发现：比如下图的一个人脸识别的例子，神经网络在这个过程中先学到了边角的概念，之后学到了五官，最后学到了整个面部的特征。

![img](https://pic4.zhimg.com/80/v2-76828eb5958db1f07224042fbf5b9377_hd.jpg)

(以上内容参考了@YJango在**如何简单形象又有趣地讲解神经网络是什么？**中的回答，侵删)

如果我们能够用一些方法来帮助我们理解这个从低级概念到高级概念的生成过程，那么就离理解神经网络的具体结构就近了很多。而这也可以逐渐帮助我们完成一个“祛魅”的过程，将调参的魔法真正变成一项可控、可解释的过程。

要理解这个概念的生成过程很重要的一点就是要**研究隐层的概念表示**，在接下来的部分中我将给大家介绍业界关于隐层分析方法的几个研究工作。

## **模型学到了哪些概念？**

要理解神经网络中每层都学到了哪些概念一个非常直观的方法就是通过对隐层运用一些可视化方法来将其转化成人类可以理解的有实际含义的图像，这方面一个非常具有代表性的一个工作就是14年ECCV的一篇经典之作：**《Visualizing and Understanding Convolutional Networks》**，这篇文章主要利用了反卷积的相关思想实现了特征可视化来帮助我们理解CNN的每一层究竟学到了什么东西。我们都知道典型的CNN模型的一个完整卷积过程是由卷积-激活-池化（pooling）三个步骤组成的。而如果想把一个CNN的中间层转化成原始输入空间呢？我们就需要经过反池化-反激活-反卷积这样的一个逆过程。整个模型的结构如下图所示：



![img](https://pic2.zhimg.com/80/v2-512837d70b3e67b37014e78dfa033709_hd.jpg)

## **反池化：**

反池化其实很好理解，以下面的图片为例，左图可以表示是池化（pooling）过程，右图表示反池化（unpooling）过程，池化过程中我们将3*3的一个pooling块中的最大值取出，而反池化则是将pooling后的值恢复成3*3的像素单元，由于我们现在只有一个激活值， 所以只将**该激活值对应原pooling块中位置的值还原回去**，其他的值设定成0。所以在max-pooling的时候，我们不光要知道pooling值，同时也要记录下**pooling值的对应位置**，比如下图pooling值的位置就是(0,1)。

![img](https://pic2.zhimg.com/80/v2-d95603eb8bd9ac270db195587f3bca9d_hd.jpg)反池化过程

**反激活：**

在典型的CNN模型中，我们一般使用Relu作为激活函数，而反激活的值和实际的激活值没有任何区别：只保留正数，其余值为0即可。

## **反卷积：**

反卷积的过程其实非常有意思，其实反卷积这个名字多多少少有些误人子弟，和真正的反卷积并没有多大关系，真实的含义应该是**转置卷积**（Transposed Convolution），CNN模型的卷积过程本质上来讲和一般的神经网络没有任何区别（只不过将一些共用参数组合成了一个滤波器的形式），都可以转变成一个**矩阵乘法的操作**（只不过对CNN模型来说是一个参数重复度很高的稀疏矩阵），我们不妨将这个稀疏矩阵表示为 ![[公式]](https://www.zhihu.com/equation?tex=C) ，那么后一层和前一层的关系就可以表示为：

![[公式]](https://www.zhihu.com/equation?tex=CX%5E%7Bl%7D+%3D+X%5E%7Bl%2B1%7D)

而在反向传播的过程中， ![[公式]](https://www.zhihu.com/equation?tex=C%5ET) 往往可以表示为**卷积层对输入层的梯度**，也就是说通过对卷积层进行适当的补0操作，再用原始的卷积核转置之后的卷积核进行卷积操作，就可以得到相应的梯度矩阵与当前卷积层的乘积，而我们在这里使用反池化-反激活之后的特征（其中包含了大部分为0的数值）进行该操作其实表征了**原始输入对池化之后的特征的影响**，因为在反激活过程中保证了所有值非负因此反卷积的过程中符号不会发生改变。

![img](https://pic3.zhimg.com/80/v2-09cd9c88f498074209c203039d1f8c36_hd.jpg)反卷积是个上采样过程

通过上面的介绍我们其实可以明白，这个反卷积的方法之所以能够成功地将CNN的隐层可视化出来，**一个关键就在于通过反激活-反池化的过程，我们屏蔽掉了很多对当前层的激活值没有实际作用的输入层的影响将其归为0，通过反卷积操作就得到了仅对当前层有实际贡献的输入层的数值——将其作为该层的特征表示**。**因为我们最后得到的这个特征表示和原输入空间的大小是一致的，其数值表示也对应着原始空间的像素点，所以在一定程度上，这是我们可以理解的一个特征表示。**



![img](https://pic2.zhimg.com/80/v2-b7dd4ff1e0a8e947aab9eea6799a42dd_hd.jpg)

从实验结果可以看出来，第二层对应着一些边角或色彩特征，第三层对应着纹理特征，第四层对应着一些如狗脸、车轮这样的局部部位，第五层则对整体的物体有较强的识别能力。



通过上面这篇论文的工作，我们可以大致地用肉眼来判断神经网络学到的概念类型，但如果能识别一些语义概念的话对我们来说可能更有意义，在这方面一个非常有代表性的工作是在CVPR 2017上发表**《Network Dissection: Quantifying Interpretability of Deep Visual Representations》**，这篇文章提出了一种网络切割(Network Dissection)的方法来提取CNN的概念表示。

所谓的网络切割(Network Dissection)方法其实分为三个步骤：

\1. 识别一个范围很广的人工标注的视觉语义概念数据集

\2. 将隐层变量对应到这些概念表示上

\3. 量化这些隐层-概念对的匹配程度

为了获得大量的视觉语义概念数据，研究人员收集了来自不同数据源的分层语义标注数据（包括颜色、材质、材料、部分、物体、场景这几个层次），如下图所示



![img](https://pic4.zhimg.com/80/v2-116d8dde6dd5250d5abb23e61fd0dd4f_hd.jpg)

而如何将隐层变量对应到这些概念表示上并获得隐层-概念对的匹配程度呢，本文提出了如下的方法：

对于每个输入图像x，获取每个隐层k的activation map ![[公式]](https://www.zhihu.com/equation?tex=A_k%28x%29) （其实也就是feature map），这样就可以得到隐层k激活值的分布，对于每个隐层k，我们都可以找到一个 ![[公式]](https://www.zhihu.com/equation?tex=T_k) 使得 ![[公式]](https://www.zhihu.com/equation?tex=P%28A_k+%3E+T_k%29+%3D+0.005) ，这个 ![[公式]](https://www.zhihu.com/equation?tex=T_k) 可以作为接下来判断区域是否激活的一个标准。

为了方便对比低分辨率的卷积层和输入层的概念激活热图 ![[公式]](https://www.zhihu.com/equation?tex=L_c%28x%29) （其实就是标注出了相关概念在图像中的代表区域），我们将低分辨率的卷积层的特征图 ![[公式]](https://www.zhihu.com/equation?tex=A_k%28x%29) 通过插值的方法扩展为和原始图片尺寸一样大的图像 ![[公式]](https://www.zhihu.com/equation?tex=S_k%28X%29) 。

之后再建立一个二元分割 ![[公式]](https://www.zhihu.com/equation?tex=M_k%28x%29%3DS_k%28x%29%5Cge+T_k) ，这样就得到了所有被激活的区域，而我们通过将 ![[公式]](https://www.zhihu.com/equation?tex=M_k%28x%29) 和输入层的概念激活热图 ![[公式]](https://www.zhihu.com/equation?tex=L_c%28x%29) 作对比，这样就可以获得隐层-概念对的匹配程度：

![img](https://pic2.zhimg.com/80/v2-b333ca2f5c5fb61158af8dfb361d4b1d_hd.jpg)

可以发现**如果匹配度高的话，那么分子就比较大（交叉范围大），分母就比较小（合并范围小），我们通过和颜色、材质、材料、部分、物体、场景不同层次的概念作匹配就能得到隐层学到的概念层次了**，这个模型的结构如下图所示：

![img](https://pic1.zhimg.com/80/v2-df0e07b3454c76c601ad2c4082191194_hd.jpg)Network Dissection的模型结构

![img](https://pic4.zhimg.com/80/v2-67346da9700155bf97fc60aeb17035ef_hd.jpg)模型在AlexNet上的实验结果

从实验结果中我们也可以发现随着层数的增加，神经网络学到的概念类型也逐渐变得高级，比如在AlexNet中，前面的卷积层对颜色、材质的识别力较强，后面的卷积层对物体、场景的识别力较强。特别是对物体的识别上，后面的卷积层有突出的优势。

## **低级到高级=泛化到特化？**

当然从低级概念到高级概念的一个过程中总是会伴随着一个非常有意思的现象：泛化性逐渐降低，特化性逐渐升高。比如在细胞层次上，人类和其他动物的区别比较小，这个层次的泛化性就高，但到组织器官层次区别就比较大，这个层次的特化性就高。Bengio团队在2014年发表的一篇工作**《How transferable are features in deep neural networks》**就是通过研究特征的可迁移性来对这个从泛化的特化的过程进行评估。

**特征在迁移任务上的表现往往是评价特征泛化性能的一个非常好的依据。**在迁移学习中，我们首先基于基础数据训练一个基础网络，然后将特征改换到另一个任务上，如果特征是具备泛化性的，那么其在迁移任务中应该也是适用的。在这个工作中，作者将1000个ImageNet的分类分成了两个组，每个组个包含大约500个分类和645000个样本。然后利用这两组数据各训练一个八层的卷积网络baseA和baseB，然后分别取第1到第7个卷积层训练几个新的网络，以第3层为例：

自我复制网络（selffer network）B3B，前三层从baseB上复制并冻结。剩余的5个卷积层随机初始化并在数据集B上训练，这个网络作为控制组

一个迁移网络（transfer network）A3B：前三层从baseA上复制并冻结。剩余的5个卷积层随机初始化并在数据集B上训练。如果A3B的任务表现和baseB一样好，那么就说明第三层的特征至少对于B任务来讲是泛化的，如果不如baseB，那么说明第三层特征就是特化的。

一个自我复制网络B3B+，网络结构和B3B一样，但所有层都是可学习的

一个迁移网络A3B+，网络结构和A3B一样，但所有层都是可学习的

这些网络的结构图如下图所示：

![img](https://pic3.zhimg.com/80/v2-485b7f0242927311349622bc170fd27e_hd.jpg)

![img](https://pic2.zhimg.com/80/v2-95c6b85e074795f74ea365c12697e4d5_hd.jpg)

![img](https://pic2.zhimg.com/80/v2-e941cf710320e28aa617b82ea41d514d_hd.jpg)

而从实验结果来看，transferAnB的随着层数的增加性能在不断下降（泛化降低，特化提升，这印证了我们对泛化特化性质随层数变化的基本判断），而控制组的selfferBnB的性能出现了先下降后上升的现象（泛化和特化都不足够的中间层既缺乏可学习性，特征的描述性又不够强，因而出现了性能下降的现象），transferBnB+和transferAnB+一直维持着比较好的性能，但其中transferAnB+的性能确是最好的，特征在迁移任务上表现出来的优势其实也对应了我们在上一节中讲的**模型本身也意味着知识**。

## **真的需要那么多层吗？**

对于神经网络来说，隐层的数量永远都是一个玄学，我们如何理解隐层的数量和模型性能之间的关系呢？Bengio在2016年还做过一个工作《**Understanding intermediate layers using linear classifier probes**》。这篇文章的思路非常简单，就是通过在每个隐层中添加一个线性探针来测试隐层的表征性能。什么是线性探针呢？也很简单，就是以每个隐藏层为输入，判别的label为输出建立一个逻辑回归模型，通过评估模型的准确率我们就能得到隐层在整个训练过程中以及训练结束之后表征性能的变化。

![img](https://pic3.zhimg.com/80/v2-673b1ffd186389f3be805eea0c64304e_hd.jpg)

![img](https://pic3.zhimg.com/80/v2-76657ada750b411a3217335fd9b33b1a_hd.jpg)

通过32个隐层在二分数据上的实验我们可以发现随着隐层的增加表征性能不断提高，但提高的比率也逐渐趋于缓慢。

![img](https://pic1.zhimg.com/80/v2-8bf019d8a4e1e1173ef64b7f23295700_hd.jpg)

在基于Minist数据训练的CNN模型上，经过10个周期的训练，第一个卷积层的表征性能提升非常明显，但之后的卷积层并没有很明显的提升。

## **小结**

本文中我们主要介绍了四个在隐层分析上有代表性的工作，这类方法在神经网络可解释性的研究中向我们揭示了隐层性质的变化和概念生成的过程，在之后要讲到的敏感性分析方法中，也会不可避免地涉及对隐层的分析。如果大家对这几个工作感兴趣，可以找到相关的文献做更深入的阅读，如果有相关的文献推荐，欢迎私信或者回复。




# 相关

- [深度学习的可解释性研究（二）——不如打开箱子看一看]
- Matthew D. Zeiler, Rob Fergus. Visualizing and Understanding Convolutional Networks[J]. 2013, 8689:818-833.
- David Bau, Bolei Zhou, Aditya Khosla, et al. Network Dissection: Quantifying Interpretability of Deep Visual Representations[J]. 2017:3319-3327.
- Yosinski J, Clune J, Bengio Y, et al. How transferable are features in deep neural networks?[J]. Eprint Arxiv, 2014, 27:3320-3328.
- Alain G, Bengio Y. Understanding intermediate layers using linear classifier probes[J]. 2016.
