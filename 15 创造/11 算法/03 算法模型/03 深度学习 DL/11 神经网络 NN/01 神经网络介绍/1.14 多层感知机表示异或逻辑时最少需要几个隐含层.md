---
title: 1.14 多层感知机表示异或逻辑时最少需要几个隐含层
toc: true
date: 2019-08-31
---

## 多层感知机表示异或逻辑时最少需要几个隐含层（仅考虑二元输入）？


首先，我们先来分析一下具有零个隐藏层的情况（等同于逻辑回归）能否表示异或运算。仅考虑二元输入的情况，设 $X$ 取值为 $0$ 或 $1$，$Y$ 的取值也为 $0$ 或 $1$，$Z$ 为异或运算的输出。也就是，当 $X$ 和 $Y$ 相同时，异或输出为 $0$，否则为 $1$，具体的真值表如表 9.1所示:

![](http://images.iterate.site/blog/image/20190413/IS6qkgy9KQsP.png?imageslim){ width=55% }


回顾逻辑回归的公式：


$$
Z=\text { sigmoid }(A X+B Y+C)\tag{9.1}
$$


其中 Sigmoid 激活函数是单调递增的：

- 当 $A X+B Y+C$ 的取值增大时，$Z$ 的取值也增大；
- 当 $A X+B Y+C$ 的取值减少时，Z的取值也减小。

而 $A X+B Y+C$ 对于 X 和 Y 的变化也是单调的：

- 当参数 $A$ 为正数时， $A X+B Y+C$ 以及 $Z$ 的取值随 $X$ 单调递增
- 当 $A$ 取负数时， $A X+B Y+C$ 和 $Z$ 随 $X$ 单调递减；当参数 $A$ 为 $0$ 时，$Z$ 的值与 $X$ 无关。

观察异或运算的真值表：

- 当 Y=0时，将 X 的取值从 0 变到 1 将使输出 Z 也从 0 变为 1，说明此时 Z 的变化与 X 是正相关的，需要设置 A 为正数；
- 而当 Y=1时，将 X 的取值从 0 变为 1 将导致输出 Z 从 1 变为 0，此时 Z 与 X 是负相关的，需要设置 A 为负数，与前面矛盾。

因此，采用逻辑回归（即不带隐藏层的感知机）无法精确学习出一个输出为异或的模型表示。<span style="color:red;">是的。</span>

然后，我们再考虑具有一个隐藏层的情况。

事实上，通用近似定理告诉我们，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数的隐藏层，当给予网络足够数量的隐藏单元时，可以以任意精度近似任何从一个有限维空间到另一个有限维空间的波莱尔可测函数。<span style="color:red;">为什么呢？通用近似定理是什么？波莱尔可测函数是什么？还是要补充下的。</span>

对通用近似定理的证明并不在面试的要求范围，不过可以简单认为我们常用的激活函数和目标函数是通用近似定理适用的一个子集，因此多层感知机的表达能力是非常强的，关键在于我们是否能够学习到对应此表达的模型参数。<span style="color:red;">嗯。</span>


在这里，我们还并不涉及模型参数的学习，而是通过精心设计一个模型参数以说明包含一个隐含层的多层感知机就可以确切地计算异或函数。

<center>

![](http://images.iterate.site/blog/image/20190413/yoElf68TpNnd.png?imageslim){ width=55% }

</center>

如图 9.1所示。图中有 $Z_1$ 和 $Z_2$ 两个隐藏单元：

- 在隐藏单元 $Z_1$ 中，$X$ 和 $Y$ 的输入权重均为 $1$，且偏置为 $1$，等同于计算 $H_{1}=X+Y-1$ ，再应用 ReLU 激活函数 $\max \left(0, H_{1}\right)$ ，其真值表如表 9.2所示。
- 同理，隐藏单元 $Z_2$ 的输入权重均为 $−1$，偏置为 $−1$，真值表如表 9.3所示。


![](http://images.iterate.site/blog/image/20190413/74SigvIcrSCQ.png?imageslim){ width=55% }

![](http://images.iterate.site/blog/image/20190413/oBIfyx7QwFQN.png?imageslim){ width=55% }

可以看到，第一个隐藏单元在 $X$ 和 $Y$ 均为 $1$ 时激活，第二个隐藏单元在 $X$ 和 $Y$ 均为 $0$ 时激活，最后再将两个隐藏单元的输出做一个线性变换即可实现异或操作，如表 9.4所示。

![](http://images.iterate.site/blog/image/20190413/HDw3JSsBev2w.png?imageslim){ width=55% }



# 相关

- 《百面机器学习》
