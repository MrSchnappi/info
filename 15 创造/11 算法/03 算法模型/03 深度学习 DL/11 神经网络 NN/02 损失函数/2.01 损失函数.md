---
title: 2.01 损失函数
toc: true
date: 2019-08-31
---
# 可以补充进来的

- 交叉熵补充下。
- 均方误差在 20 世纪 80 年代和 90 年代流行，但逐渐被交叉熵损失替代，并且最大似然原理的想法在统计学界和机器学习界之间广泛传播。使用交叉熵损失大大提高了具有 sigmoid  和 softmax 输出的模型的性能，而当使用均方误差损失时会存在饱和和学习缓慢的问题。

# 损失函数


深度神经网络设计中的一个重要方面是损失函数的选择。


在大多数情况下，我们的参数模型定义了一个分布 $p(\boldsymbol y\mid\boldsymbol x;\boldsymbol \theta)$ 并且我们简单地使用最大似然原理。这意味着我们使用训练数据和模型预测间的交叉熵作为损失函数。

有时，我们使用一个更简单的方法，不是预测 $\boldsymbol y$ 的完整概率分布，而是仅仅预测在给定 $\boldsymbol x$ 的条件下 $\boldsymbol y$ 的某种统计量。某些专门的损失函数允许我们来训练这些估计量的预测器。<span style="color:red;">想知道是什么统计量？</span>

用于训练神经网络的完整的损失函数，通常在我们这里描述的基本损失函数的基础上结合一个正则项。我们已经在第 5.2.2 节中看到正则化应用到线性模型中的一些简单的例子。用于线性模型的权重衰减方法也直接适用于深度神经网络，而且是最流行的正则化策略之一。用于神经网络的更高级的正则化策略将在第七章中讨论。







# 相关

- 《深度学习》花书
