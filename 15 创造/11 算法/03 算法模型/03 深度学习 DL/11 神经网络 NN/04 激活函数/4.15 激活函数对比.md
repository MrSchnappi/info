---
title: 4.15 激活函数对比
toc: true
date: 2019-08-31
---


## 为什么 Sigmoid 和 Tanh 激活函数会导致梯度消失的现象？


### Sigmoid

<center>

![](http://images.iterate.site/blog/image/20190413/8zDcTNPl1c2U.png?imageslim){ width=55% }

</center>

Sigmoid 激活函数的曲线如图 9.7 所示。它将输入 $z$ 映射到区间 $(0,1)$ ，当 $z$ 很大时，$f(z)$ 趋近于 $1$；当 $z$ 很小时，$f(z)$ 趋近于 $0$。其导数 $f^{\prime}(z)=f(z)(1-f(z))$ 在 $z$ 很大或很小时都会趋近于 $0$，造成梯度消失的现象。<span style="color:red;">是的。</span>

### Tanh

<center>

![](http://images.iterate.site/blog/image/20190413/48FKFkwkewK8.png?imageslim){ width=55% }

</center>

Tanh 激活函数的曲线如图 9.8所示。当 $z$ 很大时，$f(z)$ 趋近于 $1$；当 $z$ 很小时，$f(z)$ 趋近于 $−1$。其导数 $f^{\prime}(z)=1-(f(z))^{2}$ 在 $z$ 很大或很小时都会趋近于 $0$，同样会出现“梯度消失”。实际上，Tanh 激活函数相当于 Sigmoid 的平移：

$$
\tanh (x)=2 \operatorname{sigmoid}(2 x)-1 \tag{9.9}
$$

<span style="color:red;">好吧。</span>


## ReLU系列的激活函数相对于 Sigmoid 和 Tanh 激活函数的优点是什么？它们有什么局限性以及如何改进？


### 优点

1. 从计算的角度上，Sigmoid 和 Tanh 激活函数均需要计算指数，复杂度高，而 ReLU 只需要一个阈值即可得到激活值。
2. ReLU 的非饱和性可以有效地解决梯度消失的问题，提供相对宽的激活边界。<span style="color:red;">相对宽的激活边界是什么意思？</span>
3. ReLU 的单侧抑制提供了网络的稀疏表达能力。<span style="color:red;">为什么单侧抑制提供了网络的稀疏表达能力？网络的稀疏表达能力还有那些因素吗？</span>


### 局限性

ReLU的局限性在于其训练过程中会导致神经元死亡的问题。这是由于函数 $f(z)=\max (0, z)$ 导致负梯度在经过该 ReLU 单元时被置为 0，且在之后也不被任何数据激活，即流经该神经元的梯度永远为 0，不对任何数据产生响应。

在实际训练中，如果学习率（Learning Rate）设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败。<span style="color:red;">嗯，不可逆的死亡。为什么学习率较大，会导致超过一定比例的神经元不可逆的死亡？多少比例才会导致训练失败？</span>

为解决这一问题，人们设计了 ReLU 的变种 Leaky ReLU（LReLU），其形式表示为：

$$
f(z)=\left\{\begin{array}{cl}{z,} & {z>0} \\ {a z,} & {z \leqslant 0}\end{array}\right.\tag{9.10}
$$

ReLU 和 LReLU 的函数曲线对比如图 9.9 所示：

<center>

![](http://images.iterate.site/blog/image/20190413/gtJ0WK6T3kOc.png?imageslim){ width=55% }

</center>

LReLU 与 ReLU 的区别在于，当 $z<0$ 时其值不为 $0$，而是一个斜率为 $a$ 的线性函数，一般 $a$ 为一个很小的正常数，这样既实现了单侧抑制，又保留了部分负梯度信息以致不完全丢失。但另一方面，$a$ 值的选择增加了问题难度，需要较强的人工先验或多次重复训练以确定合适的参数值。<span style="color:red;">是的呀，这个 $a$ 要怎么定？平时默认的是多少？这个默认值是怎么来的？怎么知道自己的网络用偏大一点的 $a$ 好还是偏小一点的 $a$ 好？而且，这个 $a$ 能够使网络结果复现吗？</span>


基于此，有一些 ReLU 的变种：

- 参数化的 PReLU（Parametric ReLU）应运而生。<span style="color:red;">哇塞，之前好像没有听说过这个 PReLU。嗯，要总结下。</span> 它与 LReLU 的主要区别是将负轴部分斜率 $a$ 作为网络中一个可学习的参数，进行反向传播训练，与其他含参数网络层联合优化。<span style="color:red;">这个是怎么实现的？效果怎么样？</span>

- 而另一个 LReLU 的变种增加了“随机化”机制，具体地，在训练过程中，斜率 $a$ 作为一个满足某种分布的随机采样；测试时再固定下来。Random ReLU（RReLU）在一定程度上能起到正则化的作用。<span style="color:red;"> $a$ 作为一个满足某种分布的采样？什么样的分布？分布的参数怎么定？而且，为什么能在一定程度上起到正则化的作用？</span>

- 关于 ReLU 系列激活函数，更多详细内容及实验性能对比可以参考相关论文[18]。<span style="color:red;">在真实的场景中，这些不同的 ReLU 可以对最后的结果影响有多大？而且，难道我们选用的时候就是这么试吗？有没有什么理论基础说某种 ReLU 就是最好的？</span>






# 相关

- 《百面机器学习》
