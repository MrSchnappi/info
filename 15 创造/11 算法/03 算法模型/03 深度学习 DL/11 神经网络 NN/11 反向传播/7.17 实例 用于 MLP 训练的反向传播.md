---
title: 7.17 实例 用于 MLP 训练的反向传播
toc: true
date: 2019-08-31
---


# 实例 用于 MLP 训练的反向传播


作为一个例子，我们利用反向传播算法来训练多层感知机。

这里，我们考虑一个具有单个隐藏层的非常简单的多层感知机。为了训练这个模型，我们将使用小批量随机梯度下降算法。反向传播算法用于计算单个小批量上的代价的梯度。具体来说，我们使用训练集上的一小批量实例，将其规范化为一个设计矩阵 $\boldsymbol X$ 以及相关联的类标签向量 $\boldsymbol y$。网络计算隐藏特征层 $\boldsymbol H=\max\{0, \boldsymbol X\boldsymbol W^{(1)}\}$。为了简化表示，我们在这个模型中不使用偏置。假设我们的图语言包含 $relu$ 操作，该操作可以对 $\max\{0,\boldsymbol Z\}$ 表达式的每个元素分别进行计算。类的非归一化对数概率的预测将随后由 $\boldsymbol H\boldsymbol W^{(2)}$ 给出。假设我们的图语言包含 $cross_entropy$ 操作，用以计算目标 $\boldsymbol y$ 和由这些未归一化对数概率定义的概率分布间的交叉熵。所得到的交叉熵定义了代价函数 $J_\text{MLE}$。最小化这个交叉熵将执行对分类器的最大似然估计。然而，为了使得这个例子更加真实，我们也包含一个正则项。总的代价函数为


$$
J = J_{\text{MLE}} + \lambda \left ( \sum_{i, j} \left (W_{i, j}^{(1)} \right )^2 + \sum_{i, j} \left (W_{i, j}^{(2)} \right)^2 \right )
$$

包含了交叉熵和系数为 $\lambda$ 的权重衰减项。它的计算图在图 6.11 中给出。


<center>

![](http://images.iterate.site/blog/image/20190713/nOGgJShjJH0w.png?imageslim){ width=55% }

</center>

> 图 6.11 用于计算代价函数的计算图，这个代价函数是使用交叉熵损失以及权重衰减训练我们的单层 MLP 示例所产生的。

<span style="color:red;">厉害呀，很清晰。</span>

这个示例的梯度计算图实在太大，以致绘制或者阅读都将是乏味的。这显示出了反向传播算法的优点之一，即它可以自动生成梯度，而这种计算对于软件工程师来说需要进行直观但冗长的手动推导。

我们可以通过观察图 6.11 中的正向传播图来粗略地描述反向传播算法的行为。为了训练，我们希望计算 $\nabla_{\boldsymbol W^{(1)}} J$ 和 $\nabla_{\boldsymbol W^{(2)}} J$。有两种不同的路径从 $J$ 后退到权重：一条通过交叉熵代价，另一条通过权重衰减代价。权重衰减代价相对简单，它总是对 $\boldsymbol W^{(i)}$ 上的梯度贡献 $2\lambda \boldsymbol W^{(i)}$。


另一条通过交叉熵代价的路径稍微复杂一些。令 $\boldsymbol G$ 是由 $cross_entropy$ 操作提供的对未归一化对数概率 $\boldsymbol U^{(2)}$ 的梯度。反向传播算法现在需要探索两个不同的分支。在较短的分支上，它使用对矩阵乘法的第二个变量的反向传播规则，将 $\boldsymbol H^\top \boldsymbol G$ 加到 $\boldsymbol W^{(2)}$ 的梯度上。另一条更长些的路径沿着网络逐步下降。首先，反向传播算法使用对矩阵乘法的第一个变量的反向传播规则，计算 $\nabla_{\boldsymbol H} J = \boldsymbol G\boldsymbol W^{(2)\top}$。接下来， $relu$ 操作使用其反向传播规则对先前梯度的部分位置清零，这些位置对应着 $\boldsymbol U^{(1)}$ 中所有小于 0 的元素。记上述结果为 $\boldsymbol G'$。反向传播算法的最后一步是使用对 $matmul$ 操作的第二个变量的反向传播规则，将 $\boldsymbol X^\top \boldsymbol G'$ 加到 $\boldsymbol W^{(1)}$ 的梯度上。

在计算了这些梯度以后，梯度下降算法或者其他优化算法所要做的就是使用这些梯度来更新参数。

对于 MLP，计算成本主要来源于矩阵乘法。在前向传播阶段，我们乘以每个权重矩阵，得到了 $O(w)$ 数量的乘-加，其中 $w$ 是权重的数量。在反向传播阶段，我们乘以每个权重矩阵的转置，这具有相同的计算成本。算法主要的存储成本是我们需要将输入存储到隐藏层的非线性中去。这些值从被计算时开始存储，直到反向过程回到了同一点。因此存储成本是 $O(mn_h)$，其中 $m$ 是小批量中样本的数目，$n_h$ 是隐藏单元的数量。






# 相关

- 《深度学习》花书
