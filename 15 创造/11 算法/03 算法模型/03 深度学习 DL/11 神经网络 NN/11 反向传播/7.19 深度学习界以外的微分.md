---
title: 7.19 深度学习界以外的微分
toc: true
date: 2019-08-31
---


# 深度学习界以外的微分


深度学习界在某种程度上已经与更广泛的计算机科学界隔离开来，并且在很大程度上发展了自己关于如何进行微分的文化态度。更一般地，自动微分领域关心如何以算法方式计算导数。这里描述的反向传播算法只是自动微分的一种方法。它是一种称为反向模式累加的更广泛类型的技术的特殊情况。其他方法以不同的顺序来计算链式法则的子表达式。一般来说，确定一种计算的顺序使得计算开销最小，是困难的问题。找到计算梯度的最优操作序列是 NP 完全问题，在这种意义上，它可能需要将代数表达式简化为它们最廉价的形式。

例如，假设我们有变量 $p_1,p_2\ldots,p_n$ 表示概率，以及变量 $z_1,z_2,\ldots,z_n$ 表示未归一化的对数概率。假设我们定义


$$
q_i = \frac{\exp(z_i)}{\sum_i \exp(z_i)},
$$

其中我们通过指数化、求和与除法运算构建 softmax 函数，并构造交叉熵损失函数 $J=-\sum_i p_i\log q_i$。人类数学家可以观察到 $J$ 对 $z_i$ 的导数有一个非常简单的形式：$q_i-p_i$ (译者注：$\frac{\partial J}{\partial z_i} = -\frac{p_i}{q_i}\frac{\partial q_i}{\partial z_i} -\sum_{j \ne i} \frac{p_j}{q_j} \frac{\partial q_j}{\partial z_i}  = -\frac{p_i}{q_i}(q_i - q_i^2) - \sum_{j \ne i} \frac{p_j}{q_j}(-q_jq_i) =p_i(q_i - 1) + (1-p_i)q_i = q_i - p_i.$) 反向传播算法不能够以这种方式来简化梯度，而是会通过原始图中的所有对数和指数操作显式地传播梯度。一些软件库如 Theano 能够执行某些种类的代数替换来改进由纯反向传播算法提出的图。


当前向图 $\mathcal G$ 具有单个输出节点，并且每个偏导数 $\frac{\partial u^{(i)}}{\partial u^{(j)}}$ 都可以用恒定的计算量来计算时，反向传播保证梯度计算的计算数目和前向计算的计算数目是同一个量级：这可以在算法 6.2 中看出，因为每个局部偏导数 $\frac{\partial u^{(i)}}{\partial u^{(j)}}$ 以及递归链式公式（式 6.49 ）中相关的乘和加都只需计算一次。因此，总的计算量是 $O(\#\text{edges})$。然而，可能通过对反向传播算法构建的计算图进行简化来减少这些计算量，并且这是 NP 完全问题。诸如 Theano 和 TensorFlow 的实现使用基于匹配已知简化模式的试探法，以便重复地尝试去简化图。我们定义反向传播仅用于计算标量输出的梯度，但是反向传播可以扩展到计算 Jacobian 矩阵（该 Jacobian 矩阵或者来源于图中的 $k$ 个不同标量节点，或者来源于包含 $k$ 个值的张量值节点）。朴素的实现可能需要 $k$ 倍的计算：对于原始前向图中的每个内部标量节点，朴素的实现计算 $k$ 个梯度而不是单个梯度。当图的输出数目大于输入的数目时，有时更偏向于使用另外一种形式的自动微分，称为前向模式累加。前向模式计算已经被提出用于循环神经网络梯度的实时计算，例如。这也避免了存储整个图的值和梯度的需要，是计算效率和内存使用的折中。前向模式和后向模式的关系类似于左乘和右乘一系列矩阵之间的关系，例如

$$
\boldsymbol A \boldsymbol B \boldsymbol C \boldsymbol D,\tag{6.58}
$$

其中的矩阵可以认为是 Jacobian 矩阵。例如，如果 $\boldsymbol D$ 是列向量，而 $\boldsymbol A$ 有很多行，那么这对应于一幅具有单个输出和多个输入的图，并且从最后开始乘，反向进行，只需要矩阵-向量的乘积。这对应着反向模式。相反，从左边开始乘将涉及一系列的矩阵-矩阵乘积，这使得总的计算变得更加昂贵。然而，如果 $\boldsymbol A$ 的行数小于 $D$ 的列数，则从左到右乘更为便宜，这对应着前向模式。

在机器学习以外的许多社区中，更常见的是使用传统的编程语言来直接实现微分软件，例如用 python 或者 C 来编程，并且自动生成使用这些语言编写的不同函数的程序。在深度学习界中，计算图通常使用由专用库创建的明确的数据结构表示。专用方法的缺点是需要库开发人员为每个操作定义 $\text{bprop}$ 方法，并且限制了库的用户仅使用定义好的那些操作。然而，专用方法也允许定制每个操作的反向传播规则，允许开发者以非显而易见的方式提高速度或稳定性，对于这种方式自动的过程可能不能复制。

因此，反向传播不是计算梯度的唯一方式或最佳方式，但它是一个非常实用的方法，继续为深度学习社区服务。在未来，深度网络的微分技术可能会提高，因为深度学习的从业者更加懂得了更广泛的自动微分领域的进步。






# 相关

- 《深度学习》花书
