---
title: 谷歌 自动改良反向传播算法 训练速度再提升
toc: true
date: 2019-11-17
---

# 谷歌大作：自动改良反向传播算法，训练速度再提升！

大神 Geffery Hinton 是反向传播算法的发明者，但他也对反向传播表示怀疑，认为反向传播显然不是大脑运作的方式，为了推动技术进步，必须要有全新的方法被发明出来。今天介绍的谷歌大脑多名研究人员发表的最新论文Backprop Evolution，提出一种自动发现反向传播方程新变体的方法，该方法发现了一些新的方程，训练速度比标准的反向传播更快，训练时间也更短。



论文地址：

https://arxiv.org/pdf/1808.02822.pdf



![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb0I5r6fZ9wtAyZicQyw4gO0gZ1BDeJcuqibJGUqkGFTMTV7icQ7HtQSo6vVBcdK6IuWYKZMMwR2KhcBQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



大神 Geoffrey Hinton提出的反向传播算法是深度学习的基石。



1986 年，Geoffrey Hinton 与人合著了一篇论文：Learning representations by back-propagation errors，30 年之后，反向传播算法成了这一波人工智能爆炸的核心。



但去年，Hinton 在接受采访时表示，他对反向传播算法 “**深感怀疑**”，认为应该[**彻底抛弃反向传播，另起炉灶**](http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652004732&idx=1&sn=3ecf59ff77a2b9d4ad0e98272daa2b33&chksm=f121158dc6569c9bdac67de92bbf43d05cf10c776a7c29fa77f8cf110caefcb789a9b612dd4a&scene=21#wechat_redirect)。Hinton 认为，反向传播不是大脑运作的方式，我们的大脑显然不需要对所有数据进行标注。为了推动进步，必须要有全新的方法被发明出来。



尽管Hinton、以及无数研究者仍未提出全新的、能够代替传播的方法，但最近机器学习自动搜索方法取得很多成功，**反向传播算法的变体**也得到越来越多的研究。



柏林工业大学、谷歌大脑的多名研究人员在最新发表的论文 **Backprop Evolution**，提出一种**自动发现反向传播方程新变体**的方法。研究人员使用领域特定语言将更新的方程描述为原函数列表。



![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbfxibLaQXQkzoNicsL5RV1583pDIH2D2ibb08wYXgjbKzbo12D6fibxk1qQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



具体来说，研究人员采用一种基于进化的方法来发现新的传播规则，这些规则在几个epoch的训练之后可以最大限度地提高其泛化表现。他们发现了一些新的方程，它们的**训练速度比标准的反向传播更快，训练时间更短，并且在收敛时类似标准反向传播**。



自动生成反向传播方程





反向传播算法是机器学习中最重要的算法之一。已有研究对反向传播方程的变体进行了一些尝试，并取得一定程度的成功 (e.g., Bengio et al. (1994); Lillicrap et al. (2014); Lee et al. (2015); Nøkland (2016); Liao et al. (2016))。但尽管有这些尝试，反向传播方程的修改并没有得到广泛应用，因为这些修改很少对实际应用有改进，甚至有时会造成损害。



受近期机器学习自动搜索方法取得成功的启发，我们提出一种**自动生成反向传播方程**的方法。



为此，我们提出一种**领域特定语言**（domain specific language），以将这些数学公式描述为原始函数列表，并使用一种基于**进化**（evolution-based）的方法来发现新的传播规则。在经过几个epoch的训练后，搜索条件是使 generalization 最大化。我们找到了和标准反向传播效果同样好的几个变体方程。此外，在较短的训练时间内，这几种变体可以提高准确率。这可以用来改进 Hyperband 之类的算法，在训练过程中做出基于准确性的决策。



反向传播



![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbWibStdiaGwWKcJ6iavAyBBMVfnibculUJHFIZ2AdYCickPnTKz15kFiblyqg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图1：神经网络可以看作是一些计算图。前向图（forward graph）由网络设计者定义，而反向传播算法隐式地为参数更新定义了一个计算图。本研究的主要贡献是探索如何利用evolution来找到一个比标准反向传播更有效的参数更新计算图。



![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhb21TaBQev1gnZWHzpibhme2Qva4mrsEk0P903Oc8ECCj8icp9zq8gE6kA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



其中，![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbAYqfIePkgGDFOHrJic9COrDhY0vhor3nW5zsrp8EJSMdCTD2B1NRtnA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)是网络的输入，![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbgD3j2iae6t4J9feYwn3QP2KFTD3aHBia5swv0FV1jRk1pZDylO2ZzCSw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)对layer进行索引，![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhb5iahgtkJeLsJHybP8MJYCwTAsUnOF84cSDYR2IFLwEEEs7vVdWXy5ng/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)为第![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbgD3j2iae6t4J9feYwn3QP2KFTD3aHBia5swv0FV1jRk1pZDylO2ZzCSw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)层的权重矩阵。为了优化神经网络，我们要计算损失![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbyupZepcxIWUok6eODglFhHx954LgicnSKUqyQibQ5kBTQ4kzsDxCTIrw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的偏导数，这跟权重矩阵![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbHvdRGuibJoyqjIGWcicPL4EjpwdKnWxpfIync4ibdwJypVxtqKyBqyMLQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)有关。利用反向传播算法中的链式法则可以计算出这个量。为了计算隐藏激活![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbVnhcww89cpa4kIicsN7ZCMiaaMZ8hodGg6qMYNm9Pel6cgAeAx7I7l5g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的偏导数，要应用一系列运算：



![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbqjKLmWwAnFcHg3yXkqicyQ2Xt2WwUXOXGWgQW1TGn2Oz4gtgEAxDL4A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



一旦计算出![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbXeUW6T484BeSz7MGE58rLhU3V1IzxQVniaeMykwHc9ibPl7ibeToVKRvg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)，就可以将权重更新计算为：![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbLlyLia5Xh1C2tKrV89j1BicIGINOLbcRlHbfIrja6yrBY2ZvFby21ia0Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



如图1所示，神经网络可以表示为**前向和后向的计算图**。给定一个由网络设计者定义的前向计算图，反向传播算法定义了一个用于更新参数的反向计算图。但是，有可能找到一个改进的反向计算图，从而得到更好的泛化。



最近，用于机器学习的自动搜索方法已经在各种任务上取得了很好的结果，这些方法涉及修改前向计算图，依靠反向传播来定义适当的反向图。与之不同，在这项工作中，我们关注的是修改反向计算图，并使用搜索方法为![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbXeUW6T484BeSz7MGE58rLhU3V1IzxQVniaeMykwHc9ibPl7ibeToVKRvg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)找到更好的方程，产生新的训练规则。



方法



为了找到改进的更新规则，我们使用进化算法来搜索可能的更新方程（update equation）的空间。在每次迭代中，进化控制器将一批突变的更新方程发送给workers池进行评估。每个worker使用其接收到的变异方程来训练一个固定的神经网络结构，并将获得的验证精度报告给控制器。



**搜索空间**



受到Bello et al. (2017) 的启发，我们使用领域特定语言（domain-specific language，DSL）来描述用于计算![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbXeUW6T484BeSz7MGE58rLhU3V1IzxQVniaeMykwHc9ibPl7ibeToVKRvg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的方程。DSL将每个![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbXeUW6T484BeSz7MGE58rLhU3V1IzxQVniaeMykwHc9ibPl7ibeToVKRvg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)方程表示为![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbUvMiaic8JMZ9z0pK12fmVQqoKe1RB35YXvjHbWjlcsibN2j218aQTib4Rw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)，其中![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbgWsWSPUCnpC2NoKKGy3pbPzcwJ0nhaWjY0oedRGIHWxSKZgsXn3rUA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)，![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbhXe8MV0zAdEQ5pIsBIsReTicrrk4DUELFESxoOMPdrDJlUbubRKaHTw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)是可能的操作数，![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbib6r6oXTCQ1wXjLIZe3zqsuNWTgGxHM1E6IicmICxJ0UHF6RMAZyqlTg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)和![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbclP3dZWNVLztqlHMUicPq7b3pGsaYkqSeXHuRNSgichvq03QZk1ODJQg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)是一元函数，![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbtyNnsahxVpmRTh2da2ge1mSf1SiawU86kLSEjBQXdOnmzZhnJLkTD8Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)是二元函数。一元函数和二元函数的集合是手动指定的，但是函数和操作数的各个选择由控制器选择。每个组件的示例如下:



- 操作数（Operands）：W（当前层的权重矩阵），![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbqSFMTwwN3ZwG7olepaeKdvhWYqA1CEcSrfkaN4MfI1MB2V7z2DicECg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)（高斯矩阵），![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbZESx82L6Re7jK6zNiaIu3A06aetJmTn276I2dibCvMBGKFiciahC4RgFEQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)（从![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbgk5tWrMajRSLEGlB3LG0eia80bMuLegRicKd2STZGngUOQNjWe4wa5SQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)到![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbXeUW6T484BeSz7MGE58rLhU3V1IzxQVniaeMykwHc9ibPl7ibeToVKRvg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的高斯随机矩阵映射），![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhb6Q20K2Ok7WypnpDL11VUOibfT30PeK3MoQYfzk5c8Z3uXUqLA2dY4NA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)（前向传播的隐藏激活），![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbpeAzM1xnMuzZz7tjzV39micicg2fgP42bCsgNfEh29icxmXOHDDMh48uw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)（反向传播的值）。
- 一元函数![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbYNbOsZHYPCAEe65iaOBphAZx5CNMiaRVicJIDLh8UxhF1zs6ic44DuCKWA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
- 二元函数![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbYYODhpF747qnjJUnFzvf0SHRNCmGaNzJ0HjRWIbNyzuF8JyBHGow7g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



其中，![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbgD3j2iae6t4J9feYwn3QP2KFTD3aHBia5swv0FV1jRk1pZDylO2ZzCSw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)索引当前的层。实验使用的完整设置请见论文附录A。



结果得到的量![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhby1cHfjlDVBiaMVgkFBicKdoqRX3Ueqv66STYhk3wa4c03h2ApEB9CzzA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)在方程1中被用作![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbXeUW6T484BeSz7MGE58rLhU3V1IzxQVniaeMykwHc9ibPl7ibeToVKRvg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)，或者在方程的后续部分以递归方式用作![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbdicJiahvc96J6yj0G8b5QtGvHMse6pXxKOUbG1r55eLquW6bEXQEJFyA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)。在实验中，我们探索了由1到3个二元运算组成的方程。这种DSL虽然简单，但可以表示复杂的方程，例如标准的反向传播，feedback alignment，以及direct feedback alignment。



进化算法



**进化控制器**（evolutionary controller）维护一组已发现的方程。在每次迭代中，控制器执行以下操作之一：1）概率为p的情况下，控制器在搜索期间找到的N个最优竞争力的方程中随机选择一个方程，2）概率为1 - p时，控制器从population的其他方程中随机选择一个方程。



控制器随后将k个突变（mutation）应用于所选方程，其中k是从分类分布中提取的。这k个突变中的每一个只是简单地选择一个随机一致的方程组件（例如，一个操作数，一个一元函数，或者一个二元函数），然后将它与另一个随机选择的同类组件交换。某些突变会导致数学上不可行的方程，在这种情况下，控制器会重新启动突变过程，直到成功。N、p和k的分类分布是算法的超参数。



为了创建初始 population，我们简单地从搜索空间中随机抽样N个方程。此外，在我们的一些实验中，我们从一小部分预定义的方程开始（通常是正常的反向传播方程或其反馈对齐方程变体）。从现有方程出发的能力是基于强化学习的进化方法具有的优势。



实验和结果

##



在该方法中，用于**评估每个新方程的模型的选择**是一个重要的设置。规模更大、更深的网络会更真实，但需要更长的时间来训练，而较小的模型训练更快，但可能导致更新网络无法推广。我们通过使用Wide ResNets (WRN) 来平衡这两个标准，其中WRN有16层，宽度multiplier为2，并且在CIFAR-10数据集中进行训练。



**基线搜索和泛化**



在第一次搜索中，控制器提出新方程训练WRN 16-2网络20个epoch，并且分别在有或没有动量的情况下用SGD训练。根据验证准确性收集前100个新方程，然后在不同场景下进行测试：



（A1）使用20个epoch训练WRN 16-2 ，复制搜索设置；

（A2）使用20个epoch训练WRN 28-10 ，将其推广到更大的模型（WRN 28-             10的参数是WRN 16-2的10倍）；

（A3）使用100个epoch训练WRN 16-2 ，测试推广到更长的训练机制。



实验结果如表1所示：



![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb0vNiaedrNDSeqDLZ3YVyo7qZIEu4x9yFswmxIttuYMRaWyj6VSibHria5tVgiaPj4utkBAMTDErwzbfg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb0vNiaedrNDSeqDLZ3YVyo7qmr8icrMzrDW5MYELJLP8dbNPzZJVLoHXa8kXKNZVZKGwccJaJvl9obA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

表1：实验结果



从A1到A3，在每个设置中展示了两个性能最好的方程，以及两个在所有设置中都表现良好的方程。在B1中展示了4个性能最好的方程，所有结果均为5次以上的平均测试准确率。基线是梯度反向传播。比基线性能优于0.1%的结果都用粗体表示。我们用![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb0vNiaedrNDSeqDLZ3YVyo7qyJQDialJfHe7tNZj16GfKS0yytndbGVnic5qK1lGqLAkrOquK1k3Hiazw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)表示![img](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb10vKyODhH4Exmn9icQwCnhbgV5PjFYuYF5b6MBlLQMPgYWMZOKfACbvJoKHIzpLZfJLicuibDqpIESQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)。



**增加训练次数的搜索**



**之前的搜索实验发现新方程在训练开始时运行良好，但在收敛时不优于反向传播。**后一种结果可能是由于搜索和测试机制之间的不匹配，因为搜索使用20个epoch来训练子模型，而测试机制使用100个epoch。



 **一个后续方案是匹配这两个机制。** 在第二次搜索实验中，使用100个epoch训练每个子模型。 为了补偿由于使用较多的epoch进行训练而导致的实验时间增加，使用较小的网络（WRN 10-1）作为子模型。 使用较小的模型是可以接受的，因为新方程倾向于推广到更大，更真实的模型，如（A2）。



实验结果在表1中的（B1），与（A3）较为相似，即，可以找到对SGD表现较好的更新规则，但是对有动量的SGD的结果与基线相当。(A3)和(B1)结果的相似性表明，训练时间的差异可能不是误差的主要来源。 此外，具有动量的SGD对于不同的新方程是几乎不变的。



总结

##



在这项工作中，提出了一种自动查找可以取代标准反向传播的方程的方法。使用了一种进化控制器（在方程分量空间中工作），并试图最大化训练网络的泛化。探索性研究的结果表明，对于特定的场景，有一些方程的泛化性能比基线更好，但要找到一个在一般场景中表现更好的方程还需要做更多的工作。


# 相关

- [谷歌大作：自动改良反向传播算法，训练速度再提升！](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652024461&idx=3&sn=9c8cbb3e8cea783fa516391b58a653ac&chksm=f121da7cc656536ac7ef759ffa8e1f60a866a8fe1220b7b10c7715c8d0f8f4b7f39f21689680&mpshare=1&scene=1&srcid=0813pwj9CjMtwuei4Gkky4RG#rd)
