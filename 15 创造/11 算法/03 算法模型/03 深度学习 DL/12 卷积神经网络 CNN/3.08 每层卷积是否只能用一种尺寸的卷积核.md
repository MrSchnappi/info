# 每层卷积是否只能用一种尺寸的卷积核？

经典的神经网络一般都属于层叠式网络，每层仅用一个尺寸的卷积核，如 VGG 结构中使用了大量的 $3×3$ 卷积层。事实上，同一层特征图可以分别使用多个不同尺寸的卷积核，以获得不同尺度的特征，再把这些特征结合起来，得到的特征往往比使用单一卷积核的要好，如 GoogLeNet、Inception 系列的网络，均是每层使用了多个卷积核结构。

如图 5.3所示，输入的特征在同一层分别经过 $1×1$、$3×3$ 和 $5×5$ 三种不同尺寸的卷积核，再将分别得到的特征进行整合，得到的新特征可以看作不同感受域提取的特征组合，相比于单一卷积核会有更强的表达能力。<span style="color:red;">怎么进行整合的？直接相加吗？为什么这种整合是有道理的？pytorch 里面怎么写？</span>

Inception 模块结构：

<center>

![](http://images.iterate.site/blog/image/20190722/mEa58UyHhRuh.png?imageslim)

</center>
