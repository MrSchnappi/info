---
title: 3.15 一些局部连接的网络层
toc: true
date: 2019-08-31
---

## 局部连接的网络层

在一些情况下，我们并不是真的想使用卷积，而是想用一些局部连接的网络层。

在这种情况下，我们的多层感知机对应的邻接矩阵是相同的，但每一个连接都有它自己的权重，用一个 6 维的张量 $\boldsymbol W$ 来表示。$\boldsymbol W$ 的索引分别是：输出的通道 $i$，输出的行 $j$ 和列 $k$，输入的通道 $l$，输入的行偏置 $m$ 和列偏置 $n$。局部连接层的线性部分可以表示为

$$
Z_{i,j,k} = \sum_{l,m,n} [V_{l, j+m-1, k+n-1} w_{i, j, k, l, m, n}]. %这里应该是 $W$?
$$

这有时也被称为非共享卷积，因为它和具有一个小核的离散卷积运算很像，但并不横跨位置来共享参数。

下图比较了局部连接、卷积和全连接的区别。




<center>

![](http://images.iterate.site/blog/image/20190718/YPLWgjYKO94v.png?imageslim){ width=55% }

</center>

> 9.14 局部连接，卷积和全连接的比较。
>
> - (上)每一小片（接受域）有两个像素的局部连接层。每条边用唯一的字母标记，来显示每条边都有自身的权重参数。
> - (中)核宽度为两个像素的卷积层。该模型与局部连接层具有完全相同的连接。区别不在于哪些单元相互交互，而在于如何共享参数。局部连接层没有参数共享。正如用于标记每条边的字母重复出现所指示的，卷积层在整个输入上重复使用相同的两个权重。
> - (下)全连接层类似于局部连接层，它的每条边都有其自身的参数（在该图中用字母明确标记的话就太多了）。 然而，它不具有局部连接层的连接受限的特征。




当我们知道每一个特征都是一小块空间的函数并且相同的特征不会出现在所有的空间上时，局部连接层是很有用的。例如，如果我们想要辨别一张图片是否是人脸图像时，我们只需要去寻找嘴是否在图像下半部分即可。

使用那些连接被更进一步限制的卷积或者局部连接层也是有用的，例如，限制每一个输出的通道 $i$ 仅仅是输入通道 $l$ 的一部分的函数时。实现这种情况的一种通用方法是使输出的前 $m$ 个通道仅仅连接到输入的前 $n$ 个通道，输出的接下来的 $m$ 个通道仅仅连接到输入的接下来的 $n$ 个通道，以此类推。

下图给出了一个例子。对少量通道间的连接进行建模允许网络使用更少的参数，这降低了存储的消耗以及提高了统计效率，并且减少了前向和反向传播所需要的计算量。这些目标的实现并没有减少隐藏单元的数目。




<center>

![](http://images.iterate.site/blog/image/20190718/ETaakK0gzoAT.png?imageslim){ width=55% }

</center>

> 卷积网络的前两个输出通道只和前两个输入通道相连，随后的两个输出通道只和随后的两个输入通道相连。







# 相关

- 《深度学习》花书
