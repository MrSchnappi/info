---
title: 3.16 平铺卷积
toc: true
date: 2019-08-31
---

## 平铺卷积

<span style="color:red;">没大懂</span>

平铺卷积对卷积层和局部连接层进行了折衷。这里并不是对每一个空间位置的权重集合进行学习，我们学习一组核使得当我们在空间移动时它们可以循环利用。这意味着在近邻的位置上拥有不同的过滤器，就像局部连接层一样，但是对于这些参数的存储需求仅仅会增长常数倍，这个常数就是核的集合的大小，而不是整个输出的特征映射的大小。

下图对局部连接层、平铺卷积和标准卷积进行了比较。




<center>

![](http://images.iterate.site/blog/image/20190718/cnEM0MtkoGD3.png?imageslim){ width=55% }

</center>


> 局部连接层、平铺卷积和标准卷积的比较。当使用相同大小的核时，这三种方法在单元之间具有相同的连接。此图是对使用两个像素宽的核的说明。这三种方法之间的区别在于它们如何共享参数。
>
> - (上)局部连接层根本没有共享参数。我们对每个连接使用唯一的字母标记，来表明每个连接都有它自身的权重。
> - (中)平铺卷积有 $t$ 个不同的核。这里我们说明 $t=2$ 的情况。其中一个核具有标记为"a"和"b"的边，而另一个具有标记为"c"和"d"的边。每当我们在输出中右移一个像素后，我们使用一个不同的核。这意味着，与局部连接层类似，输出中的相邻单元具有不同的参数。与局部连接层不同的是，在我们遍历所有可用的 $t$ 个核之后，我们循环回到了第一个核。如果两个输出单元间隔 $t$ 个步长的倍数，则它们共享参数。
> - (下)传统卷积等效于 $t=1$ 的平铺卷积。它只有一个核，并且被应用到各个地方，我们在图中表示为在各处使用具有标记为"a"和"b"的边的核。


为了用代数的方法定义平铺卷积，令 $\boldsymbol K$ 是一个 6 维的张量，其中的两维对应着输出映射中的不同位置。$\boldsymbol K$ 在这里并没有对输出映射中的每一个位置使用单独的索引，输出的位置在每个方向上在 $t$ 个不同的核组成的集合中进行循环。

如果 $t$ 等于输出的宽度，这就是局部连接层了。

$$
Z_{i, j, k} = \sum_{l, m, n} V_{l, j+m-1, k+n-1} K_{i, l, m, n, j\% t +1, k\% t+1},
$$

这里百分号是取模运算，它的性质包括 $t\% t =0, (t+1)\% t = 1$ 等等。在每一维上使用不同的 $t$ 可以很容易对这个方程进行扩展。



局部连接层与平铺卷积层都和最大池化有一些有趣的关联：这些层的探测单元都是由不同的过滤器驱动的。如果这些过滤器能够学会探测相同隐含特征的不同变换形式，那么最大池化的单元对于学得的变换就具有不变性。卷积层对于平移具有内置的不变性。







# 相关

- 《深度学习》花书
