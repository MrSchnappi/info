---
title: 3.17 计算核的梯度
toc: true
date: 2019-08-31
---


# 计算核的梯度

实现卷积网络时，通常也需要除卷积以外的其他运算。为了实现学习，必须在给定输出的梯度时能够计算核的梯度。在一些简单情况下，这种运算可以通过卷积来实现，但在很多我们感兴趣的情况下，包括步幅大于 1 的情况，并不具有这样的性质。

回忆一下卷积是一种线性运算，所以可以表示成矩阵乘法的形式（如果我们首先把输入张量变形为一个扁平的向量）。其中包含的矩阵是关于卷积核的函数。这个矩阵是稀疏的并且核的每个元素都复制给矩阵的多个元素。这种观点能够帮助我们导出实现一个卷积网络所需的很多其他运算。

通过卷积定义的矩阵转置的乘法就是这样一种运算。这种运算用于在卷积层反向传播误差的导数，所以它在训练多于一个隐藏层的卷积网络时是必要的。如果我们想要从隐藏层单元重构可视化单元时，同样的运算也是需要的。


重构可视化单元是后面会广泛用到的一种运算，这些模型包括自编码器、RBM和稀疏编码等等。构建这些模型的卷积化的版本都要用到转置化卷积。类似核梯度运算，这种输入梯度运算在某些情况下可以用卷积来实现，但在一般情况下需要用到第三种运算来实现。必须非常小心地来使这种转置运算和前向传播过程相协调。

转置运算返回的输出的大小取决于三个方面：

- 零填充的策略
- 前向传播运算的步幅
- 前向传播的输出映射的大小

在一些情况下，不同大小的输入通过前向传播过程能够得到相同大小的输出映射，所以必须明确地告知转置运算原始输入的大小。

这三种运算——卷积、从输出到权重的反向传播和从输出到输入的反向传播——对于训练任意深度的前馈卷积网络，以及训练带有（基于卷积的转置的）重构函数的卷积网络，这三种运算都足以计算它们所需的所有梯度。

对于完全一般的多维、多样例情况下的公式，完整的推导可以参考 Goodfellow-TR2010。为了直观说明这些公式是如何起作用的，我们这里给出一个二维单个样例的版本。


假设我们想要训练这样一个卷积网络，它包含步幅为 $s$ 的步幅卷积，该卷积的核为 $\boldsymbol K$，作用于多通道的图像 $\boldsymbol V$，定义为 $c(\boldsymbol K, \boldsymbol V, s)$，就像下采样卷积一样：

$$
Z_{i, j, k}=c(\mathbf{K}, \mathbf{V}, s)_{i, j, k}=\sum_{l, m, n}\left[V_{l,(j-1) \times s+m,(k-1) \times s+n,} K_{i, l, m, n}\right]
$$

假设我们想要最小化某个损失函数 $J(\boldsymbol V, \boldsymbol K)$。在前向传播过程中，我们需要用 $c$ 本身来输出 $\boldsymbol Z$，然后 $\boldsymbol Z$ 传递到网络的其余部分并且被用来计算损失函数 $J$。

在反向传播过程中，我们会得到一个张量 $\boldsymbol G$ 满足 $G_{i, j, k} = \frac{\partial}{\partial Z_{i, j, k}} J(\boldsymbol V, \boldsymbol K)$。

为了训练网络，我们需要对核中的权重求导。为了实现这个目的，我们可以使用一个函数

$$
g(\boldsymbol G, \boldsymbol V, s)_{i, j, k, l} = \frac{\partial}{\partial K_{i, j, k, l}} J(\boldsymbol V, \boldsymbol K) = \sum_{m, n} G_{i, m, n} V_{j, (m-1)\times s+k, (n-1)\times s+l}.
$$


如果这一层不是网络的底层，我们需要对 $\boldsymbol V$ 求梯度来使得误差进一步反向传播。我们可以使用如下的函数



$$
\begin{aligned} h(\mathbf{K}, \mathbf{G}, s)_{i, j, k} &=\frac{\partial}{\partial V_{i, j, k}} J(\mathbf{V}, \mathbf{K}) \\ &=\sum_{l, m  \;\text { s.t }\; (l-1) \times s+m=j} \sum_{n, p  \;\text { s.t. } \;(n-1) \times s+p=k} \sum_{q} K_{q, i, m, p} G_{q, l, n} \end{aligned}
$$


自编码器网络是一些被训练成把输入拷贝到输出的前馈网络。一个简单的例子是 PCA 算法，将输入 $\boldsymbol x$ 拷贝到一个近似的重构值 $\boldsymbol r$，通过函数 ${\boldsymbol W}^\top \boldsymbol W \boldsymbol x$ 来实现。使用权重矩阵转置的乘法，就像 PCA 算法这种，在一般的自编码器中是很常见的。为了使这些模型卷积化，我们可以用函数 $h$ 来实现卷积运算的转置。假定我们有和 $\boldsymbol Z$ 相同形式的隐藏单元 $\boldsymbol H$，并且我们定义一种重构运算

$$
\boldsymbol R = h(\boldsymbol K, \boldsymbol H, s).
$$


为了训练自编码器，我们会得到关于 $\boldsymbol R$ 的梯度，表示为一个张量 $\boldsymbol E$。为了训练解码器，我们需要获得对于 $\boldsymbol K$ 的梯度，这通过 $g(\boldsymbol H, \boldsymbol E, s)$ 来得到。为了训练编码器，我们需要获得对于 $\boldsymbol H$ 的梯度，这通过 $c(\boldsymbol K, \boldsymbol E, s)$ 来得到。通过用 $c$ 和 $h$ 对 $g$ 求微分也是可行的，但这些运算对于任何标准神经网络上的反向传播算法来说都是不需要的。


一般来说，在卷积层从输入到输出的变换中我们不仅仅只用线性运算。我们一般也会在进行非线性运算前，对每个输出加入一些偏置项。这样就产生了如何在偏置项中共享参数的问题。对于局部连接层，很自然地对每个单元都给定它特有的偏置，对于平铺卷积，也很自然地用与核一样的平铺模式来共享参数。对于卷积层来说，通常的做法是在输出的每一个通道上都设置一个偏置，这个偏置在每个卷积映射的所有位置上共享。然而，如果输入是已知的固定大小，也可以在输出映射的每个位置学习一个单独的偏置。分离这些偏置可能会稍稍降低模型的统计效率，但同时也允许模型来校正图像中不同位置的统计差异。例如，当使用隐含的零填充时，图像边缘的探测单元接收到较少的输入，因此需要较大的偏置。



# 相关

- 《深度学习》花书
