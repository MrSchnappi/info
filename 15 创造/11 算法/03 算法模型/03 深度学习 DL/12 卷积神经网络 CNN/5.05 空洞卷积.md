---
title: 5.05 空洞卷积
toc: true
date: 2019-09-04
---

## 9.5 空洞卷积(Dilated Convolutions)

以前的 CNN 主要问题总结：

1. Up-sampling / pooling layer
2. 内部数据结构丢失；空间层级化信息丢失。<span style="color:red;">为什么说空间层级化信息丢失了？</span>
3. 小物体信息无法重建 (假设有四个 pooling layer 则任何小于 2^4 = 16 pixel 的物体信息将理论上无法重建。)<span style="color:red;">是的。理论上无法重建了。</span>


在图像分割领域，图像输入到 CNN（典型的网络比如 FCN[3]）中，FCN先像传统的 CNN 那样对图像做卷积再 pooling，降低图像尺寸的同时增大感受野，但是由于图像分割预测是 pixel-wise 的输出，所以要将 pooling 后较小的图像尺寸 upsampling 到原始的图像尺寸进行预测，<span style="color:red;">upsampling一般采用 deconv 反卷积操作，deconv 可参见知乎答案如何理解深度学习中的 deconvolution networks？</span>之前的 pooling 操作使得每个 pixel 预测都能看到较大感受野信息。<span style="color:red;">为什么说每个 pixel 预测都能看到较大感受野的信息？</span>因此图像分割 FCN 中有两个关键，一个是 pooling 减小图像尺寸增大感受野，另一个是 upsampling 扩大图像尺寸。

在先减小再增大尺寸的过程中，肯定有一些信息损失掉了，那么能不能设计一种新的操作，不通过 pooling 也能有较大的感受野看到更多的信息呢？答案就是 dilated conv。<span style="color:red;">哇塞，真的有，感觉 pooling 的存在歧视对于这种分割任务来说的确是丢失信息了的。</span>



举例如下：

<center>

![](http://images.iterate.site/blog/image/20190722/1LxePT8Y9ktc.png?imageslim){ width=55% }

</center>


> Dilated Convolution with a $3 \times  3$ kernel and dilation rate $2$

下面看一下 dilated conv 原始论文[4]中的示意图

<center>

![](http://images.iterate.site/blog/image/20190722/az2P7sNcUg10.jpg?imageslim){ width=75% }

</center>



a. 对应 $3\times 3$ 的 1-dilated conv，和普通的卷积操作一样。
b. 对应 $3\times 3$ 的 2-dilated conv，实际的卷积 kernel size还是 $3\times 3$，但是空洞为 $1$，<span style="color:red;">这个空洞的 1 是哪来的？2-1=1 吗？</span>也就是对于一个 $7\times 7$ 的图像 patch，只有 $9$ 个红色的点和 $3\times 3$ 的 kernel 发生卷积操作，其余的点略过。也可以理解为 kernel 的 size 为 $7\times 7$，但是只有图中的 $9$ 个点的权重不为 $0$，其余都为 $0$。 可以看到虽然 kernel size 只有 $3\times 3$，但是这个卷积的感受野已经增大到了 $7\times 7$（如果考虑到这个 2-dilated conv 的前一层是一个 1-dilated conv的话，那么每个红点就是 1-dilated 的卷积输出，所以感受野为 $3\times 3$，所以 1-dilated和 2-dilated 合起来就能达到 $7\times 7$ 的 conv。<span style="color:red;">为什么？没明白为什么感受野就是 $7\times 7$ 了？</span>）。
c. 是 4-dilated conv 操作，同理跟在两个 1-dilated和 2-dilated conv 的后面，能达到 $15\times 15$ 的感受野。<span style="color:red;">怎么跟在它们后面的？为什么会有这么大的感受野？怎么计算的？</span>对比传统的 conv 操作，$3$ 层 $3\times 3$ 的卷积加起来，stride 为 $1$ 的话，只能达到 $(kernel-1) * layer+1=7$ 的感受野，也就是和层数 layer 成线性关系，而 dilated conv的感受野是指数级的增长。

dilated 的好处是不做 pooling 损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。在图像需要全局信息或者语音文本需要较长的 sequence 信息依赖的问题中，都能很好的应用 dilated conv，比如图像分割、语音合成 WaveNet、机器翻译 ByteNet 中。<span style="color:red;">都要了解下。</span>

<span style="color:red;">还是没明白为什么 dilated conv 是合理的？而且，怎么更新参数呢？</span>
