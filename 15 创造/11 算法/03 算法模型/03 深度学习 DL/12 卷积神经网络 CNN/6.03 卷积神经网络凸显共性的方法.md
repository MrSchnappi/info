---
title: 6.03 卷积神经网络凸显共性的方法
toc: true
date: 2019-09-03
---

## 5.19 卷积神经网络凸显共性的方法？

<span style="color:red;">？这个问题问的有些奇怪？而且回答的也有点没有抓住重点吧？</span>

### 5.19.1 局部连接

我们首先了解一个概念，感受野，即每个神经元仅与输入神经元相连接的一块区域。

在图像卷积操作中，神经元在空间维度上是局部连接，但在深度上是全连接。局部连接的思想，是受启发于生物学里的视觉系统结构，视觉皮层的神经元就是仅用局部接受信息。对于二维图像，局部像素关联性较强。这种局部连接保证了训练后的滤波器能够对局部特征有最强的响应，使神经网络可以提取数据的局部特征；

下图是一个很经典的图示，左边是全连接，右边是局部连接。

<center>

![](http://images.iterate.site/blog/image/20190722/LDHUuU1lBKDJ.png?imageslim){ width=65% }

</center>


对于一个 1000 × 1000 的输入图像而言，如果下一个隐藏层的神经元数目为 10^6 个，采用全连接则有 1000 × 1000 × 10^6 = 10^12 个权值参数，如此巨大的参数量几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中 10 × 10 的局部图像相连接，那么此时的权值参数数量为 10 × 10 × 10^6 = 10^8，将直接减少 4 个数量级。<span style="color:red;">嗯。</span>

### 5.19.2 权值共享

权值共享，即计算同一深度的神经元时采用的卷积核参数是共享的。权值共享在一定程度上讲是有意义的，是由于在神经网络中，提取的底层边缘特征与其在图中的位置无关。但是在另一些场景中是无意的，如在人脸识别任务，我们期望在不同的位置学到不同的特征。<span style="color:red;">人脸识别任务与普通的识别任务有什么不同吗？</span>

需要注意的是，权重只是对于同一深度切片的神经元是共享的。在卷积层中，通常采用多组卷积核提取不同的特征，即对应的是不同深度切片的特征，而不同深度切片的神经元权重是不共享。相反，偏置这一权值对于同一深度切片的所有神经元都是共享的。

权值共享带来的好处是大大降低了网络的训练难度。如下图，假设在局部连接中隐藏层的每一个神经元连接的是一个 10 × 10 的局部图像，因此有 10 × 10个权值参数，将这 10 × 10 个权值参数共享给剩下的神经元，也就是说隐藏层中 10^6 个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10 × 10个权值参数（也就是卷积核的大小）。

<center>

![](http://images.iterate.site/blog/image/20190722/wJzXiyNowIxL.png?imageslim){ width=55% }

</center>


这里就体现了卷积神经网络的奇妙之处，使用少量的参数，却依然能有非常出色的性能。上述仅仅是提取图像一种特征的过程。如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像不同尺度下的特征，称之为特征图（feature map）。

### 5.19.3 池化操作

池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。<span style="color:red;">感觉这句话讲的不是特别准确吧？</span>

如下图：

<center>

![](http://images.iterate.site/blog/image/20190722/8QKkj0z4PlMR.png?imageslim){ width=85% }

</center>








# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
