---
title: 8.11 卷积神经网络如何用于文本分类任务
toc: true
date: 2019-04-14
---
# 可以补充进来的




# 深度卷积神经网络



## 卷积神经网络如何用于文本分类任务？

卷积神经网络的核心思想是捕捉局部特征，起初在图像领域取得了巨大的成功，后来在文本领域也得到了广泛的应用。对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于 N-gram。

卷积神经网络的优势在于能够自动地对 N-gram 特征进行组合和筛选，获得不同抽象层次的语义信息。<span style="color:red;">不错，很好。</span>

由于在每次卷积中采用了共享权重的机制，因此它的训练速度相对较快，在实际的文本分类任务中取得了非常不错的效果。<span style="color:red;">嗯。</span>

图 9.20 是一个用卷积神经网络模型进行文本表示，并最终用于文本分类的网络结构[20]：

<center>

![](http://images.iterate.site/blog/image/20190414/kY6AFuI8C6u7.png?imageslim){ width=55% }

</center>

<span style="color:red;">我擦嘞，这个图画的怎么看不懂了呢？</span><span style="color:blue;">嗯，看了下面的解释，非常清晰。感觉与图像的那个连接图是有一些不同，是一维的。</span>

- 输入层是一个 $N×K$ 的矩阵，其中 $N$ 为文章所对应的单词总数，$K$ 是每个词对应的表示向量的维度。每个词的 $K$ 维向量可以是预先在其他语料库中训练好的，也可以作为未知的参数由网络训练得到。这两种方法各有优势，一方面，预先训练的词嵌入可以利用其他语料库得到更多的先验知识；另一方面，由当前网络训练的词向量能够更好地抓住与当前任务相关联的特征。<span style="color:red;">嗯，听起来都有道理，一般来说用那种比较好呢？</span>因此，图中的输入层实际采用了两个通道的形式，即有两个 $N×K$ 的输入矩阵，其中一个用预先训练好的词嵌入表达，并且在训练过程中不再发生变化；另外一个也由同样的方式初始化，但是会作为参数，随着网络的训练过程发生改变。<span style="color:red;">嗯，是的，那么是怎么随着训练的过程发生变化的？</span>

- 第二层为卷积层。在输入的 $N×K$ 维矩阵上，我们定义不同大小的滑动窗口进行卷积操作：$c_{i}=f\left(w \cdot x_{i:i+h-1}+b\right)$。其中 $x_{i:i+h-1}$ 代表由输入矩阵的第 $i$ 行到第 $i+h−1$ 行所组成的一个大小为 $h×K$ 的滑动窗口， $w$ 为 $K×h$ 维的权重矩阵，$b$ 为偏置参数。假设 $h$ 为 $3$，则每次在 $2×K$ 的滑动窗口上进行卷积，并得到 $N−2$ 个结果，再将这 $N−2$ 个结果拼接起来得到 $N−2$ 维的特征向量。每一次卷积操作相当于一次特征向量的提取，通过定义不同的滑动窗口，就可以提取出不同的特征向量，构成卷基层的输出。<span style="color:red;">嗯，不同的滑动窗口提取到的特征向量是怎么拼接的？</span>

- 第三层为池化层，比如图中所示的网络采用了 1-Max 池化，即为从每个滑动窗口产生的特征向量中筛选出一个最大的特征，然后将这些特征拼接起来构成向量表示。<span style="color:red;">哇塞，这也太粗暴了吧？真的可行吗？这样太池化了吧？不过的确就把边长的文本转化成了定长的向量。嗯，厉害。</span>也可以选用 K-Max 池化（选出每个特征向量中最大的 K 个特征），或者平均池化（将特征向量中的每一维取平均）等，<span style="color:red;">K-Max 池化比 1-Max 池化好在哪里？平时真的有用这个吗？之前好像很少听说 K-Max 池化。</span>**达到的效果都是将不同长度的句子通过池化得到一个定长的向量表示**。<span style="color:red;">这个倒真的是，机智！</span>

- 得到文本的向量表示之后，后面的网络结构就和具体的任务相关了。本例中展示的是一个文本分类的场景，因此最后接入了一个全连接层，并使用 Softmax 激活函数输出每个类别的概率。<span style="color:red;">嗯。</span>

<span style="color:red;">挺好的这一段，说明的非常清楚。</span>






# 原文与相关

- 《百面机器学习》
