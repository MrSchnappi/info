---
title: 1.09 标准 RNN 前向输出流程
toc: true
date: 2019-09-03
---

## 6.8 标准 RNN 前向输出流程

以 $x$ 表示输入，$h$ 是隐层单元，$o$ 是输出，$L$ 为损失函数，$y$ 为训练集标签。$t$ 表示 $t$ 时刻的状态，$V,U,W$ 是权值，同一类型的连接权值相同。以下图为例进行说明标准 RNN 的前向传播算法：

​<center>

![](http://images.iterate.site/blog/image/20190722/v76x6xBxcow8.png?imageslim){ width=60% }

</center>


对于 $t$ 时刻：
$$
h^{(t)}=\phi(Ux^{(t)}+Wh^{(t-1)}+b)
$$
其中 $\phi()$ 为激活函数，一般会选择 tanh 函数，$b$ 为偏置。<span style="color:red;">为什么使用 tanh 函数？不使用 ReLU 函数？之前好像在花书中看到过。</span>

$t$ 时刻的输出为：

$$
o^{(t)}=Vh^{(t)}+c
$$

模型的预测输出为：<span style="color:red;">为什么没有提到 L？</span>

$$
\widehat{y}^{(t)}=\sigma(o^{(t)})
$$

其中 $\sigma​$ 为激活函数，通常 RNN 用于分类，故这里一般用 softmax 函数。<span style="color:red;">嗯，再回顾下 softmax 函数。</span>








# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
