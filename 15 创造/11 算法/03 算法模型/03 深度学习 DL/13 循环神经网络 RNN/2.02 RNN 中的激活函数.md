---
title: 2.02 RNN 中的激活函数
toc: true
date: 2019-04-19
---
# 循环神经网络中的激活函数


我们知道，在卷积神经网络等前馈神经网络中采用 ReLU 激活函数通常可以有效地改善梯度消失，取得更快的收敛速度和更好的收敛结果。那么在循环神经网络中可以使用 ReLU 作为每层神经元的激活函数吗？<span style="color:red;">是呀，循环神经网络使用的什么激活函数？</span>

ReLU，循环神经网络，激活函数

## 在循环神经网络中能否使用 ReLU 作为激活函数？


答案是肯定的，但是需要对矩阵的初值做一定限制，否则十分容易引发数值问题。<span style="color:red;">为什么呢？</span>

为了解释这个问题，让我们回顾一下循环神经网络的前向传播公式：

$$
n e t_{t}=U x_{t}+W h_{t-1}\tag{10.9}
$$

$$
h_{t}=f\left(n e t_{t}\right)\tag{10.10}
$$

根据前向传播公式向前传递一层，可以得到：

$$
net_{t}=U x_{t}+W h_{t-1}=U x_{t}+W f\left(U x_{t-1}+W h_{t-2}\right)\tag{10.11}
$$

如果采用 ReLU 替代公式中的激活函数 $f$，并且假设 ReLU 函数一直处于激活区域（即输入大于 0），则有 $f(x)=x$，$n e t_{t}=U x_{t}+W\left(U x_{t-1}+W h_{t-2}\right)$ 。继续将其展开，$net_{t}$ 的表达式中最终包含 $t$ 个 $W$ 连乘。如果 $W$ 不是单位矩阵（对角线上的元素为 1，其余元素为 0 的矩阵），最终的结果将会趋于 0 或者无穷，引发严重的数值问题。<span style="color:red;">是的。</span>

那么为什么在卷积神经网络中不会出现这样的现象呢？

这是因为在卷积神经网络中每一层的权重矩阵 $W$ 是不同的，并且在初始化时它们是独立同分布的，因此可以相互抵消，在多层之后一般不会出现严重的数值问题。<span style="color:red;">是的是的。嗯。</span>


再回到循环神经网络的梯度计算公式：

$$
\begin{aligned}\frac{\partial n e t_{t}}{\partial n e t_{t-1}}&=W \cdot \operatorname{diag}\left[f^{\prime}\left(n e t_{t-1}\right)\right]\\&=\left( \begin{array}{ccc}{W_{11} f^{\prime}\left(n e t_{t-1}^{1}\right)} & {\cdots} & {W_{1 n} f^{\prime}\left(n e t_{t-1}^{n}\right)} \\ {\vdots} & {\ddots} & {\vdots} \\ {W_{n 1} f^{\prime}\left(n e t_{t-1}^{1}\right)} & {\cdots} & {W_{n n} f^{\prime}\left(n e t_{t-1}^{n}\right)}\end{array}\right)\end{aligned}\tag{10.12}
$$


假设采用 ReLU 激活函数，且一开始所有的神经元都处于激活中（即输入大于 $0$），则 $\operatorname{diag}\left[f^{\prime}\left(n e t_{t-1}\right)\right]$ 为单位矩阵，有 $\frac{\partial n e t_{t}}{\partial n e t_{t-1}}=W$ 。在梯度传递经历了 $n$ 层之后，$\frac{\partial n e t_{t}}{\partial n e t_{1}}=W^{n}$ 。可以看到，即使采用了 ReLU 激活函数，只要 $W$ 不是单位矩阵，梯度还是会出现消失或者爆炸的现象。<span style="color:red;">再看下。</span>


综上所述，当采用 ReLU 作为循环神经网络中隐含层的激活函数时，只有当 $W$ 的取值在单位矩阵附近时才能取得比较好的效果，因此需要将 $W$ 初始化为单位矩阵。实验证明，初始化 $W$ 为单位矩阵并使用 ReLU 激活函数在一些应用中取得了与长短期记忆模型相似的结果，并且学习速度比长短期记忆模型更快，是一个值得尝试的小技巧[25]。



# 相关

- 《百面机器学习》
器学习》
