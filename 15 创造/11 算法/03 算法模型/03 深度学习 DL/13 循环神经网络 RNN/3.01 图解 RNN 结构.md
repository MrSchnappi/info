---
title: 3.01 图解 RNN 结构
toc: true
date: 2019-09-03
---

### 6.2.2 图解经典 RNN 结构

在实际应用中，我们还会遇到很多序列形的数据，如：

- 自然语言处理问题。$x1$ 可以看做是第一个单词，$x2$ 可以看做是第二个单词，依次类推。
- 语音处理。此时，x1、x2、x3……是每帧的声音信号。
- 时间序列问题。例如每天的股票价格
- 等等。

其单个序列如下图所示：

<center>

![](http://images.iterate.site/blog/image/20190722/Nz6luc74V6Uz.jpg?imageslim){ width=25% }

</center>


前面介绍了诸如此类的序列数据用原始的神经网络难以建模，基于此，RNN 引入了隐状态 $h$（hidden state），$h​$ 可对序列数据提取特征，接着再转换为输出。

为了便于理解，先计算 $h_1​$：

<center>

![](http://images.iterate.site/blog/image/20190722/VoL13PtlcvIt.jpg?imageslim){ width=42% }

</center>


注：图中的圆圈表示向量，箭头表示对向量做变换。

RNN中，每个步骤使用的参数 $U,W,b$​相同，$h_2$ 的计算方式和 $h_1​$ 类似，其计算结果如下：

<center>

![](http://images.iterate.site/blog/image/20190722/o08YPb9AhwI0.jpg?imageslim){ width=49% }

</center>


  计算 $h_3$,$h_4​$ 也相似，可得：

<center>

![](http://images.iterate.site/blog/image/20190722/1Pgj7VYuzBKl.jpg?imageslim){ width=36% }

</center>


接下来，计算 RNN 的输出 $y_1$，采用 $Softmax$ 作为激活函数，根据 $y_n=f(Wx+b)$，得 $y_1​$:<span style="color:red;">为什么这个时候才开始计算 $y_1$ ？</span>

<center>

![](http://images.iterate.site/blog/image/20190722/5XwKoPPgyRDr.jpg?imageslim){ width=44% }

</center>


使用和 $y_1​$ 相同的参数 $V,c​$，得到 $y_1,y_2,y_3,y_4​$ 的输出结构：

<center>

![](http://images.iterate.site/blog/image/20190722/KNBKJr9i7jf1.jpg?imageslim){ width=39% }

</center>


以上即为最经典的 RNN 结构，其输入为 $x_1,x_2,x_3,x_4$，输出为 $y_1,y_2,y_3,y_4$，当然实际中最大值为 $y_n$，这里为了便于理解和展示，只计算 4 个输入和输出。

从以上结构可看出，RNN结构的输入和输出等长。<span style="color:red;">嗯。</span>
