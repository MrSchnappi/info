---
title: 4.13 长短期记忆网络 LSTM
toc: true
date: 2019-04-19
---

# 可以补充进来的

- 想知道，LSTM 的 cell 在程序中是怎么实现的？还有那些求导的过程在程序中是怎么实现的？


# 长短期记忆网络


长短期记忆网络（Long Short Term Memory，LSTM）是循环神经网络的最知名和成功的扩展。

由于循环神经网络有梯度消失和梯度爆炸的问题，学习能力有限，在实际任务中的效果往往达不到预期效果。LSTM 可以对有价值的信息进行长期记忆，从而减小循环神经网络的学习难度，因此在语音识别、语言建模、机器翻译、命名实体识别、图像描述文本生成等问题中有着广泛应用。<span style="color:red;">嗯。</span>


LSTM，门控，激活函数，双曲正切函数，Sigmoid函数

## LSTM是如何实现长短期记忆功能的？

有图有真相，我们首先结合 LSTM 结构图以及更新的计算公式探讨这种网络如何实现其功能，长短时记忆模型内部结构示意如下：

<center>

![](http://images.iterate.site/blog/image/20190417/uj0jFFNjnXLH.png?imageslim){ width=55% }

</center>

与传统的循环神经网络相比，LSTM 仍然是基于 $x_t$ 和 $h_{t−1}$ 来计算 $h_t$，只不过对内部的结构进行了更加精心的设计，加入了输入门 $i_t$、遗忘门 $f_t$ 以及输出门 $o_t$ 三个门和一个内部记忆单元 $c_t$：

- 输入门控制当前计算的新状态以多大程度更新到记忆单元中
- 遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉
- 输出门控制当前的输出有多大程度上取决于当前的记忆单元。

经典的 LSTM 中，第 $t$ 步的更新计算公式为：


$$
i_{t}=\sigma\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\right)\tag{10.13}
$$

$$
f_{t}=\sigma\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\right)\tag{10.14}
$$

$$
o_{t}=\sigma\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\right)\tag{10.15}
$$

$$
\tilde{c}_{t}=\operatorname{Tanh}\left(W_{c} x_{t}+U_{c} h_{t-1}\right)\tag{10.16}
$$

$$
c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot\tilde{c}_{t}\tag{10.17}
$$


$$
h_{t}=o_{t} \odot \operatorname{Tanh}\left(c_{t}\right)\tag{10.18}
$$


其中：

- $i_t$ 是通过输入 $x_t$ 和上一步的隐含层输出 $h_{t−1}$ 进行线性变换，再经过激活函数 $\sigma$ 得到的。
- 输入门 $i_t$ 的结果是向量，其中每个元素是 $0$ 到 $1$ 之间的实数，用于控制各维度流过阀门的信息量；
- $W_i$、 $U_i$ 两个矩阵和向量 $b_i$ 为输入门的参数，是在训练过程中需要学习得到的。
- 遗忘门 $f_t$ 和输出门 $o_t$ 的计算方式与输入门类似，它们有各自的参数 $W$、$U$ 和 $b$。

与传统的循环神经网络不同的是，从上一个记忆单元的状态 $c_{t−1}$ 到当前的状态 $c_t$ 的转移不一定完全取决于激活函数计算得到的状态，还由输入门和遗忘门来共同控制。


在一个训练好的网络中：

- 当输入的序列中没有重要信息时，LSTM 的遗忘门的值接近于 $1$，输入门的值接近于 $0$，此时过去的记忆会被保存，从而实现了长期记忆功能；
- 当输入的序列中出现了重要的信息时，LSTM应当把其存入记忆中，此时其输入门的值会接近于 $1$；
- 当输入的序列中出现了重要信息，且该信息意味着之前的记忆不再重要时，输入门的值接近 $1$，而遗忘门的值接近于 $0$，这样旧的记忆被遗忘，新的重要信息被记忆。

经过这样的设计，整个网络更容易学习到序列之间的长期依赖。


## LSTM 里各模块分别使用什么激活函数，可以使用别的激活函数吗？

关于激活函数的选取，在 LSTM 中：

- 遗忘门、输入门和输出门使用 Sigmoid 函数作为激活函数；
- 在生成候选记忆时，使用双曲正切函数 Tanh 作为激活函数。

值得注意的是，这两个激活函数都是饱和的，也就是说在输入达到一定值的情况下，输出就不会发生明显变化了。如果是用非饱和的激活函数，例如 ReLU，那么将难以实现门控的效果。<span style="color:red;">嗯。</span>

使用这个激活函数的原因如下：

- Sigmoid函数的输出在 0～1 之间，符合门控的物理定义。且当输入较大或较小时，其输出会非常接近 1 或 0，从而保证该门开或关。

- 在生成候选记忆时，使用 Tanh 函数，是因为其输出在 −1~1 之间，这与大多数场景下特征分布是 0 中心的吻合。此外，Tanh 函数在输入为 0 附近相比 Sigmoid 函数有更大的梯度，通常使模型收敛更快。<span style="color:red;">与大多数场景下特征分布是 0 中心吻合？如果不是 0 为中心呢？Tanh 使模型收敛更快是这样吗？</span>


激活函数的选择也不是一成不变的。例如在原始的 LSTM 中，使用的激活函数是 Sigmoid 函数的变种，$h(x)=2 \operatorname{sig} \operatorname{moid}(x)-1$ ，$g(x)=4 \operatorname{sigmoid}(x)-2$ ，这两个函数的范围分别是 $[−1，1]$ 和 $[−2，2]$。并且在原始的 LSTM 中，只有输入门和输出门，没有遗忘门，其中输入经过输入门后是直接与记忆相加的，所以输入门控 $g(x)$ 的值是 0 中心的。

后来经过大量的研究和实验，人们发现增加遗忘门对 LSTM 的性能有很大的提升[26]，并且 $h(x)$ 使用 Tanh 比 $2^{*}$ sigmoid $(x)-1$ 要好，所以现代的 LSTM 采用 Sigmoid和 Tanh 作为激活函数。

事实上在门控中，使用 Sigmoid 函数是几乎所有现代神经网络模块的共同选择。例如在门控循环单元和注意力机制中，也广泛使用 Sigmoid 函数作为门控的激活函数。<span style="color:red;">门控循环单元？注意力机制听说过，但是门控循环单元没听说过，是 GRU 吗？补充下。为什么使用 Sigmoid 函数作为门控的激活函数？</span>


此外，在一些对计算能力有限制的设备，诸如可穿戴设备中，由于 Sigmoid 函数求指数需要一定的计算量，此时会使用 0/1 门（hard gate）让门控输出为 0 或 1 的离散值，即当输入小于阈值时，门控输出为 0；当输入大于阈值时，输出为 1。从而在性能下降不显著的情况下，减小计算量。<span style="color:red;">真的假的？这样性能不会显著下降吗？现实中真的会这么做吗？</span>

经典的 LSTM 在计算各门控时，通常使用输入 $x_t$ 和隐层输出 $h_{t−1}$ 参与门控计算，例如对于输入门的更新： $i_{t}=\sigma\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\right)$ 。其最常见的变种是加入了窥孔机制[27]，让记忆 $c_{t−1}$ 也参与到了门控的计算中，此时输入门的更新方式变为：

$$
i_{t}=\sigma\left(W_{i} x_{t}+U_{i} h_{t-1}+V_{i} c_{t-1}+b_{i}\right)\tag{10.19}
$$

总而言之，LSTM 经历了 20 年的发展，其核心思想一脉相承，但各个组件都发生了很多演化。了解其发展历程和常见变种，可以让我们在实际工作和研究中，结合问题选择最佳的 LSTM 模块，灵活地思考，并知其所以然，而不是死背各种网络的结构和公式。<span style="color:red;">对呀，要怎么选择最佳的 LSTM 模块呢？还是要深入理解才行的。还是要好好总结的，最好再一个例子中分别使用各种模块，看看效果怎么样。</span>



# 为什么 LSTM 比 RNN 更能解决记忆长时依赖的问题？


RNN 中，$S_t=tanh(Wx_t+VS_{t-1})$ 是一个复合函数，那么它求偏导的时候由于里面的激活函数是 tanh，因此在求偏导的时候，有可能是约等于 0 的，如果约等于 0，那么就意味着 RNN 学不到什么东西，而什么时候会出现约等于 0 呢？链路越长越有可能存在一个是约等于 0 的，那么这种链路很长的情况下，就会带来梯度消失。

而 LSTM 呢？它实际上是两个函数求和： $C_t=f_t*C_{t-1}+i_t*\widetilde{C}_t$  ，两个函数之和求偏导的话就是两个偏导求和，这时候，如果有一个是约等于 0 的，那么结果也不一定会是 0，也就是说，会减少出现梯度消失的情况，也就是说，即使信息出现的时间比较远，也是可能学的到的。

也就是说 LSTM 最大的变化就是把 RNN 中的连乘的形式变成求和的形式。


# 相关

- 《百面机器学习》
- 七月在线  深度学习
