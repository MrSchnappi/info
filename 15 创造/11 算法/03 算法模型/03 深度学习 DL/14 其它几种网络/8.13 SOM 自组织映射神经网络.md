---
title: 8.13 SOM 自组织映射神经网络
toc: true
date: 2019-04-01
---
# 可以补充进来的

- 这个好像之前没怎么看到过，奇怪了，而且，这个在真实的项目中使用的话效果到底是什么样的？要补充下例子
- 而且，对于这个理论还是要再补充下，感觉看的有点不透彻。



# 自组织映射神经网络


自组织映射神经网络（Self-Organizing Map，SOM）是无监督学习方法中一类重要方法，可以用作聚类、高维可视化、数据压缩、特征提取等多种用途。

在深度神经网络大为流行的今天，谈及自组织映射神经网络依然是一件非常有意义的事情，这主要是由于自组织映射神经网络融入了大量人脑神经元的信号处理机制，有着独特的结构特点。<span style="color:red;">之前好像连听都没听说过这个。奇怪了。</span>

该模型由芬兰赫尔辛基大学教授 Teuvo Kohonen 于 1981 年提出，因此也被称为 Kohonen 网络。

自组织映射神经网络


## 自组织映射神经网络是如何工作的？它与 K 均值算法有何区别？

生物学研究表明，在人脑的感知通道上，神经元组织是有序排列的；同时，大脑皮层会对外界特定时空信息的输入在特定区域产生兴奋，而且相类似的外界信息输入产生对应兴奋的大脑皮层区域也连续映像的。

例如，生物视网膜中有许多特定的细胞对特定的图形比较敏感，当视网膜中有若干个接收单元同时受特定模式刺激时，就使大脑皮层中的特定神经元开始兴奋，且输入模式接近时与之对应的兴奋神经元也接近；

在听觉通道上，神经元在结构排列上与频率的关系十分密切，对于某个频率，特定的神经元具有最大的响应，位置相邻的神经元具有相近的频率特征，而远离的神经元具有的频率特征差别也较大。大脑皮层中神经元的这种响应特点不是先天安排好的，而是通过后天的学习自组织形成的。


在生物神经系统中，还存在着一种侧抑制现象，即一个神经细胞兴奋后，会对周围其他神经细胞产生抑制作用。这种抑制作用会使神经细胞之间出现竞争，其结果是某些获胜，而另一些则失败。表现形式是获胜神经细胞兴奋，失败神经细胞抑制。

自组织神经网络就是对上述生物神经系统功能的一种人工神经网络模拟。

自组织映射神经网络本质上是一个两层的神经网络，包含输入层和输出层（竞争层）。输入层模拟感知外界输入信息的视网膜，输出层模拟做出响应的大脑皮层。

输出层中神经元的个数通常是聚类的个数，代表每一个需要聚成的类。训练时采用“竞争学习”的方式，每个输入的样例在输出层中找到一个和它最匹配的节点，称为激活节点，也叫 winning neuron；紧接着用随机梯度下降法更新激活节点的参数；同时，和激活节点临近的点也根据它们距离激活节点的远近而适当地更新参数。这种竞争可以通过神经元之间的横向抑制连接（负反馈路径）来实现。自组织映射神经网络的输出层节点是有拓扑关系的。这个拓扑关系依据需求确定，如果想要一维的模型，那么隐藏节点可以是“一维线阵”；如果想要二维的拓扑关系，那么就行成一个“二维平面阵”，如图 5.8所示。也有更高维度的拓扑关系的，比如“三维栅格阵”，但并不常见。

![](http://images.iterate.site/blog/image/20190401/PUSTAO8TjwPW.png?imageslim){ width=55% }

假设输入空间是 D 维，输入模式为 ![](http://images.iterate.site/blog/image/20190401/LysUec8WQjvm.png?imageslim)，输入单元 i 和神经元 j 之间在计算层的连接权重为![](http://images.iterate.site/blog/image/20190401/IRmwQ3SyWa46.png?imageslim){ width=55% }，其中 N 是神经元的总数。自组织映射神经网络的自组织学习过程可以归纳为以下几个子过程。


1. 初始化。所有连接权重都用小的随机值进行初始化。
2. 竞争。神经元计算每一个输入模式各自的判别函数值，并宣布具有最小判别函数值的特定神经元为胜利者，其中每个神经元 $j$ 的判别函数为![](http://images.iterate.site/blog/image/20190401/edVIyMIqHK0N.png?imageslim){ width=55% }。
3. 合作。获胜神经元 $I（x）$ 决定了兴奋神经元拓扑邻域的空间位置。确定激活结点 $I（x）$ 之后，我们也希望更新和它临近的节点。更新程度计算如下：![](http://images.iterate.site/blog/image/20190403/3xWYz8ef1wjL.png?imageslim){ width=55% }，其中 $S_{ij}$ 表示竞争层神经元 $i$ 和 $j$ 之间的距离，![](http://images.iterate.site/blog/image/20190403/RlyWc7z0dVp1.png?imageslim){ width=55% }随时间衰减；简单地说，临近的节点距离越远，更新的程度要打更大折扣。
4. 适应。适当调整相关兴奋神经元的连接权重，使得获胜的神经元对相似输入模式的后续应用的响应增强：![](http://images.iterate.site/blog/image/20190403/28eT0Soz34dG.png?imageslim){ width=55% }， 其中依赖于时间的学习率定义为：![](http://images.iterate.site/blog/image/20190403/Lvhd7OXf5BC1.png?imageslim){ width=55% }。
5. 迭代。继续回到步骤 2，直到特征映射趋于稳定。

<span style="color:red;">没有特别明白。</span>

在迭代结束之后，每个样本所激活的神经元就是它对应的类别。

自组织映射神经网络具有保序映射的特点，可以将任意维输入模式在输出层映射为一维或者二维图形，并保持拓扑结构不变。这种拓扑映射使得“输出层神经元的空间位置对应于输入空间的特定域或特征”。由其学习过程可以看出，每个学习权重更新的效果等同于将获胜的神经元及其邻近的权向量 $w_i$ 向输入向量 $x$ 移动，同时对该过程的迭代进行会使得网络的拓扑有序。


在自组织映射神经网络中，获胜的神经元将使得相关的各权重向更加有利于它竞争的方向调整，即以获胜神经元为中心，对近邻的神经元表现出兴奋性侧反馈，而对远邻的神经元表现出抑制性侧反馈，近邻者互相激励，远邻者相互抑制。近邻和远邻均有一定的范围，对更远邻的神经元则表现弱激励的作用。

这种交互作用的方式以曲线可视化则类似于“墨西哥帽”，如图 5.9所示。

![](http://images.iterate.site/blog/image/20190403/4PfhIpaRvCRL.png?imageslim){ width=55% }

自组织映射神经网络与 K 均值算法的区别如下：

1. K 均值算法需要事先定下类的个数，也就是 K 的值。而自组织映射神经网络则不用，隐藏层中的某些节点可以没有任何输入数据属于它，因此聚类结果的实际簇数可能会小于神经元的个数。而 K 均值算法受 K 值设定的影响要更大一些。
2. K 均值算法为每个输入数据找到一个最相似的类后，只更新这个类的参数；自组织映射神经网络则会更新临近的节点。所以，K 均值算法受 noise data的影响比较大，而自组织映射神经网络的准确性可能会比 K 均值算法低（因为也更新了临近节点）。
3. 相比较而言，自组织映射神经网络的可视化比较好，而且具有优雅的拓扑关系图。


## 怎样设计自组织映射神经网络并设定网络训练参数？


**设定输出层神经元的数量**

输出层神经元的数量和训练集样本的类别数相关。若不清楚类别数，则尽可能地设定较多的节点数，以便较好地映射样本的拓扑结构，如果分类过细再酌情减少输出节点。这样可能会带来少量从未更新过权值的 “死节点”，但一般可通过重新初始化权值来解决。

**设计输出层节点的排列**

输出层的节点排列成哪种形式取决于实际应用的需要，排列形式应尽量直观地反映出实际问题的物理意义。例如，对于一般的分类问题，一个输出节点能代表一个模式类，用一维线阵既结构简单又意义明确；对于颜色空间或者旅行路径类的问题，二维平面则比较直观。


**初始化权值**

可以随机初始化，但尽量使权值的初始位置与输入样本的大概分布区域充分重合，避免出现大量的初始“死节点”。一种简单易行的方法是从训练集中随机抽取 $m$ 个输入样本作为初始权值。

**设计拓扑领域**

拓扑领域的设计原则是使领域不断缩小，这样输出平面上相邻神经元对应的权向量之间既有区别又有相当的相似性，从而保证当获胜节点对某一类模式产生最大响应时，其领域节点也能产生较大响应。领域的形状可以是正方形、六边形或者菱形。优势领域的大小用领域的半径表示，通常凭借经验来选择。

**设计学习率**

学习率是一个递减的函数，可以结合拓扑邻域的更新一起考虑，也可分开考虑。在训练开始时，学习率可以选取较大的值，之后以较快的速度下降，这样有利于很快地捕捉到输入向量的大致结构，然后学习率在较小的值上缓降至 0 值，这样可以精细地调整权值使之符合输入空间的样本分布结构。




# 相关

- 《百面机器学习》
