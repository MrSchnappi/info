---
title: 02 期望最大化
toc: true
date: 2019-06-05
---

# 期望最大化



我们介绍的第一个最大化下界 $\mathcal L$ 的算法是期望最大化算法。在潜变量模型中，这是一个非常常见的训练算法。在这里我们描述 {emview} 所提出的~EM~算法。与大多数我们在本章中介绍的其他算法不同的是，EM~并不是一个近似推断算法，而是一种能够学到近似后验的算法。



EM算法由交替迭代，直到收敛的两步运算组成：

+ E步: 令 ${\boldsymbol \theta^{(0)}}$ 表示在这一步开始时的参数值。对任何我们想要训练的（对所有的或者小批量数据均成立）索引为 $i$ 的训练样本 $\boldsymbol v^{(i)}$，令 $q(\boldsymbol h^{(i)}\mid \boldsymbol v) = p(\boldsymbol h^{(i)}\mid\boldsymbol v^{(i)};\boldsymbol \theta^{(0)})$。通过这个定义，我们认为 $q$ 在\emph{当前}参数 $\boldsymbol \theta^{(0)}$ 下定义。如果我们改变 $\boldsymbol \theta$，那么 $p(\boldsymbol h\mid\boldsymbol v;\boldsymbol \theta)$ 将会相应地变化，但是 $q(\boldsymbol h\mid\boldsymbol v)$ 还是不变并且等于 $p(\boldsymbol h\mid\boldsymbol v;\boldsymbol \theta^{(0)})$。
+ M步：使用选择的优化算法完全地或者部分地关于 $\boldsymbol \theta$ 最大化

$$\begin{aligned}
\sum_i \mathcal L(\boldsymbol v^{(i)},\boldsymbol \theta,q).
\end{aligned}$$




这可以被看作通过坐标上升算法来最大化 $\mathcal L$。在第一步中，我们更新分布 $q$ 来最大化 $\mathcal L$，而在另一步中，我们更新 $\boldsymbol \theta$ 来最大化 $\mathcal L$。



基于潜变量模型的随机梯度上升可以被看作是一个~EM~算法的特例，其中~M步包括了单次梯度操作。EM~算法的其他变种可以实现多次梯度操作。对一些模型族来说，M步甚至可以直接推出解析解，不同于其他方法，在给定当前 $q$ 的情况下直接求出最优解。



尽管 E 步采用的是精确推断，我们仍然可以将~EM~算法视作是某种程度上的近似推断。具体地说，M步假设一个分布 $q$ 可以被所有的 $\boldsymbol \theta$ 值分享。当~M步越来越远离~E步中的 $\boldsymbol \theta^{(0)}$ 时，这将会导致 $\mathcal L$ 和真实的 $\log p(\boldsymbol v)$ 之间出现差距。
幸运的是，在进入下一个循环时，E步把这种差距又降到了 $0$。




EM~算法还包含一些不同的见解。首先，它包含了学习过程的一个基本框架，就是我们通过更新模型参数来提高整个数据集的似然，其中缺失变量的值是通过后验分布来估计的。这种特定的性质并非~EM~算法独有的。例如，使用梯度下降来最大化对数似然函数的方法也有相同的性质。计算对数似然函数的梯度需要对隐藏单元的后验分布求期望。EM~算法另一个关键的性质是当我们移动到另一个 $\boldsymbol \theta$ 时候，我们仍然可以使用旧的分布 $q$。在传统机器学习中，这种特有的性质在推导大~M步更新时候得到了广泛的应用。在深度学习中，大多数模型太过于复杂以致于在最优大~M步更新中很难得到一个简单的解。所以~EM~算法的第二个特质，更多为其所独有，较少被使用。





# 相关

- 《深度学习》花书
