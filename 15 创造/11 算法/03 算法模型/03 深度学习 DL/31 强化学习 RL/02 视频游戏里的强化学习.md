---
title: 02 视频游戏里的强化学习
toc: true
date: 2019-04-20
---
# 可以补充进来的

- 感觉还是要找个真实的例子补充下。


# 视频游戏里的强化学习

游戏是强化学习中最有代表性也是最合适的应用领域之一，几乎涵盖了强化学习所有的要素：

- 环境—游戏本身的状态；
- 动作——用户操作；
- 机器人——程序；
- 回馈——得分、输赢等。

通过输入原始像素来玩视频游戏，是人工智能成熟的标志之一。<span style="color:red;">是呀</span>

雅达利（Atari）是二十世纪七八十年代红极一时的电脑游戏，类似于国内的红白机游戏，但是画面元素要更简单一些。它的模拟器相对成熟，使用雅达利游戏来测试强化学习，可谓量身定做。其应用场景可以描述为：在离散的时间轴上，每个时刻你可以得到当前的游戏画面（环境），选择向游戏机发出一个行动指令（如上、下、左、右、开火等），然后得到一个反馈（奖励）。


基于原始像素的强化学习由于对应的状态空间巨大，没有办法直接使用传统的方法。于是，2013 年 DeepMind 提出了深度强化学习模型，开始了深度学习和强化学习的结合[31]。<span style="color:red;">嗯，真是厉害。</span>

传统的强化学习主要使用 Q-learning，而深度强化学习也使用 Q-learning 为基本框架，把 Q-learning 的对应步骤改为深度形式，并引入了一些技巧，例如经验重放（experience replay）来加快收敛以及提高泛化能力。<span style="color:red;">什么是经验重放？</span>

强化学习，Q-learning

## 什么是深度强化学习，它和传统的强化学习有什么不同？

<span style="color:red;">嗯，也想知道是怎么实现的。</span>

2013年，DeepMind 提出的深度强化学习仍然使用经典的 Q-learning框架[31]。

Q-learning 的本质是，当前状态 $s_j$ 、回馈 $a_j$ 、<span style="color:red;">这个回馈的翻译简直了，不如直接说 action，查了半天回馈是啥单词。</span>奖励 $r_j$，以及 $Q$ 函数之间存在关系 $Q\left(s_{j}, a_{j}\right)=E_{s_{j+1}} y_{j}$ ，其中 $y_{j}=r_{j}+\gamma \cdot \max _{a} Q\left(s_{j+1}, a\right)$ 。

如果 $s_{j+1}$ 是终态，则 $y_j=r_j$ ，在传统的 Q-learning 中，考虑状态序列是无限的，所以并没有终态。依据这个关系，可以对 Q 函数的取值做迭代改进。所以如果我们有一个四元组 $\left(s_{j}, a_{j}, r_{j}, s_{j+1}\right)$ ，我们可以用随机梯度下降法的思想对 $Q$ 函数迭代前后的平方差距 $\left(y_{j}-Q\left(s_{j},a_{j}\right)\right)^{2}$ 做一次梯度下降。


<center>

![](http://images.iterate.site/blog/image/20190420/48XK13QzL2VW.png?imageslim){ width=55% }

</center>

经典的 Q-learning 算法如图 11.5所示。为了能与 Deep Q-learning 作对比，我们把最后一步 Q 函数更新为等价的描述：令 $y_{t}=r_{t}+\gamma \cdot \max _{a} Q\left(s_{t+1}, a ; \theta\right)$ ，并对 $\left(y_{t}-Q\left(s_{t}, a_{t}\right)\right)^{2}$ 执行一次梯度下降，完成参数更新。


<center>

![](http://images.iterate.site/blog/image/20190420/mFow9K9keMcF.png?imageslim){ width=55% }

</center>

图 11.6是深度 Q-learning 算法，其中红色部分为和传统 Q-learning 不同的部分。
比较这两个算法，我们不难发现深度 Q-learning 和传统的 Q-learning 的主体框架是相同的，在每一次子迭代中，都是按照以下步骤进行。


1. 根据当前的 $Q$ 函数执行一次行动 $a_t$。
2. 获得本次收益 $r_t$ 及下一个状态 $s_{t+1}$。
3. 以某种方式获得一个四元组 $\left(s_{j}, a_{j}, r_{j}, s_{j+1}\right)$。
4. 计算 $y_j$。
5. 对 $\left(y_{j}-Q\left(s_{j}, a_{j} ; \theta\right)\right)^{2}$ 执行一次梯度下降，完成参数更新。



表 11.1是传统 Q-learning 与深度 Q-learning 的对比：

<center>

![](http://images.iterate.site/blog/image/20190420/V5qUOfdIe0Vk.png?imageslim){ width=55% }

</center>

以获得状态的方式为例，传统 Q-learning 直接从环境观测获得当前状态；而在深度 Q-learning 中，往往需要对观测的结果进行某些处理来获得 $Q$ 函数的输入状态。比如，用深度 Q-learning玩 Atari 游戏时，是这样对观察值进行处理的：在 $t$ 时刻观察到的图像序列及对应动作 $x_{1}, a_{1}, x_{2}, \dots, x_{t}, a_{t}, x_{t+1}$ ，通过一个映射函数 $\phi\left(x_{1}, a_{1}, x_{2}, \ldots, x_{t}, a_{t}, x_{t+1}\right)$ ，得到处理后的标准状态。在实际的应用中，$\varphi$ 选择最后 4 帧图像，并将其堆叠起来。<span style="color:red;">有点没有特别的清楚。到底是怎么实现的？补充一个案例进来。</span>



## 逸闻趣事 从多巴胺到强化学习

多巴胺是一种让人感到兴奋和愉悦的神经递质，由大脑分泌。多巴胺和强化学习听起来相距甚远，但出人意料的是它们在本质上其实具有很多共通性- **都是为了获得延迟到来的奖励。**

当我们获得超过期望的回报或者奖励时，大脑会释放大量多巴胺，让我们感到兴奋和愉悦。那么决定多巴胺的释放的因素是什么呢？答案是奖励和预期之间的差值。如果你是一个养宠物的人，有没有观察到这种现象：通常在给宠物喂食之前，它们便开始分泌口水。俄国科学家巴普洛夫便做过这样一个试验：在给狗喂食之前先摇铃铛，训练狗将铃响和食物联系在一起，以后没有看到食物时也会流口水。

随着检测技术的提高，**科学家发现多巴胺的释放并非来源于奖励本身，而是来自于对奖励的预期。** 当现实的回报高于预期时，会促成多巴胺的释放，让人觉得生活美好。相反的，如果现实的回报总是不及预期，多巴胺的分泌量会降低，人们也会慢慢觉得生活一成不变，缺乏乐趣。<span style="color:red;">厉害呀。</span>

平衡预期与回报之间的差距正是时间差分学习（Temporal Difference Learning）的目标：根据现实回报和预期的差值来调整价值函数的值，这与大脑分泌多巴胺的机制异曲同工。时间差分学习可以用于优化 V-function 也可以用于优化 Q-function，而本节介绍的 Q-learning 正是时间差分算法的一个特例。<span style="color:red;">什么是时间差分学习？什么是 V-function？什么是 Q-function？为什么 Q-learning 是时间差分算法的一个特例？</span>





# 相关

- 《百面机器学习》

