---
title: 05 值函数近似
toc: true
date: 2018-07-01 16:09:12
---


# 值函数近似




前面我们一直假定强化学习任务是在有限状态空间上进行，每个状态可用一个编号来指代；值函数则是关于有限状态的“表格值函数” (tabular value function)，即值函数能表示为一个数组，输入 $i$ 对应的函数值就是数组元素 $i$ 的值，且更改一个状态上的值不会影响其他状态上的值。然而，现实强化学习任务所面临的状态空间往往是连续的，有无穷多个状态。这该怎么办呢?

一个直接的想法是对状态空间进行离散化，将连续状态空间转化为有限离 散状态空间，然后就能使用前面介绍的方法求解。遗憾的是，如何有效地对状态 空间进行离散化是一个难题，尤其是在对状态空间进行探索之前.

实际上，我们不妨直接对连续状态空伺的值函数进行学习。假定状态空间为 $n$ 维实数空间 $X=\mathbb{R}^{n}$，此时显然无法用表格值函数来记录状态值。先考虑简 单情形，即值函数能表达为状态的线性函数[Busoniu et al., 2010]

$$
V_{\boldsymbol{\theta}}(\boldsymbol{x})=\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x}
$$

其中 $\boldsymbol{x}$ 为状态向量，$\boldsymbol{\theta}$ 为参数向量。由于此时的值函数难以像有限状态那 样精确记录每个状态的值，因此这样值函数的求解被称为值函数近似(value function approximation).

我们希望通过式(16.32)学得的值函数尽可能近似真实值函数 $V^{\pi}$，近似程 度常用最小二乘误差来度量：

$$
E_{\boldsymbol{\theta}}=\mathbb{E}_{\boldsymbol{x} \sim \pi}\left[\left(V^{\pi}(\boldsymbol{x})-V_{\boldsymbol{\theta}}(\boldsymbol{x})\right)^{2}\right]
$$

其中 $\mathbb{E}_{\boldsymbol{x} \sim \pi}$ 表示由策略 $\pi$ 所采样而得的状态上的期望.

为了使误差最小化，采用梯度下降法，对误差求负导数

$$
\begin{aligned}-\frac{\partial E_{\boldsymbol{\theta}}}{\partial \boldsymbol{\theta}} &=\mathbb{E}_{\boldsymbol{x} \sim \pi}\left[2\left(V^{\pi}(\boldsymbol{x})-V_{\boldsymbol{\theta}}(\boldsymbol{x})\right) \frac{\partial V_{\boldsymbol{\theta}}(\boldsymbol{x})}{\partial \boldsymbol{\theta}}\right] \\ &=\mathbb{E}_{\boldsymbol{x} \sim \pi}\left[2\left(V^{\pi}(\boldsymbol{x})-V_{\boldsymbol{\theta}}(\boldsymbol{x})\right) \boldsymbol{x}\right] \end{aligned}
$$

于是可得到对于单个样本的更新规则

$$
\boldsymbol{\theta}=\boldsymbol{\theta}+\alpha\left(V^{\pi}(\boldsymbol{x})-V_{\boldsymbol{\theta}}(\boldsymbol{x})\right) \boldsymbol{x}
$$

我们并不知道策略的真实值函数 $V^{\pi}$ ，但可借助时序差分学习，基于 $V^{\pi}(\boldsymbol{x})=r+\gamma V^{\pi}\left(\boldsymbol{x}^{\prime}\right)$ 用当前估计的值函数代替真实值函数，即

$$
\begin{aligned} \boldsymbol{\theta} &=\boldsymbol{\theta}+\alpha\left(r+\gamma V_{\boldsymbol{\theta}}\left(\boldsymbol{x}^{\prime}\right)-V_{\boldsymbol{\theta}}(\boldsymbol{x})\right) \boldsymbol{x} \\ &=\boldsymbol{\theta}+\alpha\left(r+\gamma \boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x}^{\prime}-\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x}\right) \boldsymbol{x} \end{aligned}
$$

其中 $\boldsymbol{x}^{\prime}$ 是下一时刻的状态.

需注意的是，在时序差分学习中需要状态-动作值函数以便获取策略。这里 一种简单的做法是令 $\boldsymbol{\theta}$ 作用于表示状态和动作的联合向量上，例如给状态向量 增加一维用于存放动作编号，即将式(16.32)中的 $\boldsymbol{x}$ 替换为 $(\boldsymbol{x} ; a)$ ；另一种做法是 用 0/1对动作选择进行编码得到向量 $\boldsymbol{a}=(0 ; \ldots ; 1 ; \ldots ; 0)$ ，其中“1”表示该动 作被选择，再将状态向量与其合并得到 $(\boldsymbol{x} ; \boldsymbol{a})$，用于替换式(16.32)中的 $\boldsymbol{x}$ ，这样 就使得线性近似的对象为状态-动作值函数.


基于线性值函数近似来替代 Sarsa 算法中的值函数，即可得到图 16.14的 线性值函数近似 Sarsa 算法。类似地可得到线性值函数近似 Q-学习算法。显然， 可以容易地用其他学习方法来代替式(16.32)中的线性学习器，例如通过引入核 方法实现非线性值函数近似.

<center>

![](http://images.iterate.site/blog/image/20190709/oXhuNeNBViQs.png?imageslim){ width=55% }

</center>








# 相关
- 《机器学习》周志华
