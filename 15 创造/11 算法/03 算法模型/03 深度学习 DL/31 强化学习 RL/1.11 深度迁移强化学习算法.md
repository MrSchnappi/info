---
title: 1.11 深度迁移强化学习算法
toc: true
date: 2019-09-04
---

## 10.5 深度迁移强化学习算法

传统深度强化学习算法每次只能解决一种游戏任务，无法在一次训练中完成多种任务。迁移学习和强化学习的结合也是深度强化学习的一种主要思路。<span style="color:red;">嗯。</span>

- Parisotto 等提出了一种基于行为模拟的深度迁移强化学习算法。该算法通过监督信号的指导，使得单一的策略网络学习各自的策略，并将知识迁移到新任务中。
- Rusa 等提出策略蒸馏(policy distillation)深度迁移强化学习算法。策略蒸馏算法中分为学习网络和指导网络，通过这两个网络 $Q$ 值的偏差来确定目标函数，引导学习网络逼近指导网络的值函数空间。
- 此后，Rusa 等又提出了一种基于渐进神经网络(progressive neural networks，PNN)的深度迁移强化学习算法，PNN是一种把神经网络和神经网络连起来的算法。它在一系列序列任务中，通过渐进的方式来存储知识和提取特征，完成了对知识的迁移。PNN 最终实现多个独立任务的训练，通过迁移加速学习过程，避免灾难性遗忘。
- Fernando 等提出了路径网络(PathNet)[45]，PathNet 可以说是 PNN 的进阶版。PathNet 把网络中每一层都看作一个模块，把构建一个网络看成搭积木，也就是复用积木。它跟 PNN 非常类似，只是这里不再有列，而是不同的路径。PathNet 将智能体嵌入到神经网络中，其中智能体的任务是为新任务发现网络中可以复用的部分。智能体是网络之中的路径，其决定了反向传播过程中被使用和更新的参数范围。在一系列的 Atari 强化学习任务上，PathNet 都实现了正迁移，这表明 PathNet 在训练神经网络上具有通用性应用能力，PathNet也可以显著提高 A3C 算法超参数选择的鲁棒性。
- Schaul 等提出了一种通用值函数逼近器(universalvalue function approximators，UVFAs)来泛化状态和目标空间。UVFAs 可以将学习到的知识迁移到环境动态特性相同但目标不同的新任务中。

<span style="color:red;">这么多，想看下是怎么实现的。</span>





# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
