---
title: 13 附录
toc: true
date: 2019-08-04
---
# 附录

## 迁移学习相关的期刊和会议

迁移学习仍然是一个蓬勃发展的研究领域。最近几年，在顶级期刊和会议上，越来越多的研究者开始发表文章，不断提出迁移学习的新方法，不断开拓迁移学习的新应用领域。

在这里，我们对迁移学习相关的国际期刊和会议作一小结，以方便初学者寻找合适的论文。

这些期刊会议可以在下表中找到。

<center>

![mark](http://images.iterate.site/blog/image/20190804/PxLpsPHkewpW.png?imageslim)

</center>


## 迁移学习研究学者

这里列出了一些迁移学习领域代表性学者以及他们的最具代表性的工作，以供分享。

一般这些工作都是由他们一作，或者是由自己的学生做出来的。当然，这里所列的文章比起这些大牛发过的文章会少得多，这里仅仅列出他们最知名的工作。本部分开源在了 Github [^1]，会一直有更新，欢迎补充！

**应用研究**

**1. [Qiang Yang](http://www.cs.ust.hk/~qyang/) @ HKUST**

迁移学习领域权威大牛。他所在的课题组基本都做迁移学习方面的研究。迁移学习综述《A
survey on transfer learning》就出自杨强老师课题组。他的学生们：

**1). [Sinno J. Pan](http://www.cs.ust.hk/~qyang/) @ NTU**

现为老师，详细介绍见第二条。

**2). [Ben Tan](https://sites.google.com/view/btan/home)**

主要研究传递迁移学习(transitive transfer
learning)，现在腾讯做高级研究员。代表文章：

Transitive Transfer Learning. KDD 2015.

Distant Domain Transfer Learning. AAAI 2017.

**3). [Derek Hao
Hu](https://scholar.google.com/citations?user=Ks81aO0AAAAJ&hl=zh-CN&oi=ao)**

主要研究迁移学习与行为识别结合，目前在 Snap 公司。代表文章：

Transfer Learning for Activity Recognition via Sensor Mapping. IJCAI
2011.

Cross-domain activity recognition via transfer learning. PMC 2011.

Bridging domains using world wide knowledge for transfer learning. TKDE
2010.

**4). [Vencent Wencheng
Zheng](https://sites.google.com/site/vincentwzheng/)**

也做行为识别与迁移学习的结合，目前在新加坡一个研究所当研究科学家。代表文章：

User-dependent Aspect Model for Collaborative Activity Recognition.
IJCAI 2011.

Transfer Learning by Reusing Structured Knowledge. AI Magazine.

Transferring Multi-device Localization Models using Latent Multi-task
Learning. AAAI 2008.

Transferring Localization Models Over Time. AAAI 2008.

Cross-Domain Activity Recognition. Ubicomp 2009.

Collaborative Location and Activity Recommendations with GPS History
Data. WWW 2010.

**5). Ying Wei**

做迁移学习与数据挖掘相关的研究。代表工作：

Instilling Social to Physical: Co-Regularized Heterogeneous Transfer
Learning. AAAI 2016.

Transfer Knowledge between Cities. KDD 2016.

Learning to Transfer. arXiv 2017.

其他还有很多学生都做迁移学习方面的研究，更多请参考杨强老师主页。

**2. [Sinno J. Pan](http://www.cs.ust.hk/~qyang/) @ NTU**

杨强老师学生，比较著名的工作是 TCA 方法。现在在 NTU 当老师，一直都在做迁移学习研究。代表工作：

A Survey On Transfer Learning. TKDE 2010. [最著名的综述]

Domain Adaptation via Transfer Component Analysis. TNNLS 2011.
[著名的 TCA 方法]

Cross-domain sentiment classification via spectral feature alignment.
WWW 2010. [著名的 SFA 方法]

Transferring Localization Models across Space. AAAI 2008.

**3. [Lixin Duan](http://www.lxduan.info/) @ UESTC**

毕业于 NTU，现在在 UESTC 当老师。代表工作：

Domain Transfer Multiple Kernel Learning. PAMI 2012.

Visual Event Recognition in Videos by Learning from Web Data. PAMI 2012.

**4. [Mingsheng Long](http://ise.thss.tsinghua.edu.cn/~mlong/) @ THU**

毕业于清华大学，现在在清华大学当老师，一直在做迁移学习方面的工作。代表工作：

Dual Transfer Learning. SDM 2012.

Transfer Feature Learning with Joint Distribution Adaptation. ICCV 2013.

Transfer Joint Matching for Unsupervised Domain Adaptation. CVPR 2014.

Learning transferable features with deep adaptation networks. ICML 2015.
[著名的 DAN 方法]

Deep Transfer Learning with Joint Adaptation Networks. ICML 2017.

**5. [Judy Hoffman](http://people.eecs.berkeley.edu/~jhoffman/) @ UC
Berkeley & Stanford**

Feifei Li的博士后，现在当老师。她有个学生叫做 Eric
Tzeng，做深度迁移学习。代表工作：

Simultaneous Deep Transfer Across Domains and Tasks. ICCV 2015.

Deep Domain Confusion: Maximizing for Domain Invariance. arXiv 2014.

Adversarial Discriminative Domain Adaptation. CVPR 2017.

**6. [Fuzhen Zhuang](http://www.intsci.ac.cn/users/zhuangfuzhen/) @ ICT,
CAS**

中科院计算所当老师，主要做迁移学习与文本结合的研究。代表工作：

Transfer Learning from Multiple Source Domains via Consensus
Regularization. CIKM 2008.

**7. [Kilian Q. Weinberger](https://www.cs.cornell.edu/~kilian/) @
Cornell U.**

现在康奈尔大学当老师。[Minmin
Chen](http://www.cse.wustl.edu/~mchen/)是他的学生。代表工作：

Distance metric learning for large margin nearest neighbor
classification. JMLR 2009.

Feature hashing for large scale multitask learning. ICML 2009.

An introduction to nonlinear dimensionality reduction by maximum
variance unfolding. AAAI 2006. [著名的 MVU 方法]

Co-training for domain adaptation. NIPS 2011. [著名的 Co-training方法]

**8. [Fei Sha](http://www.cse.wustl.edu/~mchen/) @ USC**

USC教授。他曾经的学生[Boqing
Gong](http://www.cecs.ucf.edu/faculty/boqing-gong/)提出了著名的 GFK 方法。代表工作：

Connecting the Dots with Landmarks: Discriminatively Learning
Domain-Invariant Features for Unsupervised Domain Adaptation. ICML 2013.

Geodesic flow kernel for unsupervised domain adaptation. CVPR 2012.
[著名的 GFK 方法]

**9. Mahsa Baktashmotlagh @ U. Queensland**

现在当老师。主要做流形学习与 domain adaptation结合。代表工作：

Unsupervised Domain Adaptation by Domain Invariant Projection. ICCV
2013.

Domain Adaptation on the Statistical Manifold. CVPR 2014.

Distribution-Matching Embedding for Visual Domain Adaptation. JMLR 2016.

**10. [Baochen
Sun](https://www.microsoft.com/en-us/research/people/baochens/) @
Microsoft**

现在在微软。著名的 CoRAL 系列方法的作者。代表工作：

Return of Frustratingly Easy Domain Adaptation. AAAI 2016.

Deep coral: Correlation alignment for deep domain adaptation. ECCV 2016.

**11. Wenyuan Dai**

著名的第四范式创始人，虽然不做研究了，但是当年求学时几篇迁移学习文章至今都很高引。代表工作：

Boosting for transfer learning. ICML 2007. [著名的 TrAdaboost 方法]

Self-taught clustering. ICML 2008.

**理论研究**

**1. [Arthur Gretton](http://www.gatsby.ucl.ac.uk/~gretton/) @ UCL**

主要做 two-sample test。代表工作：

A Kernel Two-Sample Test. JMLR 2013.

Optimal kernel choice for large-scale two-sample tests. NIPS 2012.
[著名的 MK-MMD]

**2. [Shai Ben-David](https://cs.uwaterloo.ca/~shai/) @ U.Waterloo**

很多迁移学习的理论工作由他给出。代表工作：

Analysis of representations for domain adaptation. NIPS 2007.

A theory of learning from different domains. Machine Learning 2010.

**3. [Alex Smola](https://alex.smola.org/) @ CMU**

做一些机器学习的理论工作，和上面两位合作比较多。代表工作非常多，不列了。

**4. [John Blitzer](https://alex.smola.org/) @ Google**

著名的 SCL 方法提出者，现在也在做机器学习。代表工作：

Domain adaptation with structural correspondence learning. ECML 2007.
[著名的 SCL 方法]

**5. [Yoshua
Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html) @
U.Montreal**

深度学习领军人物，主要做深度迁移学习的一些理论工作。代表工作：

Deep Learning of Representations for Unsupervised and Transfer Learning.
ICML 2012.

How transferable are features in deep neural networks? NIPS 2014.

Unsupervised and Transfer Learning Challenge: a Deep Learning Approach.
ICML 2012.

**6. [Geoffrey
Hinton](http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html) @
U.Toronto**

深度学习领军人物，也做深度迁移学习的理论工作。

Distilling the knowledge in a neural network. NIPS 2014.

## 迁移学习资源汇总

-   (可能是有史以来)最全的迁移学习资料库，(文章/资料/代码/数据):
    <https://github.com/jindongwang/transferlearning>

-   迁移学习视频教程: <https://www.youtube.com/watch?v=qD6iD4TFsdQ>

-   知乎专栏“机器有颗玻璃心”中《小王爱迁移》系列:
    <https://zhuanlan.zhihu.com/p/27336930>，用浅显易懂的语言深入讲解经典+最新的迁移学习文章

-   迁移学习与领域自适应论文分享与笔记
    Paperweekly：<http://www.paperweekly.site/collections/231/papers>

-   迁移学习与领域自适应公开数据集
    <https://github.com/jindongwang/transferlearning/blob/master/doc/dataset.md>

## 迁移学习常用算法及数据资源

为了研究及测试算法性能，收集了若干来自图像、文本、及时间序列的公开数据集。并且，为了与自己所研究的算法进行对比，收集了领域内若干较前沿的算法及其相关代码，以便后期进行算法的比较测试。

**数据集：**

表 [tb-dataset]收集了迁移学习领域常用的数据集。这些数据集的详细介绍和下载地址，在 Github [^2]上可以找到。我们还在 Benchmark [^3]上提供了一些常用算法的实验结果。

| **序号** | **数据集**     | **类型** | **样本数** | **特征数** | **类别数** |
| -------- | -------------- | -------- | ---------- | ---------- | ---------- |
| 1        | USPS           | 字符识别 | 1800       | 256        | 10         |
| 2        | MNIST          | 字符识别 | 2000       | 256        | 10         |
| 3        | PIE            | 人脸识别 | 11554      | 1024       | 68         |
| 4        | COIL20         | 对象识别 | 1440       | 1024       | 20         |
| 5        | Office+Caltech | 对象识别 | 2533       | 800        | 10         |
| 6        | ImageNet       | 图像分类 | 7341       | 4096       | 5          |
| 7        | VOC2007        | 图像分类 | 3376       | 4096       | 5          |
| 8        | LabelMe        | 图像分类 | 2656       | 4096       | 5          |
| 9        | SUN09          | 图像分类 | 3282       | 4096       | 5          |
| 10       | Caltech101     | 图像分类 | 1415       | 4096       | 5          |
| 11       | 20newsgroup    | 文本分类 | 25804      | /          | 6          |
| 12       | Reuters-21578  | 文本分类 | 4771       | /          | 3          |
| 13       | OPPORTUNITY    | 行为识别 | 701366     | 27         | 4          |
| 14       | DSADS          | 行为识别 | 2844868    | 27         | 19         |
| 15       | PAMAP2         | 行为识别 | 1140000    | 27         | 18         |




**各数据集的简要介绍：**

**1. 手写体识别图像数据集**

MNIST和 USPS 是两个通用的手写体识别数据集，它们被广泛地应用于机器学习算法评测的各个方面。USPS数据集包括 7,291张训练图片和 2,007张测试图片，图片大小为 16$\times$16。MNIST数据集包括 60,000张训练图片和 10,000张测试图片，图片大小 28$\times$28。USPS和 MNIST 数据集分别服从显著不同的概率分布，两个数据集都包含 10 个类别，每个类别是 1-10之间的某个字符。为了构造迁移学习人物，在 USPS 中随机选取 1,800张图片作为辅助数据、在 MNIST 中随机选取 2,000张图片作为目标数据。交换辅助领域和目标领域可以得到另一个分类任务 MNIST
vs
USPS。图片预处理包括:将所有图片大小线性缩放为 16$\times$16，每幅图片用 256 维的特征向量表征，编码了图片的像素灰度值信息。辅助领域和目标领域共享特征空间和类别空间，但数据分布显著不同。

**2. 人脸识别图像数据集**

PIE代表“朝向、光照、表情”的英文单词首字母，该数据集是人脸识别的基准测试集，包括 68 个不同人物的 41,368幅人脸照片，图片大小为 32$\times$32，每个人物的照片由 13 个同步的相机(不同朝向)、21个不同曝光程度拍摄。简单起见，实验中采用 PIE 的预处理集，包括 2 个不同子集 PIE1 和 PIE2，是从正面朝向的人脸照片集合(C27)中按照不同的关照和曝光条件随机选出。按如下方法构造分类任务 PIEI
vs
PIE2：将 PIE1 作为辅助领域、PIE2作为目标领域；交换辅助领域和目标领域可以得到分类任务 PIE2
vs
PIEI。这样，辅助领域和目标领域分别由不同光照、曝光条件的人脸照片组成，从而服从显著不同的概率分布。

**3. 对象识别数据集**

COIL20包含 20 个对象类别共 1,440张图片；每个对象类别包括 72 张图片，每张图片拍摄时对象水平旋转 5 度(共 360 度)。每幅图片大小为 32$\times$32，表征为 1,024维的向量。实验中将该数据集划分为两个不相交的子集 COIL1 和 COIL2：COIL1包括位于拍摄角度为 $[0\textdegree,85\textdegree]\cup[180\textdegree,265\textdegree]$(第一、三象限)的所有图片；COIL2包括位于拍摄角度为 $[90\textdegree,175\textdegree]\cup[270\textdegree,355\textdegree]$(第二、四象限)的所有图片。这样，子集 COIL1 和 COIL2 的图片因为拍摄角度不同而服从不同的概率分布。将 COIL1 作为辅助领域、COIL2作为目标领域，可以构造跨领域分类任务 COIL1
vs COIL2；交换辅助领域和目标领域，可以得到另外一个分类任务 COIL2 vs
COIL1。

Office是视觉迁移学习的主流基准数据集，包含 3 个对象领域 Amazon(在线电商图片)、Webcam(网络摄像头拍摄的低解析度图片)、DSLR(单反相机拍摄的高解析度图片)，共有 4,652张图片 31 个类别标签。Caltech-256是对象识别的基准数据集，包括 1 个对象领域 Caltech，共有 30,607张图片 256 个类别标签。对每张图片抽取 SURF 特征，并向量化为 800 维的直方图表征，所有直方图向量都进行减均值除方差的归一化处理，直方图码表由 K 均值聚类算法在 Amazon 子集上生成。具体共有 4 个领域 C(Caltech-256),
A(Amazon),
W(Webcam)和 D(DSLR)，从中随机选取 2 个不同的领域作为辅助领域和目标领域，则可构造 $4 \times 3 = 12$ 个跨领域视觉对象识别任务，如 $A \rightarrow D, A \rightarrow C, \cdots, C \rightarrow W$。

**4. 大规模图像分类数据集**

大规模图像分类数据集包含了来自 5 个域的图像数据：ImageNet、VOC
2007、SUN、LabelMe、以及 Caltech。它们包含 5 个类别的图像数据：鸟，猫，椅子，狗，人。对于每个域的数据，均使用 DeCaf @donahue2014decaf进行特征提取，并取第 6 层的特征作为实验使用，简称 DeCaf6 特征。每个样本有 4096 个维度。

**5. 通用文本分类数据集**

20-Newsgroups数据集包含约 20,000个文档，4个大类分别为 comp,
rec,sci和 talk，每个大类包含 4 个子类，详细信息如表 2.2所示。在实验中构造了 6 组跨领域二分类任务，每组任务由 4 个大类中随机选取 2 个大类构成，一个大类记为正例，另一个大类记为负例，6个任务组具体为 comp vs rec, comp vs sci, comp vs talk, rec vs sci, rec vs talk和、sci vs talk。每个跨领域分类任务(包括辅助领域和目标领域)按如下方法生成:每个任务组 p VS的两个大类 p 和 Q 分别包含 4 个子类 P1、P2、P3、P4和 Q1、Q2、Q3、Q4；随机选取 p 的两个子类(如 P1、P2)与 Q 的两个子类(如 Q1、Q2)构成辅助领域，其余子类(P的 P3 和 P4 和 Q3 和 Q4 构成目标领域。以上构造策略既保证辅助领域和目标领域是相关的，因为它们都来自同样的大类；又保证辅助领域和目标领域是不同的，因为它们来自不同的子类。每个任务组 P VS Q可以生成 36 个分类任务，总计 6 个任务组共生成 6$\times$36 =216个分类任务。数据集经过文本预处理后包含 25,804个词项特征和 15,033个文档，每个文档由 tf-idf向量表征。

Reuters-21578是一个较难的文本数据集，包含多个大类和子类。其中最大 3 个大类为 orgs,people和 place，可构造 6 个跨领域文本分类任务 orgs vs people,people vs orgs, orgs vs place, place vs orgs, people vs place和 place vs people。

**6. 行为识别公开数据集**

行为识别是典型的时间序列分类任务。为了测试算法在时间序列任务上的性能，收集了 3 个公开的行为识别数据集：OPPORTUNITY、DASDS和 PAMAP2。OPPORTUNITY数据集包含 4 个用户在智能家居中的多种不同层次的行为。DAADS数据集包含 8 个人的 19 种日常行为。PAMAP2数据集包含 9 个人的 18 种日常生活行为。所有数据集均包括加速度计、陀螺仪和磁力计三种运动传感器。

**算法：**

收集的数据表征、迁移学习等相关领域的基准算法如下表所示。我们还在持续更新的 Github [^4]上提供了各种算法的实现代码。


<center>

![mark](http://images.iterate.site/blog/image/20190804/PiGw2OwUtUai.png?imageslim)

</center>






# 相关

- [迁移学习简明手册](http://jd92.wang/assets/files/transfer_learning_tutorial_wjd.pdf)



[^1]: <https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md>
[^2]: <https://github.com/jindongwang/transferlearning/blob/master/doc/dataset.md>
[^3]: <https://github.com/jindongwang/transferlearning/blob/master/doc/benchmark.md>
[^4]: <https://github.com/jindongwang/transferlearning>
