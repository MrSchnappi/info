---
title: 2.12 多任务学习 multitask-learning
toc: true
date: 2019-09-01
---
# 多任务学习


multitask-learning


源数据和目标数据均为标记数据。


在 Fine-tuning上，我们其实只关心迁移模型在目标数据域上的学习任务完成的好不好，而不关心在源数据域上表现如何。

而 Multitask Learning是会同时关注这两点的。<span style="color:red;">为什么还要关注在源数据集上的效果？</span>

![mark](http://images.iterate.site/blog/image/20190901/ltCLnjzRj66Y.png?imageslim)

如上图，我们针对两种任务 A 和 B，我们用它们的数据一起训练 NN 模型的前几层，再分别训练模型的后面几层包括输出层，以针对各自任务输出针对性的结果，这样的好处是由于训练数据量的增加，模型的性能可能会更好。关键是要确定两个任务有没有共通性，即是不是能共用前面几层。

另外，也可以在中间几层用共同数据来训练。

<span style="color:red;">嗯，看到这个就知道为啥 multi-tasking 可以归到迁移学习里面了。</span>

## 应用实例 多语言识别

多任务学习比较成果的一个应用实例是：多语言识别。

![mark](http://images.iterate.site/blog/image/20190901/xULcTFsuQXQN.png?imageslim)

如上图，多国语言语音识别模型的前几层共享一些公共特征。

那么，就又产生了一个问题：语言迁移的范围可以有多广呢？

下图是：借助欧洲语言帮助中文的识别（在有欧洲语言数据辅助的情况下，只用大约 50 小时的中文数据可以达到用 100 小时中文数据单独训练的效果）。

<center>

![mark](http://images.iterate.site/blog/image/20190901/rYoGkXVQm7eK.png?imageslim)

</center>

<span style="color:red;">厉害。</span>

## 渐进式神经网络 Progressive Neural Network

Progressive NN

这个是 deepmind 提的，很久了。不知道最新的 连续学习 是什么进展。

<center>

![mark](http://images.iterate.site/blog/image/20190901/taxW8OrgB49q.png?imageslim)

</center>

先针对 Task1 训练一个 NN，在训练 Task2 的 NN 时，它的每一个隐层都会借用一个 Task1 中的 NN 的隐层（也可以直接设为全 0 的参数，相当于不借用），这样对 Task1 的模型性能不会有影响，也可以在 Task2 中对其已有参数进行借用。<span style="color:red;">没有很清楚，这个属于多任务学习吗？</span>



思路就是说我不能忘记第一个任务的网络，同时又能使用第一个任务的网络来做第二个任务。

为了不忘记之前的任务，他们的方法简单暴力：对所有的之前任务的网络，保留并且 fix，每次有一个新任务就新建一个网络（一列）。

而为了能使用过去的经验，他们同样也会将这个任务的输入输入进所有之前的网络，并且将之前网络的每一层的输出，与当前任务的网络每一层的输出一起输入下一层。

<center>

![mark](http://images.iterate.site/blog/image/20190901/i8gArXSVX9zF.png?imageslim)

</center>

每次有一个新的任务，就重新添加一列，然后将前几列的输出 fuse 到当前列来。

比如说，如果两个任务的 low level 特征类似，则当前任务网络中的前几层可能完全没有用处，只需要用之前任务的输出就够了。

但是一个很明显的问题是，这个网络不能学到自己的 low level feature 的网络，然后使用之前网络的 high level决策。因为 1，当 low level不一样的时候，将输入输入之前的网络就不 make sense了；更重要的是，当前列的输入根本无法输入进之前列的网络，只复用高层网络根本无从谈起。

所以这里的限制就是，两个任务需要有类似的 low level feature。当然啦，这篇文章还是有很酷的视频，也确实用到了一些任务上。

![mark](http://images.iterate.site/blog/image/20190901/UNLiSL2Xf1uD.png?imageslim)

用几句话就能够说明白这个所谓的 progressive neural networks到底是什么了！简直不能再简单！


就是：

1. 构造一个多层的神经网络，训练某一个任务，上图第一列
2. 构建第二个多层的神经网络，然后固定第一列也就是上一个任务的神经网络，将上一列的神经网络的每一层（注意是每一层）都通过 a 处理连接到第二列的神经网络的每一层作为额外输入。也就是第二个神经网络每一层除了原始的输入，还加上经过 a 处理的之前的神经网络对应层的输入。
3. 构建第三个多层神经网络，训练第三个任务，将前两列的神经网络固定，然后同上一样的方法连接到第三个神经网络中。

上图的线很清楚的表示了这个过程。

这就是把神经网络和神经网络连起来的方法！

a的作用其实主要是为了降维和输入的维度统一（与原始输入匹配），用简单的 MLP 来表示！

总的来说，就是抽取之前的神经网络的信息与当前的输入信息融合，然后训练！训练的效果就可以和没有加前面的神经网络的方法对比，如果效果好很多说明前面的神经网络有用，知识有迁移！

这种方法的好处就是之前的训练都保留，不至于像 fine tune那样更改原来的网络！而且每一层的特征信息都能得到迁移，并且能够更好的具化分析。

缺点就是参数的数量会随着任务的增加而大量增加！并且不同任务的设计需要人工知识。


# 相关

- [迁移学习（Transfer Learning）](https://blog.csdn.net/qq_32690999/article/details/78849565)
