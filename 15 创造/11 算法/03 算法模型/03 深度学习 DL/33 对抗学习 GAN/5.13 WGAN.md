---
title: 5.13 WGAN
toc: true
date: 2019-09-03
---
# 可以补充进来的

- [令人拍案叫绝的Wasserstein GAN](https://zhuanlan.zhihu.com/p/25071913)

# WGAN 如何解决训练崩溃问题？



GAN本质上是通过生成器和判别器进行对抗训练，逼迫生成器在不知晓某个数据集合真实分布的情形下，通过不断调整生成数据的分布去拟合逼近这个真实数据分布，所以计算当前训练过程中两个分布和的距离度量标准就很关键。

Wasserstein GAN的作者敏锐地指出了:原始 GAN 在计算两个分布的距离时采用的是 Jensen-Shannon Divergence（JSD），它本质上是 KL Divergence（KLD）的一个变种。JSD或者 KLD 存在一个问题：当两个分布交集很少时或者在低维流形空间下，判别器很容易找到一个判别面将生成的数据和真实数据区分开，这样判别器就不能提供有效的梯度信息并反向传导给生成器，生成器就很难训练下去，因为缺乏来自判别器指导的优化目标。

Wasserstein GAN提出了使用 Earth-Mover距离来代替 JSD 标准，这很大程度上改进了 GAN 的训练稳定性。

后续的 Fisher GAN等模型又对 Wasserstein GAN进行了进一步的改进，这些技术陆续改善了 GAN 的训练稳定性。



<span style="color:red;">没有很深理解。</span>

WGAN 作者提出了使用 Wasserstein 距离，以解决 GAN 网络训练过程难以判断收敛性的问题。

Wasserstein 距离定义如下:

$$
L={\rm E}_{x\sim{p_{data}}(x)}[f_w(x)] - {\rm E}_{x\sim{p_g}(x)}[f_w(x)]
$$


通过最小化 Wasserstein 距离，得到了 WGAN 的 Loss：

- WGAN生成器 Loss：$- {\rm E}_{x\sim{p_g}(x)}[f_w(x)]​$
- WGAN判别器 Loss：$L=-{\rm E}_{x\sim{p_{data}}(x)}[f_w(x)] + {\rm E}_{x\sim{p_g}(x)}[f_w(x)]$

从公式上 GAN 似乎总是让人摸不着头脑，在代码实现上来说，其实就以下几点：

- 判别器最后一层去掉 sigmoid
- 生成器和判别器的 loss 不取 log <span style="color:red;">为什么不取 log ？</span>
- 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数 $c$ <span style="color:red;">怎么截断？为什么？</span>

### 7.4.6 WGAN-GP：带有梯度正则的 WGAN

​实际实验过程发现，WGAN 没有那么好用，主要原因在于 WAGN 进行梯度截断。梯度截断将导致判别网络趋向于一个二值网络，造成模型容量的下降。

于是作者提出使用梯度惩罚来替代梯度裁剪。公式如下：

$$
L=-{\rm E}_{x\sim{p_{data}}(x)}[f_w(x)] + {\rm E}_{x\sim{p_g}(x)}[f_w(x)]+\lambda{\rm E}_{x\sim{p_x}(x)}[\lVert\nabla_x(D(x))\rVert_p-1]^2
$$

由于上式是对每一个梯度进行惩罚，所以不适合使用 BN，因为它会引入同个 batch 中不同样本的相互依赖关系。如果需要的话，可以选择 Layer Normalization。<span style="color:red;">LN 和 BN 有什么不同吗？</span>实际训练过程中，就可以通过 Wasserstein 距离来度量模型收敛程度了：

<center>

![](http://images.iterate.site/blog/image/20190722/kBRLswv29AyY.png?imageslim){ width=55% }

</center>

> Wass距离随迭代次数变化

上图纵坐标是 Wasserstein 距离，横坐标是迭代次数。可以看出，随着迭代的进行，Wasserstein 距离趋于收敛，生成图像也趋于稳定。
