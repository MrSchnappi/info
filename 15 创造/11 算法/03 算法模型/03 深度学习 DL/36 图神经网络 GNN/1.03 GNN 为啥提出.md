---
title: 1.03 GNN 为啥提出
toc: true
date: 2019-09-01
---
## 当下 AI 的瓶颈：

现有的 Deep Learning的一些模型无非 MLP，CNN，和 RNN。这些模型更倾向于是对现实世界的 representation learning。

有两个问题：

- **Combinatorial Generalization** （CG）：即泛化能力，如何解决利用有限样本学习无限可能的这个本质难点，即如何实现‘infinite use of finite means’，如何将 over-fitting问题在根源上解决；
- Relational Reasoning: 即学习事物之间的关系和联系，而不仅仅是讲一个 input 提特征描述为其另一种表征。


## 为何遇到如此瓶颈：

首先要从当前 DL 模型所能处理的数据说起。以 CNN 举例，它的 input 往往是 image，而 image（或者想 RNN 处理 NLP 中的 sentence 等）是一种 Euclidean domain的，展现出 grid 性质，工整对称的数据。因此，CNN所采用的工整对称的 filter 可以很好地发挥作用。然而，这一类数据是‘单调无趣’的，即它并不能描述出 entities 于 entities 之间多变的 relations。这就从根源上限制了对复杂问题的描述力。

而世界上另一种数据表现形式是 Non-Euclidean Domain，这类数据包括比如社交网络，蛋白质结构，交通网络等等展现出网络结构或者个体间相互连接的结构。这一类数据内部没有规则的 grid 结构，而是展现出一种多变的动态的拓扑。这种结构能够很好地表征个体之间的关系，适合于 relation reasoning。

其次，从学习模型(网络)的结构解释。一个深度学习模型（网络）往往是由多个深度学习 building blocks通过某种方式连接而成的。对于 CNN 而言，其基础的 building block就是 Conv layers（和 pooling layers等）。一个模型的 CG 能力，取决于两点：

1. Structured representation & computation of the building blocks：所谓 structured，我理解就是让该 block 自己内部也具有某种拓扑结构，进而使得其具有更高的可操作性，可学习性。对于一个 Conv filter而言，就是一个正方形的 grid，在空间结构上来说是 fixed 的，可学习的量只有各个权重值；直观上看，讲一个原本固定的行人 BBox 模型进化成 part-based pictorial structure模型就是一个将模型 structured 化，得到更强 RIB 的例子。一个 pictorial 模型仿照 spring 模型，由 node 和相连的 edge 构成。而这些 edges，就是描述 relations 的最好载体，也是 configurablility 的来源。

2. **Relational Inductive Bias (RIB)** available in the model: RIB，可以理解为 assumptions or known properties about the learning process, which imposes constraints on relationships and interactions among entities in the learning process. 也就是说，RIB就是模型 block 或者结构中存在的某种已掌握的定式，一种规则，使得这个学习模型不是完全 random 或者 unstructured，而是参与了一定程度人为干预的。比如在 Bayesian 学习中，prior info就是一种 RIB，在 CNN 的 conv layer中，locality就是一种 RIB，在 RNN 中，cell之间的 sequentiality（Markovan）也是一种 RIB，在 MLP 中，加入 regularization term也是一种 RIB。

Learning的过程就是不断调整 learnable 参数的值，从而不断缩小解空间，直到找到最优解。RIB的作用，就是通过增加人为可控且已知的限制，直接缩小解空间。


所以说，一个模型的 RIBs，直接决定这个模型的学习能力，和泛化能力。因为当模型的自身规则性更强之后，对于训练数据的变化就展现更强的稳定性。


**因此，如何设计一个模型（blocks），使其具有 structured representation，而且使其内部具有强的 RIBs，而且让这个模型 generally and uniformlly可以适应 both Euclidean or Non-Euclidean数据，就是突破深度学习瓶颈的关键。**



# 相关

- [GNN新作《Relational inductive biases，deep learning，and graph networks》读书笔记](https://blog.csdn.net/Trasper1/article/details/81475667)
