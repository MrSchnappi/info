---
title: 2.12 DeepWalk
toc: true
date: 2019-09-01
---
# DeepWalk 第一个无监督学习节点嵌入的算法


DeepWalk [2] 是第一个提出以无监督的方式学习节点嵌入的算法。

它在训练过程中非常类似于词汇嵌入。其动机是 graph 中节点和语料库中单词的分布都遵循幂律，如下图所示：


<center>

![mark](http://images.iterate.site/blog/image/20190901/apKQGwWOktla.png?imageslim)

</center>
该算法包含两个步骤：

1. 在 graph 中的节点上执行 random walks，以生成节点序列

2. 运行 skip-gram，根据步骤 1 中生成的节点序列，学习每个节点的嵌入

在 random walks 的每个时间步骤中，下一个节点从上一个节点的邻节点均匀采样。然后将每个序列截断为长度为 $2|w| + 1$ 的子序列，其中 $w$ 表示 skip-gram 中的窗口大小。

本文采用 hierarchical softmax 来解决由于节点数量庞大而导致的 softmax 计算成本高昂的问题。为了计算每个单独输出元素的 softmax 值, 我们必须计算元素 $k$ 的所有 $e ^ xk$。

Softmax 的定义：

$$
\operatorname{softmax}(x)_{i}=\frac{e^{x_{i}}}{\sum_{k=1}^{K} e^{x_{k}}}
$$


因此，原始 softmax 的计算时间为 $O(|V|)$，其中 $V$ 表示图中顶点的集合。

分层 softmax 利用二叉树来处理这个问题。在这个二叉树中，所有的叶子 (下图中的 v1, v2，…v8) 都表示 graph 中的顶点。在每个内部节点中，都有一个二元分类器来决定选择哪条路径。要计算给定顶点 $v_k$ 的概率，只需计算从根节点到叶节点 $v_k$ 路径上每一个子路径的概率。由于每个节点的子节点的概率之和为 1，所以所有顶点的概率之和为 1的特性在分层 softmax 中仍然保持不变。由于二叉树的最长路径是 $O(log(n))$，其中 n表示叶节点的数量，因此一个元素的计算时间现在减少到 $O(log|V|)$。

Hierarchical Softmax：

![mark](http://images.iterate.site/blog/image/20190901/0vF3oYhdKHzU.png?imageslim)




在训练完 DeepWalk GNN 之后，模型已经学习了每个节点的良好表示，如下图所示。不同的颜色表示输入图中的不同标签。我们可以看到，在输出图 (2 维嵌入) 中，具有相同标签的节点被聚集在一起，而具有不同标签的大多数节点都被正确地分开了。

<center>

![mark](http://images.iterate.site/blog/image/20190901/Stw36kN4Lu1V.png?imageslim)

</center>

然而，DeepWalk 的主要问题是缺乏泛化能力。每当一个新节点出现时，它必须重新训练模型以表示这个节点。因此，这种 GNN 不适用于图中节点不断变化的动态图。


# 相关

- [掌握图神经网络 GNN 基本，看这篇文章就够了](https://posts.careerengine.us/p/5c64eebe4337430d41ceae7a)
