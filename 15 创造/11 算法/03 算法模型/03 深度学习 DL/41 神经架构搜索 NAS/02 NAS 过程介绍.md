---
title: 02 NAS 过程介绍
toc: true
date: 2019-08-31
---

NAS 工作流程，通常从定义一组神经网络可能会用到的 “建筑模块” 开始。

比如说 Google Brain那篇 NasNet 论文，就为图像识别网络总结了这些常用模块：

<center>

![mark](http://images.iterate.site/blog/image/20190829/vHKNRA253LQr.png?imageslim)

</center>


其中包含了多种卷积和池化模块。


NAS 算法用一个循环神经网络（RNN）作为**控制器**，从这些模块中挑选，然后将它们放在一起，来创造出某种端到端的架构。

这个架构，通常会呈现出和 ResNet、DenseNet等最前沿网络架构一样的风格，但是内部模块的组合和配置有所区别。一个架构的好坏，往往就取决于选择的模块和在它们之间构建的连接。

接下来，就要训练这个新网络，让它收敛，得到在留出验证集上的准确率。这个准确率随后会用来通过策略梯度更新控制器，让控制器生成架构的水平越来越高。

过程如下图所示：


<center>

![mark](http://images.iterate.site/blog/image/20190829/uqhlAufSPF2Q.png?imageslim)

</center>


这个过程很直观了。简单来说，很有小朋友搭积木的风范：让一个算法挑出一些积木，然后把它们组装在一起，做成一个神经网络。训练、测试，根据这个结果来调整选积木的标准和组装的方式。

这个算法大获成功，NasNet论文展现出非常好的结果，有一部分原因是出于他们做出的限制和假设。

论文里训练、测试 NAS 算法发现的架构，都用了一个比现实情况小得多的数据集。当然，这是一种折衷的方法，要在 ImageNet 那么大的数据集上训练验证每一种搜索结果，实在是太耗费时间了。

所以，他们做出了一个**假设**：如果一个神经网络能在结构相似的小规模数据集上得到更好的成绩，那么它在更大更复杂的数据集上同样能表现得更好。

在深度学习领域，这个假设基本上是成立的。

上面还提到了一个**限制**，这指的是搜索空间其实很有限。他们设计 NAS，就要用它来构建和当前最先进的架构风格非常类似的网络。

在图像识别领域，这就意味着用一组模块重复排列，逐步下采样，如下图所示：


<center>

![mark](http://images.iterate.site/blog/image/20190829/mgdQxc3eB8Ys.png?imageslim)

</center>


这些模块也都是当前研究中常用的。NAS算法在其中所做的新工作，主要是给这些模块换个连接方式。

下面，就是它发现的 ImageNet 最佳神经网络架构：


<center>

![mark](http://images.iterate.site/blog/image/20190829/SebeJqb59Hdo.png?imageslim)

</center>
