---
title: 2.01 NASNet的设计策略
toc: true
date: 2019-09-04
---

### 14.6.4 NASNet的设计策略

NASNet是最早由 google brain 通过网络架构搜索策略搜索并成功训练 ImageNet 的网络，其性能超越所有手动设计的网络模型。关于 NASNet 的搜索策略，首先需要参考 google brain发表在 ICLR2017 的论文《Neural Architecture Search with Reinforcement Learning》。该论文是最早成功通过架构搜索策略在 cifar-10数据集上取得比较不错效果的工作。NASNet很大程度上是沿用该搜索框架的设计思想。

NASNet的核心思想是利用强化学习对搜索空间内的结构进行反馈探索。架构搜索图如下，定义了一个以 RNN 为核心的搜索控制器。在搜索空间以概率 p 对模型进行搜索采样。得到网络模型 A 后，对该模型进行训练，待模型收敛得到设定的准确率 R 后，将梯度传递给控制器 RNN 进行梯度更新。

<center>

![](http://images.iterate.site/blog/image/20190722/Uc29zmBf61oe.png?imageslim){ width=55% }

</center>


								架构搜索策略流程

RNN控制器会对卷积层的滤波器的尺寸、数量以及滑动间隔进行预测。每次预测的结果都会作为下一级的输入，档层数达到设定的阈值时，会停止预测。而这个阈值也会随着训练的进行而增加。这里的控制器之预测了卷积，并没有对例如 inception 系列的分支结构或者 ResNet 的跳级结构等进行搜索。所以，控制器需要进一步扩展到预测这些跳级结构上，这样搜索空间相应的也会增大。为了预测这些结构，RNN控制器内每一层都增加了一个预测跳级结构的神经元，文中称为锚点，稍有不同的是该锚点的预测会由前面所有层的锚点状态决定。

<center>

![](http://images.iterate.site/blog/image/20190722/8CUXdJLamtvz.png?imageslim){ width=55% }

</center>


								RNN控制器

NASNet大体沿用了上述生成网络结构的机器，并在此基础上做了如下两点改进：

1、先验行地加入 inception 系列和 ResNet 的堆叠模块的思想。其定义了两种卷积模块，Normal Cell和 Reduction Cell，前者不进行降采样，而后者是个降采样的模块。而由这两种模块组成的结构可以很方便的通过不同数量的模块堆叠将其从小数据集搜索到的架构迁移到大数据集上，大大提高了搜索效率。

<center>

![](http://images.iterate.site/blog/image/20190722/kUrq9B2HCIeL.png?imageslim){ width=55% }

</center>


								NASNet的 RNN 控制器

2、对 RNN 控制进行优化，先验性地将各种尺寸和类型的卷积和池化层加入到搜索空间内，用预测一个卷积模块代替原先预测一层卷积。如图，控制器 RNN 不在预测单个卷积内的超参数组成，而是对一个模块内的每一个部分进行搜索预测，搜索的空间则限定在如下这些操作中：

					• identity					  • 1x3 then 3x1 convolution
					• 1x7 then 7x1 convolution	  • 3x3 dilated convolution
					• 3x3 average pooling 			  • 3x3 max pooling
					• 5x5 max pooling			  • 7x7 max pooling
					• 1x1 convolution				  • 3x3 convolution
					• 3x3 depthwise-separable conv • 5x5 depthwise-seperable conv
					• 7x7 depthwise-separable conv

在模块内的连接方式上也提供了 element-wise addition和 concatenate 两种方式。NASNet的搜索方式和过程对 NAS 的一些后续工作都具有非常好的参考借鉴意义。
