---
title: 4.01 Transformer
toc: true
date: 2019-09-29
---
# 可以补充进来的

- 仔细补充下。

# Transformer

Transformer 是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前 NLP 里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到 Transformer 一跃成为踢开 RNN 和 CNN 传统特征提取器，荣升头牌，大红大紫。


Transformer在未来应该会逐渐替代掉 RNN 成为主流的 NLP 工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正 RNN 结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？再说 CNN，CNN在 NLP 里一直没有形成主流，CNN的最大优点是易于做并行计算，所以速度快，但是在捕获 NLP 的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显 Transformer 同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过 RNN 和 CNN。


而介绍 Transformer 比较好的文章可以参考以下两篇文章：一个是 Jay Alammar可视化地介绍 Transformer 的博客文章[The Illustrated Transformer](https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/) ，非常容易理解整个机制，建议先从这篇看起；然后可以参考哈佛大学 NLP 研究组写的“[The Annotated Transformer.](https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/2018/04/03/attention.html) ”，代码原理双管齐下，讲得非常清楚。我相信上面两个文章足以让你了解 Transformer 了，所以这里不展开介绍。



# 相关

- [深度学习中的注意力模型（2017版）](https://zhuanlan.zhihu.com/p/37601161)
