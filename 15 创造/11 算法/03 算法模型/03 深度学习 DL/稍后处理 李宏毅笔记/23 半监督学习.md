---
title: 23 半监督学习
toc: true
date: 2019-08-18
---
![mark](http://images.iterate.site/blog/image/20190818/hsx6uS0J5OpL.png?imageslim)

## 监督学习和半监督学习
![mark](http://images.iterate.site/blog/image/20190818/C4N45H0KHR9w.png?imageslim)
在 supervised 里面，你就是有一大推的 training data，这些 training data的组成是一个 function 的 input 跟 output，假设你有 R 笔 train data，每一笔 train data有 $x^r$,$\hat{y}^r $。假设 $x^r$ 是一张 image，$\hat{y}$ 是 class label。semi-supervised learning是在 label 上面，是有另外一组 unlabel 的 data，这组 data 记做 $x^u$，这组 data 只有 input，没有 output(U笔 data)。在做 semi-superised learning时，U远远大于 R(unlabel的数量远远大于 label data的数量)。semi-surprised learning可以分成两种，一种是 transductive learning，一种是 inductive learning。这两种最简单的分法是：在做 transductive 的时候，你的 unlabel data就是你的 testing data，inductive learning 就是说：不把 unlabel data考虑进来。

为什么做 semi-supervised learning，因为有人常会说，我们缺 data，其实我们不是缺 data，其实我们缺的是与 label 的 data。比如说，你收集 image 很容易(在街上一直照就行了)，但是这些 image 是没有的 label。label data 是很少的，unlabel是非常多的。所以 semi-surprvised learning如果可以利用这些 unlabel data来做某些事是会很有价值的。

我们人类可能一直是在 semi-supervised learning，比如说，小孩子会从父母那边得到一点点的 supervised(小孩子在街上，问爸爸妈妈这是什么，爸爸妈妈说：这是狗。在以后的日子里，小孩子会看到很多奇奇怪怪的东西，也没有人在告诉这是什么动物，但小孩子依然还是会判别出狗)

### 半监督学习的好处
![mark](http://images.iterate.site/blog/image/20190818/yruVftmvw2MV.png?imageslim)
为什么 semi-supervised learning有可能会带来帮助呢？假设我们现在要做分类的 task，建一个猫跟狗的 classifier，我们同时有一大堆猫跟狗的图片。这些图片是没有 label 的，并不知道哪些是猫哪些是狗。

![mark](http://images.iterate.site/blog/image/20190818/DJdfev8dM8iL.png?imageslim)
那今天我们只考虑有 label 的猫跟狗的 data，画一个 boundary，将猫跟狗的 train data分开的话，你可能就会画在中间(垂直)。那如果 unlabel 的分布长的像灰色的点这个样子的话，这可能会影响你的决定。虽然 unlabel data只告诉我们了 input，但**unlabeled data的分布**可以告诉我们一些事。那你可能会把 boundary 变为这样(斜线)。但是 semi-supervised learning使用 unlabel 的方式往往伴随着一些假设，其实 semi-supervised learning有没有用，是取决于你这个假设符不符合实际/精不精确。

![mark](http://images.iterate.site/blog/image/20190818/Y7GMMlJjKUmN.png?imageslim)
这边要讲四件事，第一个是在 generative model的时候，怎么用 semi-supervised learning。还要讲两个还蛮通用的假设，一个是 Low-density Separation Assumption，另一个是 Smoothness Assumption，最后还有 Better Representation

## 监督生成模型和半监督生成模型
### 监督生成模型
![mark](http://images.iterate.site/blog/image/20190818/62LPrTFyMlY0.png?imageslim)

我们都已经看过，supervised generative model，在 supervised learning里面有一堆 train example，你知道分别是属于 class1，class2。你会去估测 class1，class2的 probability($P(X|C_i)$)

假设每一个 class 它的分布都是一个 Gaussion distribution，那你会估测说 class1 是从μ是 $μ^1$，covariance是 $\Sigma$ 的 Gaussion 估测出来的，class2是从μ是 $μ^2$，covariance是 $\Sigma$ 的 Gaussion 估测出来的。

那现在有了这些 probability，有了这些 $μ$、covariance，你就可以估测 given 一个新的 data 做 classification，然后你就会决定 boundary 的位置在哪里。
### 半监督生成模型
![mark](http://images.iterate.site/blog/image/20190818/8hA05F8iwAUY.png?imageslim)

但是今天给了我们一些 unlabel data，它就会影响你的决定。举例来说，我们看左边这笔 data，我们假设绿色这些使 unlabel data，那如果你的 $\mu $ 跟 variance 是 $\mu ^1$,$\mu ^2,\Sigma$ 显然是不合理的。今天这个 $\Sigma$ 应该比较接近圆圈，或者说你在 sample 的时候有点问题，所以你 sample 出比较奇怪的 distribution。比如说，这两个 class label data是比较多的(可能 class2 是比较多的，所以这边 probability 是比较大的)，总之看这些 unlabel data以后，会影响你对 probability，$\mu$,$\Sigma$ 的估测，就影响你的 probability 的式子，就影响了你的 decision boundary。

![mark](http://images.iterate.site/blog/image/20190818/liVBIPV2eMlW.png?imageslim)
对于实际过程中的做法，我们先讲操作方式，再讲原理。先初始化参数(class1,class2的几率，$\mu ^1$,$\mu ^2,\Sigma$，这些值，你可以用已经有 label data先估测一个值，得到一组初始化的参数，这些参数统称 $\theta$

- Step1  先计算每一笔 unlabel data的 posterior probability，根据现有的 $\theta$ 计算每一笔 unlabel data属于 class1 的几率，那这个几率算出来是怎么样的是和你的 model 的值有关的。

- Step2 算出这个几率以后呢，你就可以 update 你的 model，这个 update 的式子是非常的直觉，这个 $C_1$ 的 probability 是怎么算呢，原来的没有 unlabel data的时候，你的计算方法可能是：这个 N 是所有的 example,$N_1$ 是被标注的 $C_1$example，如果你要算 $C_1$ 的 probability，这件事情太直觉了，如果不考虑 unlabel data的话(感觉就是 $N_1$ 除以 N)。但是现在我们要考虑 unlabel data，那根据 unlabel 告诉我们的咨询，$C_1$ 是出现次数就是所有 unlabel data它是 $C_1$posterior probability的和。所有 unlabel data而是根据它的 posterior probability决定它有百分之多少是属于 $C_1$，有多少是属于 $C_2$，$\mu^1$ 怎么算呢，原来不考虑 unlabel data时，$\mu^1$ 就是把所有 $C_1$ 的 label data都平均起来就结束了。如果今天加上 unlabel data的话，其实就是把 unlabel data的每一笔 data$x^u$ 根据它的 posterior probability做相乘。如果这个 $x^u$ 比较偏向 class1$C^1$ 的话，它对 class1 的影响就大一点，反之就小一点。(不用解释这是为什么这样，因为这太直觉了)$C_2$ 的 probability就是这样的做的 $\mu^1,\mu^2,\sum$ 也都是这样做的，有了新的 model，你就会做 step1，有了新的 model 以后，这个几率就不一样了，这个几率不一样了，在做 step2，你的 model 就不一样了。这样 update 你的几率，然后就反复反复的下去。理论上这个方法会保证收敛，但是它的初始值跟 GD 会影响你收敛的结果。

这里的 Step1 就是 Estep，而 Step2 就是 Mstep（也就是熟悉的 EM 算法）

![mark](http://images.iterate.site/blog/image/20190818/VTLAKKmm9bLb.png?imageslim)

我们现在来解释下为什么这样做的：想法是这样子的。假设我们有原来的 label data的时候，我们要做的事情是 maximum likehood，每一笔 train data 它的 likehood 是可以算出来的。把所有的 log likehood加起来就是 log total loss。然后去 maximum。那今天是 unlabel data的话今天是不一样的。unlabel data我们并不知道它是来自哪一个 class，我们咋样去估测它的几率呢。那我们说一笔 unlabel data$x^u$ 出现的几率(我不知道它是从 claas1 还是 class2 来的，所以 class1，class2都有可能)就是它在 $C_1$ 的 posterior probability跟 $C_1$ 这个 class 产生这笔 unlabel data的几率加上 $C_2$ 的 posterior probability乘以 $C_2$ 这个 class 产生这笔 unlabel data的几率。把他们通通合起来，就是这笔 unlabel data产生的几率。

接下来要做事情就是 maximum 这件事情。但是由于不是凸函数，所以你要去 iteratively solve这个函数


## 假设一：Low-density Separation
![mark](http://images.iterate.site/blog/image/20190818/XlgtnOLyNxw0.png?imageslim)
那接下来我们讲一个 general 的方式，这边基于的假设是 Low-density Separation，也就是说：这个世界非黑即白的。什么是非黑即白呢？非黑即白意思就是说：假设我们现在有一大堆的 data(有 label data，也有 unlabel data)，在两个 class 之间会有一个非常明显的红色 boundary。比如说：现在两边都是 label data，boundary 的话这两条直线都是可以的，就可以把这两个 class 分开，在 train data上都是 100%。但是你考虑 unlabel data的话，左边的 boundary 是比较好的，右边的 boundary 是不好的。因为这个假设是基于这个世界是一个非黑即白的世界，这两个类之间会有一个很明显的界限。Low-density separation意思就是说，在这两个 class 交界处，density是比较低的。

### Self-training

![mark](http://images.iterate.site/blog/image/20190818/jwQ1UXU8M7z1.png?imageslim)
Low-density separation最简单的方法是 self-training。self-training就是说，我们有一些 label data并且还有一些 unlabel data。接下来从 label data中去 train 一个 model，这个 model 叫做 $f^\ast $，根据这个 $f^\ast$ 去 label 你的 unlabel data。你就把 $x^u$ 丢进 $f^\ast$，看它吐出来的 $y^u$ 是什么，那就是你的 label data。那这个叫做 pseudo-label。那接下来你要从你的 unlabel data set中拿出一些 data，把它加到 labeled data set里面。然后再回头去 train 你的 $f^\ast$

在做 regression 时是不能用这一招的，主要因为把 unlabeled data加入到训练数据中，$f^\ast$ 并不会受影响

![mark](http://images.iterate.site/blog/image/20190818/OMoV4jjBnCuk.png?imageslim)

你可能会觉得 slef-training它很像是我们刚才 generative model里面用的那个方法。他们唯一的差别就是在做 self-training的时候，你用的是 hard label；你在做 generative mode时，你用的是 soft model。在做 self-training的时候我们会强制一笔 train data是属于某一个 class，但是在 generative model的时候，根据它的 posterior probability 它有一部分是属于 class1 一部分是属于 class2。那到底哪一个比较好呢？那如果我们今天考虑的 neural network的话，你可以比较看看哪一个方法比较好。

假设我们用 neural network，你从你的 label data得到一笔 network parameter($\theta^\ast $)。现在有一笔 unlabel data$x^u$，根据参数 $\theta^\ast $ 分为两类(0.7的几率是 class1,0.3的几率是 class2)。如果是 hard label的话，你就把它直接 label 成 class1，所以 $x^u$ 新的 target 第一维是 1 第二维是 0(拿 $x^u$train neural network)。如果去做 soft 的话。70 percent是属于 class1,30percent是属于 class2，那新的 target 是 0.7跟 0.3。在 neural network中，这两个方法你觉得哪个是有用的呢，soft这个方法是没有用的，一定要用 hard label。因为本来输出就是 0.7和 0.3，目标又设成 0.7和 0.3，相当于自己证明自己，所以没用。但我们用 hard label 是什么意思呢？我们用 hard label的时候，就是用 low-density separation的概念。也就是说：今天我们看 $x^u$ 它属于 class1 的几率只是比较高而已，我们没有很确定它一定是属于 class1 的，但这是一个非黑即白的世界，如果你看起来有点像 class1，那就一定是 class1。本来根据我的 model 说：0.7是 class1 0.3是 class2，那用 hard label(low-density-separation)就改成它属于 class1 的几率是 1(完全就不可能是 class2)。soft是不会 work 的。


### 基于熵的正则化
![mark](http://images.iterate.site/blog/image/20190818/WxerTVs6oKhm.png?imageslim)

刚才那一招有进阶版是“Entropy-based Regularization”。如果你用 neural network，你的 output 是一个 distribution，那我们不要限制说这个 output 一定要是 class1、class2，但是我们做的假设是这样的，这个 output distribution一定要是很集中，因为这是一个非黑即白的世界。假设我们现在做五个 class 的分类，在 class1 的几率很大，在其他 class 的几率很小，这个是好的。在 class5 的几率很大，在其他 class 上几率很小，这也是好的。如果今天分布很平均的话，这样是不好的(因为这是一个非黑即白的世界)，这不是符合 low-density separation的假设。

但是现在的问题是咋样用数值的方法 evaluate 这个 distribution 是好的还是不好的。这边用的是 entropy，算一个 distribution 的 entropy，这个 distribution entropy告诉你说：这个 distribution 到底是集中的还是不集中的。我们用一个值来表示 distribution 是集中的还是分散的，某一个 distribution 的 entropy 就是负的它对每一个 class 的几率乘以 log class的几率。所以我们今天把第一个 distribution 的几率带到这个公式里面去，只有一个是 1 其他都是 0，你得到的 entropy 会得到是 0($E(y^u)=-\sum_{m=1}^{5}y^u_m(lny^u)$)，第二个也是 0。第三个 entropy 是 $ln5​$。散的比较开(不集中)entropy比较大，散的比较窄(集中)entropy比较小。

所以我们需要做的事情是，这个 model 的 output 在 label data上分类整确，但在 unlabel data上的 entropy 越小越好。所以根据这个假设，你就可以去重新设计你的 loss function。我们原来的 loss function是说：我希望找一个参数，让我现在在 label data上 model 的 output 跟正确的 model output越小越好，你可以 cross entropy evaluate它们之间的距离，这个是 label data的部分。在 unlabel data的部分，你会加上每一笔 unlabel data的 output distribution的 entropy，那你会希望这些 unlabel data的 entropy 越小越好。那么在这两个中间，你可以乘以一个 weight($ln5$)来考虑说：你要偏向 unlabel data多一点还是少一点

在 train 的时候，用 GD 来一直 minimize 这件事情，没有什么问题的。unlabel data的角色就很像 regularization，所以它被称之为 entropy-based regulariztion。之前我们说 regularization 是在原来的 loss function后面加一个惩罚项(L2,L1)，让它不要 overfitting；现在加上根据 unlabel data得到的 entropy 来让它不要 overfitting。

### 半监督 SVM
![mark](http://images.iterate.site/blog/image/20190818/kwGdlp0uxj8T.png?imageslim)

那还有其他 semi-supervised的方式，叫做 semi-supervised SVM。SVM精神是这样的：SVM做的事情就是：给你两个 class 的 data，找一个 boundary，这个 boundary 一方面要做有最大的 margin(最大 margin 就是让这两个 class 分的越开越好)同时也要有最小的分类的错误。现在假设有一些 unlabel data，semi-supervised SVM会咋样处理这个问题呢？它会穷举所有可能的 label，就是这边有 4 笔 unlabel data，每一笔它都可以是属于 class1，也可以是属于 class2，穷举它所有可能的 label(如右图所示)。对每一个可能的结果都去做一个 SVM，然后再去说哪一个 unlabel data的可能性能够让你的 margin 最大同时又 minimize error。

问题：穷举所有的 unlabel data label，这是非常多的事情。这篇 paper 提出了一个 approximate 的方法，基本精神是：一开始得到一些 label，然后你每次该一笔 unlabel data看可不可以让 margin 变大，变大了就改一下。
## 假设二：Smoothness Assumption
![mark](http://images.iterate.site/blog/image/20190818/w1MRICJQ412o.png?imageslim)
接下来，我们要讲的方法是 Smoothness Assumption。近朱者赤，近墨者黑

![mark](http://images.iterate.site/blog/image/20190818/G2Ew6jQeSKiX.png?imageslim)
它的假设是这样子的，如果 x 是相似的，那 label y就要相似。光讲这个假设是不精确的，因为正常的 model，你给它一个 input，如果不是很 deep 的话，output就很像，这样讲是不够精确的。

真正假设是下面所要说的，x的分布是不平均的，它在某些地方是很集中，某些地方又很分散。如果今天 $x_1,x_2$ 它们在 high density region很 close 的话，$y^1,y^2$ 才会是是很像的。
high density region这句话就是说：可以用 high density path做 connection，可以还不知道在说什么。举个例子，假设图中是 data 的分布，分布就像是写轮眼一样，那现在假设我们有三笔 data($x_1,x_2,x_3$)。如果我们今天考虑的是比较粗略的假设(相似的 x，那么 output 就很像，那感觉 $x_2,x_3$ 的 label 比较像，但 $x_1,x_2$ 的 label 是比较不像)，其实 Smoothness Assumption更精确的假设是这样的，你的相似是要透过一个 high density region。比如说，$x_1,x_2$ 它们中间有一个 high density region($x_1,x_2$ 中间有很多很多的 data，他们两个相连的地方是通过 high density path相连的)。根据真正 Smoothness Assumption的假设，它要告诉我们的意思就是说：$x_1,x_2$ 是可能会有一样的 label，$x_2,x_3$ 可能会有比较不一样的 label(他们中间没有 high density path)。

那为什么会有 Smoothness Assumption这样的假设呢？因为在真实的情况下是很多可能成立的

![mark](http://images.iterate.site/blog/image/20190818/7mVNIowVJlPa.png?imageslim)
比如说，我们考虑这个例子(手写数字辨识的例子)。看到这变有两个 2 有一个 3，单纯算它们 peixel 相似度的话，搞不好，两个 2 是比较不像的，右边两个是比较像的(右边的 2 和 3)。如果你把你的 data 都通通倒出来的话，你会发现这个 2(最左边)跟这个 2(右边)中间有很多连续的形态(中间有很多不直接相连的相似，但是有很多 stepping stones可以直接跳过去)。所以根据 smoothness Assumption的话，左边的 2 跟右边的 2 是比较像的，右边的 2 跟 3 中间没有过渡的形态，它们两个之间是不像的。如果看人脸辨识的是，也是一样的。如果从一个人的左脸照一张相跟右脸照一张相，这是差很多的。如果你拿另外一个人眼睛朝左的相片来比较的话，会比较像这个跟眼睛朝右相比的话。如果你收集更多 unlabel data的话，在这一张脸之间有很多过渡的形态，眼睛朝左的脸跟眼睛朝向右的脸是同一个脸。



![mark](http://images.iterate.site/blog/image/20190818/sEyOcXXGb2Af.png?imageslim)
这一招在文件分类上也是非常有用的，这是为什么呢？假设你现在要分天文学跟旅游类的文章，那天文学有一个固定的 word distribution，比如会出现“asteroid,bright”。那旅游的文章会出现“yellowstone,zion等等”。那如果今天你的 unlabel data跟你的 label data是有 overlap 的话，你就很轻易处理这个问题。但是在真是的情况下，你的 unlabel data跟 label data中间没有 overlap word。为什么呢？一篇文章可能词汇不是很多，但是 word 多，所以你拿到两篇，有重复的 word 比例其实是没有那么多的。所以很有可能你的 unlabel data跟 label data之间是没有任何关系的。


![mark](http://images.iterate.site/blog/image/20190818/OtTHwjNqnvQQ.png?imageslim)

但是如果能收集到够多的 unlabeled data的话，就能得到 d1 和 d5 比较像，d5和 d6 比较像，这个像就可以一直传播过去，得到 d1 和 d3 像，同样的 d4 可以和 d2 一类。

### 聚类和标记

![mark](http://images.iterate.site/blog/image/20190818/SW5YCBqeUdqf.png?imageslim)

如何实践这个 smoothness assumption，最简单的方法是 cluster and then label。现在 distribution 长这么样子，橙色是 class1，绿色是 class2，蓝色是 unlabel data。接下来你就做一下 cluster，你可能分成三个 cluster，然后你看 cluster1 里面 class1 的 label data最多，所以 cluster1 里面所有的 data 都算是 class1，cluster2，cluster3都算是 class2、class3，然后把这些 data 拿去 learn 就结束了，但是这个方法不一定有用。如果你今天要做 cluster label，cluster要很强，因为这一招 work 的假设就是不同 class cluster在一起。可是在 image 里面，把不同 class cluster在一起是没有那么容易的。我们之前讲过说，为什么要用 deep learning，不同 class 可能会长的很像，也有可能长的不像，你单纯只有 pixel 来做 class，你结果是会坏掉的。如果你要让 class and then label这个方法有用，你的 class 要很强。你要用很好的方法来描述 image，我们自己试的时候我们会用 deep autoendcoder，用这个来提取特征，然后再进行聚类。

### 基于图的方法

刚才讲的是很直觉的方法，另外一个方法是 Graph-based Approach，我们用 Graph-based approach来表达这个通过高密度路径连接这件事情。就说我们现在把所有的 data points都建成一个 graph，每一笔 data points都是这个 graph 上一个点，要想把他们之间的 range 建出来。有了这个 graph 以后，你就可以说：high density path的意思就是说，如果今天有两个点，他们在这个 graph 上面是相的(走的到)，那么他们这就是同一个 class，如果没有相连，就算实际的距离也不是很远，那也不是同一个 class。

![mark](http://images.iterate.site/blog/image/20190818/Ly57X4PpDYFU.png?imageslim)

建一个 graph：有些时候这个 graph representation是很自然就得到了。举例来说：假设你现在要做的是网页的分类，而你有记录网页之间的 Hyperlink，那 Hyperlink 就很自然的告诉你网页之间是如何连接的。假设现在做的是论文的分类，论文和论文之间有引用之间的关系，这个引用也是 graph，可以很自然地把图画出来给你。

![mark](http://images.iterate.site/blog/image/20190818/RoOazhoTrOYO.png?imageslim)

但有时候你要想办法来建这个 graph。通常是这样做的：你要定义 $x^i,x^j$ 咋样来算它们的相似度。影像的话可以用 pixel 来算相似度，但是 performance 不太好。用 auto-encoder算相似度可能表现就会比较好。算完相似度你就可以建 graph，graph有很多种：比如说可以建 K Nearest Neighbor，K Nearest Neighbor意思就是说，我现在有一大堆的 data，data和 data 之间，我都可以算出它们的相似度，那我 K=3(K Nearest Neighbor)，每一个 point 跟他最近的三个 point 做标记。或者也可以做 e-Neighborhood:意思就是说，每个点只有跟它相似度超过某一个 threshold，跟它相似度大于的 1 点才会连起来。所谓的 edge 也不是只有相连不相连这样 boundary 的选择而已，你可以给 edge 一些 weight，你可以让你的 edge 跟你的要被连接起来的两个 data points的相似度是成正比的。怎么定义这个相似度呢？我会建议比较好的选择就是 Gaussian Radial Basis function来定义这个相似度。


怎么算这个 function 呢？你可以先算说：$x^i,x^j$ 你都把它们用 vector 来描述的话，算他们的 distance 乘以一个参数，再取负号，然后再算 exponentiation。其实 exponential 这件事在经验上还是会给你比较好的 performance。为什么用这样的方式会给你比较好的 performance 呢？如果你现在看这个 function(Gaussian Radial Basis function)它的下降速度是非常快的。你用这个 Gaussian Radial Basis function的话，你能制造出像这个图(有两个橙色距离很近，绿色这个点离橙色也蛮近，如果你用 exponential 的话，每一个点只能与非常近的点离，它跟稍微远一点就不连了。你要有这样的机制，你才能避免跨海沟的 link，所以你用 exponential 通常效果比较好。

![mark](http://images.iterate.site/blog/image/20190818/jUpQEKsQQcf4.png?imageslim)

如果我们现在在 graph 上有一些 label data，在这个 graph 上我们说这笔 data1 是属于 class1，那跟它有相连的 data points属于 class1 的几率也会上升，所以每笔 data 会影响它的邻居。光是会影响它的邻居是不够的，如果你只考虑光是影响它的邻居的话可能帮助是不会太大。为什么呢？如果说相连的本来就很像，你 train 一个 model，input很像 output 马上就很像的话，帮助不会太大。那 graph-based approach真正帮助的是：它的 class 是会传递的，本来这个点有跟 class1 相连所以它会变得比较像 class1。但是这件事会像传染病一样传递过去，虽然这个点真正没有跟 class1 相连，因为像 class1 这件事情是会感染，所以这件事情会通过 graph link传递过来。


举例来说看这个例子，你把你的 data points建成 graph，这个如果是理想的例子的话，一笔 label 是属于 class1(蓝色)，一笔 label 是属于 class2(红色)。经过 garph-based approach，你的 graph 建的这么漂亮的话(上面都是蓝色的，下面都是红色的)

![mark](http://images.iterate.site/blog/image/20190818/13OqtwtrQRoJ.png?imageslim)
这样的 semi-supervised有用，你的 data 要足够多，如果 data 不够多的话，这个地方没收集到 data，那这个点就断掉了，那这个 information 就传不过去了，比如右上图就出现四个小的 cluster。

![mark](http://images.iterate.site/blog/image/20190818/4SG5EPC00TNK.png?imageslim)
刚才是定性的说使用这个 graph，接下来说怎么定量使用这个 graph。那这个定量的使用是在这个 graph structure上面定义一个东西叫做：label的 smoothness，我们会定义说 label 有多符合我们刚才说的 smoothness assumption 的假设。

现在看这两个例子，在这两个例子都有四个 data points，data point跟 data point连接的数字代表了 weight。在左边这个例子中，你给它的 label 是(1,1,1，0)，在右边的例子中，给的 label 是(0,1,1,0)。左边的这个例子是比较 smothness 的，但是我们需要一个数字定量的描述它说：它有多 smothness。常见的做法是这样子的：这个式子是我们考虑两两有相连的 point，两两拿出来(summation over所有 data i,j)，然后计算 i,j之间的 weight 跟 y 的 label 减去 j 的 label 的平方(这个是 summation 所有 data，不管他现在是有 label 还是没有 label)。所以你看左边这个 case，在 summation over所有的 data 的时候，你只需要考虑,s=0.5(只是在计算时比较方便而已，没有真正的效用)，右边的 class s=3，这个值(s)越小越 smothness，你会希望你得出的 labelsmothness 的定义算出来越小越好。

现在看这两个例子，在这两个例子都有四个 data points，data point跟 data point连接的数字代表了 weight。在左边这个例子中，你给它的 label 是(1,1,1，0)，在右边的例子中，给的 label 是(0,1,1,0)。左边的这个例子是比较 smothness 的，但是我们需要一个数字定量的描述它说：它有多 smothness。常见的做法是这样子的：$S=\frac{1}{2}\sum_{ij}w_{i,j}(y^i-y^j)^2$。这个式子是我们考虑两两有相连的 point，两两拿出来(summation over所有 data i,j)，然后计算 i,j之间的 weight 跟 y 的 label 减去 j 的 label 的平方(这个是 summation 所有 data，不管他现在是有 label 还是没有 label)。所以你看左边这个 case，在 summation over所有的 data 的时候，你只需要考虑 $x_3,x_4$,s=0.5(只是在计算时比较方便而已，没有真正的效用)，右边的 class s=3，这个值(s)越小越 smothness，你会希望你得出的 labelsmothness 的定义算出来越小越好。

![mark](http://images.iterate.site/blog/image/20190818/7ws6LVcdnG0S.png?imageslim)
这个算式可以稍微整理整理一下，可以写成一个简洁的式子。我们把 y 串成一个 vector(现在 y 包括 label data，也包括 unlabel data)，每一个笔 label data和 label data都赋一个值给你，现在你有 R+U个 dimension vector，可以写成 y。如果你这样写的话，s这个式子可以写成 y(vector)的 transform 乘以 L(matrix)再乘以 y，L是属于(R+U)*(R+U)matrix，这个 L 被叫做“Graph Laplacian”。

这个 L 的定义是：两个 matrix 相减(L=D-W)。W就是你把这些 data point两两之间 weight connection建成一个 matrix，这个 matrix 的四个 row 个四个 columns 分别代表 data$x^1$ 到 $x^4$,D是你把 w 的每组 row 合起来。

现在我们可以用 $y^TLy$ 去评估我们现在得到的 label 有多 smothness。在这个式子里面我们会看到有 y，这个 y 是 label，这个 label 的值也就是 neural network output的值是取决于 neural parameters。这一项其实是 neural 的 depending，所以你要把 graph 的 information 考虑到 neural network的 train 的时候，你要做的事情其实就是在原来的 loss function里面加一项。假设你原来的 loss function是 cross entropy，你就加另外一项，你加的这一项是 smoothness 的值乘以某一个你想要调的参数，后面这一项 $\lambda$S其实就是象征了 regulization term。你不只要调整参数让你那些 label data的 output 跟真正的 label 越接近越好，你同时还要做到说：output这些 label，不管是在 label data还是在 unlabel data上面，它都符合 smothness assuption的假设是由这个 s 所衡量出来的。所以你要 minimize 前一项还要 minimize 后一项(用梯度下降)

其实你要算 smothness 时不一定要放在 output 的地方，如果你今天是 deep neural network的话，你可以把你的 smothness 放在 network 任何地方。你可以假设你的 output 是 smooth，你也可以同时说：我把某一个 hidden layer接出来再乘上别的一些 transform，它也要是 smooth，也可以说每一个 hidden layer的 output 都是 smooth

## Better Representation
最后一个方法是：Better Representation，这个方法的精神是：“去无存青，化繁为简”，等到 unsupervised 的时候再讲。
它的精神是这样子的：我们观察到的世界其实是很复杂的，我们在我们观察到的世界背后其实是有一些比较简单的东西在操控着我们这个复杂的世界，所以你只要能看透这个世界的假象，直指它的核心的话就可以让训练变得容易。





# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)
