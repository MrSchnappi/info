---
title: 24 无监督学习-线性降维
toc: true
date: 2019-08-18
---

![mark](http://images.iterate.site/blog/image/20190818/CgeTyWTyIzya.png?imageslim)

![mark](http://images.iterate.site/blog/image/20190818/kiAN52AFxBbq.png?imageslim)



我把 dimension reduction分为两种，一种做的事情叫做“化繁为简”，它可以分为两种：一种是 cluster，一种是 dimension reduction。所谓的“化繁为简”的意思：现在有很多种不同的 input，比如说：你现在找一个 function，它可以 input 看起来像树的东西，output都是抽象的树，把本来比较复杂的 input 变成比较简单的 output。那在做 unsupervised learning的时候，你只会有 function 的其中一边。比如说：我们要找一个 function 要把所有的树都变成抽象的树，但是你所拥有的 train data就只有一大堆的 image(各种不同的 image)，你不知道它的 output 应该是要长什么样子。

那另外一个 unsupervised learning可以做 Generation，也就是无中生有，我们要找一个 function，你随机给这个 function 一个 input(输入一个数字 1，然后 output 一棵树；输入数字 2，然后 output 另外一棵树)。在这个 task 里面你要找一个可以画图的 function，你只有这个 function 的 output，但是你没有这个 function 的 input。你这只有一大堆的 image，但是你不知道要输入什么样的 code 才会得到这些 image。这张投影片我们先 focus 在 dimension reduction这件事上，而且我们只 focus 在 linear dimension reduction上。

## 聚类



![mark](http://images.iterate.site/blog/image/20190818/bvlrSEqEULgV.png?imageslim)

那我们现在来说 clustering，什么是 clustering 呢？clustering就是说：假设有一大堆的 image，然后你就把它们分成一类一类的。之后你就可以说：这边所有的 image 都属于 cluster1，这边都属于 cluster2，这边都属于 cluster3。有些不同的 image 用同一个 cluster 来表示，就可以做到“化繁为简”这件事。那这边的问题是：要到底有几个 cluster，这个没有好的方法，就跟 neural network要有几层一样。但是你不能太多，这张有 9 张 image，就有 9 个 cluster ，这样做跟没做是一样的。把全部的 image 放在一个 cluster 里，这跟没做是一样的。要怎样选择适当的 cluster，这个你要 empirical 的来决定它

### K-means

那在 cluster 方法里面，最常用的叫做 k-means。我们有一大堆的 data，他们都是 unlabel($x^1,...x^N$)，一个 $x$ 就代表一张 image ，我们要把它做成 K 个 cluster。怎样做呢？我们要先找这些 cluster 的 center。(假设这边 $x$ 的用 vector 来表示的话，这边的 center 也都是一样长度的 vector)有 K 个 cluster 就需要有 K 个 center。那初始的 center 咋样来呢，你可以从你的 train data里面随机找 K 个 $x$ 出来，就是你的 k 个 center。

接下里，你要对所有在 train data 中的 $x$，都做以下的事情：你决定说，现在的每一个 $x$ 属于 1 到 K 中的哪一个 cluster。现在假设 $x$ 跟第 i 个 class center最接近的话，那 $x$ 就属于 $c^i$，那我们用一个 binary value(上标 n，下标 i)$b^n_i$ 来代表说第 n 个 $x$ 有没有属于第 i 个 class，如果第 n 个 x 属于第 i 个 class 的话，它的 value 就是 1，反之就是 0.接下来，你要 update 你的 cluster，方法也是很直觉的(假设你要 update 第 i 个 cluster center，你就把所有属于第 i 个 cluster 的 x 通通拿出来做平均，你就得到第 i 个 cluster 的 center，然后你要反复的做)

只所以在 initialize cluster的时候，你会想到直接从你的 database 里面去挑 K 个 $x$ 做 center，有一个很重要的原因是假设你是纯粹随机的（不是从 database 里面挑的），你很有可能在第一次 assign 这个 $x$ 的时候，就没有任何一个 x 和某一个 cluster 很像，或者某个 cluster 没有一个 $x$,

然后你 updata 的时候纯粹就是==无用功==，所以最好从 database 里面选取 K 个 $x$ 作为 cluster center。

### 层次聚类

![mark](http://images.iterate.site/blog/image/20190818/bhmUH6AnyX15.png?imageslim)



那 cluster 有另外一种方法叫做 Hierarchical Agglomerative Clusteing(HAC)，那这个方法是先建一个 tree。假设你现在有 5 个 example，你想要把它做 cluster，那你先做一个 tree structure，咋样来建这个 tree structure呢？你把这 5 个 example 两两去算它的相似度，然后挑最相似的那个 pair 出来。假设最相似的那个 pair 是第一个 example 和第二个 example merge起来再平均，得到一个新的 vector，这个 vector 代表第一个和第二个 example。现在只剩下四笔 data 了，然后两两再算相似度，发现说最后两笔是最像的，再把他们 merge average起来。得到另外一笔 data。现在只剩下三笔 data 了，然后两两算他们的 similarity，发现黄色这个和中间这个最像，然后再把他们平均起来，最后发现只剩红色跟绿色，在把它们平均起来，得到这个 tree 的 root。你就根据这五笔 data 他们之间的相似度，就建立出一个 tree structure，这只是建立一个 tree structure，这个 tree structure告诉我们说：哪些 example 是比较像的。比较早分枝代表比较不像的。

接下来你要做 clustering，你要决定在这个 tree structure上面切一刀(切在图上蓝色的线)，你如果切这个地方的时候，那你就把你的五笔 data 变成是三个 cluster。如果你这一刀切在红色的那部分，就变成了二个 cluster，如果你这一刀切在绿色这一部分，就变成了四个 cluster。这个就是 HAC 的做法

HAC跟刚才 K-means最大的差别就是：你如果决定你的 cluster 的数目，在 k-means里面你要决定 K value是多少，有时候你不知道有多少 cluster 不容易想，你可以换成 HAC，好处就是你现在不决定有多少 cluster，而是决定你要切在这个 tree structure的哪里。



## Distributed Representation

![mark](http://images.iterate.site/blog/image/20190818/jSiGiS9QEWvF.png?imageslim)



光只做 cluster 是非常卡的，在做 cluster 的时候，我们就是"以偏概全"这个就好像说：“念能力”有分成六大类，每一个人都要被 assign 成这其中的一类。要咋样决定它是属于哪一类呢？拿一杯水看看它有什么反应，就 assign 成哪一类。比如说：水满出来了就是强化系，所以小傑是强化系。这样子把每一个人都 assign 某一个系是不够的，太过粗糙的。像 bisiji 就有说：小傑其实是接近放出系的强化系能力的念能力者。如果你只是说它是强化系的，这是 loss 掉很多 information 的，你应该这样表示小傑，应该说：强化系是 0.7，放出系是 0.25，强化系是跟变化系是比较接近的，所以有强化系就有一部分的变化系的能力，其他系的能力为 0。所以你应该要用一个 vector 来表示你的 $x$，那这个 vector 每一个 dimension 就代表了某一种特值，那这件事就叫做：distributed representation。

如果原来你的 $x$ 是一个非常 high dimension的东西，比如说 image，你现在用它的特值来描述，它就会从比较高维的空间变成比较低维的空间。那这件事情就被叫做：dimension reduction。这是一样的事情，只是不同的称呼而已。

## 降维

![mark](http://images.iterate.site/blog/image/20190818/fBvfiyBObgYB.png?imageslim)

### 降维样例

那从另外一个角度来看：为什么 dimension reduction可能是有用的。举例来说：假设你的 data 分布是这样的(在 3D 里面像螺旋的样子)，但是用 3D 空间来描述这些 data 其实是很浪费的，其实你从资源就可以说：你把这个类似地毯卷起来的东西把它摊开就变成这样(右边的图)。所以你只需要在 2D 的空间就可以描述这个 3D 的 information，你根本不需要把这个问题放到这个 3D 来解，这是把问题复杂化，其实你可以在 2D 就可以做这个 task

![mark](http://images.iterate.site/blog/image/20190818/qeWS2oTVymPw.png?imageslim)



我们来举一个具体的例子：比如说我们考虑 MNIST，在 MNIST 里面，每一个 input digit都是用 28*28dimension来描述它。但是多数 28 *28dimension的 vector 你把它转成一个 image 看起来不像一个数字。你 random sample一个 28 *28vector转成一个 image，可以是这样的(数字旁边的图)，它看起来根本就不不像数字。所以在这 28 *28维空间里面 。digit这个 vector 其实是很少的，所以你要描述一个 digit，你根本就不需要用到 28 *28维，你要描述一个 digit，你要的 dimension 远比 28 *28维少。

我们举一个很极端的例子：比如说这里有一堆 3，这堆 3 如果你是从 pixel 来看待的话，你要用 28*28维来描述一个 image，然后实际上这些 3 只需要一个维度就可以来表示了。为什么呢？因为这些 3 就只是说：把原来的 3 放在这就是中间这张 image，右转 10 度就是这张，右转 2 度变它，左转 10、20度。所以你唯一要记录的只有今天这张 image 它是左转多少度右转多少度，你就可以知道说它在维的空间里面应该长什么样子。你只需要抓重这个重点(角度的变化)，你就可以知道 28 维空间中的变化，所以你只需要一维就可以描述这些 image

![mark](http://images.iterate.site/blog/image/20190818/jVjEPpEo4MFt.png?imageslim)



那怎么做 dimension reduction呢?在做 dimension reduction的时候，我们要做的事情就是找一个 function，这个 function 的 input 是一个 vector x，output是另外一个 vector z。但是因为是 dimension reduction，所以你 output 这个 vector z这个 dimension 要比 input 这个 x 还要小，这样才是在做 dimension reduction。

### 特征选择

在做 dimension reduction里面最简单是 feature selection，这个方法是：你把你的 data 分布拿出来看一下，本来在二维的平面上，但是你发现都集中在 $x_2$dimension这里，这个 $x_1$dimension没什么用，把它拿掉就只有 $x_2$dimension，你就等于做到 dimension reduction这件事了。这个方法不见得有用，因为有很多时候，你的 case 是：你任何一个 dimension 都不能拿掉。

### 主成分分析

另外一个常见的方法叫做 Principe component abalysis(PCA),PCA做的事情就是：这个 function 是一个很简单的 linear function，这个 input x跟这个 output z之间的关系就是一个 linear transform，你把这个 x 乘上一个 matrix w，你就得到它的 output z。现在要做的事情就是:根据一大堆的 x(我们现在不知道 z 长什么样子)我们要把 w 找出来



![mark](http://images.iterate.site/blog/image/20190818/UlyursmbjDup.png?imageslim)



接下来我们来介绍一下 PCA，我们刚才讲过 PCA 要做的事情就是找这个 W(z=Wx)，这个 W 咋样找呢？假设我们现在考虑一个比较简单的 case，我们考虑一个 one dimension的 case。我们现在假设只要把我们的 data project到一维的空间上，也就是 z 是一个 scale，w其实就是一个 row。那我们就用 $w^1$ 来表示 W 的第一个 row，把 x 跟 $w^1$ 做内积就得到了 $z_1$。接下来我们要问的问题是：我们要找的这个 $W^1$，应该要长什么样子。我们先假设 $w^1$ 的长度是 1(这个假设是有必要的，等下你会更清楚我们为什么要这个假设)如果的长度是 1 的话，那 $w^1$ 跟 x 做内积得到的 $z_1$ 意味着：w跟 x 是高维空间中的一个点，$w^1$ 是高维空间中的 vector，$z_1$ 就是 x 在 $w^1$ 上的投影。所以我们现在做的事情就是把一堆 x 通过 $w^1$ 变成 $z_1$(得到一堆 $z_1$，每个 x 都变成 $z_1$)那现在的问题就是这个 $w^1$ 应该长什么样子呢，怎么选取这个 $w^1$ 呢？

举例来说，这个 x 的分布(图中蓝色的点，每一个点代表宝可梦)，这个分布的横坐标是攻击力，纵坐标是防御力。那今天我要把二维投影到一维，我应该要选什么样的 $w^1$ 呢？我可以选 $w^1$ 指向这个方向(红色的箭头)，也可以选 $w^1$ 指向那个方向(橙色的箭头)，我选不同的方向得到的结果会是不一样的。那你总得给我们一个目标，我们才能知道要选咋样的 $w^1$。我们的目标是这样的：我们希望选一个 $w^1$，它经过 projection 以后得到这些 $z_1$ 的分布是越大越好，也就是我们不希望说通过这个 projection 以后所有的点通通挤在一起，把本来的 data point跟 data point之间的奇异度拿掉了。我们是希望说：经过 projection 以后，不同的 data point他们之间的区别我们仍然是可以看的出来，所以找一个 projection 方向它可以让 projection 后的 various 是越大越好。如果我们看这个例子的话，你就会觉得说：如果是选这个方向的话(红色箭头)，经过 projection 以后，可能会分布在这个 range(large variance)；如果选这个方向的话(橙色箭头)，那么你的点可以是这个 range(small variance)。所以你要选择 $w^1$ 的时候，你可能会选择 $w^1$ 的方向是 large variance这个方向。从这个图上，你可以看出 $w^1$ 其实是代表宝可梦的强度，宝可梦可能有一个 factor 代表它的强度，这个隐藏的 factor 同时影响了它的防御力跟攻击力，所以防御力跟攻击力是会同时上升的。

#### 数学推导

那我们要用 equation 来表示的话，你就会说：我们现在要去 maximize 的对象是 $z_1$ 的 variance，$z_1$ 的 variance 就是 summation over所有的 $z_1$，然后 $z_1$ 减去 $(\bar z_1)^2$。

![mark](http://images.iterate.site/blog/image/20190818/TAzgLHwqAhyV.png?imageslim)



假设你知道咋样来做(等一下来讲怎么做)，你找到一个 $w^1$，你就可以让 $z_1$ 最大，然后就结束了。你现在不只是想要投影到一维，你想要投影到更多维(二维)。现在你想要投影到二维的平面的时候，这时候你就把 x 跟另外一个 $w^2$ 做内积,$w^1,w^2$ 就是 W 的第一个 row 和第二个 row。那我们咋样来找 $w^2$ 呢？跟刚才找 $z_1$ 一样，首先假设 $w^2$ 的长度为 1，$z_2$ 的分布也是越大越好，但是你只是要让 $z_2$ 的 variance 越大越好，这样你找出不就 是 $w^1$ 吗，但是你 $w^1$ 刚才已经找过了，这样的话你就等于什么事都没有做。所以你要再加一个 condition，刚才已经找到 $w^1$ 了，这次要 $w^2$ 跟 $w^1$ 是垂直的($w1 * w^2=0$)。你先把 $w^1$，再找 $w^2$，等等，这就看你要 projection 到几维了。(projection要几维是你自己要决定的)。把 $w^1,w^2,...$ 排起来成 W 就结束了。

这个 W 是一个 orthogonal matrix，这时候，你看它的 row($w^1,w^2$)是 orthogonal，$w^1,w^2$ 的长度都是 1，所以它是一个 orthogonal matrix。

![mark](http://images.iterate.site/blog/image/20190818/J6YB3jbkYvoI.png?imageslim)

接下里的问题就是怎样来找 $w^1,w^2$(咋样来解这个问题)，这个解法是蛮容易的。经典的方法:$z_1$ 等于 $w^1x$,$z_1$ 的平均值 summation over$z_1$（公式里面少除 n），也就是 summation over$w^1x$,$w^1$ 跟 data point无关，可以提出来变为先 summation over x在 $w^1 \sum x$ 得到 $w^1$ 跟 $\bar{x}$。接下来我们要 maximize 的对象是 $z_1$ 的 variance($\sum_{z_1} (z_1-\bar{z_1})^2$)公式整理为 $\sum( w^1(x-\bar{x}))^2$。可以把这个式子做一个转化：$w^1$ 是一个 vector，$x- \bar{x}$ 是一个 vector。假设 $w^1$ 是 a，$x-\bar{x}$ 是 b，可以写成 a 的 transform 乘 b的平方，可以写成 a 的 transpose 乘以 b 再乘以 a 的 transpose 乘以 b，可以写成 a 的 transpose 乘以 b 再乘以 a 的 transpose 乘以 b 的 transpose(a的 transpose 乘以 b 是一个 scale）：找一个 $w^1$，它可以 maximizeing$(w^1)^TSw^1$（$S=Cov(x)$）。但这个是要有 constraint，如果没有 constriant 的话，这个问题会有无聊的 solution，把每个值都变无穷大，这样就结束了，所以这个问题是要有 constraint。这个问题 constraint 是：$w^2$ 的 L-Norm等于 1

![mark](http://images.iterate.site/blog/image/20190818/RN3KXae7qRIN.png?imageslim)



有了这些以后呢，我们要解这个问题。S是 covariance matrix，又是半正定。也就是说所有的 eigenvalues 都是 non-negative(比较困惑的话，可以去看李老师的现代课)。这个问题的 solution 就是：$w^1$ 是 covariance matrix的 eigenvector。它不只是一个 eigenvector，它是对应到最大的 eigenvalue $\lambda $ 那一个 eigenvector。这个就是结论

中间的过程是：首先我要用 lagrange multiplier(bitshop appendix)，式子是：$g(w^1)=(w^1)^TSw^1-\alpha ((w^1)^Tw^1-1)$，接下来你把这个 g 对所有的 w 做偏微分(w是一个 vector，有很多的 element)，令这个式子通通等于 0(偏微分)，整理一下你会得到这个式子：$Sw^1-\alpha w^1=0$。这个式子告诉我们说：solution是满足这个式子，如果写成 $Sw^1=\alpha w^1$ 的话，$w^1$ 就是 S 的一个 eigenvector。但是 S 的 eigenvector 有很多，而且你还可以找到 eigenvector 的长度等于 1。所以你接下来要做的事情是，看哪一个 eigenvector 带到这个式子里面($(w^1)^TSw^1$)可以 maximizing$(w^1)^TSw^1$。整理一下变为 $\alpha (w^1)^Tw^1$，得到结果为 $\alpha$，谁可以让这个 $\alpha$ 最大呢？答案是：$w^1$ 是对应到 largest eigenvalue 的 eigenvector 时最大，这个 $\alpha$ 是最大的 eigenvalue $\lambda _1$.



![mark](http://images.iterate.site/blog/image/20190818/PRuTP79dqrIC.png?imageslim)



那我要找 $w^2$ 的话，我们要解是这样的 equation：$(w^2)^TSw^2$  $(w^2)^Tw^2=1  $ $(w^2)^Ts^1=0$。我们要 maximizing 根据 $w^2$ 投影以后的 variance。结论是：$w^2$ 也是 covariance matrix S的一个 eigenvector，然后它对应到 $2^{nd}$ largest eigenvector $\lambda _2$。那我们现在来解它：你先写一个 function g，function里包括了你要 maximzing 的对象，还包括了两个 constraint，分别乘以 $\alpha,\beta $。接下来你对所有的参数做偏微分($w^2$ 所有的 element)。做完以后你得到这个式子($Sw^2-\alpha w^2-\beta w^1=0$)，然后坐左同时乘以 $w^1$ 的 transpose(乘以 $w^1$ 的 transpose 以后，会出现 $(w^1)^Tw^1$ 会等于 1,$(w^1)^TW^2$ 等于 0)，整理一下等于 $((w^1)^TSw^2)^T$(scale)，在整理一下得到 $(w^{2})^TSw^1$。我们已经知道 $w^1$ 是 S 的 eigenvector，而且它对应到最大的 eigenvalue $\lambda_1 $($Sw^1=\lambda w^1$)。从这边我们得到 $\beta $ 等于 0，所以剩下的 $Sw^2- \alpha w^2=0$，然后得出 $Sw^2=\alpha w^2$。$w^2$ 是一个 eigenvector，但是它是哪一个 eigenvector 呢？它是第二大 eigenvector。



![mark](http://images.iterate.site/blog/image/20190818/Ky3FKS1yoYNO.png?imageslim)



z =Wx，这里神奇的地方就是：z的 covariance 是 diagonal matrix，也就是说如果我们今天做 PCA，你原来的 data distribution可能是左边这张图，做完 PCA 以后，你会做 decorrelation，你会让你不同的 dimension 间的 covariance 是 0.也就是说你算 z 这个 vector covariance matrix的话，会发现它是(Disgonal matrix)，这样做是有好处的。假设你 PCA 得到的 feature(z)，这个新的 feature 是要其他的 model 用的，你的 model 假设说是一个 generative model，你用 gaussion 来描述某一个 class 的 distribution，而你在做 gaussion 假设的时候，你假设说 input data它的 covariance 是 diagonal，你假设不同的 dimension 之间没有 decorrelation，这样可以减少你的参数量。

你把原来的 input data做 PCA 以后，再丢给其他的 model，其它的 model 就可以假设现在的 input data它的 dimension 之间没有 decorrelation。所以它就可以用简单的 model 处理你的 input data，这样就可以避免 overfitting 的情形。

这件事情怎么说明呢？z的 covariance 是 $z-\bar{z}$ 乘以 $(z-\bar{z})^T=WSW^T$,s=Cov(x)，把 S 乘进 $w^1,...w^k$ 变成 $[Sw^1,Sw^2,...Sw^k]$,$w^1$ 是 S 的 eigenvector，$\lambda$ 是 eigenvalue，所以 $[Sw^1,Sw^2,...Sw^k]$,$w^1$ 变成 $[\lambda w^1,\lambda w^2,...\lambda w^k,]$，然后把 W 乘进去，然后就变成了 $[\lambda W w^1,\lambda Ww^2,...\lambda Ww^k,]$。($w^1$ 是 W 的第一个 row)W乘以 $w^1$ 等于 $e_1$（$w_1,...,w_n$ 是相互正交）,$e_1$ 就是 vector 第一维是 1，其它都是 0，这个东西就是 Diagonal matrix。

PCA，第一个找出的 $w^1$ 是 covariance matrix对应到最大 eigenvalue 的 eigenvector，然后找出的 $w^2$ 就是对应到第二大的 eigenvector，以此类推。有一个证明告诉你说：这么做的话，每次投影的时候都可以让 variance 最大。

#### 主成分分析的另一个角度

![mark](http://images.iterate.site/blog/image/20190818/tT3zMFLm0YyC.png?imageslim)



我们从更清楚的角度来看 PCA，你就会知道 PCA 到底在做什么。假设我们考虑的是手写的数字，我们知道这些数字其实是一些 basic component所组成的。这些 basic component可能就代表笔画。举例来说：人类所手写的数字就是这些 basic component所组成的，有斜的直线，横的直线，有比较长的直线，然后还有小圈，大圈等等，这些 basic component加起来就可以得到一个数字。这些 basic component写做 $u^1,u^2,...u^5$，这些 basic component其实就是一个一个的 vector。假设我们现在考虑的是 mnist，mnist一张 image28*28piexl，也就是 28 *28维的 vector。这些 component 其实就是 28 *28维的 vector，把这些 vector 加起来以后，你所得到的 vector 就代表了一个 diagit。如果把它写成 formulation 的话就是：$x\approx c_1u^1+c_2u^2+....c_ku^k+\bar{x}$，x代表一张 image 里面的 pixel，x等于 $c_1u^1$component乘以 $c_2u^2$ 这个 component，一直加到 $c_ku^k$component，再加上 $\bar{x}$,$\bar{x}$ 是所有 image 的平均。所以每一张 image 就是有一堆 component 的 linear conformation再加上它平均所组成的。

举例来说：7是这三个 component 加起来的结果，假设 7 就是 x 的话，$c_1=1,c_2=0,c_3=1,c_4=0,c_5=1$，你可以用 $c_1,c_2,...c_k$ 来表示一张 image。假设 component 比 pixel 的数目少的话，那么这个描述是比较有效的。7是 1 倍的 $u^1$,1倍 $u^2$,1倍的 $u^5$ 所组成的，所以 7 是一个 vector，它的第一维，第三维，第 5 维是 1.

![mark](http://images.iterate.site/blog/image/20190818/WpQXaUbGaMcH.png?imageslim)



现在把 $\bar{x}$ 移到左边，x减掉所以 image 的平均等于一堆 component linear conformation，这些 linear comformation写作 $\hat{x}$，那现在假设我们不知道这些 component 是什么，不知道 $u^1$ 到 $u^k$ 的 vector 长什么样子。那我们咋样找 K vector出来呢？我们要做的事情就是：我们要去找 K vector使得 $\hat{x}$ 跟 $x-\bar{x}$ 越接近越好，他们的差用 reconstruction error来描述。接下来我们要做事情就是：找 K 个 vector 可以 minimize 这个 reconstruction error。

在 PCA 中我们想说：我们要找一个 matrix W，x乘以 matrix W就得到了 z，把 W 的每一个 row 写出来就是 $[w_1,w_2,...,w_k]$，x乘上一个 $[(w_1)^T,(w_2)^T,...(w_k)^T]$，以此类推。那么说：是 $[w_1,w_2,...,w_k]$ 是 covariance matrix的 eigenvector，事实上你要解这个式子 $L=mi_n(u^1,...u^k)\sum \left \| (x-\bar{x}) -(\sum_{k=1}^{k}c_ku^k)\right \|$，找出 $u^1,...u^k$。由 PCA 找出的这个解 $w^1,...w^k$ 就是可以让 L 最小化



![mark](http://images.iterate.site/blog/image/20190818/PGfmSsSwfYW2.png?imageslim)



我们有一大堆的 x，现在假设有一个 $x_1$，这个 $x_1$ 减去 $\bar{x}$ 等于 $u^1$ 乘以 component weight，c上标 1 下标 1(下标 1 代表说：它是 $u^1$ 的 weight，上标 1 代表说：$x^1$ 的 $u^1$component weight)，$x^1-\bar{x} \approx c^1_1u^1+c^1_2u^2+...$.

$x-\bar{x}$ 是一个 vector(把这个 vector 拿出来),$u^1,u^2...$ 是一排 vector(把它排起来，排起来就是一个 matrix)，columns的数目是 K 个，把这些 component weight排成一排变成一个 vector，vector乘以 matrix 变成另一个 vector。我们不只是有一笔 data，$x^2-\bar{x}$ 是另外一个黄色的 vector，这个 vector($c_1^2,c_2^2,....$)乘以 matrix $u^2$ 等于另一个黄色的 vector，依次类推。

那我们把所有的 data 用这个式子来表示的话，这样就会得到一个 matrix，这个 matrix 的 cloumns 等于 data 的数目(你有 1 万笔 data，cloumns=10000)。现在 $\left\{\begin{matrix}
... &... \\
u^1 & u^2 \\
 ...& ...
\end{matrix}\right.$ 乘以这个 matrix $\left\{\begin{matrix}
c_1^1 &c_1^2 \\
c_2^1 & c_2^2 \\
 ...& ...
\end{matrix}\right.$ 跟这个 matrix 越接近越好，所以你要 minimize 左边两个 matrix 跟右边这个 matrix 之间的差距是会被 minimize 的，也就是说：用 SVD 提供给我们的 matrix 拆解方法，拆成这是三个 matrix 相乘后，跟左边的 matrix 是最接近的。

#### 奇异值分解

![mark](http://images.iterate.site/blog/image/20190818/0Te6j7ySBLVg.png?imageslim)



那要咋样来解这个问题呢？加入你有学过大一现代化，你就应该知道该咋样来解(可以参考李宏毅老师现代的课程)。每一个 matrix X，你可以用 SVD 把它拆成一个 matrix U(m *k)乘上一个 matrix $\sum$(k *k)乘上 matrix V(k *n)。这个 matrix U就是 matrix$\left\{\begin{matrix}
... &... \\
u^1 & u^2 \\
 ...& ...
\end{matrix}\right.$，这个 $\sum$V就是 $\left\{\begin{matrix}
c_1^1 &c_1^2 \\
c_2^1 & c_2^2 \\
 ...& ...
\end{matrix}\right.$。

如果我们今天用 SVD 将 X 拆成这三个 matrix 相乘，那右边三个 matrix 相乘的结果是最接近左边这个 matrix 的，那解出来的结果是什么样呢？其中 U 这个 matrix 的 K columns其实就是一组 orthonormal vector，这组 orthonormal vector是 $XX^T$ 的 eigenvector，U总共有 K 个 orthonormal vector，这 K 个 orthonormal vector对应到 $XX^T$ 最大的 k 个 eigenvalue 的 eigenvector。

你会发现，这个 $XX^T$ 就是 covariance matrix，PCA之前找出的 W 就是 covariance matrix的 eigenvector。而我们这边说做 SVD，解出来 U 的每个 column 就是 covariance matrix的 eigenvector，所以这个 U 得出的解就是 PCA 得到的解。所以我们说：PCA做的事情就是：你找出来的那些 W 其实就是 component。

#### 主成分分析和神经网络

![mark](http://images.iterate.site/blog/image/20190818/OC01WM0XIkU2.png?imageslim)



我们现在已经知道从 PCA 找出的 $w^1,...w^K$ 就是 k 个 component$u^1,...u^K$。我们现在有一个根据 component linear combination的结果叫做 $\hat{x}$`(`$\sum_{k=1}^{K}c_kw^k$)。我们希望 $\hat{x}$ 跟 $x-\bar{x}$ 的差值越小越好，你要 minimize reconstruction error。那我们现在已经根据 SVD 找出这个 W 了，那这个 $c_k$ 的值是到底多少呢？这个 $c_k$ 是每一个 image 都有自己的 $c_k$，这个 $c_k$ 就每个 image 各自找就好。如果现在要用 $c_1,...c^k$ 对 $w^k$ 做 linear combination。这个 $c_k=(x-\bar{x})w^k$，找一组 $c_k$ 可以 minimize$ \hat{x},(x-\bar{x})$

linear combinarion做的事情你可以想成用 neural network来表示它，假设我们的 $x-\bar{x}$ 就是一个 vector(这边写做三维的 vector)，假设现在只有 2 个 component(k=2)，那我们先算出 $c_1,c_2$，$c_1=(x-\bar{x})$($x-\bar{x}$）的每一个 component 乘上 $w^1$ 的每一个 component，接下来你就得到了 $c_1$。($x-\bar{x}$ 是 neural network的 input，$c_1$ 是一个 neural(linear neural )，$w^1_1,w^1_2,w^1_3$ 是 weight，)。$c_1$
乘以 $w^1$($c_1$ 乘上 $w_1$ 的第一维($w_1^1$)得到一个 value，乘上 $w_1$ 的第二维($w_2^1$)得到一个 value，乘上 $w_1$ 的第三维($w_3^1$)得到一个 value)。接下来再算一下 $c_2$，$c_2$ 乘以 $w^2$ 的结果和之前的加起来就是最后的 output，$\hat{x_1},\hat{x_2},\hat{x_3}$ 就是 $\hat{x}$。

接下来就是 minimize error，我们要 $\hat{x}$ 跟 $x-\bar{x}$ 越接近越好(也就是这个 output 跟 $x-\hat{x}$ 越接近越好)，那你可以发现说 PCA 可以表示成一个 neural network，这个 neural network只有一个 hidden layer，这个 hidden layer是 linear activation function。那现在我们 train 这个 neural network，是要 input 一个东西得到 output，这个 input 跟 output 越接近越好。这个东西就叫做 Autoencode

这边就有一个问题，假设我们现在这个 weight，不是用 PCA 的方法去找出 $w^1,w^2,...w^k$。而是兜一个 neural network，我们要 minimize error，然后用 Gradient Descent去 train 得到的 weight，那就觉得你得到的结果会跟 PCA 得到的结果一样吗？这其实是会一样的(neural network没有办法保证是垂直的，你会得到另外一组解)。所以在 linear 的情况下，或许你就想要用 PCA 来找这个 $W$ 比较快的，你用 neural network是比较麻烦的。但是用 neural network的好处是：可以 deep。

#### 缺点

![mark](http://images.iterate.site/blog/image/20190818/thSlTSrwz0r3.png?imageslim)



PCA有一些很明显的弱点，一个是：它是 unsupervised，今天加入给它一大堆点没有 label。假设你要把它 project 到一维上，PCA可以找一个可以让 data variance最大的那个 dimension，在这个 case 中，就把它 project 到这一维上(红色箭头))。有一种可能是，这两组 data point它们分别代表了两个 class，如果你用 PCA 来做 dimension reduction的话，你就会使得蓝色跟橙色的 class 被 meger 在一起，这样就无法分别了。

这时候你要怎么办呢？这时候你要引入 label data，LDA是考虑 label data考虑降维的方法，不过它是 supervised，所以这个不是我们要讲的对象。

另外一个 PCA 的弱点是 Linear，我们刚开始举得例子会说。我们刚开始举的例子说：这边有一堆点的分布是像 S 型的，我们期待说：做 dimension reduction后可以把这个 S 型曲面可以把它拉直，这件事情对 PCA 来说是做不到的。如果这个 S 型曲面做 PCA 的话，它是这样子的(如图)，就像打扁一样，而不是把它拉开，拉开这件事情 PCA 是办不到的。等下我们会讲 non-linear

#### 应用实例

![mark](http://images.iterate.site/blog/image/20190818/ExkeRtfigFT5.png?imageslim)



接下来，我们把 PCA 用在一些实际的问题上。比如说：用它来分析宝可梦的 data，宝可梦总共有 800 种宝可梦，每个宝可梦可以用 6 个 features 来表示，分别是：生命值，攻击力，防御力，特殊攻击力，特殊防御力，速度。所以每一个宝可梦就是一个 6 维的 vector，我们现在用 PCA 来分析它。

在 PCA 中有个常问的问题：我需要有多少个 component，要把它 project 到一维，二维还是三维...。要多少个 component 就好像是 neural network要几个 layer，每个 layer 要有几个 neural 一样，所以这是你要自己决定的。

那一个常见的方法是这样的：我们去计算每一个 principle components的 $\lambda $(每一个 principle component 就是一个 eigenvector，一个 eigenvector 对应到一个 eigenvalue $\lambda $)。这个 eigenvalue 代表 principle component去做 dimension reduction的时候，在 principle component的那个 dimension 上它的 variance 有多大(variance就是 $\lambda$)。


今天这个宝可梦的 data 总共有 6 维，所以 covariance matrix是有 6 维。你可以找出 6 个 eigenvector，找出 6 个 eigenvalue。现在我们来计算一下每个 eigenvalue 的 ratio(每个 eigenvalue 除以 6 个 eigenvalue 的总和)，得到的结果如图。

可以从这个结果看出来说：第五个和第六个 principle component的作用是比较小的，你用这两个 dimension 来做 projection 的时候 project 出来的 variance 是很小的，代表说：现在宝可梦的特性在第五个和第六个 principle component上是没有太多的 information。所以我们今天要分析宝可梦 data 的话，感觉只需要前面四个 principle component就好了。

![mark](http://images.iterate.site/blog/image/20190818/K1xBAHYXI1Ye.png?imageslim)





我们实际来分析一下，你做 PCA 以后得到四个 principle component就是这个样子，每一个 principle component就是一个 vector，每一个宝可梦是用 6 位的 vector 来描述。我们来看每一个 principle component做的事情是什么：如果我们看第一个 principle component，第一个 principle component每一个 dimension 都是正的，这个东西其实就代表了宝可梦的强度(如果你要产生一只宝可梦的时候，每一个宝可梦都是由这四个 vector 做 linear conformation ，每一个宝 可梦可以想成是，这六个 vector 做 linear conformation的结果，在选第一个 principle component的时候，你给它的 weight 比较大，那这个宝可梦的六维都是强的，所以这第一个 principle component就代表了这一只宝可梦的强度)。第二个 principle component它在防御力的地方是正值，在速度的地方是负值，也就是说这个防御力跟速度是成反比的，你给第二个 principle component一个 weight 的时候，你会增加那只宝可梦的防御力但是会减低它的速度。

我们把第一个和第二个 principle component 画出来的话，你会发现是这个样子(图上有 800 个点，每一个点对应到一只宝可梦)，但这样我么很难知道每一只宝可梦是什么。

![mark](http://images.iterate.site/blog/image/20190818/IcE3Ko2bj7E6.png?imageslim)



如果我们看第三个和第四个 component 的话，会发现第三个 principle component的特殊防御力是正的，攻击力和生命值是负的，说明是用生命值和攻击力换取特殊防御力的宝可梦。最后一个是：它的 HP 是正的，攻击力和防御力是负的，也就是说：它是用攻击力和防御力来换取生命力的宝可梦。

![mark](http://images.iterate.site/blog/image/20190818/5uU42rHtqKU5.png?imageslim)



我们拿它来做手写数字辨识的话，我们可以把每一张数字都拆成 component 乘以 weight，加上另外一个 component 乘以 weight，每一张 component 是一张 image(28* 28的 vector)。我们现在来画前 30component 的话(PCA)，你得到的结果是这样子的(如图所示)，你用这些 component 做 linear conformation，你就得到所有的 digit(0-9)，所以这些 component 就叫做 Eigen digits(这些 component 其实都是 covariance matrix的 eigenvector)

![mark](http://images.iterate.site/blog/image/20190818/SJTKCUA9yRPA.png?imageslim)



如果我们做人脸辨识的话，得到的结果是这样子的。找它们前 30 的 pixel component，叫做 Eigen-face。你把这些脸做 linear conformation以后就可以得到所有的脸。但是这边跟我们预期的有些是不一样的，这是不是有 bug 啊。因为我们 PCA 找出来的是 component，我们把很多 component 做 linear combine以后它会变成 face。但是现在我们找出来的不是 component，我们找出来的每一个图都几乎是完整的脸。

![mark](http://images.iterate.site/blog/image/20190818/vrrS43FB4X7k.png?imageslim)



为什么会这样呢？其实你仔细想 PCA 的特性，你会发现说，会得到这个结果是可能的。因为在 PCA 里面，weight它可以是任何值(可以是正的，也可以是负的)，所以当我们用 pixel component组成一张 image 的时候，你可以把这个 component 相加，也可以相减。所以这会导致你找出的 component 不见得是一个图的 basic 的东西。

假设我要画一个 9，那我可以先画一个 8，把下面的圈圈减掉，然后把一杠加上去(你可以先画一个复杂的图，再把多于的减掉)。这些 component 其实不见得是笔画的东西，若你要得到类似笔画的东西，你要用 Non-negative matrix factotization(NMF)。在 Non-negative matrix factotization里面，我们刚才说：PCA它可以看做是对 matrix X做 SVD，SVD是 matrix factorization的技术，它分解出来的两个 matrix 的值可以是正的，也可以是负的。那现在你要用 NMF 的话，我们会强迫所有的 component weight都是正的。是正的好处就是，现在一张 image 必须由 component 叠加得到(你不能说：我先画一个很复杂的东西，再把一些东西去掉，得到一个 digit)。如果你用 PCA 的话，你的 dimension 每一回都不见得是正的，会有一些负值，负值你是不知道要该怎么处理的。

用 NMF 的话值都是正的，那些 component 自然会形成一张 image。

![mark](http://images.iterate.site/blog/image/20190818/OTkfuR0oTrMP.png?imageslim)



在 MNIST 用 NMF 的话，你找出来的那些 piexl component它就会清楚很多，你会发现你找出来的每个东西都是笔画了。

![mark](http://images.iterate.site/blog/image/20190818/63eSpOGPA3OB.png?imageslim)



如果你要看脸的话，你就会发现说：它长的是这个样子，它比较像是脸的一些部分。

### 矩阵分解

![mark](http://images.iterate.site/blog/image/20190818/A5v9u6jBhzSb.png?imageslim)



假设我们现在做一个调查，调查每个人买公仔的数目。ABCDE代表五个人，A有 5 个春日，B有 4 个，C有 1 个，D有 1 个，E有 0 个。A有炮姐 3 个，B有 3 个，C有 1 个，D有 1 个，E有 1 个。A有姐寺 0 个，B有 0 个，C有 0 个，D有 4 个，E有 5 个。A有小唯 1 个，B有 1 个，C有 5 个，D有 4 个，E有 4 个。你会发现说：在这个 matrix 上面，每一个 table 里面的 block 并不是随机出现的。如果有两个公仔的人，那是因为每个人跟角色的背后是有一些特性的(有一些 factor 来操控这些特性)



![mark](http://images.iterate.site/blog/image/20190818/6C0kImPL1YWe.png?imageslim)



这些动漫仔会分为两种，有一种是萌傲娇的，有一种是萌天然呆的，每个人就是在萌傲娇和萌天然这个平面上一个点。那么 A 可能是偏萌傲娇的，每一个角色有傲娇属性也可能有呆萌属性，所以每个角色就是就是平面上的一个点，所以每个角色都可以用 vector 来描述它。如果某一个人萌的属性跟某一个角色它本身所具有的属性是 match 的话(它们背后的 vector 很像)，那这个人就会买这个公仔。

A,B,C它们的 vector 是左边这样子的，A是萌傲娇的，B也是萌傲娇的(没有 A 那么强)，C是萌天然呆的，每一个动漫人物角色后面也都有傲娇，天然呆这两种属性。如果它们背后属性是 match 的话，那这个人就会买这个公仔。世界操控的逻辑是这样子的，但是这些 factors(萌傲娇还是萌天然呆)这些是没有办法直接被观察的。

![mark](http://images.iterate.site/blog/image/20190818/H2TdXV6gVGCu.png?imageslim)



但是这些 factors(萌傲娇还是萌天然呆)这些是没有办法直接被观察的，因为没有人去关心他心里是想什么，你也没有办法说：每一个动漫人物它背后的属性是什么，这个是没有办法直接观察到的。



![mark](http://images.iterate.site/blog/image/20190818/MEegMSwDchy4.png?imageslim)



我们有的是动漫人物跟手上有的公仔的数目，我们要根据这个关系论出每一个人跟每一个动漫人物背后的 factor(每个人背后都有一个二维的 vector，代表他萌傲娇，萌天然呆的程度)，每个人物的背后也都有一个 vector，代表他傲娇的属性和天然呆的属性。我们知道每个人手上有公仔的数目，把它合起来看做是一个 matrix X。这个 matrix X的 row 的数目是 Otakud 的数目，它的 columns 的数目是动漫角色的数目。

那我们现在做一个假设，这个 matrix 里面的 element 它都来自于两个 vector 的 inner product。为什么 A 有 5 个春日的公仔呢？是因为 $r^A$ 跟 $r^1$ 的 inner product很大(它的 inner product是 5，所以有 5 个春日的公仔)，如 果 $r^A$ 跟 $r^1$ 的 inner product是 4，那 B 就会有 4 个炮姐的公仔，以此类推。

如果这件事你用数学来表示的话，我们可以写成这样子。把 $r^A$ 跟 $r^B$ 排成一排，把 $r^1$ 到 $r^4$ 排起来。这个 K 是 latent factor的数目，这个东西我们是没有办法知道的(我们把人分成萌傲娇萌天然呆，这样是不精确的分析方式。如果我们有更多 data 的话，我们应该更准确的知道有多少 factor，要多少 factor 这些必须是试出来，这个就行 principle component或者 neural network的层数一样，你要事先决定好)我们就假设说现在 latent factor的数目是 K，你把它当成 matrix row，就是一个 M*K的 matrix。如果你把 $r^1$ 到 $r^4$ 当做 cloumns，就是 K *N 的 matrix。如果你这个 M *K跟 K *N的 matrix 乘起来就是 M *N的 matrix。那它的每一个 dimension 就是(最上角的 dimension 就是 $r^A$ 乘以 $r^1$，第一个 row 第二个 column 是 $r^A$ 乘以 $r^2$，以此类推)matrix X。那这样的话，我们做的事情就是找一组 $r^A$ 到 $r^E$，找一组 $r^1$ 到 $r^4$，把这两个 matrix 相乘以后，跟左边这个 matrix X越接近越好(minimize error)。

这个就可以用刚才的 SVD 来解，你可能说 SVD 不是有三个解吗？这时候你看你是要把 $\sum$ 并到左边还是右边，把这个 matrix X拆成两个 matrix，然后 minimize error。

#### FM推荐系统中的应用

![mark](http://images.iterate.site/blog/image/20190818/JGxueCyEfryu.png?imageslim)



但是有时候你会遇到这个问题，就是一些 information 是 missing 的(不知道这个 information)。比如说：你不直道 ABC 手上有没有小叶撕的公仔，有可能在他所在的地区没有发行，那这个 table 只能写？

假设在 table 上有一些是问号的话，怎么办呢？你用刚才 SVD 的方法就会有点卡。那在这个 matrix 上有 missing 的 value 的话，我们还是可以做的，我们就用 gradient descent的方法来做。我们来写一个 loss function $L=\sum _{(i,j)}(r^i*r^j-n_{ij})^2$(你要 i 这个人乘以 j 这个角色的 inner product跟它购买的数量 $n_{ij}$ 是越接近越好)重点是说：我们在 summation over这些 element 的时候，我可以避开这些 missing data(我们在 summation overi,j的时候，如果值没有，我就不算它，我们只算有定义值的部分)

![mark](http://images.iterate.site/blog/image/20190818/oaWK38LA48rq.png?imageslim)



那我们就用 gradient descent来实际做一下 ，我们假设 latent factor的数目为 2。那 A 到 E 都会对应到二维的 vector，那每一个角色都会对应到一个 vector(属性)。所以我们把它在两个维度里面比较大的维度挑出来的话，你就会发现说：A,B是萌同一种属性，C,D,E是萌同一种属性，1,2有同样的属性，3,4有同样的属性。你没有办法说每一个属性分别都代表着什么(不知道那个维度代表着天然呆或者傲娇)。

你需要先找出这些 latent factor去分析它的结果，你就可以知道说(因为我们事先已经知道姐寺跟小唯是有天然呆属性)第一个维度代表天然呆的属性，第二个维度是傲娇的属性。有了这些 data 以后，你就可以预测你的 missing value。

如果我们已经知道 $r^3$,$r^A$，我们知道一个会购买公仔的数量其实是动漫角色背后的 latent factor跟人背后的 latent factor做 inner product的结果。那我们把 $r^3$ 跟 $r^A$ 做 inner product之后，你就可以预测说这个人会买多少公仔。

![mark](http://images.iterate.site/blog/image/20190818/HC4HqpNWkE7g.png?imageslim)



刚才那个 model 可以做的更精致一点，我们刚才说：A背后的 latent factor $r^A$ 跟 1 背后的 latent factor$r^1$ 得到的结果就是 table 上面的数值。但是事实上可能还会有别的因素会操控它的数值，所以更精确的写法是：$r^A$ 跟 $r^1$ 的 inner product加上某一个跟 A 有关的 scale$b_A$，再加上跟 1 有关的 scale $b_1$ 其实才等于 5。$b_A$ 代表说：A本身有多喜欢买工仔，$b_1$ 代表说：这个角色它本身会有多想让别人购买(这件事情跟属性是无关的)

所以改一下 minimize 的式子，改为 $r^i$ 跟 $r^j$ 的 inner product加上 $b_i$,$b_j$，然后你希望这个值跟 $n_{ij}$ 越接近越好，用 gradient descent来解。(你也可以在 loss function后面加上 regularization)

#### MF主题分析的应用

![mark](http://images.iterate.site/blog/image/20190818/yv2yHqgnXVd9.png?imageslim)

、

matrix factorization有很多的应用，可以应用到 topic analysis上面。如果把刚才讲的 matrix factorization的技术用到 topic analysis上面就叫做 latent semantic analysis(LSA)。就是把刚才的动漫人物换成文章，把刚才的人换成词汇。table里面的值就是 term frequency，把这个 term frequency乘上一个 weight 代表说这个 term 本身有多重要。

怎样 evaluation 一个 term 重不重要呢？常用的方式是：inverse document frequency(计算每一个词汇在整个 paper 有多少比率的 document 涵盖这个词汇，假如说，每个词汇，每个 document 都有，那它的 inverse document frequency就很小，代表着这个词汇的重要性是低的，假设某个词汇只有某一篇 document 有，那它的 inverse document frequency就很大，代表这个词汇的重要性是高的。)


在这个 task 里面，如果你今天把这个 matrix 做分解的话，你就会找到每一个 document 背后那个 latent factor，那这边的 latent factor是什么呢？可能指的是 topic(主题)，这个 topic 有多少是跟财经有关的，有多少是跟政治有关的。document1跟 document2 有比较多的“投资，股票”这样的词汇，那 document1 跟 document2 就有比较高的可能背后的 latent factor是比较偏向“财经”的

topic analysis的方法多如牛毛，基本的精神是差不多的(有很多各种各样的变化)。常见的是 probability latent semantic analysis(PLSA)和 latent Dirchlet allocation(LDA)。这跟之前在 machine learning讲的 LDA 是完全不一样的东西

### 未引入的其他相关方法

![mark](http://images.iterate.site/blog/image/20190818/JAjrVWc0ECpz.png?imageslim)





这些是一些 reference 给大家参考，这边是跟 PCA 比较有关系的。

dimension reduction的方法多如牛毛，比如说 MDS，MDS的特别是：它不需要把每一个 data 都表示成 feature vector，它要知道 feature vector跟 feature vector之间的 distance，知道这个 distance，你就可以做 dimension reduction。一般教科书举得例子会说：我现在一堆城市，你不知道咋样把城市描述成 vector，但你知道城市跟城市之间的距离(每一笔 data 之间的距离)，那你就可以画在二维的平面上。其实 MDS 跟 PCA 是有一些关系的，如果你用某些特定的 distance 来衡量两个 data point之间的距离的话，你做 MDS 就等于做 PCA。其实 PCA 有个特性是：它保留了原来在高维空间中的距离(在高维空间的距离是远的，那么在低维空间中的距离也是远的，在高维空间的距离是近的，那么在低维空间中的距离也是近的)

PCA有几率的版本，叫做 probability PCA。PCA有非线性的版本，叫做 kernel PCA。CCA是说：你有两种不同的 source，这时候你想要用 CCA。假如说你要做语音辨识，两个 source(一个是声音讯号，有那个人嘴巴的 image(可以看到这个人的唇形，可以读他的唇语))把这两种不同的 source 都做 dimension reduction，那这个就是 CCA。





# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)
