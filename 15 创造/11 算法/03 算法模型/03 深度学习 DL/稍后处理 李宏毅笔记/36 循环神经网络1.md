---
title: 36 循环神经网络1
toc: true
date: 2019-08-18
---
![mark](http://images.iterate.site/blog/image/20190818/il8lbRNb5dXR.png?imageslim)

## 应用举例

![mark](http://images.iterate.site/blog/image/20190818/jRnaVFassMEB.png?imageslim)

这边举的例子是 slot filling，我们假设订票系统听到用户说：“ i would like to arrive Taipei on November 2nd”，你的系统有一些 slot(有一个 slot 叫做 Destination，一个 slot 叫做 time of arrival)，系统要自动知道这边的每一个词汇是属于哪一个 slot，比如 Taipei 属于 Destination 这个 slot，November 2nd属于 time of arrival这个 slot。

![mark](http://images.iterate.site/blog/image/20190818/kja773Y6Ku38.png?imageslim)
这个问题你当然可以使用一个 feedforward neural network来解，也就是说我叠一个 feedforward neural network，input是一个词汇(把 Taipei 变成一个 vector)丢到这个 neural network里面去(你要把一个词汇丢到一个 neural network里面去，就必须把它变成一个向量来表示)。以下是把词汇用向量来表示的方法：

## 1-of-N encoding

![mark](http://images.iterate.site/blog/image/20190818/SUFfsY5THt1k.png?imageslim)
## Beyond 1-of-N encoding

![mark](http://images.iterate.site/blog/image/20190818/PEgGDaAtimpa.png?imageslim)
如果只是用 1-of-N encoding来描述一个词汇的话你会遇到一些问题，因为有很多词汇你可能都没有见过，所以你需要在 1-of-N encoding里面多加 dimension，这个 dimension 代表 other。然后所有的词汇，如果它不是在我们词言有的词汇就归类到 other 里面去(Gandalf,Sauron归类到 other 里面去)。你可以用每一个词汇的字母来表示它的 vector，比如说，你的词汇是 apple，apple里面有出现 app、ppl、ple，那在这个 vector 里面对应到 app,ple,ppl的 dimension 就是 1，而其他都为 0。

![mark](http://images.iterate.site/blog/image/20190818/GGNGRhXG2rpl.png?imageslim)
假设把词汇表示为 vector，把这个 vector 丢到 feedforward neural network里面去，在这个 task 里面，你就希望你的 output 是一个 probability distribution。这个 probability distribution代表着我们现在 input 这词汇属于每一个 slot 的几率，比如 Taipei 属于 destination 的几率和 Taipei 属于 time of departure的几率。

但是光只有这个是不够的，feedforward neural network是没有办法解决这个问题。为什么呢，假设现在有一个使用者说：“arrive Taipei on November 2nd”(arrive-other,Taipei-dest, on-other,November-time,2nd-time)。那现在有人说:"leave Taipei on November 2nd"，这时候 Taipei 就变成了“place of departure”，它应该是出发地而不是目的地。但是对于 neural network来说，input一样的东西 output 就应该是一样的东西(input  "Taipei"，output要么是 destination 几率最高，要么就是 place of departure几率最高)，你没有办法一会让出发地的几率最高，一会让它目的地几率最高。这个怎么办呢？这时候就希望我们的 neural network是有记忆力的。如果今天我们的 neural network是有记忆力的，它记得它看过红色的 Taipei 之前它就已经看过 arrive 这个词汇；它记得它看过绿色之前，它就已经看过 leave 这个词汇，它就可以根据上下文产生不同的 output。如果让我们的 neural network是有记忆力的话，它就可以解决 input 不同的词汇，output不同的问题。

但是光只有这个是不够的，feedforward network是没有办法 slot 这个 probability。为什么呢，假设现在有一个使用者说：“arrive Taipei on November 2nd”(arrive-other,Taipei-dest, on-other,November-time,2nd-time)。那现在有人说:"leave Taipei on November 2nd"，这时候 Taipei 就变成了“place of departure”，它应该是出发地而不是目的地。但是对于 neural network来说，input一样的东西 output 就应该是一样的东西(input  "Taipei"，output要么是 destination 几率最高，要么就是 time of departure几率最高)，你没有办法一会让出发地的几率最高，一会让它目的地几率最高。这个肿么办呢？这时候就希望我们的 neural network是有记忆力的。如果今天我们的 neural network是有记忆力的，它记得它看过红色的 Taipei 之前它就已经看过 arrive 这个词汇；它记得它看过绿色之前，它就已经看过 leave 这个词汇，它就可以根据上下文产生不同的 output。如果让我们的 neural network是有记忆力的话，它就可以解决 input 不同的词汇，output不同的问题。

## 什么是 RNN？



![mark](http://images.iterate.site/blog/image/20190818/aYkvl13bfSUf.png?imageslim)
这种有记忆的 neural network就叫做 Recurrent Neural network(RNN)。在 RNN 里面，每一次 hidden layer的 neuron 产生 output 的时候，这个 output 会被存到 memory 里去(用蓝色方块表示 memory)。那下一次当有 input 时，这些 neuron 不只是考虑 input$x_1,x_2$，还会考虑存到 memory 里的值。对它来说除了 $x_1,x_2$ 以外，这些存在 memory 里的值 $a_1,a_2$ 也会影响它的 output。

### 例子

![mark](http://images.iterate.site/blog/image/20190818/jygkgoX1kkvJ.png?imageslim)
举个例子，假设我们现在图上这个 neural network，它所有的 weight 都是 1，所有的 neuron 没有任何的 bias。假设所有的 activation function都是 linear(这样可以不要让计算太复杂)。现在假设我们的 input 是 sequence$\begin{bmatrix}
1\\
1
\end{bmatrix}\begin{bmatrix}
1\\
1
\end{bmatrix}\begin{bmatrix}
2\\
2
\end{bmatrix}...$
把这个 sequence 输入到 neural network里面去会发生什么事呢？在你开始要使用这个 Recurrent Neural Network的时候，你必须要给 memory 初始值(假设他还没有放进任何东西之前，memory里面的值是 0)

现在输入第一个 $\begin{bmatrix}
1\\
1
\end{bmatrix}$，接下来对发生什么事呢？，对左边的那个 neural 来说(第一个 hidden layer)，它除了接到 input 的 $\begin{bmatrix}
1\\
1
\end{bmatrix}$ 还接到了 memory(0跟 0)，output就是 2(所有的 weight 都是 1)，右边也是一样 output 为 2。第二层 hidden laeyer output为 4。

![mark](http://images.iterate.site/blog/image/20190818/4AqRX2BPkEWl.png?imageslim)
接下来 Recurrent Neural Network会将绿色 neuron 的 output 存在 memory 里去，所以 memory 里面的值被 update 为 2。

接下来再输入 $\begin{bmatrix}
1\\
1
\end{bmatrix}$，接下来绿色的 neuron 输入有四个
$\begin{bmatrix}
1\\
1
\end{bmatrix}\begin{bmatrix}
2\\
2
\end{bmatrix}$，output为 $\begin{bmatrix}
6\\
6
\end{bmatrix}(weight=1)$，第二层的 neural output为 $\begin{bmatrix}
12\\
12
\end{bmatrix}$。

所以对 Recurrent Neural Network来说，你就算 input 一样的东西，它的 output 是可能不一样了(因为有 memory)

![mark](http://images.iterate.site/blog/image/20190818/zxigrTHYUJr3.png?imageslim)

现在 $\begin{bmatrix}
6\\
6
\end{bmatrix}$ 存到 memory 里去，接下来 input 是 $\begin{bmatrix}
2\\
2
\end{bmatrix}$，output为 $\begin{bmatrix}
16\\
16
\end{bmatrix}$，第二层 hidden layer为 $\begin{bmatrix}
32\\
32
\end{bmatrix}$

那在做 Recurrent Neural Network时，有一件很重要的事情就是这个 input sequence调换顺序之后 output 不同(Recurrent Neural Network里，它会考虑 sequence 的 order)

## RNN架构

![mark](http://images.iterate.site/blog/image/20190818/YOcgqn1nUxUw.png?imageslim)

今天我们要用 Recurrent Neural Network处理 slot filling这件事，就像是这样，使用者说：“arrive Taipei on November 2nd”，arrive就变成了一个 vector 丢到 neural network里面去，neural network的 hidden layer的 output 写成 $a^1$($a^1$ 是一排 neural 的 output，是一个 vector)，$a^1$ 产生 $y^1$,$y^1$ 就是“arrive”属于每一个 slot filling的几率。接下来 $a^1$ 会被存到 memory 里面去，"Taipei会变为 input"，这个 hidden layer会同时考虑“Taipei”这个 input 和存在 memory 里面的 $a^1$，得到 $a^2$，根据 $a^2$ 得到 $y^2$，$y^2$ 是属于每一个 slot filling的几率。以此类推($a^3$ 得到 $y^2$)。

有人看到这里，说这是有三个 network，这个不是三个 network，这是同一个 network 在三个不同的时间点被使用了三次。(我这边用同样的 weight 用同样的颜色表示)

![mark](http://images.iterate.site/blog/image/20190818/r68Q7Fu7ujNp.png?imageslim)
那所以我们有了 memory 以后，刚才我们讲了输入同一个词汇，我们希望 output 不同的问题就有可能被解决。比如说，同样是输入“Taipei”这个词汇，但是因为红色“Taipei”前接了“leave”，绿色“Taipei”前接了“arrive”(因为“leave”和“arrive”的 vector 不一样，所以 hidden layer的 output 会不同)，所以存在 memory 里面的值会不同。现在虽然 $x_2$ 的值是一样的，因为存在 memory 里面的值不同，所以 hidden layer的 output 会不一样，所以最后的 output 也就会不一样。这是 Recurrent Neural Network的基本概念。



## 其他 RNN

![mark](http://images.iterate.site/blog/image/20190818/nKjesHF1zaGM.png?imageslim)
Recurrent Neural Networ的架构是可以任意设计的，比如说，它当然是 deep(刚才我们看到的 Recurrent Neural Networ它只有一个 hidden layer)，当然它也可以是 deep Recurrent Neural Networ。


比如说，我们把 $x^t$ 丢进去之后，它可以通过一个 hidden layer，再通过第二个 hidden layer，以此类推(通过很多的 hidden layer)才得到最后的 output。每一个 hidden layer的 output 都会被存在 memory 里面，在下一个时间点的时候，每一个 hidden layer会把前一个时间点存的值再读出来，以此类推最后得到 output，这个 process 会一直持续下去。

### Elman network &Jordan network

![mark](http://images.iterate.site/blog/image/20190818/J6RqdmcGIOOf.png?imageslim)
Recurrent Neural Networ会有不同的变形，我们刚才讲的是 Elman network。(如果我们今天把 hidden layer的值存起来，在下一个时间点在读出来)。还有另外一种叫做 Jordan network，Jordan network存的是整个 network output的值，它把 output 值在下一个时间点在读进来(把 output 存到 memory 里)。传说 Jordan network会得到好的 performance。

Elman network是没有 target，很难控制说它能学到什么 hidden layer information(学到什么放到 memory 里)，但是 Jordan network是有 target，今天我们比较很清楚我们放在 memory 里是什么样的东西。
### Bidirectional neural network

![mark](http://images.iterate.site/blog/image/20190818/l4y8uKjmPNQN.png?imageslim)
Recurrent Neural Networ还可以是双向，什么意思呢？我们刚才 Recurrent Neural Networ你 input 一个句子的话，它就是从句首一直读到句尾。假设句子里的每一个词汇我们都有 $x^t$ 表示它。他就是先读 $x^t$ 在读 $x^{t+1}$ 在读 $x^{t+2}$。但是它的读取方向也可以是反过来的，它可以先读 $x^{t+2}$，再读 $x^{t+1}$，再读 $x^{t}$。你可以同时 train 一个正向的 Recurrent Neural Network，又可以 train 一个逆向的 Recurrent Neural Network，然后把这两个 Recurrent Neural Network的 hidden layer拿出来，都接给一个 output layer得到最后的 $y^t$。所以你把正向的 network 在 input$x^t$ 的时候跟逆向的 network 在 input$x^t$ 时，都丢到 output layer产生 $y^t$，然后产生 $y^{t+1}$,$y^{t+2}$，以此类推。用 Bidirectional neural network的好处是，neural在产生 output 的时候，它看的范围是比较广的。如果你只有正向的 network，再产生 $y^t$，$y^{t+1}$ 的时候，你的 neural 只看过 $x^1$ 到 $x^{t+1}$ 的 input。但是我们今天是 Bidirectional neural network，在产生 $y^{t+1}$ 的时候，你的 network 不只是看过 $x^1$，到 $x^{t+1}$ 所有的 input，它也看了从句尾到 $x^{t+1}$ 的 input。那 network 就等于整个 input 的 sequence。假设你今天考虑的是 slot filling的话，你的 network 就等于看了整个 sentence 后，才决定每一个词汇的 slot 应该是什么。这样会比看 sentence 的一半还要得到更好的 performance。

![mark](http://images.iterate.site/blog/image/20190818/IOE0fJbTqgUa.png?imageslim)
那我们刚才讲的 Recurrent Neural Network其实是 Recurrent Neural Network最单的版本

### LSTM

那我们刚才讲的 memory 是最单纯的，我们可以随时把值存到 memory 去，也可以把值读出来。但现在最常用的 memory 称之为 Long Short-term Memory(长时间的短期记忆)，简写 LSTM。这个 Long Short-term Memor是比较复杂的。

这个 Long Short-term Memor是有三个 gate，当外界某个 neural 的 output 想要被写到 memory cell里面的时候，必须通过一个 input Gate，那这个 input Gate要被打开的时候，你才能把值写到 memory cell里面去，如果把这个关起来的话，就没有办法把值写进去。至于 input Gate是打开还是关起来，这个是 neural network自己学的(它可以自己学说，它什么时候要把 input Gate打开，什么时候要把 input Gate关起来)。那么输出的地方也有一个 output Gate，这个 output Gate会决定说，外界其他的 neural 可不可以从这个 memory 里面把值读出来(把 output Gate关闭的时候是没有办法把值读出来，output Gate打开的时候，才可以把值读出来)。那跟 input Gate一样，output Gate什么时候打开什么时候关闭，network是自己学到的。那第三个 gate 叫做 forget Gate，forget Gate决定说：什么时候 memory cell要把过去记得的东西忘掉。这个 forget Gate什么时候会把存在 memory 的值忘掉，什么时候会把存在 memory 里面的值继续保留下来)，这也是 network 自己学到的。

那整个 LSTM 你可以看成，它有四个 input 1个 output，这四个 input 中，一个是想要被存在 memory cell的值(但它不一定存的进去)还有操控 input Gate的讯号，操控 output Gate的讯号，操控 forget Gate的讯号，有着四个 input 但它只会得到一个 output

冷知识：这个“-”应该在 short-term中间，是长时间的短期记忆。想想我们之前看的 Recurrent Neural Network，它的 memory 在每一个时间点都会被洗掉，只要有新的 input 进来，每一个时间点都会把 memory
洗掉，所以的 short-term是非常 short 的，但如果是 Long Short-term Memory，它记得会比较久一点(只要 forget Gate不要决定要忘记，它的值就会被存起来)。



![mark](http://images.iterate.site/blog/image/20190818/wIt7veumJMK5.png?imageslim)
这个 memory cell更仔细来看它的 formulation，它长的像这样。

底下这个是外界传入 cell 的 input，还有 input gate,forget gate,output gate。现在我们假设要被存到 cell 的 input 叫做 z，操控 input gate的信号叫做 $z_i$（一个数值），所谓操控 forget gate的信号叫做 $z_f$，操控 output gate叫做 $z_o$，综合这些东西会得到一个 output 记为 a。假设 cell 里面有这四个输入之前，它里面已经存了值 c。

假设要输入的部分为 z，那三个 gate 分别是由 $z_i​$,$z_f​$,$z_0​$ 所操控的。那 output a会长什么样子的呢。我们把 z 通过 activation function得到 g(z)，那 $z_i​$ 通过另外一个 activation function得到 $f(z_i)​$($z_i​$,$z_f​$,$z_0​$ 通过的 activation function 通常我们会选择 sigmoid function)，选择 sigmoid function的意义是它的值是介在 0 到 1 之间的。这个 0 到 1 之间的值代表了这个 gate 被打开的程度(如果这个 f 的 output 是 1，表示为被打开的状态，反之代表这个 gate 是关起来的)。

那接下来，把 $g(z)$ 乘以 $f(z_i)$ 得到 $g(z)f(z_i)$，对于 forget gate的 $z_f$，也通过 sigmoid 的 function 得到 $f(z_f)$

接下来把存到 memory 里面的值 c 乘以​$f(z_f)$ 得到 c​$f(z_f)$，然后加起来​$c^{'}=g(z)f(z_i)+cf(z_f)$，那么 $c^{'}$ 就是重新存到 memory 里面的值。所以根据目前的运算说，这个​$f(z_i)$cortrol这个​$g(z)$，可不可以输入一个关卡(假设输入​$f(z_i)$0，那​$g(z)f(z_i)$ 就等于 0，那就好像是没有输入一样，如果​$f(z_i)$ 等于 1 就等于是把​$g(z)$ 当做输入)
。那这个​$f(z_f)$ 决定说：我们要不要把存在 memory 的值洗掉假设​$f(z_f)$ 为 1(forget gate 开启的时候)，这时候 c 会直接通过(就是说把之前的值还会记得)。如果​$f(z_f)$ 等于 0(forget gate关闭的时候)​$cf(z_f)$ 等于 0。然后把这个两个值加起来(​$c^{'}=g(z)f(z_i)+cf(z_f)$)写到 memory 里面得到 $c^{'}$。这个 forget gate的开关是跟我们的直觉是相反的，那这个 forget gate打开的时候代表的是记得，关闭的时候代表的是遗忘。那这个​$c^{'}$ 通过 $h(c^{'})$，将​$h(c^{'})$ 乘以​$f(z_o)$ 得到​$a = f(c^{'}f(z_o)$(output gate受​$f(z_o)$ 所操控，​$f(z_o)$ 等于 1 的话，就说明​$h(c^{'})$ 能通过，​$f(z_o)$ 等于 0 的话，说明 memory 里面存在的值没有办法通过 output gate被读取出来)

### LSTM举例

![mark](http://images.iterate.site/blog/image/20190818/05THRFnyH49z.png?imageslim)
LSTM例子:我们的 network 里面只有一个 LSTM 的 cell，那我们的 input 都是三维的 vector，output都是一维的 output。那这三维的 vector 跟 output 还有 memory 的关系是这样的。假设第二个 dimension$x_2$ 的值是 1 时，$x_1$ 的值就会被写到 memory 里，假设 $x_2$ 的值是-1时，就会 reset the memory，假设 $x_3$ 的值为 1 时，你才会把 output 打开才能看到输出。

假设我们原来存到 memory 里面的值是 0，当第二个 dimension$x_2$ 的值是 1 时，3会被存到 memory 里面去。第四个 dimension 的 $x_2$ 等于，所以 4 会被存到 memory 里面去，所以会得到 7。第六个 dimension 的 $x_3$ 等于 1，这时候 7 会被输出。第七个 dimension 的 $x_2$ 的值为-1，memory里面的值会被洗掉变为 0。第八个 dimension 的 $x_2$ 的值为 1，所以把 6 存进去，因为 $x_3$ 的值为 1，所以把 6 输出。

### LSTM运算举例

![mark](http://images.iterate.site/blog/image/20190818/cfyrIQtsYUTT.png?imageslim)
那我们就做一下实际的运算，这个是一个 memory cell。这四个 input scalar是这样来的：input的三维 vector 乘以 linear transform以后所得到的结果($x_1$,$x_2$,$x_3$ 乘以权重再加上 bias)，这些权重和 bias 是哪些值是通过 train data用 GD 学到的。 假设我已经知道这些值是多少了，那用这样的输入会得到什么样的输出。那我们就实际的运算一下。

在实际运算之前，我们先根据它的 input，参数分析下可能会得到的结果。底下这个外界传入的 cell，$x_1$ 乘以 1，其他的 vector 乘以 0，所以就直接把 $x_1$ 当做输入。在 input gate时，$x_2$ 乘以 100，bias乘以-10(假设 $x_2$ 是没有值的话，通常 input gate是关闭的(bias等于-10)因为-10通过 sigmoid 函数之后会接近 0，所以就代表是关闭的，若 $x_2$ 的值大于 1 的话，结果会是一个正值，代表 input gate会被打开) 。forget gate通常会被打开的，因为他的 bias 等于 10(它平常会一直记得东西)，只有当 $x_2$ 的值为一个很大的负值时，才会把 forget gate关起来。output gate平常是被关闭的，因为 bias 是一个很大的负值，若 $x_3$ 有一个很大的正值的话，压过 bias 把 output 打开。

![mark](http://images.iterate.site/blog/image/20190818/CgtE9QxyFu8F.png?imageslim)
接下来，我们实际的 input 一下看看。我们假设 g 和 h 都是 linear(因为这样计算会比较方便)。假设存到 memory 里面的初始值是 0，我们 input 第一个 vector(3,1,0),input这边 3*1=3，这边输入的是的值为 3。input gate这边($1 *100-10\approx 1$)是被打开(input gate约等于 1)。($g(z) *f(z_i)=3$)。forget gate($1 *100+10\approx 1$)是被打开的(forget gate约等于 1)。现在 0 *1+3=3($c^{'}=g(z)f(z_i)+cf(z_f)$)，所以存到 memory 里面的现在为 3。output gate(-10)是被关起来的，所以 3 无关通过，所以输出值为 0。

![mark](http://images.iterate.site/blog/image/20190818/qgRBpyb0xItm.png?imageslim)
接下来 input(4,1,0)，传入 input 的值为 4，input gate会被打开，forget gate也会被打开，所以 memory 里面存的值等于 7(3+4=7)，output gate仍然会被关闭的，所以 7 没有办法被输出，所以整个 memory 的输出为 0。

![mark](http://images.iterate.site/blog/image/20190818/2n4g8eb7YOfL.png?imageslim)

接下来 input(2,0,0)，传入 input 的值为 2，input gate关闭($\approx$ 0),input被 input gate给挡住了(0 *2=0),forget gate打开(10)。原来 memory 里面的值还是 7(1 *7+0=7).output gate仍然为 0，所以没有办法输出，所以整个 output 还是 0。



![mark](http://images.iterate.site/blog/image/20190818/76QNTjJloJPK.png?imageslim)
接下来 input(1,0,1)，传入 input 的值为 1,input gate是关闭的，forget gate是打开的，memory里面存的值不变，output gate被打开，整个 output 为 7(memory里面存的 7 会被读取出来)


![mark](http://images.iterate.site/blog/image/20190818/XqFqUy8cmH9U.png?imageslim)
最后 input(3,-1,0)，传入 input 的值为 3，input gate 关闭，forget gate关闭，memory里面的值会被洗掉变为 0，output gate关闭，所以整个 output 为 0。
## LSTM原理
![mark](http://images.iterate.site/blog/image/20190818/CTNXh4faSDUU.png?imageslim)
你可能会想这个跟我们的 neural network有什么样的关系呢。你可以这样想，在我们原来的 neural network里面，我们会有很多的 neural，我们会把 input 乘以不同的 weight 当做不同 neural 的输入，每一个 neural 都是一个 function，输入一个值然后输出一个值。但是如果是 LSTM 的话，其实你只要把 LSTM 那么 memory 的 cell 想成是一个 neuron 就好了。

![mark](http://images.iterate.site/blog/image/20190818/rJiBwEpdQlEl.png?imageslim)

所以我们今天要用一个 LSTM 的 neuron，你做的事情其实就是原来简单的 neuron 换成 LSTM。现在的 input($x_1,x_2$)会乘以不同的 weight 当做 LSTM 不同的输入(假设我们这个 hidden layer只有两个 neuron，但实际上是有很多的 neuron)。input($x_1,x_2$)会乘以不同的 weight 会去操控 output gate，乘以不同的 weight 操控 input gate，乘以不同的 weight 当做底下的 input，乘以不同的 weight 当做 forget gate。第二个 LSTM 也是一样的。所以 LSTM 是有四个 input 跟一个 output，对于 LSTM 来说，这四个 input 是不一样的。在原来的 neural network里是一个 input 一个 output。在 LSTM 里面它需要四个 input，它才能产生一个 output。

LSTM因为需要四个 input，而且四个 input 都是不一样，原来的一个 neuron 就只有一个 input 和 output，所以 LSTM 需要的参数量(假设你现在用的 neural 的数目跟 LSTM 是一样的)是一般 neural network的四倍。这个跟 Recurrent Neural Network 的关系是什么，这个看起来好像不一样，所以我们要画另外一张图来表示。


![mark](http://images.iterate.site/blog/image/20190818/66BbTcNbY7lO.png?imageslim)

假设我们现在有一整排的 neuron(LSTM)，这些 LSTM 里面的 memory 都存了一个值，把所有的值接起来就变成了 vector，写为
$c^{t-1}$(一个值就代表一个 dimension)。现在在时间点 t，input一个 vector$x^t$，这个 vector 首先会乘上一 matrix(一个 linear transform变成一个 vector z,z这个 vector 的 dimension 就代表了操控每一个 LSTM 的 input(z这个 dimension 正好就是 LSTM memory cell的数目)。z的第一维就丢给第一个 cell(以此类推)

这个 $x^t$ 会乘上另外的一个 transform 得到 $z^i$，然后这个 $z^i$ 的 dimension 也跟 cell 的数目一样，$z^i$ 的每一个 dimension 都会去操控 input gate(forget gate 跟 output gate也都是一样，这里就不在赘述)。所以我们把 $x^t$ 乘以四个不同的 transform 得到四个不同的 vector，四个 vector 的 dimension 跟 cell 的数目一样，这四个 vector 合起来就会去操控这些 memory cell运作。

![mark](http://images.iterate.site/blog/image/20190818/DUQwtLc7Vohe.png?imageslim)
一个 memory cell就长这样，现在 input 分别就是 $z$,$z^i$,$z^o$,$z^f$(都是 vector)，丢到 cell 里面的值其实是 vector 的一个 dimension，因为每一个 cell input的 dimension 都是不一样的，所以每一个 cell input的值都会是不一样。所以 cell 是可以共同一起被运算的，怎么共同一起被运算呢？我们说，$z^i$ 通过 activation function跟 z 相乘，$z^f$ 通过 activation function跟之前存在 cell 里面的值相乘，然后将 $z$ 跟 $z^i$ 相乘的值加上 $z^f$ 跟 $c^{t-1}$ 相乘的值，$z^o$ 通过 activation function的结果 output，跟之前相加的结果再相乘，最后就得到了 output$y^t$

![mark](http://images.iterate.site/blog/image/20190818/bY4XOEyXEpkB.png?imageslim)
之前那个相加以后的结果就是 memory 里面存放的值 $c^t$，这个 process 反复的进行，在下一个时间点 input$x^{t+1}$，把 z 跟 input gate相乘，把 forget gate跟存在 memory 里面的值相乘，然后将前面两个值再相加起来，在乘上 output gate的值，然后得到下一个时间点的输出 $y^{t+1}$


你可能认为说这很复杂了，但是这不是 LSTM 的最终形态，真正的 LSTM，会把上一个时间的输出接进来，当做下一个时间的 input，也就说下一个时间点操控这些 gate 的值不是只看那个时间点的 input$x^t​$，还看前一个时间点的 output$h^t​$。其实还不止这样，还会加一个东西叫做“peephole”，这个 peephole 就是把存在 memory cell里面的值也拉过来。那操控 LSTM 四个 gate 的时候，你是同时考虑了 $x^{t+1},h^t,c^t​$，你把这三个 vector 并在一起乘上不同的 transform 得到四个不同的 vector 再去操控 LSTM。

![mark](http://images.iterate.site/blog/image/20190818/Af8RsTtuIf9o.png?imageslim)
LSTM通常不会只有一层，若有五六层的话。大概是这个样子。每一个第一次看这个的人，反映都会很难受。现在还是 quite standard now，当有一个人说我用 RNN 做了什么，你不要去问他为什么不用 LSTM，因为他其实就是用了 LSTM。现在当你说，你在做 RNN 的时候，其实你指的就用 LSTM。Keras支持三种 RNN：‘’LSTM‘’,“GRU”,"SimpleRNN"
## GRU
GRU是 LSTM 稍微简化的版本，它只有两个 gate，虽然少了一个 gate，但是 performance 跟 LSTM 差不多(少了 1/3的参数，也是比较不容易 overfitting)。如果你要用这堂课最开始讲的那种 RNN，你要说是 simple RNN才行。

GRU是 LSTM 稍微简化的版本，它只有两个 gate，虽然少了一个 gate，但是 performance 跟 LSTM 差不多(少了 1/3的参数，也是比较不容易 overfitting)。如果你要用这堂课最开始讲的那种 RNN，你要说是 simple RNN才行。





# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)


