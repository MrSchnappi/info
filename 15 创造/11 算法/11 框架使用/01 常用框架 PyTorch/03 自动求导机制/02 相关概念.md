---
title: 02 相关概念
toc: true
date: 2019-12-06
---
# 相关概念


## 我们先介绍与 `torch.Tensor` 相关的

几个关键的概念：

`.requires_grad`：

- 如果设置 `.requires_grad` 为 `True`，那么将会追踪所有对于该张量的操作。

`.backward()`：

- 当完成计算后通过调用 `.backward()`，自动计算所有的梯度，这个张量的所有梯度将会自动积累到 `.grad` 属性。<span style="color:red;">什么叫自动累计到 .grad 属性？</span>

`.detach()`：

- 可以调用 `.detach()` 来阻止张量跟踪历史记录，`.detach()` 将其与计算历史记录分离，并禁止跟踪它将来的计算记录。<span style="color:red;">什么叫与计算历史记录分离？</span>

`with torch.no_grad()：`：

- 为了防止跟踪历史记录（和使用内存），可以将代码块包装在`with torch.no_grad()：`中。
- 这个在评估模型时特别有用，因为模型可能具有 `requires_grad = True` 的可训练参数，但是我们不需要梯度计算。

## 再介绍与 `Function` 相关的

`Tensor` 和 `Function` 互相连接并生成一个非循环图，它表示和存储了完整的计算历史。

主要概念如下：

`.grad_fn`：

- 每个张量都有一个 `.grad_fn` 属性，这个属性引用了一个创建了 `Tensor` 的 `Function`（如果这个张量是用户手动创建的，那么自然这个张量的 `grad_fn` 是 `None`）。<span style="color:red;">具体一般是什么 `Function` 创建了这个 `Tensor` 呢？嗯，关于这个想知道更多。</span>

`.backward()`：

- 如果需要计算导数，你可以在 `Tensor` 上调用 `.backward()`。
- 如果 `Tensor` 是一个标量（即它包含一个元素数据）则不需要为 `backward()` 指定任何参数，但是如果它有更多的元素，你需要指定一个 `gradient` 参数来匹配张量的形状。<span style="color:red;">怎么指定？</span>


注意：

- **Variable 已经取消使用**。



# 相关

- [pytorch-handbook](https://github.com/zergtant/pytorch-handbook)
- `autograd` 和 `Function` 的官方文档 https://pytorch.org/docs/autograd
