---
title: 04 前向计算
toc: true
date: 2019-12-06
---
# 可以补充进来的

- 有一点之前可能弄错了，一个是 `nn.Conv2d` 接受的 4 维的张量的每个的含义是什么，最后两个是高度宽度，还是列数行数？同时需要确认的是 `input = torch.randn(2, 1, 32, 32)` 最后的两个 32 和 `x = F.max_pool2d(F.relu(self.conv1(x)), (1, 2))` 对应的 1 和 2 和输出里面的 `torch.Size([2, 6, 28, 14])` 的 28 和 14 。
- 如果错了，那么这个文章中对应的部分要进行修改。


# 前向计算

设定输入尺寸： $32*32$。

注：这个网络（LeNet）期望的输入大小是 32×32，如果使用 MNIST数据集来训练这个网络，请把图片大小重新调整到 32×32。



```py
import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 5x5 square convolution kernel
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 30, 120) # 5*5
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (1, 2));  print(x.size())
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), (4, 2));  print(x.size())
        x = x.view(-1, self.num_flat_features(x));print(x.size())
        x = F.relu(self.fc1(x));  print(x.size())
        x = F.relu(self.fc2(x));  print(x.size())
        x = self.fc3(x);  print(x.size())
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)


input = torch.randn(2, 1, 32, 32)
out = net(input)
print(out)
```

输出：

```
Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=480, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
torch.Size([2, 6, 28, 14])
torch.Size([2, 16, 6, 5])
torch.Size([2, 480])
torch.Size([2, 120])
torch.Size([2, 84])
torch.Size([2, 10])
tensor([[-0.0101,  0.0125,  0.0205,  0.0714,  0.0003, -0.0837, -0.0238, -0.1032,
          0.0034,  0.0360],
        [ 0.0009,  0.0010,  0.0263,  0.0514, -0.0085, -0.1040, -0.0291, -0.0928,
          0.0237,  0.0367]], grad_fn=<AddmmBackward>)
```


说明，前向计算部分：

- `x = F.max_pool2d(F.relu(self.conv1(x)), (1, 2))` 将 `self.conv1(x)` 输出的内容进行非线性激活后进行最大池化处理。池化使用的是 1*2 的滑窗，即水平方向一格，竖直方向 2 格。<span style="color:red;">可见，池化是接在非线性激活后面的。</span>
- `x = F.max_pool2d(F.relu(self.conv2(x)), (4, 2))` 将上面最大池化的输入进行卷积后，做非线性激活，再做滑窗。
- `x = x.view(-1, self.num_flat_features(x))` 这里，将上面池化的输出，先用 `self.num_flat_features(x)` 统计出有多少个输出，然后用 `x.view` 来改变维度，$-1$ 说明第一个维度的尺寸由 `self.num_flat_features(x)` 的数字计算得到。
- `x = F.relu(self.fc1(x))` 对拉直的输入进行全连接，然后进行非线性激活。
- `x = F.relu(self.fc2(x))` 再次进行全连接和非线性激活。
- `x = self.fc3(x)` 进行全连接，将结果直接输出。

说明，`num_flat_features` 函数：

- `size = x.size()[1:]` 获得输入的 size，卷积层的输出的 size 一般内容为：`(batch size,channel,columns,rows)`，这里取 `(channel,columns,rows)` 部分。<span style="color:blue;"> 具体可以看下面的对于forward 计算过程中的输出尺寸的说明。</span>
- `for s in size: num_features *= s` 计算出 $channel*rows*height$ 即卷积层输出的元素的个数。

说明，输入的图像的尺寸：

- `input = torch.randn(2, 1, 32, 32)` 输入的是 batch size 为 2 的图片，即 两张图片。1 为每个图片的 channel 个数为 1，$32*32$ 为 $横向*竖向$ 尺寸。

说明，forward 计算过程中的输出尺寸：

- `torch.Size([2, 6, 28, 14])` 是经过 `x = F.max_pool2d(F.relu(self.conv1(x)), (1, 2))` 输出的尺寸，2 为输入的 batch size，一个 batch 有多少个图片输入。6 为经过卷积后输出的 channel 的个数为 6。28 为经过卷积核扫描后得到的单个 channel 的横向尺寸再进行 1 step 的池化后的结果，$(32-(5-1))/1=28$。14 为经过卷积核扫描后得到的单个 channel 的纵向尺寸再进行 2 step 的池化后的结果，$(32-(5-1))/2=14$。
- `torch.Size([2, 16, 6, 5])` 是经过 `x = F.max_pool2d(F.relu(self.conv2(x)), (4, 2))` 输出的尺寸。首先 x 现在的输入尺寸是上面的 `torch.Size([2, 6, 28, 14])`，2 仍然是 batch size ，16 是卷积层输出的 16 个 channel，6 是 $(28-(5-1))/4=6$ ，5 是 $(14-(5-1))
/2=5$。

注意：

- `nn.Conv2d` 接受一个 4 维的张量， 每一维分别是 `sSamples * nChannels * Height * Width`（`样本数*通道数*高*宽`）。
- 如果把单个样本 `(channels,height,width)` 传入网络，只需使用 `input.unsqueeze(0)` 来添加第一个维度。<span style="color:red;">嗯。好的，这个之前有用过。现在更加明确了。</span>



## 问题 1

注意：

- 我们把

```py
x = F.max_pool2d(F.relu(self.conv1(x)), (1, 2))
x = F.max_pool2d(F.relu(self.conv2(x)), (4, 2))
```

改为：

```py
x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))
```

看起来是不是很合理？

实际上是有问题的：

我们计算下修改后的x 的尺寸：

- 经过 `x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))` 后，输出尺寸为 (2,6,14,14) ，其中 $(32-(5-1))/2=14$
- 经过 `x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))` 后，输出尺寸为 (2,16,5,5)，其中 $(14-(5-1))/2=5$

可见，(2,16,5,5) 与修改前得到的 (2,16,6,5) 有了区别。

原因是什么呢？

- $(32-(5-1))/1=28$ $(28-(5-1))/4=6$
- $(32-(5-1))/2=14$ $(14-(5-1))/2=5$

可见，不同处就在于 卷积核在扫描的时候，会扣除一定的格子，扣除之后再进行除法，结果就不同了。





# 相关

- [pytorch-handbook](https://github.com/zergtant/pytorch-handbook)
