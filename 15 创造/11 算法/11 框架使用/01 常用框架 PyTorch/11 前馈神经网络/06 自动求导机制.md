---
title: 06 自动求导机制
toc: true
date: 2019-06-12
---
# 可以补充进来的

- <span style="color:red;">这个 volatile 已经过时了。。好吧，删掉重新整理下。</span>


# 自动求导机制

每个变量都有两个标志：`requires_grad` 和 `volatile`。

## requires_grad

如果一个变量定义 `requires_grad` 为 `True`，后续的这个变量的所有操作可以使用 `requires_grad`，如果一个变量定义 `requires_grad` 为 `False`，变量不需要梯度，在子图中从不执行向后计算。<span style="color:red;">嗯，大概明白了，这个 `requires_grad` 默认是什么？一般要设定成什么？</span>

例子如下。

```py
import torch
import torch.optim as optim
from torch.autograd import Variable

x = Variable(torch.randn(5, 5))
y = Variable(torch.randn(5, 5))
z = Variable(torch.randn(5, 5), requires_grad=True)
a = x + y
print(a.requires_grad)
b = x + z
print(b.requires_grad)
```

输出：

```
False
True
```


当我们在用已经训练好的模型进行训练的时候，如果想要冻结已经训练好的模型参数，我们只需要使用 `requires_grad = False` 即可冻结参数。

例如，下面的例子是我们想要冻结已经预先训练的 resnet18 网络参数，并且对 resnet18 参数进行优化，此时只要切换冻结模型中的 `requires_grad` 标志就可以了，此时已经预训练的参数就冻结了：<span style="color:red;">是的，还挺好用的，想 tensorflow 中的 freeze 差不多。</span>

```py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import torchvision

model = torchvision.models.resnet18(pretrained=True)
for param in model.parameters():
    param.requires_grad = False
model.fc = nn.Linear(512, 100) # 为什么这个地方要重新制定呢？哦，因为输出的分类不同了。
optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)
```

<span style="color:red;">嗯嗯，不错，这个例子挺好的。</span>

## volatile

<span style="color:red;">这个 volatile 已经过时了。。好吧，删掉重新整理下。</span>

`requires_grad` 为 `False`，变量不需要梯度，在子图中从不执行向后计算。`volatile` 代表 `requires_grad` 为 `False`。我们在不进行执行计算，不调用 `.backward()` 的时候，`volatile` 参数特别有用。它将使用绝对最小的内存来评估模型。<span style="color:red;">什么是绝对最小的内存来评估模型？是保存模型吗？</span>

`volatile` 不同于 `require_grad` 的传递。如果一个变量定义了 `volatile` 的操作，那么它的输出也将是 `volatile`。

例子如下。

```py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import torchvision


regular_input = Variable(torch.randn(64, 3, 7, 7))
with torch.no_grad():
    volatile_input = Variable(torch.randn(64, 3, 7, 7))
model = torchvision.models.resnet18(pretrained=True)
print(model(regular_input).requires_grad)
print(model(volatile_input).requires_grad)
```

输出：

```
True
True
```

<span style="color:red;">感觉这个例子没表明啥呀？到底这个 `no_grad()` 要怎么使用现在？</span>

# 相关

- 《深度学习框架 Pytorch 快速开发与实战》
