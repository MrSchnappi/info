---
title: 04 卷积神经网络案例
toc: true
date: 2019-06-15
---
# 可以补充进来的

- 感觉基本都知道。
- 遇到一个问题，`test_output = cnn(test_x)` 这部分取消注释之后为什么会遇到 Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at . 的问题，


# 卷积神经网络案例

下面我们通过案例来实现经典卷积神经网络。


```py
import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.utils.data as Data
import torchvision
import matplotlib.pyplot as plt

EPOCH = 3
BATCH_SIZE_TRAIN = 10
LR = 0.001
DOWNLOAD_MNIST = True

# 准备数据
train_data = torchvision.datasets.MNIST(root='./mnist/',
                                        train=True,
                                        transform=torchvision.transforms.ToTensor(),
                                        download=DOWNLOAD_MNIST,
                                        )
test_data = torchvision.datasets.MNIST(root='./mnist/',
                                       train=False)
print(train_data.train_data.size())
print(train_data.train_labels.size())

# 显示部分数据
# for i in range(1, 4):
#     plt.imshow(train_data.train_data[i].numpy(), cmap='gray')
#     plt.title('%i' % train_data.train_labels[i])
# plt.show()

# 初始化 train_loader
train_loader = torch.utils.data.DataLoader(dataset=train_data,
                                           batch_size=BATCH_SIZE_TRAIN,
                                           shuffle=True)

# test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
#                                           batch_size=batch_size,
#                                           shuffle=False)

# 用于测试的数据
test_size=300
test_x = Variable(torch.unsqueeze(test_data.test_data[:test_size], dim=1),
                  volatile=True).type(torch.FloatTensor)
test_y = test_data.test_labels[:test_size]


class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5,
                      stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)  # (16,14,14)
        )
        self.conv2 = nn.Sequential(  # (16,14,14)
            nn.Conv2d(16, 32, 5, 1, 2),  # (32,14,14)
            nn.ReLU(),
            nn.MaxPool2d(2)  # (32,7,7)
        )
        self.out = nn.Linear(32 * 7 * 7, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size(0), -1)  # 将(batch，32,7,7)展平为(batch，32*7*7)
        output = self.out(x)
        return output


cnn = CNN()
print(cnn)
params = list(cnn.parameters())
print(len(params))
print(params[0].size())

optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)
loss_function = nn.CrossEntropyLoss()

for epoch in range(EPOCH):
    for step, (x, y) in enumerate(train_loader):
        b_x = Variable(x)
        b_y = Variable(y)
        output = cnn(b_x)
        loss = loss_function(output, b_y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (step + 1) % 100 == 0:
            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f'
                  % (epoch + 1, EPOCH, step + 1, len(train_data) // BATCH_SIZE_TRAIN, loss.item()))

            # test_output = cnn(test_x)
            # pred_y = torch.max(test_output, 1)[1].data.squeeze()
            # accuracy = sum(pred_y == test_y).numpy() / test_y.size(0) # TODO 不知道有没有更好的 tensor 转 int 的方式
            # print('Epoch:', epoch, '|Step:', step,
            #       '|train loss:%.4f' % loss.data[0], '|test accuracy:%.4f' % accuracy)


test_output = cnn(test_x[:20])
pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()
print(pred_y, 'prediction number')
print(test_y[:20].numpy(), 'real number')
```

输出如下：

```
torch.Size([60000, 28, 28])
torch.Size([60000])
CNN(
  (conv1): Sequential(
    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (out): Linear(in_features=1568, out_features=10, bias=True)
)
6
torch.Size([16, 1, 5, 5])
Epoch: [1/3], Step: [100/6000], Loss: 0.5199
Epoch: [1/3], Step: [200/6000], Loss: 0.4705
略..
Epoch: [3/3], Step: [5900/6000], Loss: 0.0008
Epoch: [3/3], Step: [6000/6000], Loss: 0.0025
[7 2 1 0 4 1 4 9 8 9 0 6 9 0 1 5 9 7 8 4] prediction number
[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4] real number
```

简单说明如下：


## 数据预处理部分

**`torchvision.datasets.MNIST` 参数说明如下：**

- root：数据集，存在于根目录 processed/training.pt 和 processed/test.pt中。<span style="color:red;">嗯，看到这个的时候，想知道为什么是 .pt 文件？</span>
- train: True = 训练集，False = 测试集 <span style="color:red;">嗯。</span>
- download：如果为 True，请从 Internet 下载数据集并将其放在根目录中。如果数据集已经下载，则不会再次下载。
- transform：将原数据规范化到(0,1)区间，数据变换(0,255)≥(0,1)。<span style="color:red;">`transform=torchvision.transforms.ToTensor()` 是用来做这个的吗？</span>
- 由于加载的数据集为 array 格式，需要转换成 torch 能识别的 tensor，利用 torchvision.datasets 加载 DataLoader 中的 dataset 格式的数据，同时在获取的时候直接转换成训练所需的数据格式。

**对于图像的输出：**

我们需要把照片的像素 torch 的 tensor 数据转换成 Numpy 的形式，才能输出照片。<span style="color:red;">嗯，`train_data.train_data[i].numpy()` 是的，使用的是 `.numpy()` </span>

**`torch.utils.data.DataLoader` 参数说明如下：**

- dataset：加载数据的数据集。
- batch_size：加载批训练的数据个数。
- shuffle:True在每个 epoch 重新排列的数据。
- sampler：从数据集中提取样本。
- batch_sampler：一次返回一批索引。
- num_workers：用于数据加载的子进程数。0表示数据将在主进程中加载。
- collate_fn：合并样本列表以形成小批量。<span style="color:red;">这个是什么意思？</span>
- pin_memory：如果为 True，数据加载器在返回前将张量复制到 CUDA 固定内存中。<span style="color:red;">没懂，为什么要将张量复制到 CUDA 固定内存中？如果数据是从 generator 中临时生成的，这个也要 true 吗？</span>
- drop_last：如果数据集大小不能被 batch_size整除，设置为 True，可删除最后一个不完整的批处理。如果设为 False，并且数据集的大小不能被 batch_size 整除，则最后一个 batch 将更小。


## 定义网络结构

- class CNN 需要继承 Module。
- 需要调用父类的构造方法：super(CNN, self).__init__()。<span style="color:red;">嗯，一定要调用父类的构造方法是吧？</span>
- 在 PyTorch 中激活函数 ReLU 也算是一层 layer。
- 需要实现 forward()方法，用于网络的前向传播，而反向传播只需要调用 Variable.backward()即可。只需定义 forward 函数，并 backward自动使用函数 Autograd，可以在 forward 功能中使用任何 Tensor 操作。<span style="color:red;">嗯。</span>

## 二维卷积层

**nn.Conv2d：**

参数 kernel_size、stride、padding、dilation 也可以是一个 int 的数据，此时卷积 height 和 width 值相同；也可以是一个 tuple 数组，tuple 的第一维度表示 height 的数值，tuple 的第二维度表示 width 的数值。

参数说明如下。

- in_channels(int)：输入信号的通道。
- out_channels(int)：卷积产生的通道。
- kerner_size(int or tuple)：卷积核的尺寸。
- stride(int or tuple,optional)：卷积步长。
- padding(int or tuple,optional)：是否对输入数据填充 0。Padding可以将输入数据的区域改造成是卷积核大小的整数倍，这样对不满足卷积核大小的部分数据就不会忽略了。通过 padding 参数指定填充区域的高度和宽度。
- dilation(int or tuple,`optional``)：卷积核之间的空格。
- groups(int,optional)：将输入数据分成组，in_channels应该被组数整除。group=1，输出是所有的输入的卷积；group=2，此时相当于有并排的两个卷积层，每个卷积层计算输入通道的一半，并且产生的输出是输出通道的一半，随后将这两个输出连接起来。
- bias(bool,optional)：如果 bias=True，添加偏置。


Pooling 函数主要是用于图像处理的卷积神经网络中，但随着深层神经网络的发展，Pooling 函数相关技术在其他领域，其他结构的神经网络中也越来越受关注。

卷积神经网络中的卷积层是对图像的一个邻域进行卷积得到图像的邻域特征，亚采样层就是使用 Pooling 函数技术将小邻域内的特征点整合得到新的特征。Pooling函数确实起到了整合特征的作用。<span style="color:red;">嗯，Pooling 函数起到了整合特征的作用。</span>


池化操作是利用一个矩阵窗口在张量上进行扫描，在每个矩阵中通过取最大值或者平均值等来减少元素的个数，最大值和平均值的方法可以使得特征提取拥有“平移不变性”，也就是在图像有了几个像素的位移情况下，依然可以获得稳定的特征组合，因为平移不变性对于识别十分重要。<span style="color:red;">嗯，是的，平移不变形对于识别非常重要。</span>


Pooling 函数的结果是使得特征减少，参数减少，但 pooling 的目的并不仅在于此。Pooling函数的目的是为了保持某种不变性(旋转、平移、伸缩等)，常用的有 Mean-pooling 函数，Max-pooling 函数和 Stochastic-pooling 函数三种。<span style="color:red;">这他妈又说一遍，各种混乱，而且，Stochastic pooling 又没怎么介绍。。</span>

MaxPool2d(kernel_size,stride,padding,ceil_mode,count_include_pad)，对由几个输入平面组成的输入信号进行二维最大池化。

参数说明如下。

- kernel_size：窗口的大小。
- stride：窗口的步长。默认值为 kernel_size。
- Padding：是否对输入数据填充 0。padding可以将输入数据的区域改造成是卷积核大小的整数倍，这样对不满足卷积核大小的部分数据就不会忽略了。通过 padding 参数指定填充区域的高度和宽度。
- ceil_mode：当为 True 时，将使用 ceil 代替 floor 来计算输出形状。
- count_include_pad：当为 True 时，将包括平均计算中的零填充。默认值为 True。


输出卷积神经网络的搭建通过定义一个 CNN 类来实现，卷积层 conv1、conv2及 out 层以类属性的形式定义，各层之间的衔接信息在 forward 中定义，定义的时候要留意各层的神经元数量。<span style="color:red;">嗯。</span>通过输出卷积神经网络的参数可以看出 2 维卷积的滤波器的大小为 5×5，卷积移动步长为 1 个单元，并且对数据填充，这样对不满足卷积核大小的部分数据就不会忽略了。通过 padding 参数指定填充区域的高度和宽度。池化函数的最大池化的窗口大小为 2×2，最大池化的窗口移动的步长为 2×2,dilation 控制移动窗口的步长大小为 1。




**`torch.optim`**

torch.optim 提供了很多种更新方法，如 SGD、Nesterov–SGD、Adam、RMSProp 等，使用起来很简单。


Adam算法可以看作是修正后的算法，Adam优化算法是随机梯度下降算法的扩展式，近来其广泛用于深度学习应用中，尤其是计算机视觉和自然语言处理等任务。Adam是一种可以替代传统随机梯度下降过程的一阶优化算法，它能基于训练数据迭代更新神经网络权重。

Adam最开始是由 OpenAI 的 Diederik Kingma和多伦多大学的 Jimmy Ba在 2015 年 ICLR 论文 Adam:A Method for Stochastic Optimization中提出。Adam 通常被认为对超参数的选择具有鲁棒性，Adam通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习速率。学习速率建议为 0.001。Adam在深度学习领域内是十分流行的算法，因为它能很快地实现优良的结果。经验性结果证明 Adam 算法在实践中性能优异，相对于其他种类的随机优化算法具有很大的优势。

SGD 指 Stochastic Gradient Descent，即随机梯度下降。对于训练数据集，我们首先将其分成 n 个 batch，每个 batch 包含 m 个样本。我们每次更新都利用一个 batch 的数据，而非整个训练集。当训练数据太多时，利用整个数据集更新往往时间上不显示。batch的方法可以减少机器的压力，并且可以更快地收敛。当训练集有很多冗余时(类似的样本出现多次),batch方法收敛更快。以一个极端情况为例，若训练集前一半和后一半梯度相同，如果前一半作为一个 batch，后一半作为另一个 batch，那么在一次遍历训练集时，batch 的方法向最优解前进两个 step，而整体的方法只前进一个 step。SGD 方法的一个缺点是，其更新方向完全依赖于当前的 batch，因而其更新十分不稳定。Momentum 即动量，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前 batch 的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习得更快。<span style="color:red;">嗯。</span>

输出结果，从输出结果来看，卷积神经网络预测效果还是不错的。从上面输出的训练次数、误差大小，以及精度来看，最后的训练精度达到 99%。通过训练我们可以看出卷积神经网络模型应用在 MNIST 效果上还是很不错的。



# 相关

- 《深度学习框架 Pytorch 快速开发与实战》
