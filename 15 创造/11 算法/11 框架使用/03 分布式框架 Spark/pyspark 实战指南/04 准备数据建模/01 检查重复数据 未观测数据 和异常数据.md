---
title: 01 检查重复数据 未观测数据 和异常数据
toc: true
date: 2019-07-02
---
4.1 检查重复数据、未观测数据和异常数据（离群值）
在你充分测试这些数据并且证明值得花时间去做之前，你不应该相信这些数据，也不应该使用它们。在本节中，我们将向你展示如何处理重复数据、未观测数据以及异常数据（离群值）。
4.1.1 重复数据
重复数据是在数据集中出现在不同行，但是仔细检查之后看起来相同的观测数据。也就是说，如果你一行一行来看，某两行（或者更多行）中的所有特征可能会出现完全相同的值。
另一方面，如果你的数据是由某种 ID 形式来区分每条记录（或者例如将记录和特定的用户关联）的，那么最初看上去重复的可能不是重复数据；有时系统失败产生了错误的 ID。在这种情况下，你需要检查同一个 ID 是否真的是一个重复数据或者你需要生成一个新 ID 的系统。
考虑以下示例：
正如你所见，这里有几个问题：
·有两行 ID 等于 3 并且完全相同。
·ID为 1 和 4 的两行是一样的——唯一不同的是它们的 ID，因此我们完全可以假定它们是同一个人的数据。
·有两行的 ID 等于 5，但是这似乎是一个记录问题，因为它们看上去不是同一个人的数据。
这是一个只有 7 行的非常简单的数据集。当你有数以百万计的观测数据时，你要做什么？通常第一件我要做的事是检查是否有重复数据：比较完整数据集和运行.distinct（）方法后的数据集的数量：
从我们的 DataFrame 中返回的结果是：
如果这两个数字不同，你就会知道你的数据集中有我认为的完全重复数据：彼此相同的行。我们可以通过使用.dropDuplicates（……）方法将这些重复的行移除：
我们的数据集将如下所示（运行 df.show（））：

删除了一行 ID 为 3 的行。现在检查数据中是否有任何和 ID 无关的重复数据。我们可以立马重复之前所做的工作，但是只使用 ID 以外的列做对比：
我们应该看到还有一行是重复数据：
可以继续使用.dropDuplicates（……），不过我们会加入 subset 参数来指定只处理 id 以外的列：
subset参数指示.dropDuplicates（……）方法只查找 subset 参数指定的列；在上一个示例中，我们移除了具有相同 weight、height、age和 gender 的记录，而不是相同的 id。运行 df.show（），当我们移除 id＝1的行，因为它和 id＝4的行一致，我们会得到以下更干净的数据集：
现在，没有任何一行是重复的，即没有任何除了 ID 以外的相同行，让我们检查是否有重复的 ID。这一步要计算 ID 的总数和 ID 的唯一个数，可以使用.agg（……）方法：
以下是上述代码的输出结果：


在之前的示例中，我们首先从 pyspark.sql模块中导入了所有的函数。
这让我们能够访问各种各样的函数，这里列出的函数非常多。不过，我们还是非常鼓励你学习位于 http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#module-pyspark.sql.functions的 PySpark 的文档。
接下来，我们分别利用.count（……）和.countDistinct（……）计算 DataFrame 的行数和 id 的唯一数。.alias（……）方法允许我们对返回列指定一个好记的名称。
如你所见，总共有 5 行，但是只有 4 个唯一 ID。因为已经移除了所有的重复数据，所以我们完全可以假定这可能仅仅是 ID 数据中的偶然事件，因此我们将给每一行一个唯一的 ID。
这段代码的输出结果如下：
.monotonicallymonotonically_increasing_id（）方法给每一条记录提供了一个唯一并且递增的 ID。通过该文档，如果你的数据放置在大约不到 10 亿个分区中，每个分区的记录少于 8 亿条，ID就能被保证是唯一的。提醒：早期的 Spark 版本中，对相同 DataFrame 的多条赋值用.monotonically-monotonically_increasing_id（）方法未必会返回相同的 ID。不过这已经在 Spark 2.0中修正了。
4.1.2 未观测数据
你经常会遇到有空白的数据集。这种数值缺失发生的原因有多种，举例来说有：系统故障、人为失误、数据模式更改。
如果你的数据负担得起的话，处理缺失值最简单的方法是发现任何缺失值时都要移除所有观测数据。你必须注意不要移除太多：根据数据集中缺失值的分布，移除可能会对数据集的可能性造成严重的影响。如果在移除这些行之后，我得到的是一个非常小的数据集或者发现移除的数据超过了数据总量的 50％，我开始检查数据，看看什么特征存在最大的漏洞，也可以完全排除它们；如果一个特征的大部分值都丢失了（除非缺失的值有意义），从建模的角度出发，那它基本是无用的了。
另一个解决缺失值观测数据是在 None 的位置填充一些值。根据你的数据类型，可以选择以下几个选项：
·如果数据是离散布尔型，可以通过添加第三个类别——Missing，将其转换为一个分类变量。
·如果数据已经是分类的，你可以简单地扩展级别的数量，同时添加 Missing 类别。
·如果正在处理顺序类或者数值类数据，你可以填充任何的平均数、中间值或者一些其他预定义的值（例如，第一个四分位数（四分之一）或者第三个四分位数（四分之一），根据你的数据分布形状而定）。
考虑一个我们曾经提到的类似的例子：
在我们的例子中，处理一定数量的缺失的值类别。
分析这些行，我们发现以下内容：
·ID为 3 的行只有一条有用的信息——高度（height）
·ID为 6 的行只有一个缺失值——年龄（age）
分析这些列，我们可以发现以下内容：
·income列的大部分值都是缺失，因为它会透漏非常私人的事情。


·weight列和 gender 列都各只有一个缺失值。
·age列有两个缺失值。
为了查找每行缺少的观测数据，我们可以使用以下代码段：
输出如下：
例如，结果显示 ID 为 3 的行有 4 个缺失的观测数据，正如我们之前观测到的一样。
我们看看缺失的数据，这样当我们统计各列中缺失的观测数据时，可以决定是否要一起移除观测数据或者填充一些观测数据：
这是我们得到的：
现在我们来检查每一列中缺失的观测数据的百分比：
产生以下的输出：.count（……）方法的*参数（列名的位置）指示该方法计算所有的列。另一方面，*之前的列指示.agg（……）方法将该列表处理为一组独立的参数传递给函数。
因此，weight列和 gender 列中缺失的 14％观测数据是 height 列的两倍，并且几乎是 income 列缺失的观测数据的 72％。现在我们知道该怎么做了。
首先，我们将移除‘income’特征，因为它的大部分值都是缺失的。
现在我们看到，不需要移除 ID 为 3 的行，因为这一行在‘weight’列和‘age’中有足够的观测数据（在我们简化的例子中）来计算平均值并且填充缺失值的地方。
但是，如果你决定移除观测数据，你反而可以使用.dropna（……）方法，如下所示。在这里，我们还会利用 thresh 参数，该参数允许我们为每一行缺失观测数据的数量指定一个阈值，这就可以限定要移除的行。如果你有一个具有几十个或者几百个特征的数据集，并且只想移除这些超出某个缺失值阈值的行，这就很有用：
这段代码产生以下输出：

另一方面，如果你想要填充观测数据，可以使用.fillna（……）方法。该方法能够填充单个整型（integer）（长整型 long 也可以）、浮点型（float）或者字符串（string）；整个数据集中所有缺失的值都将用该值来填充。你也可以传递一个{'<colName>'：<value_to_impute>}形式的代码字典（dictionary）。此处的<value_to_impute>具有相同的限制，你只能传递整型、浮点型或者字符串。
如果想填充一个平均数、中间值或者其他计算值，你需要先计算出这个值，创建一个带值的代码字典，再把它传递给.fillna（……）方法。
我们是这样做的：
这段代码将产生以下结果：
我们忽略了 gender 列，因为很明显，我们计算不出一个类别（性别）变量的平均数。
我们在此使用双重转换。先把.agg（……）方法（一个 PySpark DataFrame）的输出转化成一个 pandas 的 DataFrame，再次传递一个代码字典。注意，调用.toPandas（）可能会有问题，因为该方法在本质上和 RRD 中的.collect（）是相同的。它会从工作节点收集信息并交给驱动程序。除非你有成千上万的特征，否则不太可能是前面数据集的问题。
pandas to_dict（……）方法的 records 参数指示它创建以下的代码字典：
由于我们不能计算出平均值（或者任何其他类别变量的数值度量），以将缺失的类别添加到代码字典中从而获取 gender 特征。注意，尽管 age 列的平均数是 40.40，但是填充时，df_miss_no_income.age列的类型还是会被保留——它仍然是个整型。
4.1.3 离群值
异常数据（离群值）指那些与样本其余部分的分布显著偏离的观测数据。显著的定义各不相同，但在最普遍的形式中，如果所有的值大致在 Q1-1.5IQR和 Q3＋1.5IQR范围内，IQR指四分位范围，你可以认为没有离群值；IQR定义为上分位（upper-quartile）和下分位（lower-quartile）之差，也就是分别为第 75 个百分位（Q3）和第 25 个百分位（Q1）。
再来想想一个简单的例子：

现在我们可以使用之前列出的定义来标记离群值。
首先，计算每个特征的上下截断点。我们将使用.approxQuantile（……）方法。第一个指定的参数是列名，第二个参数可以是 0 或 1 之间的其中一个数（其中 0.5是指计算的中位数）或者一个列表（在我们的例子中），第三个参数指定每个度量的一个可接受的错误程度（如果设置为 0，就会计算一个度量的准确值，但是这么做代价很大）：
界限（bounds）代码字典保存了每个特征的上下界限：
现在用它来标记我们的离群值：
这段代码生成以下的输出：


有两个离群值在 weight 特征中，两个离群值在 age 特征中。到目前为止，你应该知道如何提取这些离群值，不过这里有个代码段，列出了和其他剩余分布明显不同的值：
这段代码的输出如下：
使用本节中描述的方法，你可以快速清理即便是最大的数据集。
