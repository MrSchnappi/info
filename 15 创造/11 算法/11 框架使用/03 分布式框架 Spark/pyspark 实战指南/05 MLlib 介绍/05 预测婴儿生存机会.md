---
title: 05 预测婴儿生存机会
toc: true
date: 2019-07-02
---
5.5 预测婴儿生存机会
我们终于可以开始预测婴儿的生存机会了。在本节中，我们将构建两个模型：线性分类器（linear classifier）——逻辑回归，和非线性分类器（non-linear classifier）——随机森林。对于前者，我们使用所有特征来做处理，而对于后者，我们使用 ChiSqSelector（……）方法选出前四个特征。
5.5.1 MLlib中的逻辑回归
逻辑回归从某种程度上说，是构建任何分类模型的基准。MLlib过去使用随机梯度下降（SGD）算法来提供逻辑回归模型的评估。这个模型已经在 Spark 2.0中被弃用，而使用 LogisticRegressionWithLBFGS 模型。
LogisticRegressionWithLBFGS模型使用 Limited-memoryBroyden-Fletcher-Goldfarb-Shanno（BFGS）优化算法。这是一种接近于 BFGS 算法的拟牛顿方法。如果您擅长数学或对此感兴趣，建议您仔细阅读这篇博客文章，这是一个很好的优化算法推演过程：http://aria42.com/blog/2014/12/understanding-lbfgs。
首先使用我们的数据培训模型：
训练模型非常简单：我们只需调用.train（……）方法。需要的参数是带有 LabeledPoint 的 RDD；我们还指定了迭代次数，因此运行时间不会太长。
先使用 births_train数据集训练了模型，接下来让我们使用该模型来预测我们的测试集的分类：
上面的代码段创建了一个 RDD，其中每个元素都是一个元组，第一个元素是实际的标签，第二个元素是模型的预测。
MLlib提供了分类和回归的评估指标。让我们检查一下模型的表现：
得到结果如下：
模型表现相当不错！Precision-Recall曲线下 85％的面积指示契合得很好。在这种情况下，我们可能会预测死亡人数略有增加（真正和假正）。而这种情况实际上是一件好事，因为这样可以让医生对怀孕的母亲和婴儿做特别的关注。
受试者工作特性（Receiver-Operating Characteristic）（ROC）曲线下的面积可以理解为：与随机选择的负实例相比，模型排名几率高于随机选择的正实例。63％这个值是可以接受的。


对这些指标信息感兴趣的读者，请参阅：http://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves和 http://gim.unmc.edu/dxtests/roc3.htm。
5.5.2 只选择最可预测的特征
任何使用较少特征来准确预测类的模型，应始终优于使用复杂特征的模型。MLlib允许使用 Chi-Square选择器来选择最可预测的特征。
如何使用参见如下代码：
我们要求选择器从数据集返回四个最具预测性的特征，并使用 births_train数据集训练选择器。然后，我们使用该模型从训练和测试的数据集中仅仅提取这些特征。
.ChiSqSelector（……）方法只能用于数字特征；在使用选择器之前，分类变量需要进行散列或伪编码。
5.5.3 MLlib中的随机森林
现在做好准备构建随机森林模型了。
以下代码演示了如何做：
.trainClassifier（……）方法的第一个参数指定训练数据集。参数 numClasses 表示我们的目标变量有多少类。作为第三个参数，您可以传递一个 dictionary，其中键是 RDD 中分类特征的索引，键值表示分类特征具有的级别数。numTrees指定森林中树的数目。下一个参数告诉模型使用数据集中的所有特征，而不是只保留最具描述性的特征，而最后一个参数指定模型随机部分的种子。
让我们来看看模型表现得如何：
结果如下：

可以看出，使用较少特征的随机森林模型表现得比逻辑回归模型更好。我们来看看逻辑回归在减少特征数量的情况下，表现如何：
结果可能会让你觉得吃惊：
如您所见，两个模型都可以简化，并且仍然达到相同的准确度。鉴于此，您应该始终选择一个具有较少变量的模型。
