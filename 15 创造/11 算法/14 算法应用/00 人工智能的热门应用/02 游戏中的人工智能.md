---
title: 02 游戏中的人工智能
toc: true
date: 2019-04-27
---
# 可以补充进来的

- 看得心潮澎湃，厉害呀！


# 游戏中的人工智能


自人类文明诞生起，就有了游戏。游戏是人类最早的集益智与娱乐为一体的活动，传说四千年前就有了围棋。几个世纪以来，人们创造出不计其数的各类游戏，比如象棋、国际象棋、跳棋、扑克、麻将、桌游等。半个多世纪前，电子计算机技术诞生，自此游戏焕发了新貌。1980年前后，电视游戏（Video Game）和街机游戏（Arcade Game）开始进入人们视线，当时还是一个小众活动。20 世纪 90 年代，你是否还记得风靡街头的游戏机厅，以及走进千家万户的小霸王学习机。然后，个人计算机的普及将游戏带入了一个崭新的时代。当前，电子游戏不限于电脑，手机、平板等各类带屏平台都被游戏一一拿下。2010 年，游戏已是数千亿美元的产业，全球市场利润远超其他娱乐业。

现在，定义游戏的边界不再清晰。周末聚一帮好友吃着串玩狼人杀，是一种游戏；深夜与千里之外素不相识的网友，组织一小队去做任务，也是一种游戏。然而游戏不只有娱乐功能，还可以教孩子学英文、帮新兵熟悉战场环境，游戏营造出的奖励机制和现场体验，让学习过程事半功倍。

心理学家认为，人们玩游戏时的娱乐体验，构建在智力活动之上。游戏中层层关卡设计，代表不同级别的智力难度，玩家在过关之前，需要投入一定的脑力，观察、思考、实验、学习并动用过去积累的常识知识。

这种对智力逐级考察并及时奖励的过程，是我们产生愉悦感的来源，也是游戏和智能密不可分的联系。<span style="color:red;">嗯。</span>


## 游戏 AI 的历史

早在人工智能处于萌芽期，先驱们就产生用计算机解决一些智力任务的想法。人工智能之父——阿兰·图灵很早就从理论上提出用 MiniMax 算法来下国际象棋的思路[53]。

第一款成功下棋的软件诞生于 1952 年，记录在道格拉斯的博士论文中，玩的是最简单的 Tic-Tac-Toe 游戏（见图 14.10（a））。

<center>

![](http://images.iterate.site/blog/image/20190427/00RSHVHgz4U5.png?imageslim){ width=55% }

</center>


几年后，约瑟夫塞·缪尔开发出下西洋跳棋（见图 14.10（b））的软件，是第一款应用机器学习算法的程序，现在这个算法被人们称为强化学习。<span style="color:red;">那时候就用强化学习了吗？</span>

<center>

![](http://images.iterate.site/blog/image/20190427/1ApNjWQobBHv.png?imageslim){ width=55% }

</center>

在早期的游戏中，AI 都集中在解决经典棋类游戏的问题上，人们相信人类挑战了几百年甚至上千年的游戏，必定是人类智能的精华所在。

然后，三十年的努力，人们在树搜索技术上取得突破。1994年，乔纳森·斯卡费尔的西洋跳棋程序 Chinook 打败了人类冠军马里恩·汀斯雷[54]；2007年，他在《科学》杂志宣布“Checkers is solved”（西洋跳棋已被攻克）[55]。


长时间以来，国际象棋被公认为 AI 领域的实验用 “果蝇”，大量的 AI 新方法被测试于此。直到 1997 年，IBM 的深蓝击败世界级国际象棋大师加里·卡斯帕罗夫（见图 14.11），展现出超人般的国际象棋水平，这只 “果蝇” 终于退休了[56]。

当时深蓝运行在一个超级计算机上，现在一台普通的笔记本就能运行深蓝程序。<span style="color:red;">真的假的？这么厉害，嗯，要看下。</span>

深蓝击败象棋大师加里·卡斯帕罗夫：

<center>

![](http://images.iterate.site/blog/image/20190427/J1nn39g4Sfvb.png?imageslim){ width=55% }

</center>

游戏 AI 的另一个里程碑事件发生在西洋双陆棋上（见图 14.10（c））。1992年，杰拉尔德·特索罗开发的名叫 TD-Gammon 的程序，运用了神经网络和时间差分学习方法，达到了顶尖人类玩家的水准[57]。<span style="color:red;">哇塞！那个时候就用了神经网络和时间差分算法了？厉害！想看下这个程序。</span>

<center>

![](http://images.iterate.site/blog/image/20190427/VqQRXI01TNQM.png?imageslim){ width=55% }

</center>

随着 AI 技术的发展，经历了从高潮到低谷、从低谷到高潮的起起伏伏，时间转移到 2010 年前后，DeepMind、OpenAI 等一批 AI 研究公司的出现，将游戏 AI 推向一个新纪元，下面我们开始一一详述。


## 从 AlphaGo 到 AlphaGo Zero

面对古老的中国游戏——围棋，AI 研究者们一度认为这一天远未到来。

2016年 1 月，谷歌 DeepMind 的一篇论文《通过深度神经网络与搜索树掌握围棋》（Mastering the game of go with deep neural networks and tree search）发表在《自然》杂志上，提到 AI 算法成功运用有监督学习、强化学习、深度学习与蒙特卡洛树搜索算法解决下围棋的难题[58]。

2016年 3 月，谷歌围棋程序 AlphaGo 与世界冠军李世石展开 5 局对战，最终以 4∶1 获胜（见图 14.12）。2016 年年底，一个名为 Master 的神秘围棋大师在网络围棋对战平台上，通过在线超快棋的方式，以 60 胜 0 负的战绩震惊天下，在第 59 盘和第 60 盘的局间宣布自己就是 AlphaGo。2017 年 5 月，AlphaGo 又与被认为世界第一的中国天才棋手柯洁举行三局较量，结果三局全胜。<span style="color:red;">是呀！厉害！</span>

AlphaGo 击败围棋冠军李世石：


<center>

![](http://images.iterate.site/blog/image/20190427/Bx0XBGiHtzOT.png?imageslim){ width=55% }

</center>

从算法上讲，AlphaGo 的成功之处在于完美集成了深度神经网络、有监督学习技术、强化学习技术和蒙特卡洛树搜索算法。<span style="color:red;">是呀，特别想看下 AlphaGo 是怎么实现的。</span>

虽然人们很早就尝试使用蒙特卡洛树搜索算法来解决棋类 AI 问题，但是 AlphaGo：

- 首先采用强化学习加深度神经网络来指导蒙特卡洛树搜索算法。
- 强化学习提供整个学习框架，设计策略网络和价值网络来引导蒙特卡洛树搜索过程；
- 深度神经网络提供学习两个网络的函数近似工具
- 策略网络的初始化权重则通过对人类棋谱的有监督学习获得。

与传统蒙特卡洛树搜索算法不同，AlphaGo 提出 “异步策略与估值的蒙特卡洛树搜索算法”，也称 APV-MCTS。在扩充搜索树方面，APV-MCTS 根据有监督训练的策略网络来增加新的边；在树节点评估方面，APV-MCTS 结合简单的 rollout 结果与当前值网络的评估结果，得到一个新的评估值。<span style="color:red;">怎么这么厉害呢！</span>

训练 AlphaGo 可分成两个阶段：

- 第一阶段，基于有监督学习的策略网络参数，使用强化学习中的策略梯度方法，进一步优化策略网络；
- 第二阶段，基于大量的自我对弈棋局，使用蒙特卡洛策略评估方法得到新的价值网络。

需要指出的是，为了训练有监督版的策略网络，在 50 核的分布式计算平台上要花大约 3 周时间，如图 14.13所示。<span style="color:red;">厉害呀，现在的网络都这么厉害吗？这样长时间的训练是怎么能保证网络最后收敛的？</span>

AlphaGo 的训练：

<center>

![](http://images.iterate.site/blog/image/20190427/MC4pdt9H61Jp.png?imageslim){ width=55% }

</center>

就在众人尚未回过神来之际，AlphaGo的后继者 AlphaGo Zero横空出世，后者根本不需要人类棋谱做预先训练，完全是自己和自己下[59]。

算法上，AlphaGo Zero只凭借一个神经网络，进行千万盘的自我对弈：

- 初始时，由于没有人类知识做铺垫，AlphaGo Zero不知围棋为何物；
- 36小时后，AlphaGo Zero达到 2016 年与李世石对战期 AlphaGo 的水平；
- 72小时后，AlphaGo Zero以 100∶0的战绩绝对碾压李世石版的 AlphaGo；
- 40天后，AlphaGo Zero超越所有版本的 AlphaGo，

如图 14.14所示：<span style="color:red;">是呀，这张图之前看到过，真的震惊了。</span>

<center>

![](http://images.iterate.site/blog/image/20190427/VtQ6IfCabpDs.png?imageslim){ width=55% }

</center>

研究者们评价 AlphaGo Zero 的意义，认为它揭示出一个长期以来被人们忽视的真相-**数据也许并非必要，有游戏规则足够。** <span style="color:red;">嗯嗯，是呀！</span>

这恰和人们这几年的观点相左，认为深度学习技术是数据驱动型的人工智能技术，算法的有效性离不开海量规模的训练数据。事实上，深层次探究个中原因，有了游戏的模拟系统，千万盘对弈、千万次试错不也是基于千万个样本数据吗，只是有效数据的定义不一定指人类的知识。<span style="color:red;">嗯是的。有效数据的定义不一定指人类的知识。</span>

纵观其他经典的棋类游戏，如国际象棋、中国象棋等，无一不是基于确定性规则建立的游戏。这类游戏不仅规则明晰，而且博弈的双方均持有对称的信息，即所谓的 “完美信息”。游戏 AI 面对的问题，通常是一个搜索问题，而且是一对一的 MiniMax 游戏。原理上，记住当前局面并向下进行搜索式推演，可以找到较好的策略。当搜索空间不大时，可以把各种分支情况都遍历到，然后选出最佳方案；当搜索空间太大时，可以用一些剪枝的或概率的办法，减少要搜索的状态数。国际象棋和中国象棋的棋子较少，且不同棋子走子方式固定，用今天的超级计算机穷举不是问题。但是围棋不同，棋盘是 19×19，有 361 个落子点，一盘围棋约有 10 的 170 次方个决策点，是所有棋类游戏中最多的，需要的计算量巨大，所以穷举方式是不可能的，这也导致围棋成为最后被计算机攻克的棋类游戏。数学上，中国象棋和国际象棋的空间复杂程度大约是 10 的 48 次幂，而围棋是 10 的 172 次幂，还有打劫的手段可以反复提子，事实上要更复杂。值得一提的是，可观测宇宙的质子数量为 10 的 80 次幂。

<span style="color:red;">嗯，感觉 AlphaGo 和 AlphaGo Zero 还是要好好学习下的。尤其是对于这个项目而言，是怎么思考的，怎么实施的，软件是怎么写出来的，模块是怎么分的。</span>

## 德州扑克中的 “唬人” AI


德州扑克在欧美十分盛行，大概的规则是每人发两张暗牌，只有自己看到，然后按 3-1-1 的节奏发 5 张明牌，七张牌组成最大的牌型，按照同花顺>四条>葫芦>同花>顺子>三条>两对>对子>高牌的顺序比大小。这期间，玩家只能看到自己的两张底牌和桌面的公共牌，因此得到的信息不完全。

高手可以通过各种策略来干扰对方，比如诈唬、加注骚扰等，无限注德州扑克可以随时全下。<span style="color:red;">这么牛逼</span>

2017 年 1 月，在美国宾夕法尼亚州匹兹堡的河流赌场，一个名为 Libratus 的 AI 程序，在共计 12 万手的一对一无限注德州扑克比赛中，轮流击败四名顶尖人类高手，斩获 20 万美元奖金和约 177 万美元的筹码（见图 14.15）。

它的设计者卡耐基梅隆大学博士诺阿·布朗透露，他自己只是一个德州扑克的爱好者，并不十分精通，平时只与朋友打打五美元一盘的小牌，所以从未通过自己或其他人类的经验教 Libratus 怎么玩牌，仅仅给了它德扑的玩法规则，让它通过“左右互搏”来自己摸索这个游戏该怎么玩，如何能更大概率地获胜。也许正因为布朗未传授人类经验给 Libratus，使它玩德扑的风格如此迥异于人类，让人捉摸不透，而这对获胜十分关键，因为在玩德扑的过程中，下注要具备足够的随机性，才会让对手摸不清底细，同时也是成功诈唬住对手的关键。

与 Libratus 交手的四位人类职业玩家证实了 Libratus 下注十分大胆，不拘一格。它动不动就押下全部筹码，多次诈唬住人类对手，这让人类玩家在 20 天内只有 4 天是赢钱的，其他日子都输了。

据称，Libratus 自我学习能力非常强，人类头一天发现它的弱点，第二天它就不会再犯。<span style="color:red;">我擦，这么牛逼！</span>布朗所用的方法称为反事实遗憾最小化算法（Counterfactual Regret Minimization，CFR），可得到一个近似纳什均衡的解，基本原理是：先挑选一个行为 A 予以实施，当隐状态揭开时，计算假设选择其他非 A 行为可获得的奖励，类似计算机会成本，并将非 A 行为中的最佳收益与事实行为 A 的收益之差称为“遗憾”，如果遗憾大于零，意味着当前挑选的行为非最优，整个过程就是在最小化这个遗憾[60]。<span style="color:red;">没有很明白。想更多的了解这个。</span>

DeepStack 是另一个同样达到世界级水准的德扑 AI 程序[61]。与 Libratus 相同，DeepStack 采用自我对战和递归推理的方法学习策略；不同的是，它不是计算一个显式的策略，而是类似 AlphaGo，采用树搜索结合近似值函数的强化学习方法来决定每轮的行为，可看成一个带不完美信息的启发式搜索 AlphaGo。<span style="color:red;">想更多的了解这个。</span>

德州扑克 AI Libratus的决策过程：

<center>

![](http://images.iterate.site/blog/image/20190427/ozESvtbwbn5G.png?imageslim){ width=55% }

</center>

牌类游戏与棋类游戏不同。

国际象棋、中国象棋和围棋等都是“完美信息”游戏，也就是说，所有玩家在游戏中获得的信息是确定的、公开的和对称的。AI 攻克这些游戏的难度，主要取决于游戏过程的决策点数量，这决定了需要的计算量。

然而，扑克是一种包含很多隐藏信息的“不完美信息”游戏。玩家掌握不对称信息，只看得到自己手里的牌，却不知道对手手中的牌，更不知道对手如何猜测自己的手牌。因此，虽然一局德扑的决策点数量要少于一盘围棋，但是不确定性的加入，使得每个决策点上，玩家都要全盘进行推理，计算量难以想象。

在非对称信息博弈中，对同样的客观状态，由于每个玩家看到的信息不同，这增加了玩家状态空间的数目以及做决策的难度。如果考虑心理层面的博弈，有别于机器，人类可以 “诈唬” 来虚张声势，这被人类看作是智商和情商的完美结合。

非对称博弈中双方的猜测是彼此的，是相互影响的，故而没有单一的最优打法，AI 必须让自己的移动随机化，这样在它唬骗对方时对方才无法确定真假。举个石头剪子布的例子，如果别人一直用石头剪刀布各 1/3的混合策略，那自己就会发现好像怎么出招收益都是 0；于是每次都出石头，但是这样的话，对手就可以利用这个策略的弱点提高自己的收益。**所以好的算法就要求，基于别人已有策略得到的新策略要尽可能地少被别人利用。** <span style="color:red;">厉害！</span>

这样的研究有很实际的意义，它将来能够应用在金融谈判、拍卖、互联网安全等领域，需要 AI 在“不完美信息”的情景中做出决策，这或许正是 Libratus 擅长的。<span style="color:red;">是呀，对这个真的非常感兴趣，要仔细学习下这个。</span>



## AI电子竞技

2013 年，尚未被谷歌收购的 DeepMind 发表了一篇里程碑式的论文《用深度强化学习玩 Atari》（Playing Atari with deep reinforcement learning）[62]。Atari 2600 是 20 世纪 80 年代一款家庭视频游戏机（见图 14.16），相当于以前的小霸王学习机，输出信号接电视机，输入则是一个控制杆。研究者通常在它的模拟器 Arcade Learning Environment（ALE）上做实验[63]。

这篇论文试图让 AI 仅凭屏幕上的画面信息及游戏分数，学会打遍所有 Atari 2600 上的游戏。该文充分吸收了近些年深度学习的研究成果——深度卷积神经网络，结合强化学习的已有框架，运用经验回放的采样思路，设计出深度 Q-learning 算法，最后结果出奇地好，在很多游戏上都胜过人类高手。<span style="color:red;">是呀，太赞了！</span>

传说正是因为这点，让谷歌看上了 DeepMind。2015 年，谷歌 DeepMind 在《自然》杂志上发表了著名的文章《通过深度强化学习达到人类水平的控制》（Human-level control through deep reinforcement learning），提出了著名的深度 Q 网络（DQN），仅训练一个端到端的 DQN，便可在 49 个不同游戏场景下全面超越人类高手[64]。<span style="color:red;">仅训练一个 DQN 吗？为什么？</span>

游戏机 Atari 上的游戏：

<center>

![](http://images.iterate.site/blog/image/20190427/idMtO4LRFBeP.png?imageslim){ width=55% }

</center>

此外，在 2016 年 4 月，另一家 AI 研究公司- OpenAI 对外发布了一款用于研发和评比强化学习算法的工具包 Gym。Gym 包括了各种模拟环境的游戏，如最经典的倒立摆。该平台提供一个通用的交互界面，使开发者可以编写适用不同环境的通用 AI 算法。开发者通过把自己的 AI 算法拿出来训练和展示，获得专家和其他爱好者的点评，大家共同探讨和研究。强化学习有各种各样的开源环境集成，与它们相比，Gym 更为完善，拥有更多种类且不同难度级别的任务，如图 14.17所示。<span style="color:red;">嗯，是的呀，好！</span>

<span style="color:red;">想知道这个 Gym 现在发展的怎么样了，对应的游戏的算法现在发展的怎么样了？有没有比 Gym 更厉害的出现？</span><span style="color:blue;">嗯嗯，有，下面有介绍 Universe</span>

- 倒立摆（Cart Pole）：这是一个经典控制问题。一个杆一个小车，杆的一端连接到小车，连接处自由，杆可以摆来摆去。小车前后两个方向移动，移动取决于施加的前后作用力，大小为 1。目标是控制力的方向，进而控制小车，让杆保持站立。注意小车的移动范围是有限制的。![](http://images.iterate.site/blog/image/20190427/vWAiDPNo4v3o.png?imageslim){ width=55% }
- 月球登陆者（Lunar Lander）：这个游戏构建在 Box2D 模拟器上。Box2D 是一款 2D游戏世界的物理引擎，可处理二维物体的碰撞、摩擦等力学问题。本游戏的场景是让月球车顺利平稳地着陆在地面上的指定区域，接触地面一瞬间的速度最好为 0，并且消耗的燃料越少越好。<span style="color:red;">哇塞</span>![](http://images.iterate.site/blog/image/20190427/tvPpFY7vNDw5.png?imageslim){ width=55% }
- 双足行走者（Bipedal Walker）：同样基于 Box2D 模拟器，这个游戏中玩家可以控制双足行走者的步进姿态。具体地说，是控制腿部膝关节处的马达扭力，尽量让行走者前进得更远，同时避免摔倒。本环境提供的路面包括台阶、树桩和陷坑，同时给行走者提供 10 个激光测距值。另外，环境的状态信息包括水平速度、垂直速度、整体角速度和关节处角速度等。<span style="color:red;">嗯嗯。</span>![](http://images.iterate.site/blog/image/20190427/GJlBf9QfCf5N.png?imageslim){ width=55% }
- 毁灭战士（Doom：Defend Line）：这是一款仿 3D 的第一人称射击游戏。游戏场景是在一个密闭的空间里，尽可能多地杀死怪物和保全自己，杀死的怪物越多，奖励就越多。AI 玩家所能观察的，同人类玩家一样，只是一个第一人称的视野。<span style="color:red;">哇塞！</span>![](http://images.iterate.site/blog/image/20190427/CmHmqe28XODd.png?imageslim){ width=55% }


OpenAI 显然不满足于此。2016年年底，继 4 月发布 Gym 之后，OpenAI又推出一个新平台-Universe（见图 14.18）。Universe 的目标是评估和训练通用 AI。同 Gym上的定制游戏不同，Universe 瞄准的环境是世界范围的各种游戏、网页及其他应用，与人类一样面对相同复杂和实时程度的环境，至少在信息世界这个层面上，物理世界还有待传感器和硬件的进步。具体地讲，游戏程序被打包到一个 Docker 容器里，提供给外部的接口，人与机器一样的，谁都不能访问游戏程序的内部，只能接收屏幕上的画面，和发送键盘和鼠标指令。<span style="color:red;">哇塞！就是我想要的！嗯，现在的这个的发展怎么样了？有什么最新的进展吗？要持续关注下，要不断学习下。</span>


<center>

![](http://images.iterate.site/blog/image/20190427/7yVby5EziENm.png?imageslim){ width=55% }

</center>

Universe 的目标是让设计者开发单一的智能体，去完成 Universe 中的各类游戏和任务。当一个陌生游戏和任务出现时，智能体可以借助过往经验，快速地适应并执行新的游戏和任务。我们都知道，虽然 AlphaGo 击败了人类世界围棋冠军，但是它仍然属于狭义 AI，即可以在特定领域实现超人的表现，但缺乏领域外执行任务的能力，就像 AlphaGo 不能陪你一起玩其他游戏。为了实现具有解决一般问题能力的系统，就要让 AI 拥有人类常识，这样才能够快速解决新的任务。

因此，智能体需要携带经验到新任务中，而不能采用传统的训练步骤，初始化为全随机数，然后不断试错，重新学习参数。这或许是迈向通用 AI 的重要一步，所以我们必须让智能体去经历一系列不同的任务，以便它能发展出关于世界的认知以及解决问题的通用策略，并在新任务中得到使用。<span style="color:red;">是的是的。</span>


最典型的任务就是基于浏览器窗口的各项任务。互联网是一个蕴藏丰富信息的大宝藏。Universe 提供了一个浏览器环境，要求 AI 能浏览网页并在网页间导航，像人类一样使用显示器、键盘和鼠标。当前的主要任务是学习与各类网页元素交互，如点击按钮、下拉菜单等。将来，AI可以完成更复杂的任务，如搜索、购物、预定航班等。<span style="color:red;">哇塞！就是我想要的！</span>



## 星际争霸：走向通用 AI


面对策略类电脑游戏，挑战难点不仅仅是像素点阵组成的画面，更在于高级认知水平的表现，考察 AI 能否综合对多种单位、多种要素等的分析，设计复杂的计划，并随时根据情况灵活调整计划，尤其是即时类策略游戏，被视为 AI 最难玩的游戏。<span style="color:red;">是呀</span>

星际争霸（StarCraft）就是一款这样的游戏，于 1998 年由暴雪娱乐公司发行（见图 14.19）。它的资料片母巢之战（Brood War）提供了专给 AI 程序使用的 API，激发起很多 AI 研究者的研究热情[65]。

在平台方面，DeepMind 在成功使用深度学习攻克 Atari 游戏后，宣布和暴雪公司合作，将 StarCraft II 作为新一代 AI 测试环境，发布 SC2LE 平台，开放给 AI 研究者测试他们的算法。SC2LE 平台包括暴雪公司开发的 Machine Learning API、匿名化后的比赛录像数据集、DeepMind 开发的 PySC2 工具箱和一系列简单的 RL迷你游戏[66]。Facebook 也早在 2016 年就宣布开源 TorchCraft，目的是让每个人都能编写星际争霸 AI 程序。TorchCraft 是一个能让深度学习在即时战略类游戏上开展研究的库，使用的计算框架是 Torch[67]。<span style="color:red;">嗯，再难在卡也要把它搞定，实在不行把那个 GPU 电脑装成 Windows 的。</span>


暴雪公司出品的游戏“星际争霸”：


<center>

![](http://images.iterate.site/blog/image/20190427/xhsMu13lod74.png?imageslim){ width=55% }

</center>


在算法方面，Facebook在 2016 年提出微操作任务，来定义战斗中军事单位的短时、低等级控制问题，称这些场景为微操作场景[68]。<span style="color:red;">微操作任务是什么意思？</span>为了解决微操作场景下的控制问题，他们运用深度神经网络的控制器和启发式强化学习算法，在策略空间结合使用直接探索和梯度反向传播两种方法来寻找最佳策略。<span style="color:red;">怎么做的？想更多的了解。</span>

阿里巴巴的一批人也在 2017 年参与到这场 AI 挑战赛中，提出一个多智能体协同学习的框架，通过学习一个多智能体双向协同网络，来维护一个高效的通信协议，实验显示 AI 可以学习并掌握星际争霸中的各类战斗任务[69]。<span style="color:red;">哇塞！是怎么做到的？补充到这里。</span>

一般说来，玩星际争霸有三个不同层面的决策：

- 最高层面是战略水平的决策，要求的信息观察强度不高；
- 最低层面是微操作水平的决策，玩家需要考虑每个操控单位的类型、位置及其他动态属性，大量的信息都要通过观察获取；
- 中间层面是战术水平的决策，如兵团的位置及推进方向

星际争霸的三个决策层次：

<center>

![](http://images.iterate.site/blog/image/20190427/uzvJNeWJPcIB.png?imageslim){ width=55% }

</center>

<span style="color:red;">这些决策层次是怎么互相融合的？怎么协同作用的？嗯，想更多的知道这个。</span>

可见，即时战略类游戏对 AI 来讲有着巨大的挑战，代表着智能水平测试的最高点。<span style="color:red;">是呀，厉害的。</span>



## 为什么 AI 需要游戏？

游戏并非只有对弈。自电子游戏诞生起，有了非玩家角色（Non-Player Character）的概念，就有了游戏 AI 的强需求。引入非玩家角色，或对抗，或陪伴，或点缀，提升了游戏的难度，增强了游戏的沉浸感。与不同难度等级 AI 的对抗，也让玩家能够不断燃起挑战的欲望，增强游戏的黏性。另一方面，游戏行业也是 AI 发展最理想的试金石[70]。<span style="color:red;">是的。</span>


游戏提供了定义和构建复杂 AI 问题的平台。传统学术界的 AI 问题都是单一、纯粹的，每个问题面向一个特定任务，比如图片分类、目标检测、商品推荐等。走向通用 AI，迟早要摆脱单一任务设定，去解决多输入、多场景和多任务下的复杂问题。<span style="color:red;">是的。</span>

从这点看，游戏是传统学术问题无法媲美的，即使是规则简单的棋类游戏，状态空间规模也是巨大的，包含各种制胜策略。从计算复杂性角度看，许多游戏都是 NP-hard。在由这些难度铺设的爬山道上，研究者们相继攻克了西洋棋、西洋双陆棋、国际象棋、中国象棋和围棋，以及简单电子游戏 Atari 系列和超级马里奥等。现在，人们正把目光放在更大型、更具挑战性的星际争霸。

游戏提供了丰富的人机交互形式。游戏中人机交互是指人的各种操作行为以及机器呈现给人的各种信息，具有快节奏多模态的特征。一方面，游戏要么是回合制的，人机交互的频率一般都是秒级，有的稍长，比如围棋、大富翁等，要么是实时的，频率更短，比如极品飞车、星际争霸等；另一方面，人们通过键盘、鼠标和触摸板控制游戏中的角色，但不限于此，在一些新出的游戏中，人们还可通过移动身体、改变身体姿态和语音控制的方式参与游戏。如果将交互信息的形态考虑进来，有动作、文本、图片、语音等；如果将交互信息在游戏中的作用考虑进来，可以是以第一人称方式直接控制角色，如各类 RPG 游戏，可以是以角色切换的方式控制一个群体，如实况足球，还可以从上帝视角经营一个部落、一个公司或一个国家，如文明。

复杂的人机交互方式，形成了一个认知、行为和情感上的模式闭环——引发（Elicit）、侦测（Detect）和响应（Respond），将玩家置身于一个连续的交互模式下，创造出与真实世界相同的玩家体验。想象一下，AI算法做的不再是拟合数据间的相关性，而是去学习一种认知、行为和情感上的人类体验。<span style="color:red;">是的。</span>


游戏市场的繁荣提供了海量的游戏内容和用户数据。当前大部分 AI 算法都是数据驱动的，以深度学习为例，欲得到好的实验效果，需要的训练集都在千万级规模以上。在软件应用领域，游戏是内容密集型的。当前游戏市场，每年都会产生很多新游戏，游戏种类五花八门。因此，无论从内容、种类还是数量上，数据都呈爆炸式增长。此外，随着各类游戏社区的壮大，玩家提出了更高的要求，期待获得更好的玩家体验，游戏行业被推向新的纪元。除了游戏内容数据，随着玩家群体延伸到各年龄层、各类职业人群，用户行为数据也爆炸式增长，游戏大数据时代已然来临。<span style="color:red;">是呀</span>

游戏世界向 AI 全领域发出了挑战：

- 很多电子游戏都有一个虚拟的时空世界，各种实时的多模态的时空信号，在人与机器间频繁传送，如何融合这些信号做出更好的预测，是信号处理科学的一个难题。
- 棋类游戏不涉及虚拟世界，规则简单清晰，没有各类复杂信号，但解决这类问题也不是一件简单的事情，因为状态空间庞大，所以要设计高效的搜索方法，如国际象棋、西洋棋依靠 MiniMax 树搜索，围棋用到蒙特卡洛树搜索。此外，解决围棋问题更少不了深度学习和强化学习方法。
- 早年的电视游戏和街机游戏，都是通过二维画面和控制杆的方式实现人机交互，如果让 AI 像人一样在像素级别上操作控制杆玩游戏，就用到深度学习中最火的卷积神经网络，并与强化学习结合为深度强化学习方法。
- Jeopardy！是美国很流行的一个知识问答类真人秀，AI要解决知识问答，既要用到自然语言处理技术，也要具备一定的通识知识，掌握知识表征和推理的能力。
- 另外，规划、导航和路径选择，也是游戏中常见的 AI 问题。
- 更大型的游戏如星际争霸，场景更复杂，既是实时的又是策略的，集成了各类 AI 问题。

<span style="color:red;">是呀！真的厉害！</span>


如果上述几点理论仍无法让你信服，那么当前 DeepMind 和 OpenAI 等公司及一些大学研究机构的强力推动，研究者们产生的各种天马行空的想法，足以让你感到一种震撼，看清游戏对 AI 的巨大推动。事实上，当下越来越多的 AI 研究者，开始将游戏视作构建新型通用 AI 的超级试验场。为什么呢？


- 无进化速度的限制
    - 与经历上亿年漫长进化的人类相比，游戏提供的虚拟世界没有时间流速的限制，计算流代替了现实世界的时间流，处理器计算频率越快，计算并行度越高，沿时间轴演化的速度越快。一天的时间，已经完成百万次的迭代。
- 无限次场景和无限次重生
    - 游戏世界可以提供无限次重复的场景，智能体拥有无限次重生的机会，使得进化的试错代价大大降低。这让笔者联想到一部关于人工智能的美剧《西部世界》，里面的机器人经历一次次死亡与重生，终于迎来最后的觉醒，听上去真让人有些害怕。<span style="color:red;">嗯，迎来最后的觉醒。</span>
- 独立的世界
    - 游戏世界与现实世界独立，既可以模拟现实世界的物理规则，也可以打破物理规则，看智能体的应对策略。前者对现实世界高度仿真，有助于在开展硬件实验前，如无人车、机器人，先期探索适用的 AI 模型和算法，大大降低耗费在硬件上的成本。后者呢？在我们尚未抵达或尚未了解的极端物理世界、网络世界或其他世界，进行假设性试验，先假设一些未知的规则，再看智能体的进化轨迹，为人类的未来作打算。


当然，游戏也需要 AI，升级的 AI 会大大增加游戏的玩家体验。以前游戏中的 AI 大都是写死的，资深玩家很容易发现其中的漏洞。刚开始时，玩家找到这些漏洞并借以闯关升级，这带来很大乐趣；慢慢地，玩家厌倦了一成不变的难度和重复出现的漏洞。如果 AI 是伴随玩家逐步进化的，这就有意思了。<span style="color:red;">是呀，以后的游戏，谁家要是么有什么真的 AI 参与，估计游戏性都下降不少。</span>

还有一点，传统游戏 AI 属于游戏系统自身，获取的是程序内部数据，和玩家比有不对称优势。现在的 AI 要在玩家视角下，采用屏幕画面作为 AI 系统的输入，像一个人类玩家来玩游戏。<span style="color:red;">是呀！必须要达到的。</span>智能体与人类玩家，不仅存在对抗，还存在协作。我们甚至可以建立一个协作平台，用自然语言的方式，向 AI 传达指令，或接收来自 AI 的报告。<span style="color:red;">嗯，超级试验场。</span>


总之，在游戏这个超级 AI 试验场上，一切皆有可能。<span style="color:red;">嗯。</span>





# 相关

- 《百面机器学习》
