---
title: 2.11 LeNet-5
toc: true
date: 2019-09-03
---

## 4.1 LeNet-5

### 4.1.1 模型介绍

LeNet-5 是由 $LeCun$ 提出的一种用于识别手写数字和机器印刷字符的卷积神经网络（Convolutional Neural Network，CNN），其命名来源于作者 $LeCun$ 的名字，5 则是其研究成果的代号，在 LeNet-5 之前还有 LeNet-4 和 LeNet-1 鲜为人知。LeNet-5 阐述了图像中像素特征之间的相关性能够由参数共享的卷积操作所提取，同时使用卷积、下采样（池化）和非线性映射这样的组合结构，是当前流行的大多数深度图像识别网络的基础。

### 4.1.2 模型结构

LeNet-5 网络结构图：
<center>

![](http://images.iterate.site/blog/image/20190722/y4YQLvVNnUBn.png?imageslim){ width=80% }

</center>

如图所示，LeNet-5 一共包含 7 层（输入层不作为网络结构），分别由 2 个卷积层、2 个下采样层和 3 个连接层组成。

网络的参数配置如表 4.1所示，其中下采样层和全连接层的核尺寸分别代表采样范围和连接矩阵的尺寸（如卷积核尺寸中的 $“5\times5\times1/1,6”$ 表示核大小为 $5\times5\times1$、步长为 $1​$ 且核个数为 6 的卷积核）。

LeNet-5 网络参数配置：

|      网络层      |       输入尺寸       |          核尺寸          |       输出尺寸       |          可训练参数量           |
|:----------------:|:--------------------:|:------------------------:|:--------------------:|:-------------------------------:|
|   卷积层 $C_1$   | $32\times32\times1$  |  $5\times5\times1/1,6$   | $28\times28\times6$  |  $(5\times5\times1+1)\times6$   |
|  下采样层 $S_2$  | $28\times28\times6$  |       $2\times2/2$       | $14\times14\times6$  |       $(1+1)\times6$ $^*$       |
|   卷积层 $C_3$   | $14\times14\times6$  |  $5\times5\times6/1,16$  | $10\times10\times16$ |            $1516^*$             |
|  下采样层 $S_4$  | $10\times10\times16$ |       $2\times2/2$       |  $5\times5\times16$  |         $(1+1)\times16$         |
| 卷积层 $C_5$$^*$ |  $5\times5\times16$  | $5\times5\times16/1,120$ | $1\times1\times120$  | $(5\times5\times16+1)\times120$ |
|  全连接层 $F_6$  | $1\times1\times120$  |      $120\times84$       |  $1\times1\times84$  |        $(120+1)\times84$        |
|      输出层      |  $1\times1\times84$  |       $84\times10$       |  $1\times1\times10$  |        $(84+1)\times10$         |

> ​	$^*$ 在 LeNet 中，下采样操作和池化操作类似，但是在得到采样结果后会乘以一个系数和加上一个偏置项，所以下采样的参数个数是 $(1+1)\times6​$ 而不是零。<span style="color:red;">哦，这个之前不知道，解答了我的疑惑，但是为什么要有这个稀疏和偏置项呢？现在还有吗？用处是什么？</span>
>
> ​	$^*$ $C_3$ 卷积层可训练参数并未直接连接 $S_2$ 中所有的特征图（Feature Map），而是采用如下图所示的采样特征方式进行连接（稀疏连接），生成的 16 个通道特征图中分别按照相邻 3 个特征图、相邻 4 个特征图、非相邻 4 个特征图和全部 6 个特征图进行映射，得到的参数个数计算公式为 $6\times(25\times3+1)+6\times(25\times4+1)+3\times(25\times4+1)+1\times(25\times6+1)=1516$，<span style="color:red;">有点没大理解。</span>在原论文中解释了使用这种采样方式原因包含两点：限制了连接数不至于过大（当年的计算能力比较弱）；强制限定不同特征图的组合可以使映射得到的特征图学习到不同的特征模式。<span style="color:red;">这种限定特征图的组合来学习不同的特征模式的方式可行吗？</span>
>
> $S_2$ 与 $C_3$ 之间的特征图稀疏连接：
> <center>
>
> ![](http://images.iterate.site/blog/image/20190722/m0obhOpt91PU.jpg?imageslim){ width=55% }
>
> </center>
>
> ​	$^*$ $C_5$ 卷积层在示意图中显示为全连接层，原论文中解释这里实际采用的是卷积操作，只是刚好在 $5\times5$ 卷积后尺寸被压缩为 $1\times1​$，输出结果看起来和全连接很相似。

### 4.1.3 模型特性


- 卷积网络使用一个 3 层的序列组合：卷积、下采样（池化）、非线性映射（LeNet-5 最重要的特性，奠定了目前深层卷积网络的基础）
- 使用卷积提取空间特征
- 使用映射的空间均值进行下采样
- 使用 $tanh$ 或 $sigmoid$ 进行非线性映射
- 多层神经网络（MLP）作为最终的分类器
- 层间的稀疏连接矩阵以避免巨大的计算开销







# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
