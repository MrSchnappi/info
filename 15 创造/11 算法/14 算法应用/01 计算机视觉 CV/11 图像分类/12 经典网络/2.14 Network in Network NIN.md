---
title: 2.14 Network in Network NIN
toc: true
date: 2019-09-03
---

## 4.4 Network in Network

<span style="color:red;">再看下这个网络。没有怎么理解。</span>


### 4.4.1 模型介绍
​

Network In Network (NIN)是由 $Min Lin$ 等人提出，在 CIFAR-10 和 CIFAR-100 分类任务中达到当时的最好水平，因其网络结构是由三个多层感知机堆叠而被成为 NIN$^{[5]}$。

NIN 以一种全新的角度审视了卷积神经网络中的卷积核设计，通过引入子网络结构代替纯卷积中的线性映射部分，这种形式的网络结构激发了更复杂的卷积神经网络的结构设计，其中下一节中介绍的 GoogLeNet 的 Inception 结构就是来源于这个思想。

<span style="color:red;">子网络结构，没听说过，看下。</span>

### 4.4.2 模型结构


NIN 网络结构图：

<center>

![](http://images.iterate.site/blog/image/20190722/E4WWatTig6k0.jpg?imageslim){ width=75% }

</center>


NIN 由三层的多层感知卷积层（MLPConv Layer）构成，每一层多层感知卷积层内部由若干层的局部全连接层和非线性激活函数组成，代替了传统卷积层中采用的线性卷积核。

在网络推理（inference）时，这个多层感知器会对输入特征图的局部特征进行划窗计算，并且每个划窗的局部特征图对应的乘积的权重是共享的，这两点是和传统卷积操作完全一致的，<span style="color:red;">是呀。</span>最大的不同在于多层感知器对局部特征进行了非线性的映射，而传统卷积的方式是线性的。<span style="color:red;">对局部特征进行了非线性的映射？怎么做的？</span>

NIN的网络参数配置表 4.4 所示（原论文并未给出网络参数，表中参数为结合网络结构图和 CIFAR-100 数据集以 $3\times3$ 卷积为例给出）。

表 4.4 NIN网络参数配置（结合原论文 NIN 结构和 CIFAR-100数据给出）：

|           网络层           |       输入尺寸        |         核尺寸          |       输出尺寸        |            参数个数             |
|:--------------------------:|:---------------------:|:-----------------------:|:---------------------:|:-------------------------------:|
| 局部全连接层 $L_{11}$ $^*$ |  $32\times32\times3$  | $(3\times3)\times16/1$  | $30\times30\times16$  |  $(3\times3\times3+1)\times16$  |
|   全连接层 $L_{12}$ $^*$   | $30\times30\times16$  |      $16\times16$       | $30\times30\times16$  |       $((16+1)\times16)$        |
|   局部全连接层 $L_{21}$    | $30\times30\times16$  | $(3\times3)\times64/1$  | $28\times28\times64$  | $(3\times3\times16+1)\times64$  |
|     全连接层 $L_{22}$      | $28\times28\times64$  |      $64\times64$       | $28\times28\times64$  |       $((64+1)\times64)$        |
|   局部全连接层 $L_{31}$    | $28\times28\times64$  | $(3\times3)\times100/1$ | $26\times26\times100$ | $(3\times3\times64+1)\times100$ |
|     全连接层 $L_{32}$      | $26\times26\times100$ |     $100\times100$      | $26\times26\times100$ |      $((100+1)\times100)$       |
|  全局平均采样 $GAP$ $^*$   | $26\times26\times100$ | $26\times26\times100/1$ |  $1\times1\times100$  |               $0$               |


> 局部全连接层 $L_{11}$ 实际上是对原始输入图像进行划窗式的全连接操作，因此划窗得到的输出特征尺寸为 $30\times30$（$\frac{32-3_k+1}{1_{stride}}=30$）
>
> 全连接层 $L_{12}$ 是紧跟 $L_{11}$ 后的全连接操作，输入的特征是划窗后经过激活的局部响应特征，因此仅需连接 $L_{11}$ 和 $L_{12}$ 的节点即可，而每个局部全连接层和紧接的全连接层构成代替卷积操作的多层感知卷积层（MLPConv）。
>
> 全局平均采样层或全局平均池化层 $GAP$（Global Average Pooling）将 $L_{32}$ 输出的每一个特征图进行全局的平均池化操作，直接得到最后的类别数，可以有效地减少参数量。<span style="color:red;">怎么进行全局平均池化的？</span>

### 4.4.3 模型特点

<span style="color:red;">看起来很牛逼呀，再看下这个网络。</span>

- 使用多层感知机结构来代替卷积的滤波操作，不但有效减少卷积核数过多而导致的参数量暴涨问题，还能通过引入非线性的映射来提高模型对特征的抽象能力。
- 使用全局平均池化来代替最后一个全连接层，能够有效地减少参数量（没有可训练参数），同时池化用到了整个特征图的信息，对空间信息的转换更加鲁棒，最后得到的输出结果可直接作为对应类别的置信度。







# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
