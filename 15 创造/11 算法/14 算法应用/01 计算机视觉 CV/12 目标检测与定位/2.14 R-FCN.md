---
title: 2.14 R-FCN
toc: true
date: 2019-09-03
---

### 8.2.4 R-FCN



R-FCN

由于现在的主流网络层数越来越多，基于 Faster RCNN检测框架的方法的计算量受到了 3 个因素的影响：

1.基础网络的复杂度

2.候选框数量的多少

3.分类和位置回归子网络的复杂度（每个候选框的 box 都会独立进行前向计算）。



一般来说直接优化前两点性价比不太高。如果直接优化 RoI-wise subnetwork是否可行呢，将子网络的深度尽可能减少？分类是要增加物体的平移不变性（不同的位置都是同一个物体）；目标检测时减少物体的平移变化（目标检测需要得到物体所在的位置）。通常我们所用的网络都是 ImageNet 的分类任务训练得到的，在目标检测的时候进行 Finetune。由于得到的初始模型基于分类任务，那么会偏向于平移不变性，这和目标检测就出现了矛盾。



MSRA的 Jifeng Dai等人提出了 R-FCN，通过 position-positive score maps（位置敏感得分图）来解决这个矛盾。位置敏感得分图通过预测 RoI 中不同部位的类别投票表决产生该 RoI 的类别预测。引用原文中的例子，“如果我们的算法要识别婴儿，那么把一个目标区域分成九宫格，其中算法认为其中五个格子中的区域分别像婴儿的头、四肢和躯干，那么根据投票机制，就认为这个目标区域里的是一个婴儿。这很符合我们人类的判断逻辑。”



![mark](http://images.iterate.site/blog/image/20190905/I37ldsmCLTMR.png?imageslim)


 R-FCN沿用了 Faster RCNN 的框架结构，不同的是在 Faster R-CNN的基础上通过引入位置敏感得分图，将 RoI-wise subnetwork消灭了，直接在位置敏感得分图上利用 ROI Pooling进行信息采样融合分类和位置信息。



![mark](http://images.iterate.site/blog/image/20190905/QIFONQ2GEwWB.png?imageslim)
R-FCN 网络框架结构

![mark](http://images.iterate.site/blog/image/20190905/DbIi83nfkVHc.png?imageslim)
ResNet101为例，不同检测框架复用卷积网络层数






<span style="color:red;">没有看明白。</span>

**R-FCN 有哪些创新点？**

R-FCN 仍属于 two-stage 目标检测算法：RPN+R-FCN

1. Fully convolutional <span style="color:red;">什么是 fully convolutional ？</span>
2. 位置敏感得分图（position-sentive score maps）<span style="color:red;">什么是位置敏感得分图？</span>

> our region-based detector is **fully convolutional** with almost all computation shared on the entire image. To achieve this goal, we propose **position-sensitive score maps** to address a dilemma between translation-invariance in image classification and translation-variance in object detection.

R-FCN backbone：ResNet

ResNet-101+R-FCN：83.6% in PASCAL VOC 2007 test datasets

既提高了 mAP，又加快了检测速度

假设我们只有一个特征图用来检测右眼。那么我们可以使用它定位人脸吗？应该可以。因为右眼应该在人脸图像的左上角，所以我们可以利用这一点定位整个人脸。如果我们还有其他用来检测左眼、鼻子或嘴巴的特征图，那么我们可以将检测结果结合起来，更好地定位人脸。

现在我们回顾一下所有问题。在 Faster R-CNN 中，检测器使用了多个全连接层进行预测。如果有 2000 个 ROI，那么成本非常高。R-FCN 通过减少每个 ROI 所需的工作量实现加速。上面基于区域的特征图与 ROI 是独立的，可以在每个 ROI 之外单独计算。剩下的工作就比较简单了，因此 R-FCN 的速度比 Faster R-CNN 快。<span style="color:red;">什么意思？基于区域的特征图怎么与 ROI 是独立的？</span>


<center>

![](http://images.iterate.site/blog/image/20190722/hRE7C15tU9IP.png?imageslim){ width=45% }

</center>

> 人脸检测

现在我们来看一下 5×5 的特征图 $M$，内部包含一个蓝色方块。我们将方块平均分成 3×3 个区域。现在，我们在 $M$ 中创建了一个新的特征图，来检测方块的左上角（TL）。这个新的特征图如下图（右）所示。只有黄色的网格单元  $[2,2]$ 处于激活状态。在左侧创建一个新的特征图，用于检测目标的左上角。

<center>

![](http://images.iterate.site/blog/image/20190722/ycL6o8F57OV5.png?imageslim){ width=55% }

</center>


> 检测示例

我们将方块分成 9 个部分，由此创建了 9 个特征图，每个用来检测对应的目标区域。这些特征图叫做位置敏感得分图（position-sensitive score map），因为每个图检测目标的子区域（计算其得分）。

<center>

![](http://images.iterate.site/blog/image/20190722/e5YbVzt55tsw.png?imageslim){ width=65% }

</center>


> 生成 9 个得分图

下图中红色虚线矩形是建议的 ROI。我们将其分割成 3×3 个区域，并询问每个区域包含目标对应部分的概率是多少。例如，左上角 ROI 区域包含左眼的概率。我们将结果存储成 3×3 vote 数组，如下图（右）所示。例如，`vote_array[0][0]` 包含左上角区域是否包含目标对应部分的得分。

<center>

![](http://images.iterate.site/blog/image/20190722/hCicaNkNKNHr.png?imageslim){ width=65% }

</center>


将 ROI 应用到特征图上，输出一个 3x3 数组。将得分图和 ROI 映射到 vote 数组的过程叫做位置敏感 ROI 池化（position-sensitive ROI-pool）。该过程与前面讨论过的 ROI 池化非常接近。

<center>

![](http://images.iterate.site/blog/image/20190722/6J1AqQHDhip3.png?imageslim){ width=55% }

</center>


将 ROI 的一部分叠加到对应的得分图上，计算 `V[i][j]`。在计算出位置敏感 ROI 池化的所有值后，类别得分是其所有元素得分的平均值。

<center>

![](http://images.iterate.site/blog/image/20190722/XLwaA8fCpNwr.png?imageslim){ width=45% }

</center>

> ROI 池化

假如我们有 $C$ 个类别要检测。我们将其扩展为 $C+1$ 个类别，这样就为背景（非目标）增加了一个新的类别。每个类别有 3×3 个得分图，因此一共有 (C+1)×3×3 个得分图。使用每个类别的得分图可以预测出该类别的类别得分。然后我们对这些得分应用 softmax 函数，计算出每个类别的概率。以下是数据流图，在本案例中，$k=3$。

<center>

![](http://images.iterate.site/blog/image/20190722/r5L3XO4tURAi.png?imageslim){ width=75% }

</center>

<span style="color:red;">又看了一遍，感觉还有点模糊。</span>






# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
