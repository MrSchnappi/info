---
title: 3.15 SSD
toc: true
date: 2019-09-03
---

# SSD



SSD

SSD对 YOLO 进行了改进，达到了和两阶段方法相当的精度，同时又保持了较快的运行速度。SSD也采用了网格划分的思想，和 Faster RCNN不同的是它将所有的操作整合在一个卷积网络中完成。为了检测不同尺度的目标，SSD对不同卷积层的特征图像进行滑窗扫描；在前面的卷积层输出的特征图像中检测小的目标，在后面的卷积层输出的特征图像中检测大的目标。它的主要特点是：



- 基于多尺度特征图像的检测：在多个尺度的卷积特征图上进行预测，以检测不同大小的目标，一定程度上提升了小目标物体的检测精度。
- 借鉴了 Faster R-CNN中的 Anchor boxes思想，在不同尺度的特征图上采样候选区域，一定程度上提升了检测的召回率以及小目标的检测效果。下图是 SSD 的原理：



![mark](http://images.iterate.site/blog/image/20190905/2O65Midy8411.png?imageslim)



<span style="color:red;">没看懂。重新补充下。</span>

## SSD 的创新

1. 基于 Faster R-CNN 中的 Anchor，提出了相似的先验框（Prior box）
2. 从不同比例的特征图（多尺度特征）中产生不同比例的预测，并明确地按长宽比分离预测。

不同于前面的 R-CNN 系列，SSD 属于 one-stage 方法。SSD 使用 VGG16 网络作为特征提取器（和 Faster R-CNN 中使用的 CNN 一样），将后面的全连接层替换成卷积层，并在之后添加自定义卷积层，并在最后直接采用卷积进行检测。在多个特征图上设置不同缩放比例和不同宽高比的先验框以融合多尺度特征图进行检测，靠前的大尺度特征图可以捕捉到小物体的信息，而靠后的小尺度特征图能捕捉到大物体的信息，从而提高检测的准确性和定位的准确性。

如下图是 SSD 的网络结构图。

<center>

![](http://images.iterate.site/blog/image/20190722/uDdyNuq6P2hN.png?imageslim){ width=85% }

</center>


**1. 怎样设置 default boxes？**

SSD 中 default box 的概念有点类似于 Faster R-CNN 中的 anchor。不同于 Faster R-CNN 只在最后一个特征层取 anchor, SSD 在多个特征层上取 default box，可以得到不同尺度的 default box。在特征图的每个单元上取不同宽高比的 default box，一般宽高比在{1,2,3,1/2,1/3}中选取，有时还会额外增加一个宽高比为 1 但具有特殊尺度的 box。如下图所示，在 8x8 的 feature map 和 4x4 的 feature map 上的每个单元取 4 个不同的 default box。原文对于 300x300 的输入，分别在 conv4_3, conv7,conv8_2,conv9_2,conv10_2,conv11_2 的特征图上的每个单元取 4,6,6,6,4,4 个 default box. 由于以上特征图的大小分别是 38x38,19x19,10x10,5x5,3x3,1x1，所以一共得到 38x38x4+19x19x6+10x10x6+5x5x6+
3x3x4+1x1x4=8732 个 default box。<span style="color:red;">为什么是 38x38x4 ？</span>对一张 300x300 的图片输入网络将会针对这 8732 个 default box 预测 8732 个边界框。

<center>

![](http://images.iterate.site/blog/image/20190722/EtXdcikNOOBA.png?imageslim){ width=75% }

</center>


**2. 怎样对先验框进行匹配？**

SSD 在训练的时候只需要输入图像和图像中每个目标对应的 ground truth。先验框与 ground truth 的匹配遵循两个原则：

1. 对图片中的每个 ground truth, 在先验框中找到与其 IOU 最大的先验框，则该先验框对应的预测边界框与 ground truth 匹配。
2. 对于 1 中每个剩下的没有与任何 ground truth 匹配到的先验框，找到与其 IOU 最大的 ground truth，若其与该 ground truth 的 IOU 值大于某个阈值（一般设为 0.5），则该先验框对应的预测边界框与该 ground truth 匹配。

按照这两个原则进行匹配，匹配到 ground truth 的先验框对应的预测边界框作为正样本，没有匹配到 ground truth 的先验框对应的预测边界框作为负样本。尽管一个 ground truth 可以与多个先验框匹配，但是 ground truth 的数量相对先验框还是很少，按照上面的原则进行匹配还是会造成负样本远多于正样本的情况。为了使正负样本尽量均衡（一般保证正负样本比例约为 1：3），SSD 采用 hard negative mining，即对负样本按照其预测背景类的置信度进行降序排列，选取置信度较小的 top-k 作为训练的负样本。

**3. 怎样得到预测的检测结果？**

最后分别在所选的特征层上使用 3x3 卷积核预测不同 default boxes 所属的类别分数及其预测的边界框 location。由于对于每个 box 需要预测该 box 属于每个类别的置信度（假设有 c 类，包括背景，例如 20class 的数据集合，c=21）和该 box 对应的预测边界框的 location(包含 4 个值，即该 box 的中心坐标和宽高)，则每个 box 需要预测 c+4 个值。所以对于某个所选的特征层，该层的卷积核个数为（c+4）x 该层的 default box 个数。最后将每个层得到的卷积结果进行拼接。对于得到的每个预测框，取其类别置信度的最大值，若该最大值大于置信度阈值，则最大值所对应的类别即为该预测框的类别，否则过滤掉此框。对于保留的预测框根据它对应的先验框进行解码得到其真实的位置参数（这里还需注意要防止预测框位置超出图片），然后根据所属类别置信度进行降序排列，取 top-k 个预测框，最后进行 NMS，过滤掉重叠度较大的预测框，最后得到检测结果。

SSD 优势是速度比较快，整个过程只需要一步，首先在图片不同位置按照不同尺度和宽高比进行密集抽样，然后利用 CNN 提取特征后直接进行分类与回归，所以速度比较快，但均匀密集采样会造成正负样本不均衡的情况使得训练比较困难，导致模型准确度有所降低。另外，SSD 对小目标的检测没有大目标好，因为随着网络的加深，在高层特征图中小目标的信息丢失掉了，适当增大输入图片的尺寸可以提升小目标的检测效果。




# SSD关键源码解析


SSD（

SSD: Single Shot MultiBox Detector

）是采用单个深度神经网络模型实现目标检测和识别的方法。如图0-1所示，该方法是综合了Faster R-CNN的anchor box和YOLO单个神经网络检测思路（YOLOv2也采用了类似的思路，详见

YOLO升级版：YOLOv2和YOLO9000解析

），既有Faster R-CNN的准确率又有YOLO的检测速度，可以实现高准确率实时检测。在300*300分辨率，SSD在VOC2007数据集上准确率为74.3%mAP，59FPS；512*512分辨率，SSD获得了超过Fast R-CNN，获得了80%mAP/19fps的结果，如图0-2所示。SSD关键点分为两类：模型结构和训练方法。模型结构包括：多尺度特征图检测网络结构和anchor boxes生成；训练方法包括：ground truth预处理和损失函数。本文解析的是SSD的tensorflow实现源码，来源

balancap

/

SSD-Tensorflow

。本文结构如下：

1，多尺度特征图检测网络结构；

2，anchor boxes生成；

3，ground truth预处理；

4，目标函数；

5，总结

![img](https://pic2.zhimg.com/80/v2-d0252b7d1408105470b88ceb45054725_hd.png)

图0-1 SSD与MultiBox，Faster R-CNN，YOLO原理（此图来源于作者在eccv2016的PPT）

![img](https://pic2.zhimg.com/80/v2-0213e22e8b0d96f8854e82d796c83a71_hd.png)

图0-2 SSD检测速度与精确度。（此图来源于作者在eccv2016的PPT）

**1 多尺度特征图检测网络结构**

SSD的网络模型如图1-1所示。

![img](https://pic1.zhimg.com/80/v2-7f7f3c99d20df97455e8bcfce7876d30_hd.png)



图1-1 SSD模型结构。（此图来源于原论文）

模型建立源代码包含于ssd_vgg_300.py中。模型多尺度特征图检测如图1-2所示。模型选择的特征图包括：38×38（block4）,19×19（block7），10×10（block8），5×5（block9），3×3（block10），1×1（block11）。对于每张特征图，生成采用3×3卷积生成 默认框的四个偏移位置和21个类别的置信度。比如block7，默认框（def boxes）数目为6，每个默认框包含4个偏移位置和21个类别置信度（4+21）。因此，block7的最后输出为(19*19)*6*(4+21)。

![img](https://pic1.zhimg.com/80/v2-5964f6dff6dbbd435336cde9e5dfc988_hd.png)

图1-2 多尺度特征采样（此图来源：[知乎专栏](https://zhuanlan.zhihu.com/p/24954433)）

其中，初始化参数如下：

```py
"""
Implementation of the SSD VGG-based 300 network.

The default features layers with 300x300 image input are:
  conv4 ==> 38 x 38
  conv7 ==> 19 x 19
  conv8 ==> 10 x 10
  conv9 ==> 5 x 5
  conv10 ==> 3 x 3
  conv11 ==> 1 x 1
The default image size used to train this network is 300x300.
"""
default_params = SSDParams(
    img_shape=(300, 300),#输入尺寸
    num_classes=21,#预测类别20+1=21（20类加背景）
    #获取feature map层
    feat_layers=['block4', 'block7', 'block8', 'block9', 'block10', 'block11'],
    feat_shapes=[(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)],

    anchor_size_bounds=[0.15, 0.90],
    #anchor boxes的大小
    anchor_sizes=[(21., 45.),
                  (45., 99.),
                  (99., 153.),
                  (153., 207.),
                  (207., 261.),
                  (261., 315.)],
    #anchor boxes的aspect ratios
    anchor_ratios=[[2, .5],
                   [2, .5, 3, 1./3],
                   [2, .5, 3, 1./3],
                   [2, .5, 3, 1./3],
                   [2, .5],
                   [2, .5]],
    anchor_steps=[8, 16, 32, 64, 100, 300],#anchor的层
    anchor_offset=0.5,#补偿阀值0.5
    normalizations=[20, -1, -1, -1, -1, -1],#该特征层是否正则，大于零即正则；小于零则否
    prior_scaling=[0.1, 0.1, 0.2, 0.2]
    )
```

建立模型代码如下，作者采用了[TensorFlow-Slim](https://link.zhihu.com/?target=https%3A//github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)（类似于keras的高层库）来建立网络模型，详细内容可以参考[TensorFlow-Slim](https://link.zhihu.com/?target=https%3A//github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)网页。

```py
#建立ssd网络函数
def ssd_net(inputs,
            num_classes=21,
            feat_layers=SSDNet.default_params.feat_layers,
            anchor_sizes=SSDNet.default_params.anchor_sizes,
            anchor_ratios=SSDNet.default_params.anchor_ratios,
            normalizations=SSDNet.default_params.normalizations,
            is_training=True,
            dropout_keep_prob=0.5,
            prediction_fn=slim.softmax,
            reuse=None,
            scope='ssd_300_vgg'):
    """SSD net definition.
    """
    # End_points collect relevant activations for external use.
    #用于收集每一层输出结果
    end_points = {}
    #采用slim建立vgg网络,网络结构参考文章内的结构图
    with tf.variable_scope(scope, 'ssd_300_vgg', [inputs], reuse=reuse):
        # Original VGG-16 blocks.
        net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')
        end_points['block1'] = net
        net = slim.max_pool2d(net, [2, 2], scope='pool1')
        # Block 2.
        net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')
        end_points['block2'] = net
        net = slim.max_pool2d(net, [2, 2], scope='pool2')
        # Block 3.
        net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')
        end_points['block3'] = net
        net = slim.max_pool2d(net, [2, 2], scope='pool3')
        # Block 4.
        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')
        end_points['block4'] = net
        net = slim.max_pool2d(net, [2, 2], scope='pool4')
        # Block 5.
        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')
        end_points['block5'] = net
        net = slim.max_pool2d(net, [3, 3], 1, scope='pool5')#max pool

        #外加的SSD层
        # Additional SSD blocks.
        # Block 6: let's dilate the hell out of it!
        #输出shape为19×19×1024
        net = slim.conv2d(net, 1024, [3, 3], rate=6, scope='conv6')
        end_points['block6'] = net
        # Block 7: 1x1 conv. Because the fuck.
        #卷积核为1×1
        net = slim.conv2d(net, 1024, [1, 1], scope='conv7')
        end_points['block7'] = net

        # Block 8/9/10/11: 1x1 and 3x3 convolutions stride 2 (except lasts).
        end_point = 'block8'
        with tf.variable_scope(end_point):
            net = slim.conv2d(net, 256, [1, 1], scope='conv1x1')
            net = slim.conv2d(net, 512, [3, 3], stride=2, scope='conv3x3')
        end_points[end_point] = net
        end_point = 'block9'
        with tf.variable_scope(end_point):
            net = slim.conv2d(net, 128, [1, 1], scope='conv1x1')
            net = slim.conv2d(net, 256, [3, 3], stride=2, scope='conv3x3')
        end_points[end_point] = net
        end_point = 'block10'
        with tf.variable_scope(end_point):
            net = slim.conv2d(net, 128, [1, 1], scope='conv1x1')
            net = slim.conv2d(net, 256, [3, 3], scope='conv3x3', padding='VALID')
        end_points[end_point] = net
        end_point = 'block11'
        with tf.variable_scope(end_point):
            net = slim.conv2d(net, 128, [1, 1], scope='conv1x1')
            net = slim.conv2d(net, 256, [3, 3], scope='conv3x3', padding='VALID')
        end_points[end_point] = net

        # Prediction and localisations layers.
        #预测和定位
        predictions = []
        logits = []
        localisations = []
        for i, layer in enumerate(feat_layers):
            with tf.variable_scope(layer + '_box'):
                #接受特征层的输出，生成类别和位置预测
                p, l = ssd_multibox_layer(end_points[layer],
                                          num_classes,
                                          anchor_sizes[i],
                                          anchor_ratios[i],
                                          normalizations[i])
            #把每一层的预测收集
            predictions.append(prediction_fn(p))#prediction_fn为softmax，预测类别
            logits.append(p)#概率
            localisations.append(l)#预测位置信息

        return predictions, localisations, logits, end_points
```

**2 anchor box生成**

对每一张特征图，按照不同的大小（scale） 和长宽比（ratio） 生成生成k个默认框（default boxes），原理图如图2-1所示(此图中，默认框数目k=6，其中5×5的红色点代表特征图，因此：5*5*6 = 150 个boxes)。

每个默认框大小计算公式为：![[公式]](https://www.zhihu.com/equation?tex=s_%7Bk%7D%3Ds_%7Bmin%7D+%2B%5Cfrac%7Bs_%7Bmax%7D-s_%7Bmin%7D++%7D%7Bm-1%7D%28k-1%29%2Ck%5Cin+%5B1%2Cm%5D+)，其中，m为特征图数目，![[公式]](https://www.zhihu.com/equation?tex=s_%7Bmin%7D+)为最底层特征图大小（原论文中值为0.2，代码中为0.15），![[公式]](https://www.zhihu.com/equation?tex=s_%7Bmax%7D+)为最顶层特征图默认框大小（原论文中为0.9,代码中为0.9）。

每个默认框长宽比根据比例值计算，原论文中比例值为![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+1%2C2%2C3%2C1%2F2%2C1%2F3+%5Cright%5C%7D+)，因此，每个默认框的宽为![[公式]](https://www.zhihu.com/equation?tex=w_%7Bk%7D%5E%7Ba%7D+%3Ds_%7Bk%7D%5Csqrt%7Ba_%7Br%7D+%7D++)，高为![[公式]](https://www.zhihu.com/equation?tex=h_%7Bk%7D%5E%7Ba%7D+%3Ds_%7Bk%7D%2F%5Csqrt%7Ba_%7Br%7D+%7D++)。对于比例为1的默认框，额外添加一个比例为![[公式]](https://www.zhihu.com/equation?tex=s_%7Bk%7D%5E%7B%27%7D+%3D%5Csqrt%7Bs_%7Bk%7Ds_%7Bk%2B1%7D%7D+)的默认框。最终，每张特征图中的每个点生成6个默认框。每个默认框中心设定为![[公式]](https://www.zhihu.com/equation?tex=%28%5Cfrac%7Bi%2B0.5%7D%7B%7Cf_%7Bk%7D+%7C%7D%2C%5Cfrac%7Bj%2B0.5%7D%7B%7Cf_%7Bk%7D+%7C%7D+%29),其中，![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%7C+f_%7Bk%7D+%5Cright%7C+)为第k个特征图尺寸。

![img](https://pic4.zhimg.com/80/v2-e128c01e26456fa24502e2c05bf46e1b_hd.png)

![img](https://pic3.zhimg.com/80/v2-e6f0dd799661fff724853435b976a82e_hd.png)

![img](https://pic3.zhimg.com/80/v2-64a521f37e62fe79c9b5d11746eb6686_hd.png)

图2-1 anchor box生成示意图（此图来源于[知乎专栏](https://zhuanlan.zhihu.com/p/24954433)）

源代码中，默认框生成函数为ssd_anchor_one_layer()，代码如下：

```python
#生成一层的anchor boxes
def ssd_anchor_one_layer(img_shape,#原始图像shape
                         feat_shape,#特征图shape
                         sizes,#预设的box size
                         ratios,#aspect 比例
                         step,#anchor的层
                         offset=0.5,
                         dtype=np.float32):
    """Computer SSD default anchor boxes for one feature layer.

    Determine the relative position grid of the centers, and the relative
    width and height.

    Arguments:
      feat_shape: Feature shape, used for computing relative position grids;
      size: Absolute reference sizes;
      ratios: Ratios to use on these features;
      img_shape: Image shape, used for computing height, width relatively to the
        former;
      offset: Grid offset.

    Return:
      y, x, h, w: Relative x and y grids, and height and width.
    """
    # Compute the position grid: simple way.
    # y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]
    # y = (y.astype(dtype) + offset) / feat_shape[0]
    # x = (x.astype(dtype) + offset) / feat_shape[1]
    # Weird SSD-Caffe computation using steps values...

    """
    #测试中，参数如下
    feat_shapes=[(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)]
    anchor_sizes=[(21., 45.),
                      (45., 99.),
                      (99., 153.),
                      (153., 207.),
                      (207., 261.),
                      (261., 315.)]
    anchor_ratios=[[2, .5],
                       [2, .5, 3, 1./3],
                       [2, .5, 3, 1./3],
                       [2, .5, 3, 1./3],
                       [2, .5],
                       [2, .5]]
    anchor_steps=[8, 16, 32, 64, 100, 300]


    offset=0.5

    dtype=np.float32

    feat_shape=feat_shapes[0]
    step=anchor_steps[0]
    """
    #测试中，y和x的shape为（38,38）（38,38）
    #y的值为
    #array([[ 0,  0,  0, ...,  0,  0,  0],
     #  [ 1,  1,  1, ...,  1,  1,  1],
    # [ 2,  2,  2, ...,  2,  2,  2],
    #   ...,
     #  [35, 35, 35, ..., 35, 35, 35],
    #  [36, 36, 36, ..., 36, 36, 36],
     #  [37, 37, 37, ..., 37, 37, 37]])
    y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]
    #测试中y=(y+0.5)×8/300,x=(x+0.5)×8/300
    y = (y.astype(dtype) + offset) * step / img_shape[0]
    x = (x.astype(dtype) + offset) * step / img_shape[1]

    #扩展维度，维度为（38,38,1）
    # Expand dims to support easy broadcasting.
    y = np.expand_dims(y, axis=-1)
    x = np.expand_dims(x, axis=-1)

    # Compute relative height and width.
    # Tries to follow the original implementation of SSD for the order.
    #数值为2+2
    num_anchors = len(sizes) + len(ratios)
    #shape为（4,）
    h = np.zeros((num_anchors, ), dtype=dtype)
    w = np.zeros((num_anchors, ), dtype=dtype)
    # Add first anchor boxes with ratio=1.
    #测试中，h[0]=21/300,w[0]=21/300?
    h[0] = sizes[0] / img_shape[0]
    w[0] = sizes[0] / img_shape[1]
    di = 1
    if len(sizes) > 1:
        #h[1]=sqrt(21*45)/300
        h[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[0]
        w[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[1]
        di += 1
    for i, r in enumerate(ratios):
        h[i+di] = sizes[0] / img_shape[0] / math.sqrt(r)
        w[i+di] = sizes[0] / img_shape[1] * math.sqrt(r)
    #测试中，y和x shape为（38,38,1）
    #h和w的shape为（4,）
    return y, x, h, w
```

**3 ground truth预处理**

训练过程中，首先需要将label信息（ground truth box，ground truth category）进行预处理，将其对应到相应的默认框上。根据默认框和ground truth box的jaccard 重叠来寻找对应的默认框。文章中选取了jaccard重叠超过0.5的默认框为正样本，其它为负样本。

源代码ground truth预处理代码位于ssd_common.py文件中，关键代码如下：

```python
#label和bbox编码函数
def tf_ssd_bboxes_encode_layer(labels,#ground truth标签，1D tensor
                               bboxes,#N×4 Tensor（float）
                               anchors_layer,#anchors，为list
                               matching_threshold=0.5,#阀值
                               prior_scaling=[0.1, 0.1, 0.2, 0.2],#缩放
                               dtype=tf.float32):
    """Encode groundtruth labels and bounding boxes using SSD anchors from
    one layer.

    Arguments:
      labels: 1D Tensor(int64) containing groundtruth labels;
      bboxes: Nx4 Tensor(float) with bboxes relative coordinates;
      anchors_layer: Numpy array with layer anchors;
      matching_threshold: Threshold for positive match with groundtruth bboxes;
      prior_scaling: Scaling of encoded coordinates.

    Return:
      (target_labels, target_localizations, target_scores): Target Tensors.
    """
    # Anchors coordinates and volume.
    #获取anchors层
    yref, xref, href, wref = anchors_layer
    ymin = yref - href / 2.
    xmin = xref - wref / 2.
    ymax = yref + href / 2.
    xmax = xref + wref / 2.
    #xmax的shape为((38, 38, 1), (38, 38, 1), (4,), (4,))
(38, 38, 4)
    #体积
    vol_anchors = (xmax - xmin) * (ymax - ymin)

    # Initialize tensors...
    shape = (yref.shape[0], yref.shape[1], href.size)
    feat_labels = tf.zeros(shape, dtype=tf.int64)
    feat_scores = tf.zeros(shape, dtype=dtype)
    #shape为（38,38,4）
    feat_ymin = tf.zeros(shape, dtype=dtype)
    feat_xmin = tf.zeros(shape, dtype=dtype)
    feat_ymax = tf.ones(shape, dtype=dtype)
    feat_xmax = tf.ones(shape, dtype=dtype)

    #计算jaccard重合
    def jaccard_with_anchors(bbox):
        """Compute jaccard score a box and the anchors.
        """
        # Intersection bbox and volume.
        int_ymin = tf.maximum(ymin, bbox[0])
        int_xmin = tf.maximum(xmin, bbox[1])
        int_ymax = tf.minimum(ymax, bbox[2])
        int_xmax = tf.minimum(xmax, bbox[3])
        h = tf.maximum(int_ymax - int_ymin, 0.)
        w = tf.maximum(int_xmax - int_xmin, 0.)

        # Volumes.
        inter_vol = h * w
        union_vol = vol_anchors - inter_vol \
            + (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
        jaccard = tf.div(inter_vol, union_vol)
        return jaccard
    #条件函数
    def condition(i, feat_labels, feat_scores,
                  feat_ymin, feat_xmin, feat_ymax, feat_xmax):
        """Condition: check label index.
        """
        #tf.less函数 Returns the truth value of (x < y) element-wise.
        r = tf.less(i, tf.shape(labels))
        return r[0]
    #主体
    def body(i, feat_labels, feat_scores,
             feat_ymin, feat_xmin, feat_ymax, feat_xmax):
        """Body: update feature labels, scores and bboxes.
        Follow the original SSD paper for that purpose:
          - assign values when jaccard > 0.5;
          - only update if beat the score of other bboxes.
        """
        # Jaccard score.
        label = labels[i]
        bbox = bboxes[i]
        scores = jaccard_with_anchors(bbox)#计算jaccard重合值

        # 'Boolean' mask.
        #tf.greater函数返回大于的布尔值
        mask = tf.logical_and(tf.greater(scores, matching_threshold),
                              tf.greater(scores, feat_scores))
        imask = tf.cast(mask, tf.int64)
        fmask = tf.cast(mask, dtype)
        # Update values using mask.
        feat_labels = imask * label + (1 - imask) * feat_labels
        feat_scores = tf.select(mask, scores, feat_scores)

        feat_ymin = fmask * bbox[0] + (1 - fmask) * feat_ymin
        feat_xmin = fmask * bbox[1] + (1 - fmask) * feat_xmin
        feat_ymax = fmask * bbox[2] + (1 - fmask) * feat_ymax
        feat_xmax = fmask * bbox[3] + (1 - fmask) * feat_xmax
        return [i+1, feat_labels, feat_scores,
                feat_ymin, feat_xmin, feat_ymax, feat_xmax]
    # Main loop definition.
    i = 0
    [i, feat_labels, feat_scores,
     feat_ymin, feat_xmin,
     feat_ymax, feat_xmax] = tf.while_loop(condition, body,
                                           [i, feat_labels, feat_scores,
                                            feat_ymin, feat_xmin,
                                            feat_ymax, feat_xmax])

    # Transform to center / size.
    #计算补偿后的中心
    feat_cy = (feat_ymax + feat_ymin) / 2.
    feat_cx = (feat_xmax + feat_xmin) / 2.
    feat_h = feat_ymax - feat_ymin
    feat_w = feat_xmax - feat_xmin
    # Encode features.
    feat_cy = (feat_cy - yref) / href / prior_scaling[0]
    feat_cx = (feat_cx - xref) / wref / prior_scaling[1]
    feat_h = tf.log(feat_h / href) / prior_scaling[2]
    feat_w = tf.log(feat_w / wref) / prior_scaling[3]
    # Use SSD ordering: x / y / w / h instead of ours.
    feat_localizations = tf.stack([feat_cx, feat_cy, feat_w, feat_h], axis=-1)
    return feat_labels, feat_localizations, feat_scores


#ground truth编码函数
def tf_ssd_bboxes_encode(labels,#ground truth标签，1D tensor
                         bboxes,#N×4 Tensor（float）
                         anchors,#anchors，为list
                         matching_threshold=0.5,#阀值
                         prior_scaling=[0.1, 0.1, 0.2, 0.2],#缩放
                         dtype=tf.float32,
                         scope='ssd_bboxes_encode'):
    """Encode groundtruth labels and bounding boxes using SSD net anchors.
    Encoding boxes for all feature layers.

    Arguments:
      labels: 1D Tensor(int64) containing groundtruth labels;
      bboxes: Nx4 Tensor(float) with bboxes relative coordinates;
      anchors: List of Numpy array with layer anchors;
      matching_threshold: Threshold for positive match with groundtruth bboxes;
      prior_scaling: Scaling of encoded coordinates.

    Return:
      (target_labels, target_localizations, target_scores):
        Each element is a list of target Tensors.
    """
    with tf.name_scope(scope):
        target_labels = []
        target_localizations = []
        target_scores = []
        for i, anchors_layer in enumerate(anchors):
            with tf.name_scope('bboxes_encode_block_%i' % i):
                #将label和bbox进行编码
                t_labels, t_loc, t_scores = \
                    tf_ssd_bboxes_encode_layer(labels, bboxes, anchors_layer,
                                               matching_threshold, prior_scaling, dtype)
                target_labels.append(t_labels)
                target_localizations.append(t_loc)
                target_scores.append(t_scores)
        return target_labels, target_localizations, target_scores


#编码goundtruth的label和bbox
    def bboxes_encode(self, labels, bboxes, anchors,
                      scope='ssd_bboxes_encode'):
        """Encode labels and bounding boxes.
        """
        return ssd_common.tf_ssd_bboxes_encode(
            labels, bboxes, anchors,
            matching_threshold=0.5,
            prior_scaling=self.params.prior_scaling,
            scope=scope)
```

**4 目标函数**

SSD目标函数分为两个部分：对应默认框的位置loss（loc）和类别置信度loss（conf）。定义![[公式]](https://www.zhihu.com/equation?tex=x_%7Bij%7D%5E%7Bp%7D%3D%5Cleft%5C%7B+1%2C0+%5Cright%5C%7D++) 为第i个默认框和对应的第j个ground truth box，相应的类别为p。目标函数定义为：



其中，N为匹配的默认框。如果N=0，loss为零。![[公式]](https://www.zhihu.com/equation?tex=L_%7Bconf%7D+)为预测框![[公式]](https://www.zhihu.com/equation?tex=l)和ground truth box ![[公式]](https://www.zhihu.com/equation?tex=g)的Smooth L1 loss，![[公式]](https://www.zhihu.com/equation?tex=%5Calpha+)值通过cross validation设置为1。



![img](https://pic2.zhimg.com/80/v2-f7f9cd187a7e4cf8fb2c430a844bdc5d_hd.png)



定义如下：

![img](https://pic1.zhimg.com/80/v2-c59028fcd350680c60002216cac34434_hd.png)

其中，



为预测框，



为ground truth。



为补偿（regress to offsets）后的默认框（



）的中心，



为默认框的宽和高。



![[公式]](https://www.zhihu.com/equation?tex=L_%7Bconf%7D+)定义为多累别softmax loss，公式如下：



![img](https://pic3.zhimg.com/80/v2-b5772e77cfe447103133b90c05a807ee_hd.png)

目标函数定义源码位于ssd_vgg_300.py，注释如下：



```python
# =========================================================================== #
# SSD loss function.
# =========================================================================== #
def ssd_losses(logits, #预测类别
               localisations,#预测位置
               gclasses, #ground truth 类别
               glocalisations, #ground truth 位置
               gscores,#ground truth 分数
               match_threshold=0.5,
               negative_ratio=3.,
               alpha=1.,
               label_smoothing=0.,
               scope='ssd_losses'):
    """Loss functions for training the SSD 300 VGG network.

    This function defines the different loss components of the SSD, and
    adds them to the TF loss collection.

    Arguments:
      logits: (list of) predictions logits Tensors;
      localisations: (list of) localisations Tensors;
      gclasses: (list of) groundtruth labels Tensors;
      glocalisations: (list of) groundtruth localisations Tensors;
      gscores: (list of) groundtruth score Tensors;
    """
    # Some debugging...
    # for i in range(len(gclasses)):
    #     print(localisations[i].get_shape())
    #     print(logits[i].get_shape())
    #     print(gclasses[i].get_shape())
    #     print(glocalisations[i].get_shape())
    #     print()
    with tf.name_scope(scope):
        l_cross = []
        l_loc = []
        for i in range(len(logits)):
            with tf.name_scope('block_%i' % i):
                # Determine weights Tensor.
                pmask = tf.cast(gclasses[i] > 0, logits[i].dtype)
                n_positives = tf.reduce_sum(pmask)#正样本数目

                #np.prod函数Return the product of array elements over a given axis
                n_entries = np.prod(gclasses[i].get_shape().as_list())
                # r_positive = n_positives / n_entries
                # Select some random negative entries.
                r_negative = negative_ratio * n_positives / (n_entries - n_positives)#负样本数
                nmask = tf.random_uniform(gclasses[i].get_shape(),
                                          dtype=logits[i].dtype)
                nmask = nmask * (1. - pmask)
                nmask = tf.cast(nmask > 1. - r_negative, logits[i].dtype)

                #cross_entropy loss
                # Add cross-entropy loss.
                with tf.name_scope('cross_entropy'):
                    # Weights Tensor: positive mask + random negative.
                    weights = pmask + nmask
                    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits[i],
                                                                          gclasses[i])
                    loss = tf.contrib.losses.compute_weighted_loss(loss, weights)
                    l_cross.append(loss)

                #smooth loss
                # Add localization loss: smooth L1, L2, ...
                with tf.name_scope('localization'):
                    # Weights Tensor: positive mask + random negative.
                    weights = alpha * pmask
                    loss = custom_layers.abs_smooth(localisations[i] - glocalisations[i])
                    loss = tf.contrib.losses.compute_weighted_loss(loss, weights)
                    l_loc.append(loss)

        # Total losses in summaries...
        with tf.name_scope('total'):
            tf.summary.scalar('cross_entropy', tf.add_n(l_cross))
            tf.summary.scalar('localization', tf.add_n(l_loc))
```

**5 总结**

本文对[SSD: Single Shot MultiBox Detector](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1512.02325)的tensorflow的关键源代码进行了解析。本文采用的源码来自于[balancap](https://link.zhihu.com/?target=https%3A//github.com/balancap)/[SSD-Tensorflow](https://link.zhihu.com/?target=https%3A//github.com/balancap/SSD-Tensorflow)。源码作者写得非常详细，内容较多（其它还包括了图像预处理，多GPU并行训练等许多内容），因此只选取了关键代码进行解析。在看完论文后，再结合关键代码分析，结构就很清晰了。SSD代码实现的关键点为：1，多尺度特征图检测网络结构；2，anchor boxes生成；3，ground truth预处理；4，目标函数。SSD和YOLOv2类似，可以实现高准确率下的实时目标检测，是非常值得研究和改进的目标检测方法。











# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
- [SSD关键源码解析](https://zhuanlan.zhihu.com/p/25100992)
