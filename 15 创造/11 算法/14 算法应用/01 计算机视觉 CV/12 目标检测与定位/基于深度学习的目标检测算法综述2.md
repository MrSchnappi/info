---
title: 基于深度学习的目标检测算法综述2
toc: true
date: 2019-11-17
---
**基于深度学习的目标检测算法综述**分为三部分：

**1. Two/One stage算法改进。**这部分将主要总结在two/one stage经典网络上改进的系列论文，包括Faster R-CNN、YOLO、SSD等经典论文的升级版本。

**2. 解决方案。**这部分我们归纳总结了目标检测的常见问题和近期论文提出的解决方案。

**3. 扩展应用、综述。**这部分我们会介绍检测算法的扩展和其他综述类论文。



上篇文章（[基于深度学习的目标检测算法综述（一）](https://zhuanlan.zhihu.com/p/40047760)）介绍了**1. Two/One stage算法改进**。本篇文章将详细介绍**2. 解决方案，**这部分我们归纳总结了目标检测的常见问题和近期论文提出的解决方案。

![img](https://pic4.zhimg.com/80/v2-99ee5e628fb2efd2873d6dcb7e36326b_hd.jpg)

## **2. 解决方案**

本部分针对小物体检测、不规则形状物体检测、检测算法中正负样本不均衡问题、物体被遮挡、检测算法的mini-batch过小、物体之间的关联信息被忽略等问题提出了解决方案。在这个部分最后本篇综述将介绍四篇改进网络结构以提升检测效果（包括准确率和速度）的论文，如改进基础网络以提升准确率的DetNet和针对移动端优化速度的Pelee网络等。

## **2.1 小物体检测**

## **2.1.1 Feature Pyramid Networks for Object Detection**

**论文链接：**[https://arxiv.org/abs/1612.03144](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1612.03144)

**开源代码：**[https://github.com/unsky/FPN](https://link.zhihu.com/?target=https%3A//github.com/unsky/FPN)

**录用信息：**CVPR2017

**论文目标：**

引入Top-Down 结构提升小物体检测效果。

**核心思想：**

Feature Pyramid Networks (FPN) 是比较早提出利用多尺度特征和Top-Down结构做目标检测的网络结构之一，虽然论文中整个网络是基于Faster R-CNN 检测算法构建，但其整体思想可以广泛适用于目前常见的大部分目标检测算法甚至分类等的其他任务中。

整体来讲， FPN 解决的问题如下：只用网络高层特征去做检测，虽然语义信息比较丰富，但是经过层层pooling等操作，特征丢失太多细节信息，对于小目标检测这些信息往往是比较重要的。所以，作者想要将语义信息充分的高层特征映射回分辨率较大、细节信息充分的底层特征，将二者以合适的方式融合来提升小目标的检测效果。

![img](https://pic2.zhimg.com/80/v2-be68faff320a952f551bfd8773a224bd_hd.jpg)特征利用方法

上图中作者首先介绍了四种常见的特征利用的方式，这里我们一一说明。图中a部分展示了利用图像特征金字塔的方式做预测的方法，即通过将预测图片变换为不同尺寸的图片输入网络，得到不同尺寸的特征进行预测。这种方法行之有效，可以提升对各个尺寸目标的检测效果。

![img](https://pic2.zhimg.com/80/v2-3fca1cd40e98b3d950337313fe564bc5_hd.jpg)

上图是imageNet 数据集中各个物体的尺寸分布，可以看到大部分物体尺寸集中在40-140像素之间。当我们采用imageNet 的pretrain参数初始化我们的基础网络时，网络实际上对40-140像素间的物体是较为敏感的。所以当物体目标过大过小的时候效果都会打折扣。而图像多尺度金字塔预测的方法也解决了这一问题。这个方法虽然行之有效但是缺点是效率低下。尤其是应用于类似于Faster R-CNN 这样的网络，迫于速度和显存的压力，端对端的训练难以实现。通常这种方法只应用于网络预测的阶段。

图中b部分展示的方法为利用单一尺度图片的最高层信息进行预测的方法，是平时最广泛被使用的一种方法，简单高效。但是缺点是由于尺度单一，应对多尺度的目标效果受限。图中c部分展示了利用特征金字塔来做预测的方法。即采用不同尺寸不同深度的特征层分别进行预测，每层的感受野和特征信息的丰富程度都不一样，对不同尺寸的目标响应也有所区别。其中高层特征更适合用于检测大目标，而低层特征细节信息更加丰富，感受野也偏小，更适合用于检测小目标。我们经常使用的SSD检测算法即使用了这种思路。该方法的缺点是低层的特征信息因为层数较浅，语义信息不太丰富，所以小目标的检测效果仍然不尽如人意。图中d部分即为FPN的解决方案，利用Top-Down结构，融合了高层和底层的特征信息，使得底层的语义信息仍然很丰富的同时保持较大的分辨率，提升小物体的检测效果。

![img](https://pic2.zhimg.com/80/v2-44b34ae0530a3f895dd9e1542ab00d21_hd.jpg)

上图展示了FPN网络的特征融合方式，其中高层特征通过最近邻差值的方式增大两倍的尺寸，而底层的特征经过一个1*1的卷积做降维操作，这两层特征分别作像素级的相加完成融合。融合之后的特征可以经过一个3*3的卷积层之后输入用来预测，也可以再重复上面的操作，和更低层的特征进行融合。

**算法效果：**

![img](https://pic3.zhimg.com/80/v2-327154788f5c0c592cc1eca75c03344e_hd.jpg)

上图展示了各个算法在COCO数据集上的对比。

## **2.1.2 Beyond Skip Connections Top Down Modulation for Object Detection**

**论文链接：**[https://arxiv.org/abs/1612.06851](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1612.06851)

**开源代码：**无

**录用信息：**CVPR2017

**论文目标：**

\1. 有效利用网络前层信息。使其既包含小物体细节信息，又包含高层抽象语义信息，提高小物体召回率。

\2. 避免直接特征叠加导致维度过高。

**核心思想：**

本文是Google 对标Facebook FPN的一个算法，用与FPN不同的方式实现了Top Down结构，主要是为了融合低层的细节特征和高层语义特征来提升小物体检测效果的一个方法。

本论文提出的Top Down modulation的结构主要关键点在于modulation这一过程，在该算法中，高底层的信息融合不是像FPN一样像素级叠加，而是通过卷积进行融合。由神经网络自主的选择选择哪些特征进行融合，实现这一“调制”过程。

一、整体网络结构：

![img](https://pic2.zhimg.com/80/v2-dffbf31d7e58497321a9c64bfdf85541_hd.jpg)整体网络结构图

首先对自下而上的CNN网络加入一个自上而下的Top Down网络，使用 lateral connections连接起来。通过这些connections 筛选合适的特征，通过Top Down 网络进行特征融合。

二、Top Down Modulation （TDM）网络基础模块：

![img](https://pic3.zhimg.com/80/v2-1e06d6716916d020898a391b96e96476_hd.jpg)

基础单元由两个部分组成：

L：a lateral module

侧向连接主要提取低层信息特征，并筛选可用特征。

T : a Top Down module

邻近层语意信息的传递，及特征融合。

算法细节以VGG16为例：

![img](https://pic4.zhimg.com/80/v2-2c5371d8aab7cae5c16b71098039350b_hd.jpg)

下文具体分析VGG16 ，Conv2层。

![img](https://pic2.zhimg.com/80/v2-dc6cc069297decd76a98e064eb8e712d_hd.jpg)

解构流程：

\1. 将上层Top Down的feature map经过3 x 3 x 64卷积，再接Relu激活函数，最后接UpSample上采样。

\2. 将bottom up对应的feature map经过3 x 3卷积，再接Relu激活函数。

\3. 将L模块的feature map 与 T模块的feature map concat。

\4. 接1 x 1 x 128卷积做特征融合。

三、训练方法：

以预训练好的原网络作为开始，论文发现逐步构建bottom-up的网络比一次性构建要好。用整体网络结构图举例说明，论文先添加（ ![[公式]](https://www.zhihu.com/equation?tex=L_%7B4%7D) , ![[公式]](https://www.zhihu.com/equation?tex=T_%7B5%2C4%7D) ），并且用 ![[公式]](https://www.zhihu.com/equation?tex=T_%7B4%7D) out获取目标检测的特征。训练完（ ![[公式]](https://www.zhihu.com/equation?tex=L_%7B4%7D) , ![[公式]](https://www.zhihu.com/equation?tex=T_%7B5%2C4%7D) ）模块后，再添加（ ![[公式]](https://www.zhihu.com/equation?tex=L_%7B3%7D) , ![[公式]](https://www.zhihu.com/equation?tex=T_%7B4%2C3%7D) ），并且用一个新的 ![[公式]](https://www.zhihu.com/equation?tex=T_%7B3%7D) out获取检测特征，以此类推。每添加一对模块，整个网络都是端到端训练的。

网络设计准则：

1. Top Down的深度通常是越深越有效，但是过于深的时候如VGG融合到conv1深度，性价比就相当低了。

\2. 侧向连接和bottom-up的连接应该减少特征维度。

\3. 网络训练需要采取一层一层增加，冻结之前层的方式。这样可以保证训练收敛稳定。

四、算法总结：

整体来讲高层和底层信息都经过卷积进行了降维操作，控制了计算量的增加，如VGG16最后一层输入RPN的特征数由512减小为256。同时特征融合是concat一起的，避免了特征叠加对信息造成的负面影响。另外需要说明一点，由于RPN网络输入特征图会变大很多倍，如果还遵循原始Faster R-CNN逐像素判断正负例的做法，网络会变得很慢，所以作者采取通过更改stride step来控制计算量保持不变。如特征图面积增大4倍，stride就设置为2。

**算法效果：**

![img](https://pic1.zhimg.com/80/v2-ef4a98af30e895cef7bc4bdf358872d0_hd.jpg)

从效果表格上看小物体识别率得到了提升，说明TDM结构发挥了作用。笔者实际使用时也发现TDM结构就速度和效果来讲，相较类似于像素叠加式的方式也要好一些。

另外，关于TDM、FPN和DSSD在Top-Down结构设计上的区别已在本综述的第一篇文章中详细阐述，这里不再重复，具体细节请查看上一篇文章的DSSD部分。



## **2.2 不规则形状物体的检测**

## **2.2.1 Deformable Convolutional Networks**

**论文链接：**[https://arxiv.org/abs/1703.06211](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.06211)

**开源代码：**[https://github.com/msracver/Deformable-ConvNets](https://link.zhihu.com/?target=https%3A//github.com/msracver/Deformable-ConvNets)

**录用信息：**ICCV2017

**论文目标：**

提升可变性物体的检测效果。卷积神经网络由于其固定的几何结构限制了其几何变换的建模能力。传统方法通常使用数据增强(如仿射变换等)、SIFT特征以及滑动窗口算法等方法来解决几何形变的问题。但这些数据增强方法都需假设几何变换是固定的，无法适应未知的几何形变，手工设计特征和算法也非常困难和复杂。所以需要设计新的CNN结构来适应空间几何形变。

**核心思想：**

论文主要是引进可形变卷积和可形变RoI pooling来弥补CNN在几何变换上的不足。同时实验也表明在深度CNN中学习密集空间变换对于复杂的视觉任务（如目标检测和语义分割）是有效的。

一、可形变卷积

1.标准卷积采用固定的kernel在输入的feature map上采样;可形变卷积则是在标准卷积采样的基础上，每一个采样位置分别在水平和竖直方向进行偏移，已达到不规则采样的目的，即卷积具有形变能力。

2.偏移是网络学习出来的，通常是小数；论文采用双线性差值的方式求这些带有小数采样点在feature map上对应的值。

3.偏移量可以采用标准卷积来实现。

4.图文说明

![img](https://pic2.zhimg.com/80/v2-d94c0d13d2600521525829ef981db8e1_hd.jpg)

二、可形变RoI pooling

1.RoI pooling是把不同大小的RoI(w*h)对应的feature map 统一到固定的大小(k*k);可形变RoI pooling则是先对RoI对应的每个bin按照RoI的长宽比例的倍数进行整体偏移(同样偏移后的位置是小数，使用双线性差值来求)，然后再pooling。

2.由于按照RoI长宽比例进行水平和竖直方向偏移，因此每一个bin的偏移量只需要一个参数来表示，具体可以用全连接来实现。

3.图文说明

![img](https://pic3.zhimg.com/80/v2-b08488b577045608aa099392849fb916_hd.jpg)

**算法效果：**

三层3*3可形变卷积分别在背景(左),小目标(中),大目标(右)上的采样结果(共9^3=729个点)

![img](https://pic4.zhimg.com/80/v2-79d88fd48031deb83f05c06ae15c3b1b_hd.jpg)



## **2.3 解决正负样本不均衡的问题**

## **2.3.1 Focal Loss for Dense Object Detection**

**论文链接：**[https://arxiv.org/abs/1708.02002](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.02002)

**开源代码：**[https://github.com/unsky/RetinaNet](https://link.zhihu.com/?target=https%3A//github.com/unsky/RetinaNet)

[https://github.com/unsky/focal-loss](https://link.zhihu.com/?target=https%3A//github.com/unsky/focal-loss)

**录用信息：**ICCV2017

**论文目标：**

论文作者认为，one stage检测精度之所以低于two stage方案的一个重要原因是检测样本不均衡：easy negative samples数量大大超过其他正样本类别。这使得训练过程中，easy negative samples的训练loss主导了整个模型的参数更新。

为解决这个问题，论文提出了focal loss。在计算交叉熵损失时，引入调制因子，降低概率值较大的easy negative samples的loss值的权重，而对于概率值较小的误分类样本的loss值，权重保持不变。以此提高占比较低的误分类别的样本在训练时对loss计算的作用。

**核心思想：**

一、Focal Loss

Focal Loss是在Cross Entropy上进行的改进,主要有一个γ参数(实际应用中会增加Balanced
Cross Entropy中的α参数),γ是作者引入用来关注hard example的参数,减少hardexample对loss的影响。下图是其公式及效果曲线图:

![img](https://pic3.zhimg.com/80/v2-48247a11d181d0ffe4cda2470e1602ae_hd.jpg)

Focal Loss的具体形式并不是很重要,主要是发现在目标检测中正负样本极度不平衡导致检测精度低的问题，论文也验证了其它形式的loss。虽然先前的检测器也考虑到这个问题并提出解决方案(例如OHEM),但是这种修改loss并且由数据驱动的形式更加有效和简洁。

二、RetinaNet

作者为了证明Focal Loss的有效性,设计了一个one-stage的网络结构，并命名为RetinaNet，其是以ResNet+FPN作为基础网络来提取特征,然后分别接两个子网络进行识别和定位。其中rpn的Top-Down pathway和lateral connetions(横向融合)用来构建大量多尺度的特征金字塔,然后用anchor机制在其上产生box用于下一步的分类和回归。下图是RetinaNet的结构示意图:

![img](https://pic1.zhimg.com/80/v2-1505662e2dfa3f31d1c8db198bf31534_hd.jpg)

**算法效果：**

论文阐述了one stage检测器相比two stage检测器精度低的问题以及原因，并通过设计新的loss给出了解决方案。

![img](https://pic1.zhimg.com/80/v2-62eaf47074dc9c030af3314ad2514960_hd.jpg)



## **2.3.2 Chained Cascade Network for Object Detection**

**论文链接：**

[http://openaccess.thecvf.com/content_ICCV_2017/papers/Ouyang_Chained_Cascade_Network_ICCV_2017_paper.pdf](https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Ouyang_Chained_Cascade_Network_ICCV_2017_paper.pdf)

**开源代码：**[https://github.com/wk910930/ccnn](https://link.zhihu.com/?target=https%3A//github.com/wk910930/ccnn)

**录用信息：**ICCV2017

**论文目标：**

物体检测任务中背景样本会比前景多很多，在训练中会造成正负样本不均衡的情况。作者设计出一套级联的方式来去除无用的背景，浅层中的无用信息过滤掉后无需进入更深的网络中计算，后阶段的特征和分类器只负责处理少量更困难的样本。由于需处理样本数量减少（主要减少了简单样本），此级联网络可节省训练预测时间，且更困难样本经过级联后作用会加强，所以检测效果也会有一定提升。

**核心思想：**

主要的网络结构如图所示，可以分为两个部分：前阶段层和深阶段语义层。

![img](https://pic4.zhimg.com/80/v2-f7c927a5443d32da7cd8f2c2d657b0e3_hd.jpg)

在浅层中，通过多级级联，网络可以拒绝来自背景的无用RoI。经过RoI pooling后的特征用来做分类，但被上一层拒绝的RoI不会再用他们的信息做RoI pooling了。在上下文级联阶段，那些没有被拒绝的RoI用来生成特征。特征再经过级联，最终没有被拒绝的部分用来做最终的预测。

作者设计的网络在训练和预测阶段都有层级网络，可以在节省训练和预测时间的情况下提升检测效果。因为在浅层网络拒绝RoI在深层网络不会用来被使用。 前阶段的网络用来学简单的分类，可以去除那些比较容易区分的背景，而深层用来学难的分类。以图为例，第一层，学习是不是哺乳动物。第二层学习是哪种动物如羊牛猪。最后一层学习区分都有角的动物，如牛羊。

![img](https://pic3.zhimg.com/80/v2-48bf844b51529704849b8ba64a85c76a_hd.jpg)

作者的创新主要可以归于以下几点：把级联分类器用在物体检测中，分类器级联中在上一层的分类得分在下一层也会被用到。不同层用到不同的特征，这些特征可以是不同深度、不同参数、分辨率和语义信息。上一层的特征可以被用到当前层，作为本层的先验知识。而所有的处理，如框回归、特征级联、分类级联都是在一个完整的网络中可以E2E处理。

**算法效果：**

![img](https://pic2.zhimg.com/80/v2-92c84b981f856c66c4c82a981fea15dd_hd.jpg)



## **2.3.3 RON-Reverse Connection with Objectness Prior Networks for Object Detection**

**论文链接：**[https://arxiv.org/abs/1707.01691](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1707.01691)

**开源代码：**[https://github.com/taokong/RON](https://link.zhihu.com/?target=https%3A//github.com/taokong/RON)

**录用信息：**CVPR2017

**论文目标：**

该算法结合 region free 和 region based 检测算法的优点，主要尝试解决两个问题：

1. 多尺度目标定位预测的问题。本文提出了RON的检测方法，首先利用一个类似于FPN的Top-Down 的连接方式，融合高层特征和底层特征（其目的在这里不做过多复述），并将融合后的不同分辨率的多层特征用来进行预测。与此同时利用一个objectness prior map过滤掉大量背景区域，提高检测效率。
2. 难例挖掘的问题。提出难例挖掘方法，提出了一套自己的选择正负样本的逻辑，以及训练时loss更新的策略，但整体和其他算法的思路差别不是很大。

**核心思想：**

文章介绍了two-stage目标检测算法的流程，包括四个步骤：a）利用CNN网络提取图像特征；b）在图像的特征图的基础上确定带检测的proposals，去掉大量的背景区域（相对于滑窗来讲）；c）对提取的proposals进行分类与精修；d）去掉重复的预测框，输出最终的检测结果。而region free的(如SSD)的算法精简了b，c两个步骤，将检测问题转变为一个single shot的问题。通过人为设置的一些固定的proposals直接让CNN学习如何对目标进行定位回归，加快了检测速度；



![img](https://pic1.zhimg.com/80/v2-268e2fd907814d7a28b081a87524d900_hd.jpg)网络结构

其网络结构图大致如上图所示，就Top-Down的结构来讲，考虑到该论文的发表时间，其实并无新意，它利用了类似于FPN的连接方法，即一个反卷积将高层特征图上采样，低层特征图经过一个3*3卷积处理，然后两者相素级相加。这种方式其实没有太多特点，采用别的其他方式应该也能达到大致相同的效果。融合后的特征层用来做最终的结果预测，其reference box的设置方法也和SSD的default box的设置思路大致类似，即采用如下公式：

![img](https://pic3.zhimg.com/80/v2-3cc27873b2b0197e06b63508f02564a6_hd.jpg)

其中Smin定义为图像1/10尺寸，而每个像素点对应5个aspect ratios { 1 3 , 1 2 , 1, 2, 3}。

而在读这篇论文时真正感到有新意的是作者引入了objectness prior map用来指导训练和预测，网络对每个预测的特征层都通过3*3＊2卷积产生了一个分辨率完全一致的objectness
prior map，其channel的个数为10，其后接一个Softmax的分类用于确定该像素所对应的default box为前景或背景。

![img](https://pic1.zhimg.com/80/v2-b021cd97b51bd3fdf13e793576e3bfc4_hd.jpg)

如图所示，图中的热力图样子的图片为objectness prior map各个通道的均值，可以看到不同尺度的物体均只在特定的层产生了响应。利用这样的objectness prior map，可以过滤掉大量的背景区域，也同时提高了多尺度物体的检测性能。

在训练时作者采用了和其他检测算法类似的方法，通过特定的overlaps的阈值对每一个default box进行了类别划分，前传的阶段objectness prior map和网络的预测输出同时进行，而反传阶段，先对objectness prior map的网络进行更新，然后通过设定的阈值，只对objectness prior map超过阈值的区域的正例进行更新。整体的正负样本比例保持在1:3，负样本是随机选取的。

在目标预测阶段，其结构也有一点改进，如下图所示，主要工作是将分类的模块改成了Inception的结构提高分类的准确率。

![img](https://pic4.zhimg.com/80/v2-ff32a2804534da725dc7c2a3658a9833_hd.jpg)

![img](https://pic3.zhimg.com/80/v2-b4d5cd41abff26b772547ff2ff7e5932_hd.jpg)

**算法效果：**

![img](https://pic1.zhimg.com/80/v2-888d6cafb27fb0064a120d6d7f43e744_hd.jpg)

## **2.4 被遮挡物体检测**

## **2.4.1 Soft-NMS -- Improving Object Detection With One Line of Code**

**论文链接：**[https://arxiv.org/abs/1704.04503](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1704.04503)

**开源代码：**[https://github.com/bharatsingh430/soft-nms](https://link.zhihu.com/?target=https%3A//github.com/bharatsingh430/soft-nms)

**录用信息：**ICCV2017

**论文目标：**

![img](https://pic1.zhimg.com/80/v2-b983fc1c8bc4e730ef5dde86f2c2a21c_hd.jpg)

传统NMS的处理方法是对检测框按检测得分排序，留下得分最高的，其他的如果IoU大于一定阈值就直接将其得分赋值为0（等于去掉）。这样的话，如果遇到上面这种情况就会有问题：红框是最好的物体检测框，但是绿框和红框的IoU过大，所以按照NMS策略，绿框就直接去除，检测不到了。作者设计了一种新的NMS方法解决这个问题。

**核心思想：**

计算NMS时，不是简单的去除值不是最高的物体，而是给IoU较大的框设置较小的得分。伪代码如下，修改的这行代码就是把传统的NMS改进为soft-NMS。

![img](https://pic4.zhimg.com/80/v2-7b4c70d256c0024e9d28e2244b1c30a7_hd.jpg)

作者使用两种方法将传统NMS改进为soft-NMS：

线性方法：





![img](https://pic4.zhimg.com/80/v2-5efce90a5ed1a3b060689f5a0bbb6c9f_hd.jpg)

高斯加权：

![img](https://pic3.zhimg.com/80/v2-b0a87ee09033c68ae688a7f4b1d8be16_hd.jpg)

**算法效果：**

由于soft-NMS主要关注的是互相遮挡的物体，从下图中的结果提升来看，像鸟、船这种容易被遮挡的物体的提升比较明显。

![img](https://pic2.zhimg.com/80/v2-8c182837b9257ccbe7fb7c4d9f8cb345_hd.jpg)



## **2.4.2 RRC: Accurate Single Stage Detector Using Recurrent Rolling Convolution**

**论文链接：**[https://arxiv.org/abs/1711.06897](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1711.06897)

**开源代码：**[https://github.com/xiaohaoChen/rrc_detection](https://link.zhihu.com/?target=https%3A//github.com/xiaohaoChen/rrc_detection)

**录用信息：**CVPR2017

**论文目标：**

提升被遮挡物体或小物体的检测效果。

**核心思想：**

SSD虽然在时间消耗和准确度上取得了不错的平衡，但是它对小物体、被遮挡的物体、同类重叠物体、照片过曝光或者欠曝光的物体检测效果不好。其原因主要是这些物体的预测位置与ground truth的IoU(Intersection over Union)有时会低于0.5。从下图的效果图（左侧图）来看虽然分类正确，位置也给出来了，但是与ground truth的偏差会很大，如果通过在训练过程中针对多尺度预测时，把输入的每一个特征图提高对应的anchor boxes的IoU阈值的话，SSD的预测的准确度会大幅度下降。针对于这种情况，本文提出了Recurrent Rolling Convolution这种结构，在生成anchor大于0.5的时候（本文设置为0.7）基于SSD的one stage模型针对于上所说的小物体、被遮挡物体等所预测的物体的位置会有较高的IoU。

![img](https://pic4.zhimg.com/80/v2-790548d37b45664c81100424010bb587_hd.jpg)

2 网络设计结构

2.1 总体设计

![img](https://pic3.zhimg.com/80/v2-c3921f1f9cfb4b580da9afd058c97692_hd.jpg)

上图为连续两次RRC迭代的流程图。以VGG16的网络结构为例，RRC在第一次迭代的计算需要输入的特征图包括conv4_3， FC6，conv8_2，conv9_2，conv10_2。在每一次迭代的每一个bottom up和Top-Down的阶段（图中上半部分为第一次迭代和下半部分为第二次迭代中）从左往右箭头使得网络历经 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes+1%5Ctimes+19) 的卷积和池化操作后，加在高一级卷积层的前面。从右往左箭头使得网络历经 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes+1%5Ctimes+19) 的卷积和反卷机操作后，加在低一级卷机层的后面。这样原来的特征图的厚度为256+38/19，然后经过一个 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes+1%5Ctimes+256) 的卷机操作以后生成回原来的channel尺寸，生成的特征图分别为conv4_3_2， FC6_2，conv8_2_2，conv9_2_2，

conv10_2_2至此完成了一轮RRC迭代。论文中RCC迭代在实验中一共使用了5次，这些权重参数在每次迭代是共享的，如果不是共享的话会有过拟合和一些未知的偏差问题(it will be more prone to overfitting and cause unexpected bias)。RRC是一个循环的过程，每一次迭代特征图都会将信息聚合来预测回归。正如前面讨论过的，这些相关特征信息包含上下特征信息，这对于检测具有那些小物体和被遮挡物体等特征提取具有重要作用。每一次迭代有一个单独的损失函数来训练学习。这确保了模型在每一次迭代过程中不会退化达到真正的进步。因为RRC可以多次进行，由此产生的特征图，具有更好的深度信息。

2.2 loss函数

下图中的 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctau%5C) 为框的回归和分类的预测运算， ![[公式]](https://www.zhihu.com/equation?tex=F) 为上述所说的RRC迭代，作者说到在每一次做预测还是做迭代的时候权重参数分别都是共享的，这样网络就变成类似rnn的结构（An important insight is that if the weights in F and τ are shared over steps respectively, this is a recurrent network.）。所以每一次做预测在训练过程中都会产生loss，和SSD类似，分类用交叉熵损失函数，框回归用smoothL1。

![img](https://pic2.zhimg.com/80/v2-6c2d80cd9abb197bf898f2b2ff0fe98d_hd.jpg)

2.3 实现的一些细节

对于网络的体系结构， RRC迭代5次，使用5层的卷积图来预测框回归和分类任务。实验通过验证RRC的 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) 卷积进行，证明了模型结果是有效的。对于数据增强，除了SSD中采用的数据增强方法外，论文还随机调整HSV颜色空间中图像的曝光度和饱和度扩大了1.3倍。此外实验使用KITTI数据集, 该数据集包含了许多挑战性的物体比如小和严重遮挡车辆和行人，并在特征图上使用生成anchor的阈值为0.7。在该数据集包含7481个样本进行训练和验证，和其他7518幅图像进行测试。在实验中没有使用任何其他数据集来提升结果。因为KITTI数据集中的图像尺寸比较大（1272x375），所以要调整了conv4_3中推断anchor对应的规模从0.1变成0.066。作者还去掉了在SSD中的全局池化层并且将conv10_2的anchor scale设置为0.85。

**算法效果：**

需要注意的是由于本文主要解决的是遮挡物体检测的问题，作者使用了KITTI数据集（汽车）进行测试。KITTI数据的物体互相遮挡比较常见，可以更好的反映出算法的优化效果。

![img](https://pic4.zhimg.com/80/v2-e320a7636f192f7ae1611e61c35c9ccf_hd.jpg)



## **2.5 解决检测mini-batch过小的问题**

## **2.5.1 MegDet: A Large Mini-Batch Object Detector**

**论文链接：**[https://arxiv.org/abs/1711.07240](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1711.07240)

**开源代码：**无

**录用信息：**CVPR2018

**论文目标：**

解决大规模Batch Size的训练问题。

缩短训练时间，提高准确率。

**核心思想：**

对于图片分类任务，网络只需识别其中主要的物体，由于其占图片的比例较大，导致所需图片的尺寸相对较小，因此相应的Batch Size很大。而对于检测任务，需要检测一些精细的物体，经深度神经网络训练之后，图片的尺寸以2的倍数减小。因此，为了保证最后一张特征图上小尺寸物体依然占有一定比例，需要输入一张较大尺寸的图片，然而这将导致图像检测所需的显存同比例增长，使得已有的深度学习框架无法训练大规模Batch Size的图像检测模型。

小 batch-size有如下缺点：a.训练时间久，b.不能为BN提供准确的数据，c.正负样本比例不均衡，d.梯度不稳定的缺点。然而增加Batch Size需要较大的learning rate，但是如果只是简单地增加learning rate，可能导致网络不收敛，如果用小的lr确保网络收敛，效果又不好。具体改进如下：

1．Variance Equivalence，即使用梯度方差等价代替梯度等价。在之前的工作中经常使用Linear Scaling Rule，即当Batch Size变为原来的k倍时，lr(learning rate)也应该变为原来的k倍。但是在分类里，损失只有Cross-Entropy，然而在检测里，图像之间有不同的Ground-Truth分布。结合两个任务的不同，分类任务的lr规律并不适合检测任务。基于此，论文提出了方差等价来代替梯度等价。

2．Warmup Strategy，即刚开始训练的时候，lr设置的小，随着每次迭代，逐渐增加lr。例如，16-batch的FPN学习率为0.02，在训256的MegDet时，如果设置lr=0.02*16,会导致模型发散，因此需要逐渐增加lr，让模型适应。随后随着训练逐渐减少lr，确保网络精度提高。

3．Cross-GPU Batch Normalization，在分类里，图片大小通常为224或299，所以一块TitanX可以容纳32张图，或者更多。然而对于检测来说，检测器需要接受各种尺寸的图片，因此会限制一个设备容纳的图片数。所以，需要跨GPU从多个样本提取不同的数据完成BN。如下图所示：

![img](https://pic1.zhimg.com/80/v2-5a4695d0953d139ff9507660fc65b764_hd.jpg)

BN操作需要对每个batch计算均值和方差来进行标准化。其中对于多卡的具体做法是，首先聚合算均值，然后将其下发到每个卡，随后再计算minibatch的方差，最后将方差下发到每个卡，结合之前下发的均值进行标准化。

**算法效果：**

论文提出了warmup learning rate和跨GPU BN机制，使训练时间缩短（从33到4小时），而且准确率更高。使用同样在COCO数据集上训练的FPN框架，使用256的batch-size比使用16的batch-size，验证集精度更高，且训练时间更少（如下图所示）。使用MegDet做backbone的检测网络获得了COCO2017冠军。

![img](https://pic1.zhimg.com/80/v2-805feef9a4de3e77e0ab7623428e7b14_hd.jpg)



## **2.6 关注物体之间关联性信息**

## **2.6.1 Relation Networks for Object Detection**

**论文链接：**[https://arxiv.org/abs/1711.11575](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1711.11575)

**开源代码：**[https://github.com/msracver/Relation-Networks-for-Object-Detection](https://link.zhihu.com/?target=https%3A//github.com/msracver/Relation-Networks-for-Object-Detection).

**录用信息：**CVPR2018

**论文目标：**

绝大多数的检测类论文对目标物体都是独立检测，忽略了目标物体彼此之间的关联性信息。本文作者受NLP领域注意力方法启发，提出了一种有效度量物体关联性信息的新方法。作者将该方法用于物体识别、重复框消除等阶段，均取得了显著的效果，实现了一种完全的端到端物体检测。在实际应用中，relation module可以用来替换NMS。

**核心思想：**

本文主要创新点有两方面。一方面是提出了物体关联性信息有效度量的新方法：relation module。另一方面是该方法在物体识别和重复框消除中的应用。在物体识别中，relation module可以将关联性信息融入提取后的特征中，并能保持特征维数不变。因此该方法可以方便地嵌入到各种检测框架中，用于提升性能。在重复框消除中，使用relation module 替换NMS，使得重复框消除这一环节能够更有效的融入整个学习过程，下面具体介绍下relation module以及它在物体识别和重复框消除中的应用。

一、Relation module

示意图如下:

![img](https://pic2.zhimg.com/80/v2-f194a1731fda4708656743f34e9b1055_hd.jpg)

其中 ![[公式]](https://www.zhihu.com/equation?tex=f_%7BA%7D%5E%7Bn%7D) 代表第n个物体的apperance特征(卷积特征）， ![[公式]](https://www.zhihu.com/equation?tex=f_%7BG%7D%5E%7Bn%7D) 代表第n个物体的geometric特征（4D的bounding box信息）。

上图右边的输出 ![[公式]](https://www.zhihu.com/equation?tex=f_%7BR%7D%5E%7Bn%7D) 即代表了作者提出的关联性信息（第n个物体）

对于上图右边的每一项，作者都给出了具体的解释和计算公式，读者可以阅读原文了解细节。需要指出的是，通过一系列的变换和限制，在concat所有的relation，并与原来的特征叠加后，特征的维数并没有发生改变。

二、物体识别应用

现在绝大多数的检测算法均采用如下处理步骤：

![img](https://pic1.zhimg.com/80/v2-91efc0eb6423aa0a38dde2b1143efb90_hd.jpg)

如前所述，relation module并不改变特征的维数，所以可以非常方便的添加到fc层之后，形成2fc+RM（relation module）的结构，实现如下结果：

![img](https://pic2.zhimg.com/80/v2-f08187c1ca8793458914877ba38cea39_hd.jpg)

其中 ![[公式]](https://www.zhihu.com/equation?tex=r_%7B1%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=r_%7B2%7D) 代表了relation module的重复次数。

三、重复框消除应用

作者将重复框消除当做了一个二分类问题。对于每个ground truth，只有一个detected object被归为correct类，而其余的都被认为是duplicate类。

对于每个detected object，首先按照物体识别阶段的输出 ![[公式]](https://www.zhihu.com/equation?tex=S_%7B0%7D)（分类分数）进行排名，即将scores转化为rank，并与 ![[公式]](https://www.zhihu.com/equation?tex=f%5E%7Bn%7D)（物体识别阶段的输入特征）、 ![[公式]](https://www.zhihu.com/equation?tex=bbox%5E%7Bn%7D) （物体识别阶段的输出坐标）一起作为relationmodule的输入（需要经过维数映射）。

将relation module的输出经过一个线性分类器和Sigmoid函数后，得到分数 ![[公式]](https://www.zhihu.com/equation?tex=S_%7B0%7D)。 ![[公式]](https://www.zhihu.com/equation?tex=S_%7B0%7D%5Ctimes+S_%7B1%7D) 即为最终的分类分数。

IoU超过阈值 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta) 的detected object都会作为当前ground truth的候选框。在候选框中， ![[公式]](https://www.zhihu.com/equation?tex=S_%7B0%7D%5Ctimes+S_%7B1%7D) 最大的（非IoU最大）分类为correct，其余为duplicate。

示意图如下:

![img](https://pic3.zhimg.com/80/v2-ef684ba8ca78ac0e6a357e486a4c4b7a_hd.jpg)

在训练和推理中，作者都给出了详细的策略，读者可以阅读原文了解细节。

需要指出的是，不管是NMS或SoftNMS，在学习过程中，重复框消除只是一种孤立的处理手段，并不参与模型参数的更新，而采用relation module方法，重复框消除既可以单独训练，也可以融入到整个模型参数的更新中，从而实现完全的end to end训练。

**算法效果：**

作者在COCO数据集上进行了实验，采用了ResNet-50和ResNet-101作为基础网络。

关物体识别效果，作者实验了geometric 特征的作用、relation的个数、relation module的个数，结果如下所示：

![img](https://pic2.zhimg.com/80/v2-19bebec6fa362757f79cb232ddbbe219_hd.jpg)

关于重复框消除效果，作者对比实验了NMS、SoftNMS以及本文方法，结果如下：

![img](https://pic3.zhimg.com/80/v2-72a668d23b13b6ac4ea8524cd5d934fa_hd.jpg)

关于端到端训练效果，作者对比了Faster R-CNN、FPN、DCN三种检测框架。针对每一种框架，分别使用了 2fc +SoftNMS -> 2fc+RM +SoftNMS -> 2fc+RM +e2e(end-to-end)三种网络结构，实验结果如下：

![img](https://pic2.zhimg.com/80/v2-7bfbced8670537335b19b863bb01c3d1_hd.jpg)



## **2.7 改进网络结构以提升效果**

## **2.7.1 DetNet: A Backbone network for Object Detection**

**论文链接：**[https://arxiv.org/abs/1804.06215](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1804.06215)

**开源代码：**[guoruoqian/DetNet_pytorch](https://link.zhihu.com/?target=https%3A//github.com/guoruoqian/DetNet_pytorch)

**录用信息：**ECCV2018

**论文目标：**

讨论分类模型和检测模型各自特点，重新设计了一个适合检测任务的网络模型。

**核心思想：**

该论文提出一种专门用于目标检测的主干网络结构DetNet，主要的贡献在于探讨了当前主流的CNN网络对于目标检测任务上的弊端。当前常用的CNN网络结构都是基于分类任务所设计的，其特点是存在大量的pooling层，最后的特征图会被压缩的非常小。Pooling层会造成特征的部分位移实际上对位置信息很重要的检测任务来讲比较不友好。而分类任务被分类的部分往往在图像中占主体，最后特征层比较小其实并无影响，而且这样感受野变大使得单位特征可以包含更丰富的信息。但是对于检测任务来讲，目标在图中所占比例是不固定的，有可能非常小。低分辨率高感受野的特征会丢失太多细节信息导致小物体的检测困难。所以适合检测的网络应当有着较高的分辨率，但是这样会带来计算量增长和感受野降低的弊端。DetNet 通过精细的设计平衡了这两个问题。

在论文中作者以ResNet50为例，将ResNet50通过改进变成适合检测任务的网络。文中提到味了保证公平性，ResNet50网络的前4个stage未作改变，而在第4个stage之后引入的空洞卷积的操作。

![img](https://pic1.zhimg.com/80/v2-8d86a625a5d1531218f832d076adf29c_hd.jpg)

![img](https://pic4.zhimg.com/80/v2-dd0035ed70cb25d19296ec39cfbf7607_hd.jpg)

如上图所示，该网络在第四个stage之后便不在进行下采样，特征的分辨率不会进一步降低。此时特征的stride step＝ 16.而引入空洞卷积的操作便是为了克服感受野小的问题，但是值得注意的一点是，随着网络的变深，空洞卷积的尺度并没有相应的变大。作者是因为变大效果不好所以没有变大还是无意而为之也没有进行讨论，这个其实是个挺值得大家去做的一个实验，没有这个实验我们没法得出到底多大的感受野范围是最适合目标检测任务的。与此同时，为了进一步减小计算量，自stage4，网络channel的数量便不再增多，而是保持一样。这样的网络结构使得该网络也同样适合用和检测中常见的Top-Down结构同时使用，由于预测层分辨率没有变化，所以使得高层特征和底层特征融合更加的容易，免去了上采样的步骤。

我认为论文与其说提出了一个全新的检测适用的网络倒不如说改进了分类网络的不足，或是说提出了一个重新设计检测网络的思考方向。目标检测算法发展到现在，各个流程趋于稳定与同质化。在底层特征的研究可以作为一个新的方向，CNN在卷积的过程中如何更好的体现位置坐标信息，如何对于不同尺寸的物体都保持较好的响应，都是优化检测任务的重点。本论文的网络设计虽然看似简单，但思考的出发点实际上是非常深刻且能解决实际问题的。而且其中一些操作，如预测层的channel没有继续变多，是否也能体现出检测网络的一个特性？其背后的原因又是什么？希望未来可以看到更多基于特定任务的特性而单独设计的网络结构。

**算法效果：**

![img](https://pic4.zhimg.com/80/v2-94f4e7906be237bb0f3053b8c1cadc57_hd.jpg)

## **2.7.2 RefineDet：Single-Shot Refinement Neural Network for Object Detection**

**论文链接：**[https://arxiv.org/abs/1711.06897](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1711.06897)

**开源代码：**[https://github.com/sfzhang15/RefineDet](https://link.zhihu.com/?target=https%3A//github.com/sfzhang15/RefineDet)

**录用信息：**CVPR2018

**论文目标：**

通过实现对ssd default box的二次精修提升检测效果。

**核心思想：**

one stage的网络结构，位置框和物体的类是在同一个特征提取层来做回归和分类预测的，这种的网络运算速度虽然快但是准确度不够高，准确度不够高的一个重要原因是因为框的正负样本数目比例失衡严重，two stage 的网络由于引入了Region Proposal Networks使得框的回归任务精度变高，该网络筛出了大量的负样本框（正负样本比例控制在1:3）解决了正负样本不平衡的问题。RefineDet是基于SSD的改进算法，该算法主要是bottom up（网络结构图上半部分）的网络结构来回归粗略位置参数来调整anchor的位置以及框的二分类（是否是物体的位置）任务，用Top-Down（网络结构图下半部分）的网络结构相对于调整的anchor的参数来回归精细物体位置和框内物体的分类任务。可以看出bottom up的运算就是来解决正负样本框数目不平衡问题的。

![img](https://pic1.zhimg.com/80/v2-f33c357130bcae1fefa7a4e063430774_hd.jpg)

网络总体来说就是把SSD这个one stage模型，通过加入Top-Down的方法变成two stage模型。bottom up阶段（论文称之为arm，anchor refinement module）为常规SSD多尺度预测阶段，做预测所提取的特征图分别为：conv4_3，conv5_3，fc7，conv6_2。每一个特征图都会有两个子网络分支，为别为预测anchor位置的子网络mbox_loc（3组w, h, x, y，子网络卷积层的channel为12）和预测是否为anchor类别的子网络mbox_conf（3组0,1，子网络卷积层的channel为6），筛选出的负例样本置信度高于0.99的就不会传入到Top-Down阶段（论文称之为ODM，object detection module）以此来控制正负样本的比例不均衡问题。

将arm阶段预测出来的结果调整anchor参数（conf:0/1,w,h,x,y图中标记为refined anchors），将特征图conv4_3，conv5_3，fc7，conv6_2输入给TCB单元（transfer connection block）得到P3, P4, P5, P6传入给ODM阶段。TCB单元实质上就是Top-Down结构，作用就是使得多尺度特征图的信道融合以此来丰富特征。最后生成的特征图为：P3, P4, P5, P6（其中P3, P4, P5, P6的生成分别对应arm中的 conv4_3，conv5_3，fc7，conv6_2相对应，特征图的尺寸分别是 ![[公式]](https://www.zhihu.com/equation?tex=512%5Ctimes64%5Ctimes64) 、 ![[公式]](https://www.zhihu.com/equation?tex=512%5Ctimes32%5Ctimes32) 、 ![[公式]](https://www.zhihu.com/equation?tex=512%5Ctimes16%5Ctimes16) 、 ![[公式]](https://www.zhihu.com/equation?tex=512%5Ctimes8%5Ctimes8) ）。有一个细节就是当conv4_3，conv5_3层在做anchor 的预测亦或者是做Top-Down的TCB操作的时候为了防止反向传播的剃度过大导致loss输出nan，这两层会经过一个L2normlization操作然后分别扩大常量倍数，scale的值分别为10和3 ，以网络结构如下：

![img](https://pic2.zhimg.com/80/v2-33ee8e9ae537c3d73bb6789776cca995_hd.jpg)

在Top-Down阶段做框精细调整的回归和物体分类任务。每一个特征图都会有两个子网络分支，为别为精细调整anchor位置的子网络mbox_loc（3组w, h, x, y，子网络卷积层的channel为12）和预测是否为物体类别的子网络mbox_conf（3组81类样本，因为是分类coco数据集，子网络卷积层的channel为243）。

网络训练过程是端到端的训练方法，loss函数也是常规的分类Softmax和目标检测的框回归smoothL1。损失函数公式如下所示：

![img](https://pic2.zhimg.com/80/v2-b7df12660c4057e88137c4512fc7ae81_hd.jpg)

其中，i为一个batch里面预测anchor的序列， ![[公式]](https://www.zhihu.com/equation?tex=l_%7Bi%7D%5E%7B%2A%7D) 为第i个anchor的真实标签， ![[公式]](https://www.zhihu.com/equation?tex=g_%7Bi%7D%5E%7B%2A%7D) 为第i个anchor的真实位置参数(w, h, x, y)。 ![[公式]](https://www.zhihu.com/equation?tex=p_%7Bi%7D)和 ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bi%7D)为第i个anchor在arm阶段预测的是否为前景置信度和位置参数。 ![[公式]](https://www.zhihu.com/equation?tex=c_%7Bi%7D)和 ![[公式]](https://www.zhihu.com/equation?tex=t_%7Bi%7D)为第i个anchor在ODM阶段预测物体类别的景置信度和位置参数。 ![[公式]](https://www.zhihu.com/equation?tex=N_%7Barm%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=N_%7Bodm%7D) 分别是在arm阶段和ODM阶段预测出的anchor为正样本的个数。 ![[公式]](https://www.zhihu.com/equation?tex=L_%7Bb%7D) 为二分类（anchor box是否是物体）的交叉熵损失函数，也成对数损失函数， ![[公式]](https://www.zhihu.com/equation?tex=L_%7Bm%7D) 为多类别分类Softmax损失函数，![[公式]](https://www.zhihu.com/equation?tex=L_%7Br%7D) 为Faster R-CNN中的 smoothL1损失函数。在公式中 [![[公式]](https://www.zhihu.com/equation?tex=l_%7Bi%7D%5E%7B%2A%7D%5Cgeq1) ]的意思是，如果anchor是正样本时，该值为1，anchor是负样本时，该值为0。因此[ ![[公式]](https://www.zhihu.com/equation?tex=l_%7Bi%7D%5E%7B%2A%7D%5Cgeq1) ] ![[公式]](https://www.zhihu.com/equation?tex=L_%7Br%7D) 指出了RefineDet网络只关心anchor正样本的loss值，负样本的loss值忽略不计。

在coco的数据集上，选用vgg16的网络结构，全部的图片resize到 ![[公式]](https://www.zhihu.com/equation?tex=512%5Ctimes512) 的条件下batch size设置为32，使用4张显卡来训练，0.9动量，0.0005的权重衰减率，前280k次的迭代使用的学习率为10e-3，后80k和40k次的迭代使用的学习率分别为10e-4、10e-5。在推理阶段arm将置信度高于0.99的负样本过滤掉，ODM只会输出置信度前400个物体预测样本，然后使用非极大值抑制（阈值为0.45），只会留下置信度为前200个预测样本作为最终的输出结果。

**算法效果：**

![img](https://pic3.zhimg.com/80/v2-333b122cef3a815ddcdbdab9645b3c16_hd.jpg)

![img](https://pic2.zhimg.com/80/v2-ea148f5def9f1e1648191970ff01553d_hd.jpg)

以上效果为在VOC 和 COCO 数据集上的mAP对比。



## **2.7.3 Pelee: A Real-Time Object Detection System on Mobile Devices**

**论文链接：**[https://arxiv.org/abs/1804.06882](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1804.06882)

**开源代码：**[https://github.com/Robert-JunWang/Pelee](https://link.zhihu.com/?target=https%3A//github.com/Robert-JunWang/Pelee)

**录用信息：**CVPR2018

**论文目标：**

在网络结构设计上更加精致，可以在移动端进行实时物体检测。

**核心思想：**

\1. Two-Way Dense Layer

Pelee使用两路卷积层来得到不同尺寸的感受野。

![img](https://pic1.zhimg.com/80/v2-6b49520991a5a8a6de1cfef02883938c_hd.jpg)

左边先通过1 x 1卷积，后面接3 x3卷积，使其对小物体有更好表现。

右边先通过1 x 1卷积，两个3 x 3卷积代替5 x 5卷积，有更小的计算量和参数个数，且对大物体有更好表现。

\2. Stem Block

![img](https://pic3.zhimg.com/80/v2-b0e3efb27ba116f3f41c96d6ad96a2ca_hd.jpg)

网络前几层对于特征的表达十分重要，Pelee 采用了类似于DSOD算法的stem block结构，经实验可知该结构可以在不增加过多计算量情况下提高特征表达能力。

\3. 动态调整bottleneck layer通道数

bottleneck layer通道数目随着输入维度的变化而变化，以保证输出通道的数目不会超过输出通道。与原始的 DenseNet 结构相比，实验表明这种方法在节省 28.5% 的计算资源的同时仅会对准确率有很小的影响。

\4. 过渡层不压缩

实验表明，DenseNet 提出的压缩因子会损坏特征表达，Pelee 在转换层中也维持了与输入通道相同的输出通道数目。

\5. Bn层与卷积融合,起到加速的作用。

舍弃了以前的bn-relu-conv采用了conv-bn-relu形式。Bn层与卷积层融合加快推理速度(融合部分计算)，同时后面接了1 x 1卷积提高表达能力。

作者将SSD与Pelee相结合，得到了优于YOLOV2的网络结构。

使用Pelee作为基础网络对SSD改进：

\1. 舍弃SSD的38 x 38的feature map

\2. 采用了Residual Prediction Block结构，连接了不同的层。

\3. 用1 x 1卷积代替SSD里的3 x 3卷积做最终物体的分类与回归。减少了21.5%的计算量。

具体网络结构：

![img](https://pic3.zhimg.com/80/v2-345667c990eb969449b0fd56667d140e_hd.jpg)分类网络结构图



![img](https://pic2.zhimg.com/80/v2-ebba779ea4880c13489e7249402e2201_hd.jpg)

首先图片输入后经过Stem Block结构，再通过stage1, stage2,stage3,stage4及后续卷积操作得到相应feature map，接resblock结构后进行分类及位置回归。

![img](https://pic2.zhimg.com/80/v2-956cf7f00ff2a283762b360e06066989_hd.jpg)

**算法效果：**

结合SSD，在VOC2007 数据集上达到了76.4% 的 mAP，在 COCO 数据集上达到了 22.4% 的 mAP。在iPhone 6s 上的运行速度是 17.1 FPS，在iPhone 8 上的运行速度是 23.6 FPS。

![img](https://pic4.zhimg.com/80/v2-f28e62e8161aca6101450351a73fae17_hd.jpg)

![img](https://pic1.zhimg.com/80/v2-9f3cd7cd85e5cf0d981543b2b0863b90_hd.jpg)



## **2.7.4 Receptive Field Block Net for Accurate and Fast Object Detection**

**论文链接：**[https://arxiv.org/pdf/1711.07767.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1711.07767.pdf)

**开源代码：**[https://github.com/ruinmessi/RFBNet](https://link.zhihu.com/?target=https%3A//github.com/ruinmessi/RFBNet)

**录用信息：**未被会议收录

**论文目标：**

一些检测论文会依赖很深的CNN网络来提升效果，但此类网络会牺牲运行速度。在RFB论文中，作者由视觉感受野（Receptive Fields）出发提出了感受野RFB模块（Receptive Fields Block）。通过膨胀卷积和增加Inception结构等方法使得网络结构的感受野变大，这样可以在不增加网络深度的前提下保持较高的检测效果和较快的运行速度。

**核心思想：**

通过改进SSD特征提取网络，使卷积核的感受野可以覆盖更多范围，提升检测效果。算法主要是对网络中卷积的结构进行改进，相当于把SSD的基础网络替换为一个类似于Inception的网络，并将普通卷积改为了膨胀卷积，使得每个卷积的感受野变得更大。RBF网络通过模仿人类感受野使基础网络可以学到更多尺度的信息，从而在不增加参数的前提下提升准确率。

RFBNet主要创新可以参考以下两张图：

![img](https://pic4.zhimg.com/80/v2-121c4290e76eb8f434fa6f725b17b067_hd.jpg)

![img](https://pic1.zhimg.com/80/v2-c236e36088d96f8dde4e5ecbc4f07350_hd.jpg)

作者参考Inception结构，将SSD的基础网络改进为多Branch结构。每个RFB（感受野模块）由不同大小的普通卷积＋Dilation Conv构成。如1*1卷积接3*3卷积，3*3卷积接3*3膨胀卷（感受野为9*9），5*5卷积接3*3膨胀卷积（感受野为15*15）。然后将这三个结构concat在一起共同作用。膨胀卷积如图所示，虽然3*3的卷积的参数个数和普通卷积一样，但其覆盖范围更大。

其实每个卷积核不覆盖很小的范围在deformable conv论文中也早有提及。作者在对比中提到，deformable的每个像素的作用是相同的，但RFB结构可以通过对不同尺度的卷积设定不同权重使不同尺度的信息的作用不同。

在实现过程中，作者使用了两种不同类型的RFB：

![img](https://pic2.zhimg.com/80/v2-c71b766b706e876f2f76fe9ffe8680a9_hd.jpg)

A结构分支更多，卷积核更小，且没有5*5卷积核（作者在使用中使用两个3*3卷积代替5*5卷积）这两种构造在最后的SSD物体检测网络中的位置是不同的。根据作者的说法，在更靠前的网络，为了模仿人类更小的感受野，所以使用了更多分枝，且卷积核更小。事实上在使用过程中，只有第一层用了A结构。

![img](https://pic4.zhimg.com/80/v2-577e1ea82b990385a42e163c90b3d07f_hd.jpg)

我们看一下整体的网络结构，可以看到RFB a结构只在提取VGG43的特征时使用，其他的RFB都是B结构。另一个有趣的现象是最后几层依然使用了原始的卷积操作。因为在这些层feature map的尺寸已经很小了。较大的卷积核（5*5）不能运行在上面。

**算法效果：**

VOC数据集的mAP可以达到80.5%。作者在其他基础网络上也测试了准确率，发现也有提升。证明RFB结构的效果提升具有普遍性。此外，作者尝试了使用RFB网络从零开始训练。最终的mAP为77.6 (DSOD为77.7)，整体表现差不多。

![img](https://pic4.zhimg.com/80/v2-14e77e37c91e302cd2a3f0d17bad032b_hd.jpg)

![img](https://pic3.zhimg.com/80/v2-a970c263d4d520ad29ca31f80c7fcb52_hd.jpg)



# 相关

- [基于深度学习的目标检测算法综述（二）](https://zhuanlan.zhihu.com/p/40020809)
