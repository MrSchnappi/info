---
title: 3.11 CTPN
toc: true
date: 2018-11-10
---

# 可以补充进来的

- [场景文本检测—CTPN算法介绍](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247485005&idx=1&sn=0d4fb43b8db2a8046c64a9cfcbf3f478&chksm=fdb69bdacac112cce05c8b735b4f8b1ccf2348bea55a30af2055fc328958bb8f1ffd0f819bd2&scene=21#wechat_redirect)


# CTPN



**CTPN模型**

CTPN是目前流传最广、影响最大的开源文本检测模型，可以检测水平或微斜的文本行。文本行可以被看成一个字符 sequence，而不是一般物体检测中单个独立的目标。同一文本行上各个字符图像间可以互为上下文，在训练阶段让检测模型学习图像中蕴含的这种上下文统计规律，可以使得预测阶段有效提升文本块预测准确率。CTPN模型的图像预测流程中，前端使用当时流行的 VGG16 做基础网络来提取各字符的局部图像特征，中间使用 BLSTM 层提取字符序列上下文特征，然后通过 FC 全连接层，末端经过预测分支输出各个文字块的坐标值和分类结果概率值。在数据后处理阶段，将合并相邻的小文字块为文本行。

![mark](http://images.iterate.site/blog/image/20190729/zFT8emNbkmo1.png?imageslim)（选自 arXiv: 1609.03605，’Detecting Text in Natural Image with Connectionist Text Proposal Network’）


detecting text in natural image with connectionist text proposal network

# 简介

本文将对 CTPN 这篇文章的思路做一个详细的介绍，同时对代码进行解读。
论文地址：[arxiv](https://arxiv.org/pdf/1609.03605.pdf)
作者 github 地址：[github](https://github.com/tianzhi0549/CTPN)
tensorflow版本地址：[tensorflow](https://github.com/eragonruan/text-detection-ctpn)
作者提供的版本使用的 caffe，没有提供训练的代码，但是有一个 online 的 demo


# 论文的关键 idea

- 文本检测的其中一个难点就在于文本行的长度变化是非常剧烈的。因此如果是采用基于 faster rcnn等通用物体检测框架的算法都会面临一个问题？怎么生成好的 text proposal？这个问题实际上是比较难解决的。因此在这篇文章中作者提供了另外一个思路，检测一个一个小的，固定宽度的文本段，然后再后处理部分再将这些小的文本段连接起来，得到文本行。检测到的文本段的示意图如下图所示。
  ![img1](http://slade-ruan.me/img/in-post/ctpn/in-post-1022-01.png)
- 具体的说，作者的基本想法就是去预测文本的竖直方向上的位置，水平方向的位置不预测。因此作者提出了一个 vertical anchor的方法。与 faster rcnn中的 anchor 类似，但是不同的是，vertical anchor的宽度都是固定好的了，论文中的大小是 16 个像素。而高度则从 11 像素到 273 像素变化，总共 10 个 anchor.
- 同时，对于水平的文本行，其中的每一个文本段之间都是有联系的，因此作者采用了 CNN+RNN的一种网络结构，检测结果更加鲁棒。

------

# pipeline

整个算法的流程主要有以下几个步骤：（参见下图）

- 首先，使用 VGG16 作为 base net提取特征，得到 conv5_3的特征作为 feature map，大小是 W×H×C
- 然后在这个 feature map上做滑窗，窗口大小是 3×3。也就是每个窗口都能得到一个长度为 3×3×C的特征向量。这个特征向量将用来预测和 10 个 anchor 之间的偏移距离，也就是说每一个窗口中心都会预测出 10 个 text propsoal。
- 将上一步得到的特征输入到一个双向的 LSTM 中，得到长度为 W×256的输出，然后接一个 512 的全连接层，准备输出。
- 输出层部分主要有三个输出。2k个 vertical coordinate，因为一个 anchor 用的是中心位置的高（y坐标）和矩形框的高度两个值表示的，所以一个用 2k 个输出。（注意这里输出的是相对 anchor 的偏移）。2k个 score，因为预测了 k 个 text proposal，所以有 2k 个分数，text和 non-text各有一个分数。k个 side-refinement，这部分主要是用来精修文本行的两个端点的，表示的是每个 proposal 的水平平移量。
- 这是会得到密集预测的 text proposal，所以会使用一个标准的非极大值抑制算法来滤除多余的 box。
- 最后使用基于图的文本行构造算法，将得到的一个一个的文本段合并成文本行。![img2](http://slade-ruan.me/img/in-post/ctpn/in-post-1022-02.png)

------

# 一些细节

## vertical anchor

- k个 anchor 的设置如下：宽度都是 16 像素，高度从 11~273像素变化（每次乘以 1.4）
- 预测的 k 个 vertical coordinate的坐标如下：![img3](http://slade-ruan.me/img/in-post/ctpn/in-post-1022-03.png)
- 与真值 IoU 大于 0.7的 anchor 作为正样本，与真值 IoU 最大的那个 anchor 也定义为正样本，这个时候不考虑 IoU 大小有没有到 0.7，这样做有助于检测出小文本。
- 与真值 IoU 小于 0.5的 anchor 定义为负样本。
- 只保留 score 大于 0.7的 proposal

## BLSTM

- 文章使用了双向的 LSTM，每个 LSTM 有 128 个隐层
- 加了 RNN 之后，整个检测将更加鲁棒，具体效果见下图。上图是加了 RNN，下图是没有 RNN 的结果。![img4](http://slade-ruan.me/img/in-post/ctpn/in-post-1022-04.png)

## 训练

- 对于每一张训练图片，总共抽取 128 个样本，64正 64 负，如果正样本不够就用负样本补齐。这个和 faster rcnn的做法是一样的。
- 训练图片都将短边放缩到 600 像素。

# 代码注解

reimplement的 github[地址](https://github.com/eragonruan/text-detection-ctpn)

## 准备训练数据

- 正如上文所提的，这个网络预测的是一些固定宽度的 text proposal，所以真值也应该按照这样来标注。但是一般数据库给的都是整个文本行或者单词级别的标注。因此需要把这些标注转换成一系列固定宽度的 box。代码在 prepare_training_data这个文件夹。
- 整个 repo 是基于 RBG 大神的 faster rcnn改的，所以根据他的输入要求。要再将数据转换为 voc 的标注形式，这部分代码也在 prepare_training_data这个文件夹。

## 一些预测结果

- 这个 repo 没有实现文章中提到的 side refinement部分。
- 以身份证检测做了个例子，但是模型是可以应用到各种水平文本检测的应用中的。
- 部分检测的结果如下所示：


![](http://images.iterate.site/blog/image/181106/LmEDH92jG0.png?imageslim){ width=55% }



![](http://images.iterate.site/blog/image/181106/HJ3ChhEb61.png?imageslim){ width=55% }



# 相关

- [论文阅读与实现--CTPN](http://slade-ruan.me/2017/10/22/text-detection-ctpn/)
)
