---
title: 4.02 结合 Attention
toc: true
date: 2019-09-15
---

**第四个分类：借鉴 Attention**

由于 NLP 领域兴起的 Attention 模型的重大影响，其也进入了文字检测与识别的视野，激发出一些新想法，代表性成果有：

1）CRNN，
2）ASTER，
3）FAN。

![img](https://pic2.zhimg.com/80/v2-d826691f221f9f3017c154d3be6a340d_hd.jpg)

在讲 Attention 之前，首先讲一下旷视科技 TPAMI 2017 的一个工作，称之为 CRNN，其底层用 CNN 提取特征，中层用 RNN 进行序列建模，上层用 CTC loss 对目标进行优化。它是一个端到端可训练的文字识别结构，但并未使用 Attention。目前，CRNN 已成长为该领域的一个标准方法，在 GitHub 上已开源。

![img](https://pic1.zhimg.com/80/v2-fe14a767ef55fb625173e111cc7cd5fc_hd.jpg)

随后，旷视科技在 TPAMI 2018 提出一个称之为 ASTER 的解决方案。由于文字存在倾斜、弯曲等问题，在识别阶段，检测也不一定是最理想的，这时需要分两步做识别。第一步是给定一张输入图像，把其中的文字矫正到一个有利于识别的状态；第二步则是进行识别。这里需强调的是矫正过程是网络自动学习的，并不存在标注。

![img](https://pic2.zhimg.com/80/v2-336f889612ac14dd6012f2200fb58b35_hd.jpg)

那么，如上所示，ASTER 主要有矫正和识别两个模块。矫正模块在 STN 的基础上做了优化，使得控制点的预测更精确；识别模块则是一个经典的 CNN+RNN 同时带有 Attention 的结构，可以对序列进行预测。

![img](https://pic1.zhimg.com/80/v2-7df9defb35f02f0c3f6f5a0370413ed0_hd.jpg)

当然，Attention 本身也存在一些问题，比如 ICCV 2017 收录论文《Focusing Attention: Towards Accurate Text Recognition in Natural Images》提出了 FAN 这一工作。

某些情况下，Attention 预测不一定准确，如图 a 中 Attention 把后两个点定位在 “K” 上，造成 Attention 点发生重合，进而导致定位不准确与漂移。FAN 方法通过约束 Attention 点位置，将其锁定为目标文字中心，这样就避免了重合、漂移等情况，Attention 区域变得更整齐，最终识别效果也得到了提升。
