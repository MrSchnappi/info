---
title: 2.11 pix2pix
toc: true
date: 2019-09-03
---

# 有监督图像翻译：pix2pix

在这篇 paper 里面，作者提出的框架十分简洁优雅（好用的算法总是简洁优雅的）。相比以往算法的大量专家知识，手工复杂的 loss。这篇 paper 非常粗暴，使用 CGAN 处理了一系列的转换问题。下面是一些转换示例：

<center>

![](http://images.iterate.site/blog/image/20190722/b0RBu2Y4vfLK.png?imageslim){ width=75% }

</center>

> pix2pix 结果示例

上面展示了许多有趣的结果，比如分割图 $\longrightarrow$ 街景图，边缘图 $\longrightarrow$ 真实图。对于第一次看到的时候还是很惊艳的，那么这个是怎么做到的呢？我们可以设想一下，如果是我们，我们自己会如何设计这个网络？

**直观的想法**？

最直接的想法就是，设计一个 CNN 网络，直接建立输入-输出的映射，就像图像去噪问题一样。可是对于上面的问题，这样做会带来一个问题。**生成图像质量不清晰。**<span style="color:red;">嗯。</span>

拿左上角的分割图 $\longrightarrow$ 街景图为例，语义分割图的每个标签比如“汽车”可能对应不同样式，颜色的汽车。那么模型学习到的会是所有不同汽车的平均，这样会造成模糊。<span style="color:red;">是呀，学习到的是平均。</span>

<center>

![](http://images.iterate.site/blog/image/20190722/3rI3fhwolCHf.png?imageslim){ width=75% }

</center>

> pix2pix语义地图 L1loss 结果

**那么如何解决生成图像的模糊问题呢？**

这里作者想了一个办法，即加入 GAN 的 Loss 去惩罚模型。GAN 相比于传统生成式模型可以较好的生成高分辨率图片。思路也很简单，在上述直观想法的基础上加入一个判别器，判断输入图片是否是真实样本。模型示意图如下：

<center>

![](http://images.iterate.site/blog/image/20190722/eiVH8dRJzMLs.png?imageslim){ width=75% }

</center>

> pix2pix模型示意图

上图模型和 CGAN 有所不同，但它是一个 CGAN，只不过输入只有一个，这个输入就是条件信息。原始的 CGAN 需要输入随机噪声，以及条件。这里之所有没有输入噪声信息，是因为在实际实验中，如果输入噪声和条件，噪声往往被淹没在条件 C 当中，所以这里直接省去了。<span style="color:red;">嗯，还是很赞的，想更多的知道这个。感觉的确是可行的，而且，没有必要从噪音中进行生成。从已知的信息进行生成更好。</span>

### 7.3.3  其他图像翻译的 tricks

从上面两点可以得到最终的 Loss 由两部分构成：

- 输出和标签信息的 L1 Loss。
- GAN Loss

测试也使用 Dropout，以使输出多样化。<span style="color:red;">没看懂下面的式子：</span>
  $$
  G^*=arg\mathop {\min }\limits_G \mathop {\max }\limits_D \Gamma_{cGAN}(G,D)+\lambda\Gamma_{L1}(G)
  $$


采用 L1 Loss 而不是 L2 Loss 的理由很简单，L1 Loss 相比于 L2 Loss保边缘（L2 Loss基于高斯先验，L1 Loss基于拉普拉斯先验）。<span style="color:red;">什么叫 L2 loss 基于高斯先验，L1 loss 基于拉普拉斯先验？怎么就保边缘了？</span>


GAN Loss 为 LSGAN 的最小二乘 Loss，并使用 PatchGAN(进一步保证生成图像的清晰度)。PatchGAN 将图像换分成很多个 Patch，并对每一个 Patch 使用判别器进行判别（实际代码实现有更取巧的办法），将所有 Patch 的 Loss 求平均作为最终的 Loss。<span style="color:red;">没明白，这个厉害吗？</span>






# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
