---
title: 01 线性变换和非线性变换
toc: true
date: 2018-08-29
---






# 第 2 章深度学习和计算机视觉中的 基础数学知识


## 可以补充进来的

- <span style="color:red;">这个暂时没有进行整理，涉及到数学的，暂时优先整理 SIGAI 的</span>


对于基础数学概念的扎实理解，会帮助对深度学习中一些听上去很复杂的技术有直观而清晰的认识。


约定如下：向量用小写加黑斜体表示，凡是 涉及矩阵乘法的公式中，输入向量默认均为列向量。




## 线性变换和非线性变换

线性代数是几乎所有理工科的基础科目，而线性变换就是基础中的基础。

### 线性变换的定义

线性变换是指具有如下性质的函数 $\math{T}$ :

对于向量《和 V，有

r("+v) = r(") + r(v)    (公式 2-1)

对于标量 a，有

7(av) = ar(v)    (公式 2-2)

公式 2-1叫做加性(additivity)，通俗来说就是两个向量的和经过变换后等于两个向 量经过变换后的和。公式 2-2叫做齐性(homogeneity)，意思是给一个向量缩放一个倍数 在变换和变换后再缩放这个倍数结果是一样的。一般来说，在机器学习和视觉中最常遇到 的线性变换是矩阵乘法，也就是如下形式的线性变换。

| r(x1,x2,---,x„) = | a2Ax}+a22x2+--a2nxn | =    | 、i，i^2,1 | 气 2    •a2,2 • | •气。   | Vx2  | (公式 2-3) |
| ----------------- | ------------------- | ---- | ---------- | -------------- | ------- | ---- | --------- |
|                   |                     |      | _am,\      | am,2           | • am,n_ | _v   |           |

所以后面如果提到线性变换基本都是指矩阵乘法。概念讲完了，接下来从一个简单的 小例子讲起。

2.1.2高中教科书中的小例子

首先一起来复习一个高中数学的经典例子：坐 标变换。如图 2-1所示，在（1,0）位置有一个点儿 问：要把这个点逆时针旋转 60°到点 5 的位置， 如何做到？旋转后的位置坐标是什么？

根据高中课本，答案是：

一 siO=去

/、    /、 R （公式 2-4）

h=sin〔?K+c°s〔*K了 这是高中数学里的一个基本问题。我们来把这

个变换写成矩阵乘法的形式：



图 2-1高中数学的坐标变换问题

XB

_ys

Jcos ⑼

」Lsin（^）

-sin（^） cos （汐）



（公式 2-5）

其中知 71/3。这就是一个典型的线性变换。所以我们知道，线性可以实现旋转，那么 再进一步，如果希望能从 B 到 C，也就是 6 和原点连线的中点呢？很自然的，我们想到把 x轴和 y 轴都缩小为原来的 1/2就可以，所以只要把变换矩阵中对应的系数，也就是每一行 都乘以 0.5，则得到了将 d 变换到 C 的过程：

"xcl = ro.5

0

0.5

」一 Lo

「0.5    0

| COS（汐）  | -sin（汐） |      |
| ---------- | ---------- | ---- |
| sin （汐） | cos（0）   | yA_  |

（公式 2-6）

_[0    0.5

0.5 cos （0） -0.5 sin （0） xA 0.5sin（汐）0.5cos（沒）yA

所以矩阵乘法除了能旋转，还能进行缩放，有了这两个操作我们已经能够在给定坐标 系内任一起点和终点都能写出一个对应的变换了。那么再把问题进一步，如果不是这种有 规律的旋转加缩放的，矩阵，而是任意 2x2 的矩阵呢？代表的几何意义又是什么？在直接解 答这个问题前先来看另一个重要的概念：投影。

2.1.3点积和投影

说到投影又需要一起来回顾一个高中数学知识：点积。对于两个向量《，veR\其中 U=[u\, W2, Wn], V=[Vl, v2, vw], w 和 V 的点积定义如下：

w v = Zw/v/=wivi+w2v2+-*-wwvw    （公式 2-7）

也许有的读者看到这里会说，这不是内积吗？没错，这也是内积，内积是点积的推广， 是一个更广泛的概念，或者说，点积就是欧几里得空间的标准内积。其实在深度学习和计 算视觉用到的范围内，这些细节并不重要。我们可以认为点积和内积是一回事。

根据公式 2-7，有一个很常用的公式，即向量的长度的计算。



\Ju - u

（公式 2-8）

知道了定义，来看看点积的几何意义。先从小学数学开始，2x5=10，这个大家都知道， 2的 5 倍，等于 10。如果不把这么-•个简单的乘法看成是两个标量相乘，而是想象一下这 个乘法发生在二维平面的 x 轴上，如图 2-2a所示。



图 2-2投影示意图

两个向量(2,0)和向量(5,0)，它们的点积是 2x5+0x0=10，如果只看其中*轴的部分，和 乘法没有区别，从几何方面来看，就是以一个倍数对另一个量进行拉伸或收缩。

接下来让(5,0)这个向量保持长度，逆时针“旋转”成为(3,4)，再和(2,0)做点积， (2,0)-(3,4)=2x3+0x4=6o通过图 2-2b来形象理解一下，(3,4)在 x 轴上的分量的长度是 3, x 轴恰好科 2,0)指的方向重合，所以也可以说(3,4)在(2,0)指向的方向上的分量的长度是 3。而 (2,0)在自己方向上的长度就是向量本身的长度，也就是 2。这两个标量的乘积 2x3=6就是 这两个向量的点积的结果。

事实上这就是两个向量点积的几何意义：一个向量《在另—^向量 v 方向上的分量的 长度，和 v 的长度相乘得到的值。其中“在 v 上的分量的长度，称之为《在 v 上的投影(更 严谨的说法叫标投影，scalar projection)。为什么叫投影呢？来看图 2-2b，想象一下有个 平行光光源，发出的光和*轴，也就是(2,0)的方向垂直，则可以想象(3,4)在(2,0)上投出的 影子的长度，则恰好是(3,4)在(2,0)上分量的长度。

图 2-2中的例子是(2,0)刚好落在 x 轴，是为了方便理解和演示。接下来一起看看两个 任意方向向量做点积的情况，如图 2-3所示。

如图 2-3a，两个二维向量《和 1^，之间的夹角为 0，则《在 v 上的投影长度为 n 的长 都是|«|COS(0)，这个长度再乘上 v 的长度|v|就是

= |w||v|cos（汐）

（公式 2-9）

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-24.jpg)

当然，点积具有交换性，所以对这个几何意义的理解也一样，《和 V 的点积也可以看作是 V 在《上的投影乘以《的长度。公式 2-9的推导也很简单，最常见的一种推导如图 2-3b所示，把 n和 v 看作是三角形的两边，贝嘴三边的长度为根据高中数学中的三角形余弦定理，有

|«-v|2 =|«|2 +|v|2 -2|«||v|cos(0)    (公式 2-10)

而直接展开|«-v|2，有

|w - v|2 =(w-v)-(w —V)

= " « + v v-2(« v)    (公式 2-11)

=|«|2 +|v|2 -2(w. v)

公式 2-10和公式 2-11的最后一项应该是相等的，则推导出公式 2-9。接下来把公式 2-9 换个形式：

cos（沒）

（公式 2-12）

也就是说两个向量夹角的 cos 值，就是这两个向量方向上的单位向量的点积。这是一 个非常有用的结论，因为两个向量夹角的 cos 值是这两个向量相似性的重要度量，夹角越 小说明两个向量所指的方向越相近，而公式 2-12给了我们一个非常简便的计算这种相似性 的方法。另外，虽然我们的示例和推导都是二维的，但是这个结论在高维度也普遍适用， 并且在机器学习中是个很有用的公式，在后面的实例中还会见到。

2.1.4矩阵乘法的几何意义(1)

1.投影角度的理解

了解了点积的几何意义，再回过头来看看矩阵乘以向量的几何意义。从公式 2-3以及 公式 2-7点积的定义可以知道，矩阵乘以一个向量的计算，事实上就是矩阵每一行的行向 量和待乘向量的点积所形成的新向量，所以有

| r（x15x2,•••,%„） = | + 气 2x2+    ■ | = I（X） = | x_气* • X | （公式 2-13） |
| ------------------- | ------------- | ---------- | --------- | ------------ |
|                     |               |            |           |              |

其中紙，*代表第 Z*行的行向量(叫，叫 2, ...，X为(Xb X2, ...，Xw)To根据 2.1.3节点积 的几何意义，这个运算可以看作是 X 在<1，•上的投影长度，乘以本身的长度。如果我们把 <1，看作是一个坐标轴的单位向量，那么矩阵乘法运算后的向量的每一维值对应的就是 X 在 这个坐标轴上的投影长度。也就是说，这个变换计算的是向量 X 在以 A 作为每个坐标轴单 位向量的新坐标系下的坐标，这就是从投影角度来看待矩阵乘法的几何意义。

现在回到 2.1.2节中的旋转和缩放的小例子，旋转矩阵如下：

| cos  | 〔£  | -sin ⑼ |      | 12   | _4T2 |
| ---- | ---- | ------ | ---- | ---- | ---- |
| sin  |      | cosf^l |      |      | 1    |
|      |      |        |      | _ 2  | 2    |

所以变换后的坐标轴的单位向量是 x’=(l/2, i/5/2)和卢=(^/2, 1/2)，如图 2-4左图所示， 所以在标准坐标系中的(0，1)，也就是横轴上的向量，在以 X’和/为坐标轴的坐标系中的就 不再是(0,1)，而是如图 2-4右图所示的一个落在第一象限的向量的坐标。在新坐标系中每 个轴的值如前所述，就是向量在每个轴上的投影长度乘以每个轴向量的长度。



图 2-4从投影角度理解逆时针旋转 n/3

2.坐标映射角度的理解

从投影角度理解矩阵乘法虽然最能体现投影的意义，但却不是最直观的。第一是因为 人脑中最形象的空间的参照系都是正交的，简单来说无论二维还是三维(或是脑补的高维 图像)，坐标系的轴都是互相垂直。从本节讲的内容来理解，就是任何一根轴上的向量在 其他轴上的投影都是 0。所以如果变换矩阵的行向量互相正交，那么还可以像图 2-4中一 样，形象地理解为旋转，但是如果变换矩阵的行向量不是正交的，甚至哪怕行向量的长度 不是 1 的情况下，就很难形象地想象了，例如下面的矩阵乘法：

[;]«]

画出(1,1)和行向量，和变换后的向量(1,5)的位置，如图 2-5a所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-26.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-27.jpg)

图 2-5从位置映射的角度理解矩阵乘法

在图 2-5a中，(1，1)在两个行向量上的投影长度仍然可以形象地理解，但是由于(2,-1) 和(1，4)并不互相垂直，所以还是难以形象地理解矩阵乘法的变换。

从投影角度形象理解矩阵乘法的第二个困难是参考坐标系的变换，对于人们来说最直 观的坐标系就是标准的笛卡尔坐标系，以二维为例子就是(1,0)所在为横轴，(0,1)所在为纵 轴的这种坐标系。在执行矩阵乘法的线性变换时，无论是变换前的向量，还是变换后的向 量，都是以笛卡尔坐标系为参考的。所以换个角度，自始至终都在笛卡尔坐标系下，首先 来考虑下面的问题：横轴和纵轴的单位向量在矩阵乘法之后对应的向量分别是什么？还是 以图 2-5中的矩阵乘法为例子，过程如下，对于横轴单位向量(1,0)，有

4」|_0

2x1

1x1

4x0」L1」

对于纵轴单位向量(0,1)，有

| _2   | -f   | '0'  |      | _2x0 | -lxf  |      | "-1" |
| ---- | ---- | ---- | ---- | ---- | ----- | ---- | ---- |
| 1    | 4    | 1    |      | 1x0  | 4x1 _ |      | 4_   |

结论一目了然，对于横轴，也就是第一个维度的单位向量，变换后对应的就是变换矩 阵的第一列的列向量(2,1)，对于纵轴，也就是第二个维度的单位向量，变换后对应的就是 变换矩阵的第二列的列向量(-1,4；)。这个结论可以很容易地推广到高维情况，对于第/维度 的单位向量，变换后对应的就是变换矩阵中的第 z*列的列向量。

可以把这种变换形象地理解成一种坐标的映射，具体到图 2-5a的例子，就是图 2-5b中 的情况，经过变换后原来的(1，0)对应的新坐标是(2,1)，(0,1)对应的新坐标是(-1,4)。在这种 对应关系下，考虑由(0,0)，(1,0), (1,1)和(0,1)围起来的单位长度的小方框，经过变换后相 当于被“拉伸”成为由(0,0)，(2,1), (1,5)和(_1，4)围起来的四边形，所以在单位方框中右上 角的顶点，在变换后就是被拉伸后四边形的对应顶点(1,5)。

再回过头来看看最开始讲的高中教科书里逆时针旋转 k/3的例子，如图 2-6所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-28.jpg)

比起投影的角度更加直观了，横轴单位向量变换后对应的坐标正是逆时针旋转 71/3。 在这种理解下，对于任何矩阵乘法的变换，都可以很形象地理解为对变换前的区域进行旋 转和沿特定方向缩放结合一起的操作，让原来区域经过形变后映射到了一个新的区域里。 比如图 2-7中，实现了对一个区域的切变和沿 x 轴翻转。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-29.jpg)



图 2-7通过矩阵乘法实现对一个区域的切变和翻转

如果希望变换后的坐标有位移，只需要在变换后的结果上加一个位移向量就可以，如 图 2-8所示。



图 2-8通过增加偏置项实现位移

如果把图 2-8中的问题想象成一个回归问题，只通过矩阵乘法的话是无法把图 2-8a中 的笑脸变换为图 2-8b中实线的笑脸，而添加了位移向量之后，则能够轻松拟合。这个位移 向量偏置在机器学习中是一种常见的参数，通常被称为偏置（bias），而形如的变 换形式也是机器学习中最常见的变换，称做仿射变换。简单来说，仿射变换就是一个线性 变换接着一个位移。

从拟合的角度看，偏置对结果的影响主要和样本的分布相关，一般来说当样本方差大、 维度高的时候，则偏置的影响就会小一些。偏置的引入让变换的灵活度更大，但却不再是 线性变换，并且形式上变得比 Mx 更复杂，一个常用的办法可以把化为尸 Ux 的 形式，推导如图 2-9所示。

如推导的第 2 行所示，加上位移向量的时候，可以看作位移向量是 1 前面的系数，这 样就如图 2-9中灰色方框标识的，通过把位移向量加到矩阵的最后一列，同时在待变换向 量后添加一个值为 1 的维度，把位移向量/偏置直接包含在矩阵乘法之中。

| "1   | 1"   | X    | +    | T    |      |
| ---- | ---- | ---- | ---- | ---- | ---- |
| 0    | -1   | _y_  |      | _2_  | L    |

lxx+lxx Oxy + (-l)xy



lxx + lxx + 1    lxx + lxx + lxl

Oxy+ (-l)x^ + 2」I Oxy+ (-l)xy+ 2x1

1 1 0    -1

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-33.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-34.jpg)

图 2-9通过在向量末添加 1 的方式将偏置/位移向量包含在矩阵中

从形式上看，，。这样只是个公式上的小把戏，从维度的角度来看，通过增加维度，变换 可以更加灵活。

2.1.5本征向量和本征值

一提到本征向量，很多人可能会想到通过本征值的定义对等式进行变化后，对着一元 二次方程求解本征值的头疼经历。本书不打算讲这么多关于本征向量和本征值的细节，我 们一起来从定性的角度形象感受一下本征向量。当然，一开始还是先讲公式，下面来看看 本征值和本征向量的定义，对于一个非零向量 X 和一个矩阵/（，如果有标量 A 使得：

Ax = Ax    （公式 2-14）

则称 2 为/I的本征值，x为对应的本征向量。从定义来看，本征向量的意思就是说对经过 变换后，这个向量并没有发生方向的变化（或是完全反向，如果 2 为负值的话）。其实在 当前更多的中文教材中，本征向量和本征值有另外一个名字叫特征向量和特征值。个人认 为“特征向量/值”的名字并不是很好，因为 eigen 这个词有“固有，不变”的含义，例如

就是微分算子的本征函数。

基于 2.1.4节的理解，我们来直观感受一下本征向量和本征值的几何含义，为方便讨论, 都以单位向量为例，考虑如下矩阵：

1 1 -2 4

这个矩阵变换的本征向量分别为（_1/人,-l/V2）W（-l/V5 ,-2/75），对应的本征值分别 是 2 和 3。注意：为了方便讨论，这里使用大于 0 的本征值。首先来看看向量（1，0）和（0，1） 经过变换后的情况，如图 2-10所示。

如图 2-10a所示，根据 2.1.4节关于矩阵乘法几何含义的理解，（1,0）向量所示的黑色实 线箭头变换后对应的则是第一列的向量（1，_2），而（0，1）所示的浅色实线箭头变换后对应的是 第二列的向量（1，4）。显然这两个向量都发生了方向的变化。接下来看看图 2-10b中对两个 本征向量变换后的情况。（-1/VL-i/W）是黑色实线箭头,（-V W「2/々）是浅色实线箭头， 变换后的两个向量和变换前的向量方向完全一致，其中黑色虚线箭头的长度是黑色实线箭 头长度的 2 倍，浅色虚线箭头的长度是浅色实线箭头长度的 3 倍，这就是本征值的几何含 义：变换会将对应本征向量方向上的向量进行缩放，缩放的倍数就是本征值。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-35.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-36.jpg)

a）    b）

图 2-10理解本征向量的几何含义

上面的例子用的矩阵是一个非对称矩阵，在机器学习中比较常见的情况是对称矩阵， 尤其是正定矩阵。正定矩阵的定义如下：对于任意非零的向量 X，和一个对称矩阵/（，如 果有

xtAx > 0    （公式 2-15）

则称矩阵 d 是正定矩阵。从之前讲到的点积的几何意义，正定矩阵可以理解为一个向量经 过正定矩阵变换后，和自身的点积大于 0，说白了就是正定矩阵对应的变换不会把变换后 的向量变到向量本身所垂直的平面的另一侧。具体到二维的例子就是，怎么变，变换后的 向量和自身的夹角都不会大于 90°。考虑如下正定矩阵：

'1.5 0.5"

0.5 1.0

本征向量分别是(0.85,0.53)和(-0.53,0.85)，对应的本征向量为 1.81和 0.69，还是按照 图 2-10的方式画出来如图 2-lla所示，深色实线箭头为单位向量，浅色虚线箭头为变换后 的向量。

可以看到，两个本征向量是互相垂直的。正定矩阵的本征向量有什么特别之处呢？来 看图 2-llb，想象有个单位长度的向量，把这个向量绕着原点旋转，并画出变换前和变换 后的轨迹，则这个向量显然画出了一个圆，而变换后的向量画出的轨迹是一个椭圆。如 图 2-llc所示，而这个正定矩阵对应的本征向量，则正好分别是椭圆长短轴所指的方向， 本征值则是椭圆的半长轴和半短轴的长度。从几何上理解就是正定矩阵变换前后的空间里 可以找到一组正交的向量，这组正交向量变换后仍是正交的，且方向不变，空间只是沿着 这组正交向量的方向上发生了拉伸/收缩。如果有接触过主成分分析(Principal Component Analysis, PCA)的读者肯定已经看出来了，是二维的，没错，这就是 PCA 的底层思想， 关于 PCA 的细节，后面的章节也会讲到。但是这个结论在高维度也普遍适用，并且在机器 学习中是个很有用的公式，在后面的实例中还会见到。



图 2-11正定矩阵的本征向量和变换性质

2.1.6矩阵乘法的几何意义(2)

从旋转和拉伸角度理解矩阵乘法的几何意义

了解了本征值和本征向量的几何意义，我们知道了一个正定矩阵对应的变换其实就是 沿着本征向量的方向进行了缩放。那么从旋转和缩放的角度如何看待正定矩阵的变换呢？ 还是考虑图 2-11中的例子，对于图 2-11例子中的变换矩阵

'1.5 0.5'

0.5 1.0

很难直观想象出沿着特征向量方向(0.85,0.53)和(-0.53,0.85)进行缩放的几何过程。在 2.1.2 节中的公式 2-6中，我们知道如果沿着横轴和纵轴方向进行缩放，那么形式就非常简单了， 如二维平面中，用一个变换对横轴缩放 a 倍，纵轴缩放 6 倍的矩阵如下：

a 0 0 b

该矩阵是一个对角矩阵，对应维度上的元素就是要缩放的倍数。2.1.2节中和 2.1.4节 中也讲了旋转矩阵和对应的几何理解，那么对于二维平面的情况，我们很自然地想到，只 要用一个旋转矩阵，把原来空间中对应特征向量的方向旋转到对应*轴和轴，然后进行 简单的缩放，然后再用一个矩阵变换旋转回去，不就和直接乘以一个变换矩阵等效了吗？ 按照这个思路来试一下。

第一步，已知两个特征向量的方向，现在要把(0.85,0.53) “转回” x轴的位置，只需要 把当前的％轴转到(0.85,0.53)沿 x 轴对称的位置，所以根据 2.1.4节中的形象理解，第一个 列向量就是(0.85,-0.53)。同样的对于(-0.53,0.85)，要“转回、轴，则需要把当前轴转到

(-0.53,0.85)沿轴对称的位置，也就是(0.53,0.85)，所以变换矩阵就是 _ 0.85    0.53"

-0.53 0.85_

也就是从图 2-12a到图 2-12b的情况。注意其实就是特征向量作为行向量的矩阵。接 下来就是简单的沿着％轴和 y 轴方向进行缩放，其中缩放的倍数分别是两个特征向量对应 的特征值，也就是进行如下的矩阵变换。

'1.81 0 0    0.69

这一步对应图 2-12b到图 2-12c，可以看到，变换前的方格里的笑脸已经被扭曲成了斜 着的四边形，接下来就是最后一步，也就是第一步的“逆旋转”，其实就是逆变换，注意 到旋转矩阵都是正交矩阵，所以逆变换就是转置，也就是特征向量作为列向量的矩阵。

'0.85 -0.53—

0.53    0.85

最后就得到了图 2-12d，所以在这个过程中相当于把变换矩阵按照分解成了 3个子变换矩阵：

| "1.5 | 0.5] _ | 「0.85   | -0.53" | "1.81 | 0    |      | "0.85 | 0.53~ |
| ---- | ------ | -------- | ------ | ----- | ---- | ---- | ----- | ----- |
| 0.5  | 1.0」  | '\|_0.53 | 0.85 _ | 0     | 0.69 | □    | -0.53 | 0.85  |

图 2-12将一个正定矩阵的变换分解为分布的“旋转一缩放一旋转操作”

其中第一次和最后一次的变换是单纯旋转，中间的变换是单纯地沿坐标轴缩放。







2.1.7奇异值分解

图 2-12的例子讲的是正定矩阵，那么对于一般情况下的矩阵变换呢，是否这种基于本 征向量和本征值，然后用单纯旋转和缩放的组合解释的几何意义也能推广呢？答案是肯定 的，而且形式上和对于正定矩阵的分解很类似，也是“翻转一缩放一翻转”，这种更一般 的分解就是奇异值分解(Singular Value Decomposition, SVD)。定义如下：

M=UXUt    (公式 2-16)

其中 I 是一个对角矩阵，就我们在机器学习领域内的问题而言，和 K 为正交矩阵。这里 不打算探讨过多的奇异值分解的细节，而是专注于几何层面的理解。下面还是以二维的例 子来演示，考虑如下的矩阵及其奇异值分解。

| P 31_ | 「0.66  | -0.76- | "5.40 | 0 '  | "0.38 | 0.92" |
| ----- | ------- | ------ | ----- | ---- | ----- | ----- |
| Li 4  | 义 0.76 | 0.66   | 0     | 0.93 | -0.92 | 0.38  |

还是按照类似图 2-12所示的策略，先画出(0.38,0.92)和(-0.92,0.38)两个第一次旋转后 会转到横轴和纵轴的向量，还有虚线笑脸，来一起看看一步步直到最终的变换，如图 2-13 所示。



「5.40

L o



d)    c)

0°93

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-40.jpg)

图 2-13将一个一般的矩阵变换分解为分布的“旋转一缩放一旋转”操作

所以现在我们知道，任何一个矩阵乘法执行的线性变换都是可以分解为旋转一缩放-> 旋转，而且其实正定矩阵的奇异值分解就是 1/和 K 是同一个矩阵的情况，就像前面说的， 奇异值分解可以看作是本征值和本征向量的一种推广。

2.1.8线性可分性和维度

线性可分的定义是指，对于两类不同的《维点的集合為和如果存在一个 n 维向 量 w G Rn和一个标量 6 使得：



\> 0, XG Xo < 0, XG Xj

（公式 2-17）

则称和而是线性可分的。从几何上理解，一个 Z2 维的线性组合 ivx+b代表的是《维空 间中的一个超平面，也就是说能被一个超平面分开的两类集合，就是线性可分的。具体来 看看从一维到三维的例子，如图 2-14所示。

图 2-14a中在一维直线上，将两类点分开的超平面就是一个点。图 2-14b中，分开两 类点的超平面是一条直线。图 2-14c中则是一个平面分开了两个不同类别的点。对于这种 二分类问题，在《维空间中，wx+Z^O则是描述判别这两类不同模式的超平面。至于线性 不可分，如图 2-15&所示的是一个经典的表达异或(XOR)函数的线性不可分例子。

|      | △    | O    |
| ---- | ---- | ---- |
|      | O    | △    |
|      |      |      |

a)



图 2-14线性可分的例子

b)

图 2-15线性不可分的例子

在图 2-15b中，一种类别的点集“包围”着另一种类别的点集。对于这种情况，无论 怎么画直线，都不可能将两类点分开。

那么线性可分和前面讲的线性变换有什么联系呢？通过前面的内容，我们对矩阵所对 应线性变换有了比较形象的定性理解。总结一下就是矩阵会对空间进行旋转，沿指定方向 缩放的操作。线性变换再结合平移向量一起，这类变换被称为仿射变换。在同维度下，仿 射变换下有些特性可能会发生变化，如长度、面积、角度和距离等，有些量则保持不变， 如直线的平行性质，还有线性可分的性质。形象来理解，如果对变换前的空间标上格子， 那么变换后格子的相对位置是不会发生变化的，所以以二维为例，如果变换前可以被直线 二分类的空间，变换后对应的空间和分界线都是一样的，如图 2-16所示。

也就是说同维度的仿射变换不改变线性可分或不可分的性质。那如果变换一下维度 呢？结论对于线性不可分还是一样，只要是线性/仿射变换，线性不可分的变换后还是线性 不可分，而对于线性可分的例子，则有可能会变得线性不可分。我们还是用图 2-14中的例

| ---------------1 |          |               |
| ---------------- | -------- | ------------- |
|                  | \        | FI「_•:       |
|                  | \        |               |
| -(               | .5    0J | D    OS    L0 |

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-43.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-44.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-45.jpg)

子，先考虑变换到低的维度，如图 2-17所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-46.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-47.jpg)

图 2-16仿射变换下对空间的扭曲

A



（通

|      | a    a |      |
| ---- | ------ | ---- |
|      |        | vX/X |

|      |      |                 |
| ---- | ---- | --------------- |
|      |      | aX/......    >y |

(0,1)



![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-50.jpg)

图 2-17二维样本通过线性变换到一维

在图 2-17中，分别把样本位置和 x 轴的单位向量（1,0），轴的单位向量（0，1），和 d 点 方向的单位向量（1/2，-人/2）做矩阵乘法，也就是点积，相当于求出了样本在这些向量上的 投影。可以看到，在 jv 轴和」向量的投影仍然是线性可分的，但是在 x 轴上的投影上，样 本则混在了一起，无法用一个点分开。下面再来看看把二维的样本通过矩阵乘法投影到高 维，只需要用一个三行两列的矩阵，如图 2-18所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-51.jpg)



图 2-18二维样本通过线性变换到三维

可以看到，经过投影到高维的矩阵乘法，二维平面在三维空间里还是分布在一个平面 上，所以线性可分性还是维持。

虽然我们只看了一维到三维的情况，不过不同维度之间的线性变换的规律是类似的， 高维变换到低维可能会让本来线性可分的样本变得不可分，而低维变换到高维则不会破坏 线性可分性。

总体而言，在高维空间中的样本更容易被线性分开，直观想象一下，一群挤在一条线 上的样本和一群同样数量分布在三维空间中的样本，哪个感觉更容易被线性分开？当然是 后者了。

2.1.9非线性变换

通过 2.1.8节我们知道如果是线性不可分的样本，通过线性变换到更高维的空间后，仍 然是线性不可分的。但是注意到，低维的样本变换到更高维空间后，事实上仍处在原来维 度大小的一个超平面上。如图 2-18所示的例子，二维空间中所有的样本在变换到三维空间 后还是处在一个二维平面上，但是和二维情况不同的是，在二维空间内，无论如何都不能 改变样本的分布，而在三维空间内却多出了一个维度！下面来考虑如图 2-19a的情况，在 垂直二维 x-y平面的方向上增加第三个维度 z。



图 2-19非线性变换让线性不可分样本变得线性可分

显然是线性不可分的，一个很自然的想法是通过非线性变换得到一个 z 轴的值，来考 虑如下的非线性变换：

z = /(^y) = (x-°-5)2 +(y-o.5)2

得到的结果在三维空间中如图 2-19b所示，如果只看 z 轴，如图 2-19c所示。可以看 到，无论是只考虑非线性变换后在低维空间的效果(图 2-19c)还是同时考虑到非线性变 换和高维空间的效果(图 2-19b)，都有无数个超平面可以轻松地将样本分开。在图 2-19b 和图 2-19c中分别标出了 z=0.1所在的分界作为例子。

注意到因为图 2-19a中的样本中心是在(0.5,0.5)，所以在非线性变换时需要对 x 和 分 别减去 0.5。这一过程可以看作是为了能让非线性变换起到最好的效果而做的仿射变换，所 以当遇到线性不可分的样本，就可以考虑先做仿射变换，然后进行非线性变换。如果效果 不好呢？可以考虑更灵活的非线性变换，更高的维度甚至到无限维，SVM里的核方法就含 有这一思想。或者可以考虑再来一次仿射变换+非线性变换，至此神经网络呼之欲出了， 这部分内容在后面我们再详细探讨。

2.2概率论及相关基础知识

概率论在各种应用学科中未必有线性代数那么广泛，但也是极其重要的一门学科。机 器学习作为一门面向数据的学科，概率论的重要性更加凸显。在默认读者已经具备高中概 率基本知识的前提下，本节将面向应用层面需要，简要回顾及介绍概率论及相关知识里和 机器学习联系最紧密的部分。

2.2.1条件概率和独立

先上定义：如果有事件力和見在已知 B 事件发生的条件下，事件发生的概率为 P（A\B）O通过定义，一个直观的感受是条件概率描述了依赖性。从反面的角度来说，就是 一个事件是否是独立的。

所以我们来考虑这两种情况：第一种情况，如果事件发生的概率确实依赖于 5 事件， 则当需要计算事件的概率时，就不得不考虑 5 事件。于是 d 事件和 B 事件同时发生的概 率为 P（AB）=P（A\B）P（B）O进一步的，如果用 P（/（|方）表示 5 不发生时发生的概率，则 d 事件发生的概率可以表示为 P（A）=P（A\B）P（B^P（A\B ）（1-P（^）），这实上就是在求边缘概 率。另一方面，＞4事件和 B 事件同时发生的概率也可以表示用已知 d 事件发生时 B 事件的 概率乘以？（事件发生的概率，所以有这就是著名的贝叶斯 公式；第二种情况，如果」事件发生的概率和公毫无关系，则有 d 事件和 B 事件同时发生的概率则为各自发生概率的乘积

说个简单例子来体会一下，假设你住在二楼，每到晚饭后，楼下经常会出现跳广场舞 的大妈。在不下雨的好天气里，大妈们兴致盎然，不过考虑到体能问题，只有 90%的时候 会出现，也就是八跳舞|不下雨＞0.9。而遇上雨雪天气，大妈们仍有一颗风雨无阻的心， 出现在广场上的概率大概是 50%，所以 P（跳舞|下雨）=0.5。而你居住的城市里，下雨天出 现的概率是 40%，即 P（下雨）=0.4，总结如表 2-1所示。

| 表 2-1广场舞大妈出现概率一览 |               |                 |
| --------------------------- | ------------- | --------------- |
| 说    明                    | P（下雨）=0.4 | P（不下雨）=0.6 |
| 跳舞的条件概率              | 0.5           | 0.9             |
| 不跳舞的条件概率            | 0.5           | 0               |

先来计算一下大妈们在雨中舞蹈的概率 P（下雨，跳舞＞^（跳舞|下雨）P（下雨）=0.5X 0.4=0.2，也就是说大妈们在雨中坚持舞蹈的景象有 20%的情况会出现。再来算算总体而言， 大妈们跳舞的概率，P（跳舞）=P（跳舞|下雨）P（下雨）+P（跳舞|不下雨 VX 不下雨＞0.5X0.4+ 0.9X0.6=0.74，所以大部分时间大妈们都会跳舞的。接下来再算算，大妈们跳舞的情况下， 下雨天出现的概率，P（下雨|跳舞）*（跳舞|下雨）P（下雨）/P（跳舞）=0.5X0.4/0.74=0.27。虽然 这个概率可以计算，不过注意到这并不代表着因果关系。下雨是影响是否跳舞的因素，而 跳舞是不会影响是否下雨的。所以因果关系能推出条件概率，但是条件概率并不代表着因 果关系。

那么条件独立的例子呢？想象一下你住的城市从不下雨，不过有雾霾。面对雾霾和对 舞蹈的热爱，大妈们选择了后者，完全不受影响。所以这种情况下跳舞就是个独立事件， 如果要计算大妈们在雾霾中舞蹈的概率，只需要 P（霾中舞蹈（跳舞）P（雾霾）。假如是在 北京的话，根据 2015 年的统计数据，这个概率是 0.9x0.49=0.44。

2.2.2期望值、方差和协方差

考虑离散情况，期望值的定义如下：

E[X] = XxiPM    （公式 2-18）

/=1

从定义来看就是变量值和其对应的概率的乘积，在整个定义域上的求和，简单说就是 以概率为加权系数的求和。当然这是定义，联系实际，白话的解释就是在长时重复观测下， 目标数据的平均值。例如，我们观察到跳广场舞大妈们的身高服从一个高斯分布，分布的 中心是一米六二，大妈们身高的期望值就是一米六二。

那么接下来我们希望知道大妈们的身高差异是否很大，也就是一组数据的离散程度如 何，这个时候可以用方差：

Var{X} = e\（X - E[X]）2y    （公式 2-19）

方差开方的话就是标准差，都可以用来度量一组结果的离散程度。通常在计算方差的 时候，下面这个公式更常用一些。

Kar（X） = £[（%_£[X]）2]

= E^X2- 2XE[X] + （^[^]）2]

（公式 2-20）

= e\_X2~]-2E [X] E [X] +（E[X]）2 = e[x2]-（e[x]）2

方差度量的是单个变量的离散程度，如果是多维变量的话则需要协方差，协方差定义 如下：

COV （X,Y） = E[（X-E[X]）（Y-E[Y]}] = E[XY]-E[X]E[Y]

（公式 2-21）

从定义上来看，协方差度量的是两个变量离散程度的大小以及相互之间的趋势的一致 性。如果两个变量相应的观测每次都相对于期望值的趋势一致，比方说*大于£（幻时，相 应的 y 也大于 F（y），而 x 小于£（JQ0^, y也小于£（y），那么协方差的值就大于 0；如果 x 和对应的^每次观测到的相对于期望值的趋势相反，则协方差小于 0；如果 z 和 r 并没有 趋势上的关联，则协方差会接近 0。所以协方差实际上包含了两个变量之间的统计相关性。

在实际问题中，我们常将协方差表示为一个矩阵，考虑一组变量不，為，其协方 差矩阵定义如下：

| cov^X,)    | cov(Xl9X2)  | •⑽（冬人）   |
| ---------- | ----------- | ------------ |
| cov[X29X{) | cov(^X2,X2) | • cov(X2,Xn) |
| cov(Xn,X,) | cov(Xn,X2)- | • cov(Xn,Xn) |

(公式 2-22)

协方差矩阵能描述方差，还有变量之间的趋势相关性，不过因为不同维度之间的协方 差大小依赖于两个维度方差的大小，所以比较起来并不直接。一种更容易让人理解的方式 是相关系数矩阵，定义如下：

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-54.jpg)

(公式 2-23)

其中，巧和 a，分别是两个维度的标准差，也就是协方差矩阵对角元素开方的值。从相 关系数矩阵的定义也可以知道，相关系数矩阵的值等效于将数据做了方差归一化之后的协 方差矩阵的值。比起协方差，相关性系数对趋势是否相同的度量更加一目了然，而且使得 不同量级之间的两两趋势相似度可以直接量化比较。

还是用广场舞大妈的例子来理解协方差包含的趋势信息。如我们获取了每个广场舞大 妈的年龄和跳舞时长的数据，多半会发现这两个数据的协方差以及相夭性系数是个负值。 因为一般来说年龄越大，体力越差，跳舞时间就会越短。当然也不排"fe有个别大妈天赋异 禀，80多岁了还能蹦跶全场，不过总体而言这个年龄和跳舞时长的相反趋势是靠谱的。

那么年龄和跳舞时长的协方差绝对值的大小呢？这个取决于参加广场舞的大妈们的年 龄段。如果有限制，只允许 50〜60岁的大妈参加，那么也许 60 岁的大妈跳半小时累了， 50岁的大妈则能多跳到一个小时；再考虑另一种情况，不限年龄，也许有的十几岁小姑娘 也凑热闹，跳了 5个小时直到半夜。90岁的老奶奶也来锻炼，5分钟就歇了。那么后者的 协方差绝对值显然大于前者。

2.2.3 熵

熵在物理和信息论中都是一个重要的概念，用来衡量一个分布的无序程度。从机器学 习角度，我们来看看概率和信息论中熵的定义：

H（X）= £[-log2 （尸⑷）]=-玄尸⑷log2 （尸⑷）    （公式 2-24）

配合一个简单的二值例子来理解一下，如图 2-20所示。

来看图 2-20a，想象箭头是一些小磁针，只能指向上和下两个方向。有些读者可能看 出来了，就是 Ising 模型，不过这里不讨论相变，只是简单赋给小磁针一个向上的概率 Pup。

假设在没有任何外界场影响的情况下，小磁针的指向随机，也就是说向上和向下的概 率相等，都是 0.5，向上的概率记作 Pup。这时候磁针的指向完全随机没有规律性，是最混 乱无序的情况，把 Pup=0.5, PdQwn=卜 Pup=0.5带入公式 2-24，得到熵的值为_（0.5xlOg2（0.5）+ 0.5xlog2（0.5））=lo

t t It 1 t H t I

Pup=0.5, Entropy=\

t t    t t    t t    1 t    t    t

Pup=0.9, Entropy=QAl

t t    t t    t t    t t    t    t

Pup—> 1，Entropy^) a)



图 2-20熵和有序程度

如果有一个外界的场作用在这些磁针上，场的强度刚好让磁针有 0.9的概率指向上方， 也就是 Pup=0.9 , Pdown=l-^up=0.1 ，同样带入公式 2-24，熵的值为 _(0.9xlog2(0.9)+ 0.1xlOg2(0.1))=0.47o此时观察到的磁针的有序程度变高了，熵变小了。

再考虑更极端的情况，如果一个很强的场加到了磁针上，导致 PUP—1，这时候小磁针 基本上都是向上了，有序程度最高。因为 log2(0)=-oo，所以我们计算 l@HOg2 (巧二 0。

总结来说就是，无序程度越高，熵越大。我们把小磁针向上的概率从 0 到 1 对应的熵 画出来，如图 2-20b所示，熵在最混乱的时候，也就是 Pup=PdQwn=0.5时达到最大。

下面来介绍熵和平均编码长度。

信息论中，熵代表着根据信息的概率分布对信息编码所需要的最短平均编码长度。举 个简单的例子来理解一下这件事情：假设有个考试作弊团伙，需要连续不断地向外传递 4 选 1 单选题的答案。直接传递 ABCD 的 ASCII 码的话，每个答案需要 8 个位(bit)的二进 制编码，从传输的角度，这显然有些浪费。信息论最初要解决的，就是数据压缩和传输的 问题，所以这个作弊团伙希望能用更少的 bit 编码来传输答案。很简单，答案只有 4 种可 能性，所以二进制编码需要的长度就是取 2 为底的对数：

log2(4)= 2

2个 bit 就足够进行 4 个答案的编码了(00,01,10,11)。在上面这个例子中，其实隐含 了一种假没，就是 4 个答案出现概率是相等的，均为 77=1/4，所以编码需要长度的计算可 以理解为如下的形式：

log2(4)= log



-log2(l/4) = -log2(7?)

此时已经有些像熵的定义了。回顾一下熵的定义，正是_log2(p)的期望值，所以我们把 这个思路也套用一下：

H(^) = £[-log2(P(^))] = -    P(x)log2(P(x))

xe{A,B,C,D}

这正是熵，因为 ABCD 出现的概率均为^=1/4，所以上面式子算出来结果刚好是 2。 从这个角度，嫡就是对每个可能性编码需要长度的期望值。

答案出现概率相等的例子可能并不贴近实际，有人认为“不知道选什么的时候就选 C”，

这个信息是可以帮助作弊团队改善编码长度的。假设 A 出现的概率不变仍为 1/4, C出现 的概率变成了 1/2, B 和 D 则分别是 1/8: P(A)=l/4, P(B)=l/8, P(C)=l/2, P(D)=l/8。在这 种情况下，考虑到传递答案的过程中，C出现的次数(概率)最高，所以可以为 C 用短一 些的编码，而出现次数较少的 B 和 D 则可以考虑用长一些的编码。这样的话，平均下来， 对于一定的信息总量，需要的编码总长度就会少一些。根据熵的定义的思路，对于出现概 率为 P 的事件，考虑用长度为-log2Q?)的二进制进行编码。所以考虑如表 2-2所示的编码。

表 2-2按出现概率进行最短编码

| A    | B    | C    | D    |
| ---- | ---- | ---- | ---- |
| 10   | 110  | 0    | 111  |

下面对照熵的公式来计算一下编码长度的期望值，也就是平均编码长度:

H⑷=-Z P(x)log2(P(x))

xe{A»B,C，D}

=一〔去 l0g2 (去)+ 去 10§2(|) + | l0g2 (去)+ 去 l0§2 (I)〕

=—x2 + -x3 + -xl + -x3 = 1.75 4    8    2    8

这个编码长度比 2 还小，并且不是整数。再详细点理解一下，假设 f 弊团伙要传递 200 个答案出去。为了方便说明，这 200 个答案中 ABCD 出现的次数恰好念严格和其出现概率 成比例，也就是 A50 次，B25次，C100次，D25次。所以传递 200 个答案一共需要的 bit 数是：

50x2 + 25x3 + 100x1 + 25x3 = 350

那么平均下来就是每个答案耗费了 350/200=1.75个 bit 编码长度。在实际情况中，并 不是每个信息都可以单纯按照上面的两个例子进行二进制编码。比如一个事件出现概率为 0.3，那么我们并不知道该如何用一个_log2(0.3)=1.74个 bit 的二进制编码表示。但是平均编 码长度的概念是可以拓展的，代表了对随机变量的平均不确定度的度量。比如 ABCD 这 4 个答案出现概率相等时，是一种最无序最不确定的状态，谁也蒙不准下一个答案是什么。 但是如果 C 出现概率高了，那么答案就显得不是那么没规律，考试者选 C 时的信心就高了 一些。也可以回到小磁针的例子理解一下。当向上向下概率均为 0.5时，小磁针指向的不 确定度最高，如果要传递的信息至少需要 1 位编码，当向上概率逼近 1 时，如果很确定小 磁针的指向，那还需要编码吗？

最后，举例只是为了举例，考试作弊属于三观不正，另外据笔者亲身经历，选择题选 C的论点其实是个谎言。

2.2.4 最大似然估计(Maximum Likelihood Estimation, MLE)

在机器学习中，常常面临的一个问题通常是有了观测到的数据，需要一个模型分布来 解释，这个时候最大似然估计是一个不错的方法。首先来了解一下似然函数。

（公式 2-25）

L{e\X} = P{X\6}

直观理解就是给定了观测到的数据，和分布的形式，把分布的参数作为输入，得到在 该组参数下观测到的数据*在该分布下的概率。根据似然函数的描述，有个很自然的问题 就是，如果给定观测的数据和分布，如何才能找到一组参数，让分布和数据最大程度地吻 合？这个问题就是最大似然估计要解决的。顾名思义，最大似然估计要解决的是下面问题。

4 = argmax[L（0| JT）]    （公式 2-26）

具体来说，对于一组观测到的数据｛xlyx2，...A«h最大似然的参数为下面的问题：

^ = argmax    | 0）    （公式 2-27）

用一个简单例子来看一下，如图 2-21所示，为一组观测到的数据在 3 组不同的正态分 布参数下的可视化例子。



图 2-21最大似然估计的例子

?

图 2-21左边是标准正态分布，均值;z=0，标准差 0=1，求出似然函数值为 6.03x10' 可以看到显然拟合度不是很好，样本都在分布的右半边。中间是随便取 0=3，求出似 然函数值为一个更小的值 2.37x1（Th，视觉上的拟合度更差了，都集中在中心区域。右边是 似然最大的一组参数//=0.77, a=0.58，似然函数值 2.91x10'样本看上去也像是服从这个 分布了。

在实际的使用中样本量巨大的情况下，乘法计算量相对很大，而且连续相乘可能会得 到非常小的值。无论是从数值计算的效率还是误差的角度，都是希望避免的情况。所以比 起原始的似然函数，实际应用中使用的一般都是对数似然函数（log-likelihood）。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-58.jpg)

6-arg max    [P（调

arg max

e

J log （尸（调）    （公式 2-28）

采用取对数后，不仅将乘法化成了加法，并且求导也因为 log 函数的性质变得更简单。 求最大似然常常需要优化算法，求导的简化让算法效率得到了提高。另外 log 也会让不同 量级的数值更加容易比较，也会优化求最大似然的数值计算效率和稳定性。关于更多优化 算法和数值计算的知识，本章后面会讲到。

2.2.5    KL 散度(Kullback-Leibler divergence)

2.2.4节中讲了如何让数据和分布的吻合度最高，也就是说似然函数可以衡量数据和分 布的相似度。另一种常见的衡量相似度的办法是 KL 散度，定义如下：

久 L…lie) = ZPW(公式 2-29)

xed    Q\X)

乍一看并不是一个很直观的定义，下面来展开一下：

= -[P(x)lOg(2(x)) + ZP(x)lOg(P(x))    (公式 2-30)

xeQ    xeQ

=H(P,Q)-H(P)

首先将 log 里的项拆成减法，然后展开。注意看第二行，后面的一项恰好是熵的定义， 只不过对数底不一样(2为底叫做 bit，e为底叫做 nat: natural unit)，还有前边没有负号。 因为考虑了不同的分布，我们把这一项记作 H(P)。前边一项 H(八 0)也眷类似熵的形式，不 同的是尸和 log(Q(x))做乘法，这一项被称做尸和 0 的交叉熵(cross entropy)。

前面已经讲解过熵代表着信息量，H(P)代表着基于 P 分布自身的编码长度，也就是最 优的编码长度。而则代表着用 P 的分布去 0 分布的信息，自然需要更多的编码长 度。并且两个分布差异越大，需要的编码长度越长。所以两个值相减是大于等于 0 的一个 值，代表冗余的编码长度，也就是两个分布差异的程度。在信息论中，KL散度的另一个 名字叫做相对熵(relative entropy)。

另外根据 KL 散度的定义，D^P\\Q^D^Q\\PV所以 KL 散度并不能作为距离度量， 尽管 KL 散度还有一个名字叫 KL 距离。

2.2.6    KL散度和 MLE 的联系

2.2.4节中讲了如何让数据和分布的吻合度最高，因为 KL 散度表示的是两个分布的差 异，所以最小化 ICL 散度是等效于 MLE 的。下面用一个不严谨的推导来说明这件事，还是 从 KL 散度的展开出发。

Acl (川 l2) = — 2LpWlog(2W)-"⑺    (公式 2-31)

xeQ

假设真实分布式 P，采样的分布是 Q。从 Q 中抽样了 n个样本｛xl9x2，...^｝，来求出对 P(x)的估计：

其中＜5(x)是狄拉克函数，当 x=0时 J(x)=l，否则(5(x)=0。把这一项带入到公式 2-31， 得到：

1 n

£>KL(PH2) = -Z~Z^(X/_X)1°g(2(X))-//(P)

xeQ z=l

因为是采样离散的值，所以 4 中的项只有的时候狄拉克函数才为 1，也 «=1

就是说这项可以化为 1，所以有如下：

1 n

^KL（^iie）=—Eiog（e（xz.））-H（p）

n /=i

可以看到第一项除了前面的系数-l/n和后边的一项//（P），其实就是对数似然函数。这 样最小化 KL 散度就和 MLE 建立了联系。

2.3维度的诅咒

维度是数据的一个非常重要的性质，在第 2 章讲的一些线性变换的性质，在二维情况 下进行形象理解后，这些性质是可以推广到高维情况的。然而并不是所有低维度的性质和 现象都可以推广到高维情况，在高维的世界里，有许多低维空间中顺理成章的事情不再成 立，并且由于很难形象理解，这些高维度中的变化往往是让人感到非常头疼的。这些高维 空间中的困难就被称作维度的诅咒（Curse of Dimensionality） o

维度的诅咒体现在很多方面，本书只对两个大方面进行一些简单的探讨。我们会从最 基本的一些性质出发，进行简单的推导，如果对推导不感兴趣的读者，可以略过这些细节, 定性理解结论就好。

2.3.1采样和维度

数据对于机器学习算法就像燃料之于火箭，本节先来看看高维度对数据的诅咒。先考 虑一个简单的情况，对于一个长度为 1 的线段，均匀地采样 10 个点，如图 2-22a所示。



c)

图 2-22采样数量随着维度的变化

而到了二维情况，以同样的间隔进行采样，则需要 102=100个，如图 2-22b。对于三维情况 则是 103=1000，如图 2-22c。也就是说如果要保持每一维度的采样间隔，则采样点数量随着维度 是指数增加，相应的计算在高维情况下是不现实的，这便是一个高维下的诅咒，我们可以定性 将这个问题理解为高维度下如果要对采样空间有一个足够的认识，需要的样本是指数增加的。

要解决这个问题，也许有人要提到蒙特卡洛(Monte Carlo, MC)方法的思想，没错， 蒙特卡洛是高维度下采样和求解积分的一个非常有效的手段，不过随机采样的效果其实也 是受到样本维度和积分要求解的函数的复杂性影响的。当然蒙特卡洛方法和维度的关系不 是我们要探讨的重点，不过可以从随机采样的方法出发，一步步来看看一些其他的高维空 间下的奇妙性质。

2.3.2高维空间中的体积

我们来考虑一个最简单的情况，在一个二维空间里的单位圆里均匀采样。最简单的想 法就是先用蒙特卡洛生成和单位圆外切的正方形中的样本，然后通过判断到原点的距离保 留下在圆内部的样本，如图 2-23a所示。



图 2-23在二维球体(圆)和三维球体(球)中进行随机均匀采样

这种方法虽然简单，但是有一个问题是位于圆外的采样点被浪费了，根据圆面积公式 和正方形面积公式可以算出这种办法的采样效率为：

7ir2 _ 7tr2 _ 7i

(2?7=47~4

其中 r 为半径，在图 2-23例子中 r=l。可以看到，虽然浪费了边角的采样点，但是采 样效率还是接近 80%的。接下来看看三维空间中的情况，如图 2-23b，同样是先在一个立 方体内进行采样，然后保留内切球内部的采样点，则采样效率是球体积除以立方体体积。

4    3    4    2

— Tir —兀 r 3_= 3_ = 1 (2r)3    8r3    6

采样效率瞬间降至接近 50%。接下来继续考虑更高维度情况，对于一个《维超球体， 其体积为：

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-61.jpg)

(公式 2-32)

其中 r（x）为 Gamma 函数，对于《为正整数的情况，这个公式可以写成我们更容易理

解的形式如下。

n-\

n

(2tt)2 rn

F(«-2)

(公式 2-33)

2x4x6x."(n-2)xn

，n是偶数

公式前半部分虽然更容易理解了，不过还是很复杂，我们重点看后半部分，对于第《 维超球体而言，其体积就是〃-2维超球体乘上 2nr2ln，是一种递归关系。对于单位球体而 言则是 2k/〃，注意到当^<7的时候，这一项是大于 1 的，也就是说随着维度的增加，体积 是增加的。而《>7时，这一项小于 1，于是随着维度的增加，单位超球体的体积是越来越 小的。所以总体来说，随着维度的增加，超球体的体积是先上升后下降的（超球体的表面 积也有类似趋势，有兴趣的读者可以自行调研）。如果把 1 维也考虑进来，则对于单位超 球体，其体积随维度的趋势如图 2-24所示。

如果之前从没有接触过高维空间，这个结论可以让我们从体积角度对高维空间有了一 个初步的反直观的认识。可以看到，随着维度的增加，单位超球体的体积是趋于 0 的。然 后再回到采样的问题上，知道了 n维超球体的体积，只要除以维超立方体的体积 r=（2r）” 就能求出高维情况下的采样效率，公式这里就不再展开了，直接看结果，如图 2-25所示。



图 2-24单位（超）球体体积随维度的变化    图 2-25单位（超）球体体积占外切立方体的体积比

可以看到，到了第 10 维的时候，采样效率已经很接近 0 了。这又是一个不那么直观的 结论，这个结论也让我们明白一开始提到的在超球体内采样的方法是完全不现实的。从另 一个角度来理解，这代表随着维度的增加，随机采样的点几乎都不会出现在单位超球体内， 也就是说离原点的距离都大于 1。我们进一步想一个问题，如果不要求保留单位超球体， 而是保留半径为 2、为 5，或者一个更大数的情况呢？分析的方法也类似，只要看随着维度

的增加，总会超过 27：r2，所以对于 n»27rr2的情况，球体内出现采样点的概率总会趋向于 0。直观上来理解就是所有的点在高维空间里都会远离圆心，而且随着维度越高，就离得越 远，为了更好地理解这个现象，2.3.3节将从另一个更加基础的度量来考察一下。

2.3.3高维空间中的距离

首先还是考虑 n 维超立方体中的均匀采样，先考虑低维情况，从二维开始，我们来计 算一下均匀分布的样本到原点的欧式距离的分布。因为是均匀分布，所以样本到原点距离 的累积分布函数(Cumulative Distribution Function, CDF)分布实际上可以看作是随着圆 形半径的扩大，正方形内部的圆的面积占正方形面积的比，因为圆和正方形都是沿着 x 轴 和轴对称，所以实际计算的时候，为方便我们可以只考虑第一象限内的单位正方形内的 情况，如图 2-26a所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-63.jpg)



a)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-65.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-66.jpg)

图 2-26求解均匀分布样本到原点距离的分布

在图 2-26a的右图中，方形面积为 1，我们用表示弧长在单位正方形内的部分，则 CDF 为：

外)=

因为 F(l)就是正方形面积，所以样本到原点距离的概率密度函数(Probability Density Function, PDF)就简化为

;?(r) = /(r)

计算分为两种情况，一种是时，如图 2-26b所示，很简单就是直角对应的弧长， 也就是圆周长的 1/4，等于 7tr/2。当 r>l时，如图 2-26c所示，弧长为 2 咖，而 cos(7t/4-^)=l/r,

所以二维情况下，正方形内均匀采样的样本到原点距离的分布如下:

丌厂



,r^l

,r〉l

（公式 2-34）

对于其他维度的情况，也可以分为在超球体内和超球体外的不同区间分别求出分布， 不过比起二维情况要复杂得多，我们不再花时间在公式上，直接来看看二维〜四维情况均 匀采样的样本到原点距离的概率密度分布，如图 2-27所示。



图 2-27二维到四维空间中均匀采样点到原点距离的概率密度分布

2.3.4中心极限定理和高维样本距离分布的近似

从图 2-27中可以看到，随着维度的增加，在超球体内部的样本到中心距离的分布是呈 指数上升趋势的，也就是说在原点附近的样本数量是急速下降的，从图 2-27中，x=0.25所 示的虚线就能清晰地看到这个趋势。而另一方面，在四维的时候，可以看到分布在单位超 球体表面附近的样本已经不是最多的，总之，从二维到四维己经能看到样本总体开始远离 中心的趋势，符合上一部分中观察到的结论。那么更高维的情况呢？能否分析出一个很明 确的趋势呢？这时候可以考虑借助概率论中经典的中心极限定理，首先简单介绍一下中心 极限定理：

对于 Z7 个独立同分布变量及，為，...，見，如果采样分布的期望为…方差为 a2，则当 n 足够大时，样本之和的抽样分布可以近似为均值为方差为的正态分布。

根据中心极限定理，可以把均匀分布中的一个样本的每一维都看作是一个独立同分布 的样本，到原点的欧式距离的平方则是每一维的值的平方之和，所以当维度足够高时，均 匀分布样本到原点距离的平方也近似服从正态分布。

考虑我们的例子，每个维度都是一个-1〜1的均匀分布，其方差为：

而求方差的一个公式如下:

Var{X)= E(X^-E^    (公式以)

其中£(X)为 0，所以 A2 的期望值也就是每一维平方的采样平均值为：

/Z=£(^2) = Far(^) = |

同样，根据方差公式 2-2i，对于 y 的方差有：

M^)=叫 M2)-五 M2 =去-去=去

所以对于〃维空间每一维在-1〜1均匀采样的样本，到原点距离的平方服从：

描述的分布，而该分布的标准差线性依赖于 i/V^，而均值则线性依赖于 1 加，所以标 准差和均值的比线性依赖于 7^加，也就是 1/士，也就是说随着 Z2 趋于 00，标准差和均值的 比值是趋于 0 的。形象理解就是，在高维空间中的超立方体中的均匀采样，每个点到原点 的距离都是差不多的。

而且这个结论对于任何独立同分布的情况都是适用的，比如在实际应用中最常见的正 态分布，如果每一维都是互相独立的相同的标准分布的话，则〃维情况下，样本到原点的 距离 r 服从 chi 分布(就是卡方分布的开方)，其概率密度函数为：

P（r，n） =

1 2

2 2 rn~\

（公式 2-36）

其中 r(x)为 Gamma 函数。我们把这个分布在《=100,000和《=10 000的曲线画出 来，如图 2-28所示。



图 2-28高维度卜正态分布采样点到原点距离的分布

可以看到，在高维正态分布中，采样点到原点距离都集中在很小的一个“峰”里。除 了到原点的距离，在高维空间中，两点之间的距离也会有趋同的趋势，比如我们还是考虑 最为常见的正态分布，因为正态分布的对称性，对于每一个维度，两个样本的差就相当于 两个相同正态分布下样本的和，而对于正态分布，有如下性质。

考虑两个独立的正态分布以及从中抽取的样本及和為：

考虑这两个样本的和)^及+及，有：

也就是说 r 服从一个正态分布，其均方差为两个分布的均方差之和，所以两个样本之 间的距离和样本到原点的距离一样，也会随着维度的增加渐渐趋同。所以在高维度空间中， 距离成了一个不再那么可靠的度量。比如在二维空间中，我们会很直观地认为两个距离很 近的样本会有更大概率拥有相同或相近的属性，而在高维空间中，这样的判断就很不可靠 了。除了公式的推导，从定性的角度可以理解为，随着维度的增加，每一维度对距离的贡 献都变得不再重要，所以在独立分布下，各个维度最后对总距离的贡献的效果都被平均掉 了。另一方面也可以理解为，维度虽然增加了，不过采样数量通常不会随着维度的增加而 指数增加。所以维度越高，样本在高维空间中是呈现越来越稀疏的趋势，定性来看就是距 离其他样本越来越远了。

2.3.5数据实际的维度

前面列举的高维度例子，数据的每个维度间都是互相独立，然而这并非大多数实际应 用中的情况，在实际的应用中，数据的每个维度之间通常都会有相关性，而这种相关性通 常会让高维的“诅咒”不再那么可怕，下面先来看一个非常简单的二维例子，如果一个协 方差矩阵为

1    0.99

0.99    1

中心在原点的二维正态分布，50个采样如图 2-29所示。

可以看到，样本其实分布在所在的直线附近，所以如果把每个样本都投影到

所在的直线的话，就相当于把二维降到了一维，而降维后的样本也是可以近似描述原始的 分布的。这种情况在高维空间中也是很常见的，因为样本本身的相关性，所以一个几千或 者几万维的数据，很可能实际用一个几百维的数据就可以近似描述了。

上面提到的是统计相关性只是一个方面，再来看另一个简单的例子，考虑一个二维分 布，是由 l-(x-0.3)2-O-0.7)2再加上一个很小的噪声产生，如图 2-30所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-70.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-71.jpg)

图 2-29强相关的二维正态分布样本    图 2-30可以用一个曲面近似描述的三维样本

理所当然的，这样的样本可以用产生这些样本的曲面近似描述，也就是 /x，y)=l-(x-0.3)2-Cv-0.7)2，所以实际数据的维度也是不到三维的，更近一步，如果样本是 三维空间中的一条直线加上小噪声产生的，那么理所当然的这些样本可以被直线近似描述， 实际的维度只有 1。所以虽然高维的“诅咒”听起来非常可怕，但实际上更常见的情况是， 数据常常是在一个等效维度更低的子区域中。

除了这些抽象的样本，实际应用中接触到的样本更是如此，比如我们来考虑一个 100X 100分辨率的灰度图像，每个像素的取值为 0〜255，则可以看作是一个 10000 维的数据， 每一维度的取值为 0〜255。如果每个像素都是服从均匀随机分布并且互相独立，则可能 的图像样本一共有 25610 胃个，在这种情况下，我们看到的图像大多会是如图 2-31a所示 的情况。



b)

图 2-31每个像素独立的图像采样和实际生活中图像的采样

而在实际生活中看到的图像，如果缩放到 100X100 分辨率的灰度照片的样本数量，则 会远小于 2561g()()()，比如图 2-31b中的例子。这也是一种相关性的表现，比如 2-31b中海鸟 身上的像素，因为毛色较纯，一个白色像素周围的像素通常来说也会是偏白色的，这就是 一种正的相关性。我们视觉能识别的图像，其实背后隐含的就是一种空间上的相关性。所 以相对于 10000 维的空间来说，能被表示成人类可识别图像的 100X100 分辨率灰度图，只 是在这 10000 维空间里的一个非常小的子区域内。

2.3.6局部泛化

2.3.5节花了不少篇幅介绍了数据和维度的关系，而真正将数据利用起来并处理解决实 际问题的还是从数据到结果的映射，也就是函数。数据和函数综合在一起的效果，也就决 定了一个问题的复杂度。

在机器学习中有一个常见的概念叫做局部泛化(Local Generalization)，意思是对于两 个输入样本 x 和 X1，如果这两个样本之间的距离很近，则对于要学习的函数，而言，/x)和 D的值应该也相差不多。下面通过图 2-32中的一维例子来形象理解一下。



图 2-32 —维例子理解局部泛化

对于三个 X 轴的一维样本 Xo, X!和其中％0和^很靠近，X2离得较远，则 Xxo)和 的值会比较接近，而/%2)和/%0)//^)都差得比较多。所以这是对函数值平滑性的一个自然 而然的结论。很多非参数机器学习方法背后正是基于这一思想，比如 SVM, KNN和高斯 过程(Gaussian Process)。

而到了高维度下，基于局部泛化的方法也会遭遇维度的“诅咒”，首先，基于局部泛 化的方法有个假设是函数足够光滑，如果函数不那么光滑的情况下，需要描述函数的样本 就会随着函数的复杂度而增加。比如图 2-32中实线的情况，如果在 x2 左边函数的极小值 处取一个样本，再在附近取一个样本，利用二次方或者其他的插值方式就已经能够较好 的近似了。我们把问题简化一下，把左边阴影的区域看作是一个区域，右边看作是另一个 区域。如果/在这两个区域的值有较大差别，则需要两个不同样本描述。那么考虑二维情 况，第二个维度上也可以找到两个区域，所以空间被划分为 22=4个区域，如果函数在这 4 个区域上的值都有差别，比如类似 XOR 的函数，则至少需要 4 个样本来描述。依此类推， 对于《维空间，如果每一维都有两个区域，且和其他维度形成的全排列都对应一个独特的 值，那么则至少需要 2”个样本对函数进行近似描述，本质上讲，就是说需要描述的函数是 非常不平滑的。

这种指数级上升的样本需求，显然在高维情况下是难以达到的。当然，并不是每一个 函数都非常不平滑，但是另一方面，也不是数据的每一维度对函数只有两个区域的不同取 值，所以总体而言，随着维度的增加，用局部泛化来描述一个函数是对样本数目有着指数 增长的需求的。这就是为什么对于有些高维情况而言，用 SVM 训练出的模型，所有样本 都是支持向量(Support Vector) o

除了函数本身复杂度的影响，数据本身在高维空间中的特性也会影响局部泛化。在 2.3.5节中也讲过，随着维度的增加，样本在空间中的分布趋同和趋于稀疏，所以局部的概 念就变得弱了。从一个更简单的角度来理解，可以认为在低维度下，一个样本能罩住的局 部的范围相对整个样本空间来说的比例比起高维的情况大，比如图 2-22中，一维情况下 0.1间隔的样本只需要 10 个就能罩住整个单位空间了，而到了三维则需要 100 个，更高维 度则需要更多，而且是指数变化。所以如果采样的数目不变的情况下，高维度局部泛化的 能力显然就没有低维空间强了。

2.3.7函数对实际维度的影响

2.3.6节讲过虽然数据本身维度可能很高，不过大多数实际情况¥’；数据是分布在一个 低维度的子空间内。当数据和函数一起作用时，也有类似的情况。比如一个简单的情况， 考虑在一个足够小的局部空间内，函数可以被线性近似，则有如下表达式：

,(x)~aw2+…认

在这样一个可以被线性表达式近似的局部空间内，线性系数绝对值的大小就决定了每 个维度对结果贡献的多少。通常来说这些系数是不相等的，事实上很常见的情况是这些系 数里只有很少的部分有较大的绝对值，其他大部分都会相比起来非常小甚至可以忽略不计。

当然这里举的例子是线性近似的情况，是一个帮助理解的最简单的例子。对于大多 数图像相关的问题，类似的性质是普遍存在的，通常来说最常见情况是，对于一个输入 X，有：

x* = /(x)

X’是经过变换得到的变量，如果 X’本身维度就很低，或者可以在某些情况下被近似成一 个低维度的向量则通常经过一个逆变换或是近似逆变换的函数得到的值是可以近似原 始的输入的。

I〜广(<)

只要厂对 X 的近似足够好，则真正处理问题的时候就不用面对原始的、可能维度 很高的而是也就说在函数作用下的的维度才是要面临的实际维度。

在有些变换下，X’本身维度就不高，直接达到了降低维度的效果。而有的情况则需要 做一定的近似得到一个等效的低维向量 X’5，比如下面的例子：

x* = (3.0, 0.003, -0.01, 2.4, 0.05, -0.02, -0.005, 0.0001, 1.0, 0.025)

和之前线性近似中系数的性质类似，少数维度很大的值，其他维度的值小到可以忽略。 于是就忽略掉那些很小的维度，得到下面的只有少量维度不为 0 的近似。

x1 - x's = (3.0, 0, 0, 2.4, 0, 0, 0, 0, 1.0, 0)

这样等效于实际处理的维度降低了，在这种形式中，大多数值都是 0，少数起实际作 用的非 0 值很稀疏地分布在一些维度中，所以这种形式就称为是一种稀疏的表达。

2.3.8    PCA——什么是主成分

既然高维度带来了如此多的灾难，一个很自然的想法就是降低维度，然后在低维度下 解决问题。比如 2.3.7节里提到的 x’=/(x)，如果 x’相对于 x 是一个维度很低的向量，则相当 于达到了降维的效果。关于降维，先举一个最简单的例子，即主成分分析(Principal Component Analysis, PCA)

考虑如下的协方差矩阵对应的高斯分布的样本，如图 2-33所示。



图 2-33有相关性的二维高斯分布样本

可以看到，因为 x 和轴有相关性，所以样本呈现出沿着方向分布的趋势。如果 用一个椭圆将样本“圈”起来，则样本的偏差最大的方向是沿着椭圆的长轴，最小的方向 是沿着椭圆的短轴。所以在这个例子中，可以认为沿着长轴的方向就是数据变化剧烈程度 的主要方向，稍微正式些的说法就是数据在这个方向上的投影拥有最大的方差，这个方向 也就是主成分所在的方向。而短轴所指的方向，同时也是和长轴正交的方向，就是在这个 二维例子中数据变化剧烈程度最小的方向。

2.3.9 PCA——通过本征向量和本征值求主成分

那么应该如何求出这个变化剧烈程度最大的方向呢？还是考虑图 2-33中所示的样本 X， 如果这些样本是经过一个标准差为 1，两个维度相互独立的高斯分布样本芥；，先经过沿每 个轴的缩放(£)，再乘以一个正交矩阵 C/旋转而得到的，如图 2-34所示。



图 2-34有相关性的二维高斯分布样本

所以有：

X = ULXV

并且中的每个行向量就是要求的投影方向，其中对应缩放值最大的方向就是主成分 所在的方向。之所以有这么别扭的一项，是为了方便接下来的推导公式：因为 I 的每个维 度的均值都是 0，所以根据协方差矩阵 Z 的计算公式，有如下：

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-76.jpg)

（公式 2-37）

（公式 2-38）

其中;V是样本个数，因为&是方差为 1，且两个维度间没有相关性的样本，所以有:

去°]=[o 1.

另一方面 I 是个对角矩阵，将这两个结论带入，如下： z = ulltut =UL2Ut

是不是觉得眼熟？回忆一下 2.1.6节中的讲解，这正是在求本征向量。虽然这里并不是 一个严格的证明，但是从直观的角度联系起了协方差矩阵的本征向量、本征值和主成分方 向及大小的关系。讲到这里基本就很清晰了。所以我们来求一下图 2-33所示的协方差矩阵 的特征值和特征向量，得到结果是本征向量为（斤/2, W/2）和（-W/2, V2/2），对应的本征 值分别为 1.5和 0.5。可以看到本征值大的显然就对应着最主要的成分，本征值小就对应着 不那么重要的成分。协方差矩阵的本征向量代表的几何含义已经很清楚了，而根据前面对 协方差矩阵公式的推导，本征值就是样本投影到对应本征向量上之后的值的方差大小。所 以从图 2-33中所示椭圆的角度看，主成分分析中主成分大小和本征值的区别在于数据分布 所在的“椭圆”的轴的长度是正比于本征值开根号（标准差），而不是本征值本身。

• 61 •



2.3.10 PCA——通过主成分分析降维

前面所讲例子中数据的相关性并不是很大，下面用图 2-29中强相关的例子来测试一

下，其协方差矩阵的本征向量仍然是（人/2, W/2）和（-W/2, 72/2），对应的本征值分别为 1.99和 0.01。我们把原始数据投影在两个本征向量上之后的新的数据分布画出来，如 图 2-35b所示。



*

投影





b）沿着协方差矩阵本征向量方向投影后的样本

-1

一 2

•- 4_I_I_I-L-

-2-1012

a）强相关的二维样本

4近似

| /    |      | 2    |          |             |       |
| ---- | ---- | ---- | -------- | ----------- | ----- |
| • •  |      | 1    |          |             |       |
|      |      |      |          |             |       |
|      |      |      |          |             |       |
| *    | 重建 | 0    | • ••- •  | • ......... | ••….- |
|      |      |      |          |             |       |
| Z    |      | -1   |          |             |       |
| ✓    |      | —2   | -3    -2 | -1 6        | 1 2   |

d）只保留主成分之后样本在原空间的重建

c）只保留主成分之后样本在主成分方向的投影

图 2-35通过 PCA 二维降维到一维的例子

如图 2-35所示，根据前面矩阵乘法的意义，我们既可以把这个过程按最原始的理解， 认为是样本在两个正交方向上的投影，也可以理解为将样本旋转，让方差最大方向处在 x 轴。无论哪个理解，都可以看到数据的长短轴比是〜1.4/0.1，数据在两个轴上 的分布，无论是按照方差还是标准差，^轴的成分相对于*轴的主成分而言都非常小，所 以可以只取第一个主成分，忽略掉第二个成分的贡献，这样就能利用其对应的本征向量把 二维数据降到一维，也就是从图 2-35b到图 2-35c的过程。而这个过程相当于只保留了本 征值最大向量对应的本征向量的方向，具体到计算就是让数据和这个方向的本征向量做点 积。在这种近似后，可以把数据再“转”回原来的空间中，如图 2-35d所示，可以看到虽 然损失了一些信息，但是大体保留了样本分布的形状，是一种不错的近似。

这些例子讲的是二维的，推广到高维也非常直接，对于一个《维的数据，只要知道了 数据的协方差矩阵，就可以直接对协方差矩阵求本征向量和对应的本征值，然后按照本征

值排序。如果数据的维度间存在很强的相关性，则会观察到排在前面的本征值明显大于后 面的本征值。这个时候就可以根据取前面的个本征向量作为新的 W 维空间的坐标 轴所在，达到了从〃维到 W 维的降维。降维操作也非常简单，就是把本征值最大 W 个作为 一个新的矩阵，和原数据相乘就可以了。整个过程如图 2-36所示。

Y=LrX'

|                | ] J               |
| -------------- | ----------------- |
| -• • 0MS5U     |                   |
|                | •• U "::.mirnf} ■ |
|                | .MCSUU) J         |
| ••             |                   |
|                | ::s;              |
| 执行降维 <^==I |                   |

减均值，归一化

求协方差矩阵

%

=>

求本征值和本征 向量并按照本征 值大小排序



大的前 m 个主成分 <==

| r---义 1 又 2 |      |
| ----------- | ---- |
|             | Un   |

图 2-36主成分分析降维的流程

注意到数据一上来有一步减去均值再除以标准差的操作，这个预处理通常是为了避免 各个不同维度间变量量级差距太大带来的影响。减去均值在机器学习中是对数据预处理的 一种基本操作，除以标准差则视需要而定。比如考虑二维情况，第一个维度方差是 1 000, 第二个维度只有 1 的话，那无论这两个维度间的相关性是多少，经过 PCA 之后找到的主成 分方向都近似第一个维度。当然是否一定要做这样的预处理视情况而定。

有的时候除了对于输入数据要做这样的处理，对于 PCA 降维后的数据，根据后续处理 需求的不同，也会需要做类似的操作。前面已经讲了变换后的数据每一维度的标准差就是 本征值开方，所以这个处理如下：

r, =_1=丄

1 '拉

在实际应用中，有可能因为选取的主成分比较多，而数据本身在一些维度上相关性极 强，排在后面的本征值会过小，导致白化后得到一个过大的值，这种情况需要在分母里再 加一个很小的但是计算可以接受的项&

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-81.jpg)

这个操作通常被称为 PCA 白化。

2.3.11 PCA——归一化和相关性系数

在前面内容中介绍过相关性系数 r，根据定义再对照 PCA 降维中的预处理步骤，会发

现预处理之后其实就相当于基于相关性系数矩阵执行了 PCA。这样做的好处除了前面提到 的降低不同维度量级影响，最根本来说是直接获取了维度间相关性的信息，比如图 2-37所 示的例子。

1    0.5

r —

9

1.5

1.5

1

0.5    1



-5    0    5

a)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-83.jpg)

图 2-37两种强相关分布的例子

仅看协方差矩阵 Z 的话，很难直观感受出相关性的大小，而直接看相关系数矩阵的话, 则一目了然。这种直观性也体现在画出来的样本分布上，从图 2-37b中更容易一眼看出两 个维度的正相关性。

总的来说，如果正相关性越强，则值越接近 1，负相关性越强，值越接近-1，没有相 关性则为 0。比如下面图 2-38中所示的 5 个例子。



^1    r=0.5    r=0

图 2-38不同相关性下典型的分布

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-85.jpg)

2.3.12 PCA——什么样的数据适合 PCA

下面举两个例子说一下，如图 2-39所示。

图 2-39a的样本，两个维度的相关性只有-0.04，可是一看就知道，样本之间有着非常 强的相关性，是处在一条螺线上。

图 2-39b的样本，两个维度的相关性高达 0.95，但是如果做了 PCA并降维到一维的话, 显然 Z 字形的信息就全部丢失了。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-86.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-87.jpg)

图 2-39两种强相关分布的例子

在 PCA 中，我们说到相关性，指的是统计意义上的相关性，所以适用于 PCA 的数据 也是统计意义上有强相关性的数据。像是图 2-39中所示的这两种例子，非线性的降维办法

如局部线性嵌入(Local Linear Embedding, LLE)等会是更有效的办法。

/

2.3.13其他降维手段

在 2.3.12节中也提到过，总体而言，如果一个输入经过某个变换后，得到的是低维的 输出，并且这个输出能够通过逆变换近似重建输入的话，这种变换就可以认为是执行了降 维。这里简单地提一下一些常见的其他降维手段，有兴趣的读者可以自行进行更多研究。

PCA是线性降维手段中非常经典的一种，为了引入非线性，人们想出了在求协方差之 前先做一次非线性变换，再求协方差矩阵的办法，这个是 Kernel PCA。能解决一些非线性 下的降维，比如一种数据围着另一种数据的情况。

PCA是一种基于数据本身的分布进行降维的手段，算是无监督的。如果已经知道了数 据的类别，一个思路是让原始数据经过投影，在低维空间上不同类别的分布中心尽可能远 离，相同类别的分布方差尽可能小，这个就是线性判别分析(Linear Discriminant Analysis, LDA) o

前面也提到过，当数据是如图 2-39所示时，在局部区域内有着非常强的相关性，而且 样本在这个局部区域内可以被一个低维度空间很好描述，那么就可以认为数据是在一个低 维的流形(manifold)上的。Isomap和前面提到的局部线性嵌入都是针对这种情况的降维 办法。

神经网络也是可以执行降维的一种手段，当对于一个输入，隐藏单元小于输入维度数 量时，常常能达到降维的效果，比如 Tom Mitchell在其经典教材《机器学习》中举过的用 神经网络重建输入的经典例子，用一个 8 输入、8输出，包含 3 个隐藏单元的单隐藏层网 络，重建长度为 8 的 one-hot编码，网络能够自动学习出 8 个二进制编码，这也是一种降 维。事实上神经网络中，降维非常常见，从各种自编码手段，到最近开始吸引眼球且英文 名字很霸气的生成式对抗网络(Generative Adversarial Networks, GAN)，都可以看到用

作降维。关于神经网络的内容第 3 章会详细讨论。

有的时候样本数量并不大，我们可以知道每两个样本之间的距离，或者是差异性的某

种度量。那么一个想法就是将这种高维空间中的距离或者差异性的大小，通过变换后在低 维空间中也得到保持。距离和差异性在低维空间中可以表示成欧氏距离，于是有了多维缩 放(Multi-Dimensional Scaling, MDS)；也可以表示成条件概率，于是有了随机近邻嵌入

(Stochastic Neighbor Embedding, SNE)可视化专用版 t-SNE。一般来说这种类型的降维 更多用于数据可视化。另外还有最近华人学者 JianTang 提出了 LargeVis，和 t-SNE相比大 大减少了计算消耗，效果拔群。

2.4卷 积

卷积神经网络是深度学习中应用最广泛的一种网络，而卷积就是这种网路的基础。本 节就来了解卷积的概念，性质和计算方式。

2.4.1点积和卷积

前面已经介绍过点积是计算两个向量相似性的一种重要度量，本节详细讲一下这种性 质，先来看图 2-40所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-88.jpg)



dot product=0 b)



c)

图 2-40不同向量点积的例子

图 2-40中是三组不同的向量求点积的结果；如果从信号的角度来看，图 2-40a中是一 个形如 sin 的信号和本身求点积；图 2-40b中则是该信号和一个形如 cos 的信号求点积；而 图 2-40c是该信号和一个随机产生的向量求点积。可以看到图 2-40a中因为两个信号相似 程度最高，所以得到的值最大是 5；而图 2-40b形

如 cos 的信号和 sin 信号点积后因为差了 7C/2的相 位，点积为 0；图 2-40c中随机信号的例子和 sin 信号则得到了负值，从视觉上来说也是差异最高的 两个信号。

JIIIJ

1UI1

所以直观上来说形状越是相似的两个向量，点 积得到的值倾向越大。当然这里的例子都是没有归

-1.0

一化的，所以并不是完全一致的信号就会拥有最大    dotproduct=6.47

的点积，比如图 2-41所示的例子。    图 2-41点积的例子

虽然如此，通过上面的例子还是可以看出，大趋势仍然是形状越是相似的向量，得到 的结果则越大，反之则越小。

2.4.2 —维卷积

有了对点积的直观理解，接下来看看卷积。先看一维情况，对于两个离散信号 f 和 g, (离散)卷积一般定义如下：

(/*g)[d=    (公式 2-39)

如果看到这个公式，尤其是对/WZ2感到很别扭也没关系，公式并不重要，直观来理解 就是，一个信号倒序之后在不同位置和另一个信号中一段同样大小的部分做了点积。以一 维为例子，如图 2-42所示。

0 1 0

zero padding 1    3 0 1 0

13

22

23

12

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-91.jpg)

1 2 3 0 21 0

O^-zero padding 1

Vo

图 2-42卷积的计算和卷积的 3 种方式

根据卷积的定义，做卷积的两个信号的顺序是没有影响的，也就是说谁是卷积核，谁 是待卷积信号都一样。不过在图 2-42中，我们规定每一行中上面的(1,2,1)作为卷积核，根 据定义，把卷积核倒序一下，就是在不同位置做点积的信号，在这个例子中还是(1，2，1)。

由图 2-42可知，整个卷积的过程就是(1，2，1)沿着一个方向(如图 2-42，从左至右)“划 过”待卷积的信号，同时在每一个位置上得到对应的点积的值。

在这个过程中，如果自始至终卷积核都在“信号内”，也就是两条虚线竖线内的部分， 则最后得到的结果的长度会小于待卷积信号的长度。假设待卷积信号的长度是〃，卷积核 大小是则结果的长度是 n-w+1。在图 2-42所示的例子中，就是 6-3+1=4。这种卷积的 方式称为 valid，结果是(8,9,4,2)。

如果卷积核的中心刚好是从待卷积信号的第一个元素“划”到最后一个元素，也就是 图 2-42所示中第二行到倒数第二行的情况，则需要把原来的信号扩展长度。一般来说扩展 的方式是在原来信号的边缘添加 0 元素，这个过程通常称为零填充(zero padding)。通过零 填充，卷积结果(4,8,9,4,2，1)的长度和待卷积信号长度一样，这种卷积的方式称为 same。

当然，也可以通过零填充把卷积核能够划过的位置扩展到最大，也就是图 242 中从第 一行到最后一行，则结果长度是 n+m-\ ，在这个例子中就是 6+3-1=8 ，结果是 (1,4,8,9,4,2,1,0)，这种方式称为 foil。

下面用一个一维卷积的例子来直观感受一下卷积的意义。我们把一个长度为 20 的 sin 信号，左右各接一个长度为 20 的随机信号作为一个新的待卷积信号，然后根据定义把一个 倒序之后就是 sin 波形的信号作为卷积核，卷积方式是 same，如图 2-43所示。



图 2-43卷积结果作为待卷积信号在卷积核上的响应

图 2-43中，面部分是待卷积信号，下面是卷积结果。可以看到，在形如 sin 的信号 划过被卷积信号的形如 sin 的区域时，卷积结果的值最大。从信号的角度，可以把卷积看 作是卷积核作为一个滤波器，卷积的结果则是被卷积信号在这个滤波器上的响应。所以大 体上越是和卷积核倒序之后相似的信号越是会获得较大的响应。

2.4.3卷积和互相关

前面提到了形象理解卷积意义的时候，相当于要把卷积核倒序排列，然后在不同位置 和相同大小的区域做点积。这个倒序总是让人感到很别扭，事实上不用倒序的操作也是有 定义的，叫做互相关，定义如下：

(/®g)h]= S /W^[w + w]    (公式 2-40)

ZM=—

因为形式的不同，互相关并不像卷积一样满足交换律。本书并不打算深究这两种的区 别，因为和机器学习相关的实际应用中，这两种操作本质上没有区别，只是形式上不一样。 另外为了方便，接下来的内容中如果出现卷积核，都默认是以互相关中的形式，除非特别 说明，否则不去探讨卷积核本身的元素排列顺序。

2.4.4二维卷积和图像响应

二维情况下的卷积和一维并没有本质上的不同，下面略过公式，直接来形象理解，如 图 2-44所示。

stride=l

-6    3

-7    -

-13 -11

0 -1 0-3

-6 0 6 -13 4 9 -10 0 10

2    4-6

-2 0-4



a)

o o 1 0 3 0 7 0 2 0 3 0 0 0

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-94.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-95.jpg)

|      | r—■ 0 |      | •0   | 0    | "o"  | !0 ： | 0    | 0    |              | 0    | 0         | 0    | 0    | 0      | 0    | 0     | 0    | 0    |
| ---- | ----- | ---- | ---- | ---- | ---- | ----- | ---- | ---- | ------------ | ---- | --------- | ---- | ---- | ------ | ---- | ----- | ---- | ---- |
| 0    | 1     | 3    |      | 4    | 1    | ：o:  | 0；  | 1    | 3 ； 2：； 4 | 1    | 0 :       | 0    | 1    | 3      | 2    | 4     | 1    | 0    |
|      | 丄    | 丄   |      | 2    | 3    | :o:   | 0;   | 5    | 3 ; 1 ：; 2  | 3    | 0 :       | 0    | 5    | 3      | 1    | 2     | 3    | 0    |
| 0    | 1     | 1    | 5    | 1    | 7    | 0     | 0：  | 1    | 1 • 5：; 1   | 7    | 0 :...... | 0    | 1    | 1-5-bJ | 7    | 0     |      |      |
| 0    | 2     | 9    | 0    | 6    | 2    | 0     | 0    | 2    | 9 0 6        | 2    | oJ        |      |      | V      |      | ! 6 : | 2 :  | o:   |
| 0    | 3     | 1    | 3    | 4    | 3    | 0     | 0    | 3    | 1 3 4        | 3    | 0         | 0    | 3    | 1      | 3    | ! 4 : | 3 :  | o:   |
| 0    | 0     | 0    | 0    | 0    | 0    | 0     | 0    | 0    | 0 0 0        | 0    | 0         | k    | J    |        |      | hJ    | o:   | o:   |

b)

图 2-44二维卷积，图像的响应

如图 2-44a所示，待卷积二维信号是个 5X5 的矩阵，为了执行 same 方式的二维卷积, 在这个 5X5 矩阵的外围各加了长度为 1 的零填充称为一个 7X7 的矩阵。卷积核则是一个 3 X3的矩阵，最左边一列为 1，最右边一列为_1，中间一列为 0。

图 2-44a中最上面的例子是卷积核在 7X7 矩阵上无论是横向还是纵向，每次划动一格,

因为是 same 方式，最后生成一个和原矩阵大小相同的 5X5 矩阵。虽然这个例子中矩阵很 小，但在应用到实际图像时，因为图像是二维的，所以卷积核需要计算点积的位置数目和 图片边长像素个数成平方比。这种情况下，如果还是以每次划动一个元素的方式做卷积， 通常计算量并不小，所以在二维卷积中一个常用的手段是按一定间隔“划动”卷积核，达 到降采样的目的。而这个划动间隔被称为 stride，所以每次移动一个元素就是 stride=l。而 图 2-44a中最下面的例子就是每次移动两个元素的情况，也就是 strides，最后生成一个 3X3的矩阵。从维度的观点来看，这也相当于是一种降维。

在图 2-43中，我们看到了一维信号经过特定的核卷积之后，生成的大致是信号中与核 相似部分的响应。在二维信号中，也有类似的性质，如图 2-44b所示，左边是一张砖瓦的 黑白照片，有明显的横向纹理和竖向纹理。我们让图 2-44a中的卷积核和这张照片做卷积， 就会得到一张响应图。为了方便显示，把得到的结果取绝对值，就是图 2-44b中右边的图 像。可以看到，虽然原图中横向和竖向的条纹都很明显，但是经过卷积之后，只有竖向条 纹对应的位置得到了明显不同于其他区域的响应。所以形如图 2-44a中所示的卷积核，在 这个例子中起到了纵向边缘查找的作用。

2.4.5卷积的计算

首先进行一个约定，接下来的内容里如果提到卷积，一般都是指二维卷积。卷积的计 算分为两大类，一类是将卷积操作转换为矩阵乘法，还有一类是利用卷积在频域的特性将 卷积转换为频域的乘法。我们不打算探讨太多细节，大概了解一下这两类办法。

1.用矩阵乘法计算卷积

/

把卷积转换为矩阵乘法的思路非常直接，既然卷积在给定位置上的计算本质上是个点 积，那么就可以把卷积的全过程转化成不同位置的点积计算。下面以图 2-45中所示一个最 简单的例子来讲一下。



图 2-45将卷积转化为矩阵乘法

如图 2-45所示，当核在左上角位置时，不直接考虑是卷积核与左上角的

子矩

阵做点积，而是换个思路，通过零填充把卷积核扩成一个和待卷积信号一样大的矩阵 "1 2 0_

3 4 0，这样核与待卷积信号的长度一样，也可以按照对应位置做点积，并且方便的

0 0 0

是，用这个办法可以把卷积核在每个不同位置的情况都转化成一个和待卷积信号大小一样 的矩阵，然后按照对应位置做点积。沿着这个思路，把每一个卷积核所在的位置都用零填 充扩展，并且再多做一步，就是按照一行一行的顺序展开成一个一维向量，就组成了图 2-45 中最中央的那个 4 行 9 列的矩阵。所以很自然的，把待卷积信号也展开成一个一维向量， 这样就构成了矩阵乘法中乘号右边的那一项。得到的结果是个四维的向量，这个结果“折 叠”回去就得到了最终的结果。于是二维卷积就这样被转换成了矩阵乘法。

如果读者注意就会发现中间的矩阵中，卷积核展开之后，每次位置移动就相当于所有 元素向右移动了一个位置。因为第二次已经移动到了矩阵的右边缘，所以下一个位置核换 行了，因此可看到中间矩阵的第三行和第二行相比向右移动了两个位不过大体而言能 看出每一行的元素向右移动位置的趋势。这种矩阵的原型是一种叫做耗普利兹（Toeplitz） 矩阵，也就是矩阵对角线，以及对角线平行的元素都相等的矩阵（二维卷积更正式的名字 叫块循环矩阵，doubly block circulant matrix）。总而言之，就是把卷积核移动位置通过矩 阵每一行元素位置的移动来实现。

在 Caffe 中，默认的卷积实现就是基于矩阵乘法的办法。

2.在频域计算卷积

空域的卷积在频域就对应着乘法，这是在信号领域常用的一个结论：

f *geF G    （公式 2-41）

这里就不过多介绍细节了，大体来说就是通过 FFT 转换到频域做乘法，然后再变换回 来就得到卷积后的结果。基于这种方法的卷积实现，在优化后的性能可以在 GPU 上达到很 好的效率，是主流的快速计算卷积的算法，在大部分框架下都有对应实现，比如 Torch, Theano 和 cudnn。

2.5数学优化基础

几乎所有的机器学习中，都面临减小模型预测值和数据之间误差的问题，这类问题一 般都可以被转化为求一个目标函数的最小值问题，称为优化问题。本节将介绍和深度学习 相关优化问题中的最基本的概念和几种常见方法。

2.5.1最小值和梯度下降

下面用最简单的一维情况举例子，如图 2-46所示。 如图 2-46所示为一个一维函数的曲线。最中间



图 2-46函数中的不同区域

的值是这个函数的最小值(global minima)；最小 值右边有一个局部最小值(local minima)，又称为 极小值。简单来说，对于一个给定的函数，优化问 题就是寻找最小值的问题。

1.梯度下降法

俗话说“人往高处走，水往低处流。”很自然地我们会想到，如果把曲线的形状也看 成山峰山谷，那么是否可以以一种方法模拟自然界中“水往低处流”的过程，来解决寻找 最小值的问题呢？目前的答案就是梯度下降算法。

算法 2-1梯度下降法

给定目标函数和初始点

重复：

= xt + rj/\xt 停止，如果 lAx/l < e

算法描述不难理解，就是先求出所在位置的梯度，然后这个梯度取负就是*向着极小 值前进的方向。x軍新的时候，将梯度乘以一个系数…控制更新时步长的大小，这个值称 为学习率。对应到具体例子，来看在图 2-46中最小值左边的方块。一开始方块所处的位置 在全局最小值左边，并且单调下降就能够接近全局最小值。方块初始位置梯度很大，所以 第一次迭代的箭头前进得比较多。第二次迭代的时候梯度小了一些，所以迭代步长也变小， 不过最终接近了全局最小值。再来看右边的三角，可以看到迭代的步长也随着梯度的下降 而减小。不过也正因为这个问题，三角进入右边的极小值之后，就被“陷”住了。

2.极值和鞍点

所以可以看到梯度下降的两个问题：第一，在梯度小的区域，算法的迭代速度会特别 慢。从道理上讲，在梯度小的区域慢速迭代是合理的，但是在实际应用中，如果算法迭代 到一个非常长但是梯度很小的区域时，就会陷入一种极其缓慢的停滞状态。比如图 2-46中 标出的一段梯度很小，彳艮“平”的区域，这种区域称之为停滞区(plateau)。第二，如果 存在一个梯度为 0，或是梯度值小于更新的标准的区域，则算法会在此处停止迭代，而这 个区域未必是全局最小值。最常见的一种情况之一是极小值，虽然不是全局最小，但是算 法会在极小值停住。除了极小值，在优化中还有一种很常见的情况叫鞍点(saddle point), 也就是在鞍点的梯度为 0，但是又并非局部最大/小值。

极小值是一个比较容易形象理解的概念，即使在高维度无法形象想象，通过脑补也不 难理解。鞍点则复杂一些，如图 2-47中所示，分别是一维和二维鞍点的例子。



图 2-47a是一维鞍点的例子，一般来说一维情况下鞍点的心态虽然会各不相同，但大 都可以分为图 2-47中的这两类：左边高右边低，或者左边低右边高。而到了二维的情况时， 因为自由度变高了，则情况也复杂很多，比如图 2-47b中左边的图，从一个方向上看去， 两边高，在该方向上，鞍点所在位置是极小值。而和该方向垂直的方向则是个极大值，外 型上看上去像个马鞍，这也是鞍点名字的由来。而图 2-47b中右边的鞍点例子则是三个方 向都较大，一个方向较小。因为自由度的增加，在二维情况下还能有很多种情况的鞍点。 总体来说，随着维度的增加，由于定义和性质，极值点的情况总体还是不变，对于极小值， 需要周围任意方向在一定范围内都大于极值所在。而鞍点则五花八门，各式各样。

所以随着维度的增加，如果稍微定性考虑一下极值和鞍点，对于任一个极值点，该点 在任一维度上都是极值，而所有维度都是极值的概率，是相对而言最低的；相对于极值点， 鞍点的条件则宽松一些，所以鞍点出现的概率会高很多；在实际算法优化的过程中，梯度 很小的区域也就是停滞区，往往会让梯度下降法停滞很久，甚至当梯度小于一定值之后， 停滞区事实上就和鞍点没什么区别了，这种情况发生的几率相对来说则更高。

所以总结起来，就是梯度下降法的问题基本都是由于鞍点/停滞区和极值导致。其中高 维空间里，比起极值点来说，鞍点/停滞区是一个更普遍的让人头疼的问题。

2.5.2 冲量（Momentum）

1.惯性和冲量

对于梯度下降法存在的难题，也可以从对自然界的物理过程中找到线索。想象一下山 谷中的一颗圆形小松果，如果从高处滚动到了一片很平缓的区域，它并不会因为平缓而停 止滚动。如果到了 -个小的坑里，也会继续滚动，但是否能滚动出小坑继续前进，取决于 当前的速度。这两种情况帮助小松果继续前进的是惯性，在梯度下降法中，-个类似的手 段也可以帮助改进，就是冲量（momentum）。

其实从物理意义上来看 momentum 这个词翻译成动量也许更加合适，不过多数中文教 材中都翻译成冲量，所以这里也沿用这个翻译。在理解的时候最好还是从动量/惯性的角度 来看，加入冲量之后的梯度下降算法改进版如下：

算法 2-2考虑冲量的梯度下降法

给定目标函数 D 和初始点 Xo，以及初始动量 v0

重复：

Axr = -V/x,）

Vt+\ = yvt + rj/\xt

X/+1    = xt + vr+i

停止，如果达到停止标准

在梯度下降算法的基础上，一个代表上一时刻的冲量的项 r 被加入进来，并且每-次 迭代都会乘上一个衰减系数卜在冲量项的影响下，算法迭代就相当于带上了 “惯性”， 前次迭代位置前进的方向可以影响到下一次迭代。这样当算法经过鞍点或是停滞区时，就 不至于停下来做过于缓慢的迭代，而经过并不是很“深”的极值时，就可以借助冲量项带 来的“惯性”冲出极值所在的“坑”。

除此以外因为算法的改变，停止算法的标准也不再是梯度小于一个阈值。停止算法的 标准可以是冲量小于某个值，梯度小于某个值，或是用户给定一个次数就停止。

\2. Nesterov Accelerated Gradient Descent方法（NAG）

NAG是最基本的冲量梯度下降法的一个改进，其提出者 Yurii Nesterov是凸优化领域 的一位神级人物。算法如下：

算法 2-3 Nesterov冲量梯度下降法

给定目标函数/x）和初始点 xG，以及初始动量冲

重复：

Ax, = -▽私+yvz）

V/+i = yvt + rj/\xt = Xr + Vr+i

停止，如果达到停止标准

所以和基本的基于加入冲量的梯度下降算法 2-2比起来，只是求梯度的位置不再是当 前位置，而是假设如果沿着当前冲量乘以衰减系数前进一步之后所在的位置，如图 2-48 所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-99.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-100.jpg)

图 2-48基本的冲量梯度下降法和 NAG 示意图

直观想象一下这个过程，好比骑了一辆山地自行车，向下俯冲。一般的冲量法就像是 骑到一个地方，根据当前的坡度，决定车往哪个方向拐。而 NAG 则是用眼睛看一下前方， 判断出坡度，然后根据前方的坡度决定车往哪个方向拐。也就是说，根据预判对当前的路 径进行修正，避免走冤枉路。

NAG在凸优化问题中，尤其是对平滑度较高的函数有很好的效果。不过在深度学习的

优化问题中，NAG是不是更好就不一定了，还需要尝试。

这里也简单提一句凸优化(Convex Optimization)，和深度学习中一般的优化问题比

起来，凸优化是一个相对既简单又复杂的领域。简单来说可以认为凸优化就是要求解的函 数局部最小就是全局最小。其实是一般优化问题的一个子问题，所以需要考虑的问题就相 对简单了。不过比起机器学习中接触的优化手段而言，凸优化也是一个很难的领域，在凸 优化的限定下，发展出了很多复杂而严谨的方法，但这些方法被应用到深度学习中的很少， NAG算是一个。

2.5.3牛顿法

1.牛顿-拉普森(Newton-Raphson, NR)算法

介绍优化中的牛顿法之前，先来介绍一下用于数值计算的牛顿-拉普森算法。牛顿-拉 普森算法是用来寻找实值方程的近似解的一种数值算法，算法表达如下：

算法 2-4牛顿-拉普森算法 给定欲求的方程和初始点 xo

重复：    /

Ax = -Xx，)//(xz) xt+\ =    + Ax

停止，如果 l/U)| < e



可以看到，牛顿法其实就是给函数在当前所在位置做一个一阶展开，然后用这个一阶 近似的解作为下一次迭代的位置。

2.牛顿法

了解了牛顿-拉普森算法，再来了解一下牛顿法，还是用一个一维例子来讲解，如 图 2-50所示。

在图 2-50中，上面一排表示 y(x)的曲线，下面一排表示的导数的曲线。例子 中使用的是一个凸函数，所以找到极值就相当于找到的解所在位置，因此把牛顿-拉普森算法套用过来在/(X)上一用就可以了。下面只需要把算法 2_4稍加修改：

算法 2-5牛顿法求极值（一维） 给定和初始点&

重复：

x 什 1    = x，-    / ⑹//’⑹

停止，如果 1/%V)I < e

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-102.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-103.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-104.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-105.jpg)

我们举例子是从牛顿-拉普森算法说起，其实也可以从泰勒展开的角度看待牛顿法。考 虑 x+Ax处，将用泰勒展开到二阶：

/(x + Ax)« /(x) + /»(x)Ax + i/"(x)(Ax)2

(公式 2-42)

这个展开相当于在 x 附近用二次函数对/x）做了个近似。在这个近似中，7W,/（x）和/’（x） 相当于系数，自变量是 Ax。对于一个二次函数，通过高中知识知道，对于 ox2+fcc+c，极值 在对称轴处。所以在公式 2-42的近似中，当

Ax

尸⑷

尸⑷

时，在/x）附近展开的二次近似函数的值/x+Ax）就是最小值。如果/%）本身就是个二次函 数的话，那么这一次计算就已经找到了/x）的极值。如果/x）不是二次函数呢？那就可以在 x+Ax处再做一次二次函数近似，找到新的 Ax 作为迭代步长，直至逼近真正的极值，这正 是牛顿法。

所以从这个角度理解，牛顿法就是每次迭代的时候，在所在位置对要求解的函数做一 个二次近似，然后直接用这个近似的最小值作为下一次迭代的位置。把单纯梯度看作线性 近似的话，当迭代步长不大的时候，二阶的近似通常比线性近似好一些，否则就很难说。

前面举的都是一维的例子，不过思路都是一样，推广到高维的算法如下：

算法 2-6牛顿法求极值

给定/x）和初始点 x0

重复：

X/+1    = Xt -    v/x，)/単 X')

停止，如果 e

| 其中///X)是二阶偏导矩阵，又称为海森(Hessian)矩阵， | 定义如下： |        |        |            |
| -------------------------------------------------- | ---------- | ------ | ------ | ---------- |
|                                                    | r av       | 9V     | SV J   |            |
|                                                    | dx}2       |        | dx'dxn |            |
|                                                    | d2f        | d2f    | 97     |            |
| (x) = /f/ (x,, x2, • • •,    )=                    | dx2dxl     | dx22   | dx2dxn | (公式 2-43) |
|                                                    | d2f        | 97     | 97     |            |
|                                                    |            | dXndX2 |        |            |

所以可以看到，虽然高维度和一维的牛顿法思路本质上没什么不同，但是海森矩阵在 高维度下的计算很大，并且在分母上需要求逆，所以计算起来不现实。这是限制牛顿法在 实际应用中使用的一个重要因素。

2.5.4学习率和自适应步长

在牛顿法中还可以发现一个特点，就是迭代步长是二阶近似的解 fc 解，所以不需要指 定学习率。不需要指定学习率从某种角度来说是一个优点，因为如果学习率没有定好，会 通过影响迭代步长从而影响迭代效果。下面以最基本的梯度法举例，如图 2-51所示。



丰*串 J>J>J 肀 2V 的的的 id低 A'i <=过过

d)

图 2-51学习率对梯度下降的影响

图 2-51中 a、b、c图分别是同一个曲线用 3 种不同的学习率进行梯度下降，图 2-5la 是学习率过大的情况，这种情况下在迭代过程中很容易因为在一个梯度较大的区域，获得 了一个很长的步长导致越过了极值点，甚至进而在别处梯度大的地方再次以一个大步长迭 代，最后导致不收敛。图 2-5lc是学习率过小的情况，虽然最后算法通常能够收敛，但是 迭代的步数会非常多，尤其是在梯度接近 0 的区域。所以一个合适的学习率需要既可以保 证收敛，又能保证效率，比如图 2-51b。不同学习率对应的收敛曲线如图 2-51d所示，合适 的学习率通常可以帮助优化收敛到一个较优的最小值(实线)；如果学习率过低，这个收 敛的过程则会很慢(短线段虚线)，而过高的学习率虽然在一开始收敛迅速，后期却常常 难以继续收敛(点虚线)；如果学习率再高，很可能一开始就不收敛，这个情况图 2-51中 就没有画出了。总之，一个合适的学习率非常重要，不过困难的地方在于，对于不同的任 务，一个最合适的学习率通常是需要进行尝试的。

2.5.5    学习率衰减(Learning Rate Decay)

根据优化过程在不同阶段的特点，一个大体的思路就是前期使用较大的学习率加速收 敛，后期用较小的学习率保证稳定，这就是学习率衰减背后的思想。这里用最常见的按步 长衰减学习率的策略为例子，公式如下：

###### & = ^ase./stepsize」    (公式 2-44)

其中是基础学习率，y是一个小于 1 的衰减系数，是一个触发衰减的阈值， 当前迭代的步数除以这个阈值向下取整作为 y 的指数。下面来考虑 7-0.1, stepSize=\QQ00Q, 意思就是每迭代 100000 步，学习率就下降为之前的十分之一。

在这个方法中，步长和衰减系数都是经验值。除了这种按步长衰减的公式，还有按照 指数衰减、按倒数衰减、按照多项式衰减等。形式虽然不一样，但是大同小异，都是要一 个下降的函数，并且参数是经验值。如果经验不足或者经验值难以确定的时候，自适应学 习率的办法才是很多人更想要的。

2.5.6    AdaGrad：每个变量有自己的节奏

在基本的梯度下降法优化中，有个一个常见问题是，要优化的变量对于目标函数的依 赖是各不相同的。对于某些变量，已经优化到了极小值附近，但是有的变量仍然在梯度很 大的地方，这时候一个统一的全局学习率是可能出现问题的。如果学习率太小，则梯度很 大的变量会收敛很慢，如果梯度太大，已经优化差不多的变量可能会不稳定。

针对这个问题，当时在伯克利加州大学读博士的 Jhon Duchi，提出了 AdaGradCAdaptive Gradient)，字面上理解就是自适应学习率。AdaGrad的基本思想是对每个变量用不同的 学习率，这个学习率在一开始比较大，用于快速梯度下降。随着优化过程的进行，对于已 经下降很多的变量，则减缓学习率，对于还没怎么下降的变量，则保持一个较大的学习率

用(x,),•表示第 f 个变量 xz•在第 f 次迭代时的值，(V/fe)),•表示函数在 f 次迭代时，对第/ 个变量^的梯度值，则算法更新梯度的公式如下：

)，二(XA — ~厂；    ■ r•(▽/(')),•    (公式 2-45)

拓(W));

其实就是每个变量都随着学习的进行，根据历史学习率累积总量来决定当前学习率减 小的程度。根据 AdaGrad 的特性，对于下面这种函数会比一般的梯度下降有很大的优势， 如图 2-52中等高线所示的情况。

图 2-52中，中间区域上下方向梯度变化明显，但是左右方向梯度非常平缓 般的梯度下降，学习率过大的话则会沿着图 2-52中的 上下方向震荡，如果学习率过小则会像图 2-52中的虚 线箭头所示，到达 B 位置后，以一个极其缓慢的速度 向 A 点前进。

如果是一

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-107.jpg)

在机器学习的应用中，AdaGrad非常适合样本稀 疏的问题，因为稀疏的样本下，每次梯度下降的方向，

以及涉及的变量都可能有很大的差异。AdaGrad的缺 点是虽然不同变量有了各自的学习率，但是初始的全 局学习率还是需要手工指定。如果全局学习率过大，

图 2-52在不同变量(方向)上梯度 差距很大的一个示意

优化同样不稳定；而如果全局学习率过小，因为 AdaGrad的特性，随着优化的进行，学习率会越来越 小，很可能还没有到极值就停滞不前了。

2.5.7 AdaDelta的进一步改进

AdaGrad中存在的问题在 AdaDelta 中得到了一个解决方案。AdaDelta是第 1 章中提到 过的 Mathew Zeiler在 Google 实习时独立完成的工作。和 AdaGrad 比超来 AdaDelta 的主要 改进有两点。第一点是将累计梯度信息从全部历史梯度变为当前时间向前的一个窗口期内 的累积，具体来说在计算历史梯度信息的时候采用的是下面的公式：

^[g2]z    (公式 2-46)

相当于对历史梯度信息的累计乘上一个衰减系数 P，然后用(1-p)作为当前梯度平方的 加权系数相加，这样就隐式实现了梯度平方在一定时间窗口内的累积。这种历史梯度信息 用类似 AdaGrad 的方法开方后作为分母项，公式如下：

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-108.jpg)

（公式 2-47）

其中 e 是一个非常小的项防止分母为 0。忽略这一项，则分母就是均方根(Root Mean Square, RMS)。用(Ax,),•表示每次更新的步长，则公式如下：



（公式 2-48）

虽然梯度累积后学习率一直下降的问题解决了，可是手动设置学习率//仍然是个问题。 AdaDelta中用第二个改进解决这个问题。第二个改进主要是受到牛顿法和 LeCun 关于牛顿 法近似工作的启发。LeCun在 1988 年的时候提出用 Hessian 矩阵的对角线近似 Hessian 矩 阵，进而更新步长的时候可以用如下公式：

\+ £



（公式 2-49）

这个步长被称为伪牛顿步长（pseudo-Newton step），其中 e 还是一个用来防止分母为 0的项。根据这个关系 Matthew 在论文中用了一个看上去有些多余的推导：

& =蠢二去=差    （公式 2-50）

dx2 dx2

大意就是，步长可以用梯度除以用于近似 Hessian 的二阶导数来表示。所以换一下等 式的顺序，也就是箭头右边的形式，等号左边的项是用来近似 Hessian 的逆。这一项可以 表示成等号右边的形式，步长除以导数：昏把步长除以导数的这一项和公式 2-48中梯

dx

度前的系数——进行类比，发现分母上的 RMS[g]，和的量纲是一样的，并且定性

J,

来说具有相同的物理意义（都是导数），所以作为分子的 A7 和 Ax 也是应该有相同的量纲 及物理意义。其实说了这么多，就是要说明下面的式子：

Ax Ax

dx dx

简单来说，就是对应的物理量要有相同的量纲。Matthew远论文里还有不少推导用于 说明这件事，并指出了一般的梯度法和 AdaGrad 都没有正确对应的量纲，这些细节这里就 略过了。    .

总之，按照这＜个道理，以及对函数平滑性的假设，把 Ar 的历史累积也和沿一样如法 炮制，只不过用的是心…就得到了 AdaDelta的更新公式：

(^)/ =

(RMS[Ax]4.

(■s[g],)z

(公式 2-51)

AdaDelta除了梯度和梯度历史信息，只额外引入了一个保存 Ar 历史信息的量，就完 成了对 Hessian 对角近似的某种程度上的近似。另外，AdaDelta的一个巨大的优点就是不 用手动指定学习率。

2.5.8其他自适应算法

AdaGmd和 AdaDelta 算是最有代表性的两种自适应算法，在之后比较流行的还有和 AdaDelta很像的 RMSProp、Adam、Adamax等算法，在一定程度上来说都能看到这两种算 法的影子，在这里就不详细介绍了。

在深度学习的实际应用中，因为问题的高维度和高复杂性的特点，具体哪种优化算法 更适合还需要具体的尝试。一般情况下带冲量的梯度下降法还是最主流的，不过对于收敛 不好的情况，自适应算法常常能收到奇效。不过一种常见的情况是，在优化的后期，自适 应算法尤其是 AdaDelta 和 RMSProp 常常会反复“震荡”，效果不如带冲量的梯度下降法。

2.5.9损失函数

优化问题只是一个单纯的函数值最小化问题，和机器学习关联起来，靠的是如何定义 要优化的函数，这个函数从优化的角度来讲叫做目标函数(objective function)，机器学习 中一般把这个函数叫做损失函数(loss function)，或是代价函数(cost function)，本书中 我们采用损失函数的名字。

因为中文翻译的关系，损失函数显得不是那么顾名思义。损失函数度量的是模型预测 的值和真实值之间的误差。举个简单例子，如图 2-53所示。





L(a,b)

图 2-53数据和线性拟合的误差

图 2-53中是一个典型的线性回归问题，包含一组数据点和函数的曲线。也 许看到这里，很多人一下就想到了最小二乘法，我们不妨就用最小二乘法的优化判据作为 损失函数：

” 2

j=i

A(a，6)就是损失函数，其中(X/，”)为图 2-53中的数据点，为数据的个数，在图 2-53所 示的例子中，n=9o对于损失函数而言，用于拟合数据的公式 ox+6中的参数么 Z)，也就是 斜率和截距，就成了损失函数中的变量。我们画出损失函数在一定区间上的值，如图 2-53 中右上的图。可以看到这样就已经转化成了一个典型的优化问题了，并且这个情况中还是 个凸优化。不过可以看到如右上图的曲面所示，直接用平方和作为损失函数，很难一眼看 出哪里是最低点。因为中间一段梯度很小的区域，不仅是肉眼，即使梯度法也会在这片区 域缓慢下降。面对这种情况，可以考虑取 log，让不同量级之间的值更容易比较，特别是 增加小值区域的敏感度，将损失函数更新如下：

£（«，/?） =



![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-113.jpg)

结果就是图 2-53所示中右下图的曲面。这样肉眼非常容易就能分辨出最低点所在，算 法收敛也容易得多。可以看到，损失函数在机器学习问题中的重要性。损失函数不仅定义 了问题，其设计还直接决定了优化算法收敛的性能，甚至是否能够成功收敛。

2.5.10分类问题和负对数似然

机器学习中除了像上部分中对实数值做回归，还有一种典型问题是分类问题。以最简 单的二分类为例，考虑一组数据｛（%1^1）知 2，^2），...,（^«）｝，其中_ye｛0,l｝，也就是将 X 分为 两类。我们尝试用一个模型/X）来学习分类的规律，给定 X，经过/X）计算结果也是 0 或 1。 很自然地会想到用比较结果是否相等来作为损失函数：

£ =    （只，，⑷）

其中/是判断两个量是否相等的函数，如果相等则为 0，否则为 1:

l，xo 0, x=y

虽然这种损失函数非常直接简单，但是不连续且高维度不可计算。现在换个思路，考 虑不让函数直接输出 0 或者 1，而是让一个处处可导容易计算的函数最后的输出是结果为 0 或者 1 的概率：其中 0 是参数。最后只需要找到对应概率最大的分类结果，就是 模型预测的结果：

y = arg max[P（K = y

在这样的框架下，机器学习要解决的问题就是找到一组参数使得对给定标注数据

（x/,yz），模型在正确标签上预测的概率都达到最大，这正是 2.2.4节中讲到过的最大似然估 计。所以可以把计算最大似然估计的思路用到这里，采用对数似然函数：

（尸（妁 I 易，

1=1



d = arg max e

在机器学习中，都习惯用优化算法求最小值，所以通常用的是负对数似然（Negative Log-Likelihood, NLL）:

6 = arg min

e

-EM 尸（妁 Ix,，

/=i

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-115.jpg)

因为概率总是小等于 1，所以 NLL 的值总是大于 0，更加直观。这是在分类问题中常 用的一种损失函数，并且能轻易拓展到多分类问题。通常这种替代直接比较输出的损失函

• 82 •



数又称做代理函数（Surrogate Loss Function） 0

2.5.11逻辑回归



图 2-54平面二分类的例子

逻辑回归并没有什么逻辑，英文是 logistic regression。逻辑回归是统计中的一种经典方法，发明者 是在统计学教材里经常出现的 David Cox。这里之所以放 到数学优化部分讲是因为在机器学习领域，逻辑回归和 分类问题的联系十分紧密。

还是考虑简单二分类问题，如图 2-54所示。

图 2-54中是两种分布在平面上的样本，划定出一条 线性分类边界（虚线），认为在分类边界左下方的都是 △，右上方的都是 O。上个部分已经讲了，直接对标签 进行回归，不如去计算样本属于某个标签的概率。通过 肉眼观察，我们发现规律，离分类边界的距离越远的，

则属于该半边平面类别的可能性越高。这对于一个线性分类边界已经是一个足够好的判定 标准。所以可以考虑计算出每个点到分类边界的距离。并且可以定义这个距离的方向性， 我们定义在分类边界左下方的点，这个距离是负值，否则这个距离就是正值。这等效于将 所有点的坐标都投影到一个垂直于分类边界，指向右上方的方向上（x）。

所以接下来要做的就是把样本到分类边界的距离转化为概率。这时候 logistic 函数可以 登场了。logistic函数定义如下：

1

（公式 2-52）

/w=

wx+ft是个仿射变换，无论;c是任何维度，最后经过这个仿射变换都会化为一个标量。 具体到例子中,x是个-维的变量。所以我们更关心仿射变换后的值对应的概率，令 Z=wx+6,

把;_1了的曲线画在图 2-55中。 l + e"r



图 2-55标准 logistic 函数

这种形式又叫做标准 logistic 函数。如图 2-55所示，标准 logistic 函数是个关于(0,0.5) 旋转对称的函数，值域是(0,1)。/的值越小于 0 的时候，函数值小于 0.5，否则大于 0.5。我 们可以把这个函数的输出看做是概率，还是考虑△和 O 分类的例子。比如 Z=x，也就是直 接拿到分类边界的距离作为计算概率的依据。显然，x算出来大于 0 的时候，分类结果为 O，并且 x 值越大，分类为△的概率越接近 1；反之若 x 小于 0，则分类结果为△, x离远 点越远，分类为 O 的概率也越低，因为 P(A)=1-P(O)，所以分类为△的概率越高。这正好 是前面的直观判断。接下来找到一组合适参数的过程就很显然了，似然最大化，前面已经 讲过了。

所以简单来说，逻辑回归把任何输入通过变换化成 0〜1之间的数值，用来代表概率。 也就是说任何一个能合理将值化为对应二分类概率的函数理论上都是可行的。其中 logistic 函数除了合理以外，其形式还易于 NLL 及求导计算，所以在分类问题中被大肆使用。

2.5.12 Softmax：将输出转换为概率

对于一个 logistic 函数，如果令 z=(w，x+Z?)/2，则可以化为如下形式:

/（x） = g（z

_J__    ez

（公式 2-53）

1 + e_2z e2 + e"z

根据定义，z是一个关于 x 的仿射变换，其实就代表着;c为某一类别的概率的一种原 始度量。这时候再来看公式 2-53的形式，logistic函数做的事情，其实就是把一个样本属 于两个类的可能性的这种原始度量通过 ez 做指数运算，求和作为分母。然后对于某一类， 直接用 ez 转化的值作为分子，这个比值作为最后样本属于某一类的概率。所以就是利用 ez 可以把任何实数转换成一个大于 0 的数的特性，做了归一化而已。

还是考虑图 2-54所示的例子，假设 w=l, b=Qo如果一个样本投影在 x 轴上的值为 2, 则 z=l，所以该样本属于 O 的概率大一些。因为图 2-54所示二分类例子的对称性，从△的 角度来看，z=-l，最后计算概率的时候就是

上面说的这种思路，可以轻易地拓展到多分类问题，就是 softmax。对于一个《分类问 题，给定输入 x 属于第 z’类(yz)的一种原始度量 h(x,yi), softmax计算的属于某一类的概率为:

eA（x’乃）

P{y\x） =-—-—    （公式 2-54）

艺 e“）

7=1

2.5.13链式求导法则

前面几节讲了那么多，每节都提到了一个重要的名词即梯度。本节讲一下梯度计算中, 和深度学习紧密关联的链式求导法则。

对于一个函数 y=/g(x))，令 w=g(x)，则有：

（公式 2-55）

dy _ dy dw dx du dx

这就是链式求导法则。其实就是对复合函数求导的一种性质，而这种性质常常能简化 梯度的计算。公式 2-55给的是一维情况下的链式求导法则，同样的原理也可以拓展到多变 量多个复合函数甚至多输出的情况。比如考虑 x=(xl9x2，...A/7)，y=(y\,y2,-• -(x), g2(x),...,gXx))，则链式法则如下：

A(x)= J/(^x))yg(x)    (公式 2-56)

J代表雅可比矩阵(Jacobian matrix)，详细如下:



，1 2 >2 . 2 979X办 ax,:

dy2

dym dym    dK

dx' dx2    dxn

| ¥    | 3g2  | ^Sk  |
| ---- | ---- | ---- |
| 办 2  | 如 2  | 办 2  |
|      | dg2  |      |
| dym  | dym  | dym  |
| _3g, | dg2  | dsk  |

| ;3&     | dx2如 2    .dx2 | ..dXn |
| ------- | -------------- | ----- |
| ■:\^L   | dSk            |       |
| 上知」_ | dx2            |       |

这个公式看起来有些密集，不过理解起来并不难。既然是矩阵乘法，所以等式左边的 每一个元素运算规则其实没有区别。只要理解了其中一个，其他的都是同样套路。先来 看公式中用虚线方框标出的部分，也就是计算等式左边第一个元素，其实就是对偏微分 求点积：

{dg, ?3g2



![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-120.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-121.jpg)

和一维情况下单个输出的公式相比，只是变成了求和的形式。形象来理解就是力通过 多条路径把对乃的一阶影响传递到 M。因为是一阶函数，可以把梯度理解为一种表示输出 对输入变化敏感度大小的系数，所以很自然地把结果加一块就是总效果了。

下面还是来一步步执行一个非常简单的例子加深理解。为简化书写，令简写 为 g'，g2同样，考虑下面的函数：

gI(x1,x2) = x1 +x2

g2(%j,X2 ) = ^X2

y = /(gpg2) = ln(&) + eg2 =ln(x! +i2) + eXlX2

相当于最终输出维度为 1，输入维度是 2，中间的复合函数两个。根据公式 2-56, Jy 和人化简为只有第一行，如下：

| dy dy   |      | dy dy      |
| ------- | ---- | ---------- |
| dxl dx2 |      | L如 1 知 2」 |

3& 3%, 9g2

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-122.jpg)

根据函数的定义，需要求导的 6 个导数如下:

办 _ 1 _    1

也 g, X, +x2

^- = g2=e^ dg2

^ = 1, ^- = 1 3x, dx2

所以有:

如 2

9x,

i办 3g2 9^i 9g, 3-^1 dg2 dx} dy = dy dg} | dy dg2 dx2 9g, dx2 dg2 dx2

x2.

dg2

dx2

x,

•l + eX|X2

x, +x2

1

•1 +

,x\x2

X2

*X1

X, +x2

1

\+ x2e

x\x2

dx, dj?n dXo x, +x2    xi + xi

如果把这个过程用图的形式来表示，则如图 2-56所示

\+ %e

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-123.jpg)

看到图 2-56也许读者会觉得眼熟。没错，第 1 章出现过一个结构类似的示意图，表示 的是一个人工神经网络。



# 相关

- 《深度学习与计算机视觉》
