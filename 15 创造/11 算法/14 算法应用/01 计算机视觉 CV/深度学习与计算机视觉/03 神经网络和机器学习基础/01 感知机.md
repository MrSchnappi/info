---
title: 01 感知机
toc: true
date: 2018-08-29
---
# 第 3 章神经网络和机器学习基础

## 可以补充进来的

- <span style="color:red;">暂时没有进行整理，对于神经网络的基础已经整理了很多的，需要把哪些融合之后再把这个整理进去。</span>

第 1 章已经从历史和其他角度讲述了神经网络的前世今生。本章将深入细节，一探究 竟。本章将从神经网络讲起，然后引出一些机器学习的基础概念并进行简单讨论。

3.1感知机

深度学习始于神经网络，神经网络始于感知机。

3.1.1 基本概念

在第 1 章里已经提到过感知机。本节就详细聊聊 这个在人工神经网络中最基础的结构。感知机由 Frank Rosenblatt在 1957 年第一次提出，结构如图 3-1所示。



5?

图 3-1感知机示意图

这种结构以一个向量作为输入，计算输入每一维 度的值的线性组合，也就是第 2 章讲过的线性变换， 然后和一个阈值进行比对，高于阈值则输出 1，否则 输入_1。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-125.jpg)

1,    + w2x2 + •

-1, WjXj + w2x2 +

xnwn+b>0 ■xnwn + Z? 0

（公式 3-1）

简单来说就是加权求和，然后再和 b 进行比大小。

3.1.2感知机和线性二分类

从结构上讲，感知机和神经元的相似之处在于：第一，多个输入到一个节点；第二， 神经元总输入电位超过阈值电位后，将释放一个输出，这对应感知机以 o 为阈值对应 1 和 -1的不同输出。功能上就大不一样了，感知机这样一种简单的结构实质能解决的是在多维 空间中的线性分类问题，如图 3-2所示的一个二维平面上的例子。

把这个平面上的不同样本二分类的一个感知机，分界线对应如图 3-3中所示的虚线。



图 3-2二维平面上线性可分样本

^!+2%2-1=0



图 3-3二维平面上线性可分样本和一个线性分界

图 3-3中，O表示/x心＞0的样本，也就是标签结果为 1 的，对应分类虚线上方的部 分；△表示/xlrv2）＜0的样本，结果标签为_1，对应分类虚线下方的部分。

3.1.3激活函数

那么 Frank Rosenblatt的感知机和神经网络有什么联系呢？感知机事实上就是人工神 经网络的最小单元，这个结构里有两个最基本的成分：计算输入向量的一个线性变换；对 线性组合的结果进行阈值判断，实际上就是非线性变换。或者更简单来说，把阈值和线性 变换放一起，则是第 2 章提到的仿射变换，所以感知机本质上就是一个仿射变换接一个非 线性变换。把公式 3-2简化一下，用 x 来表示向量（xbx2,    用 w 来表示对应系数的向

量，sgri表示大于 0 输出 1，小等于 0 则输出-1的函数，得到公式 3-3如下：

y    /（x） = sgn（＞v-x + Z?）    （公式 3-3）

其中 wx 表示两者的点积。更一般地，按照前面所说，把这个公式里的仿射变换+非 线性变换的特点提取出来，可以表示为公式 3-4。

f{x）=g{w^x + b}    （公式 3-4）

g（*）表示一个非线性变换。在机器学习领域，这种非线性变换通常被称为激活函数。 激活函数可以是 sgn 函数，也可以是其他函数，比如可以是连续且光滑的 tanh 函数（如 图 3-4a所示），或是 sigmoid 函数（如图 3-4b所示），其实就是第 2 章提到过的 logistic 函数在一维输入下的特例。这些正是经典全链接神经网络的常见基本单元。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-128.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-129.jpg)

图 3-4 tanh函数和 sigmoid 函数

另外也可以是 ReLU(Rectified Linear Unit)激活函数，这是 2012 年之后深度学习中被用 到最多的一个经典神经元结构(如图 3-5所示)。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-130.jpg)

感知机是个如此简单的模型，说白了就是乘积一累加运算一判断大小，非常适合计算 机模拟，这也是为什么这个很早就提出来的简单结构如今会成为神经网络基础的重要原因。

3.2神经网络基础

本节将从直观形象的角度，用最简单的例子一步步了解一个神经网络是如何完成基本 分类的任务。

3.2.1从感知机到神经网络

了解了感知机，就可以开始探索最基本的神经网络了 经典的两层神经网络结构。

先来看一个如图 3-6所示的最



输出

图 3-6 —个简单的神经网络的例子

最原始的输入层(X1?X2)和 3 个不同感知机相连，这 3个感知机有 3 个输出，然后再和一个感知机相连，最 终输出在这样一个经典的网络结构中，输入后面的 一层叫隐藏层，因为通常在训练和使用的时候，其输出 对使用者来说是不可见的，然后是输出层。很容易发现 这样的网络结构有以下两个特点。

分层结构，如果把输入也当成一层，则每一层有一定数量的输出作为下一层的输入。 从这个角度来说，可以把神经网络看作是对一个向量进行分步变换，每一层的输入向量经 过这一层感知机变换之后，相当于变成了一个新的向量，并且新向量的维度等于这一层感 知机单元的数量，这样一层层变换直到形成最后的输出。如图 3-6所示的网络结构，输入 层的向量传递到隐藏层之后，变成了一个三维向量，而这个三维向量到达输出层之后，最 终变成一维。如果从函数的角度来看，整个神经网络的作用就是一个向量 x，经过了变换 之后成了一个向量 y 而已。

每一层的输出都和下一层所有的感知机输入相连，也就是通常所说的全连接（Fully Connected）。所以在这种经典的结构中，对于一个《层（包含输入层和输出层）的网络， 权值的数目和神经元数目的关系如下：

（公式 3-5）

/=0

其中 W 代表权值的总数，八代表第 f 层的感知机数量。可以看到，当网络层数不多的 时候，随着感知机单元数量的增加，权值数目是平方增加的趋势。最近几年 ImageNet 竞赛 的网络有去掉全连接层的趋势，也是因为全连接层的单元数未必很多但是参数却很多，常 常会占据一个模型中的大部分参数。

3.2.2最简单的神经网络二分类例子

了解了神经网络的两个特点，那么这样一种分层的、全连接的结构和单个的感知机分 类比起来强大在哪里呢？先来看图 3-7中所示的一个平面上二分类的小例子。

这里面产生的样本是由曲线

=＞〔27^+|)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-132.jpg)

作为分界线，以上的样本标签为 1，以下的样本标为 0。因为产生样本的分界线是弯曲的， 所以对只能做线性二分类的单个感知机来说，显然无能为力。那么轮到神经网络出场了， 我们建立一个非常/简单的用于二分类的两层的神经网络，结构如图 3-8所示。

/°o

o o /



输出

输入 隐藏层 输出层 图 3-8用于二分类图 3-7所示样本的神经网络结构

A    A A

△ △

△    △    A

△ A

0.2    0.4    0.6    0.8    L0

图 3-7二维平面上线性不可分样本的例子

其中隐藏层的激活函数采用 Sigmoid，输出激活函数采用的是第 2 章介绍过的 Softmax 函数。经过训练后，这样一个网络能得到如图 3-9所示的结果。

图 3-9中，虚线以上部分（浅色）是训练好的模型分类为 1 的区域，虚线以下的部分 （深色）是模型分类为 0 的区域。可以看到，这么一个简单的神经网络成功找到了一个类 似折线的二分类边界，将数据分开了。再仔细看一下图 3-9还会发现，这个边界像是由两 条不同方向的直线“拼”起来的，只是交界部分的直线不那么直了。在远离交界的部分还

是能清晰看出直线边界的，如图 3-9中的两条虚线所示，后面会讲到这是为什么。

下面来详细分析一下这个网络到底做了什么。输入层就不用介绍了。从隐藏层看起，

也就是两个用 Sigmoid 作为激活函数的感知机，在这样一个神经网络中，隐藏层是可以分 类弯曲边界的关键，这在第 2 章已经提到过，所以如果是线性不可分的样本，经过仿射变 换后，仍然是不可分的。这个时候，就要依靠非线性变换了，也就是 Sigmoid 激活函数起 的作用。我们把隐藏层第一个感知机 hl 的输出作为新的横轴，第二个感知机 h2 的输出作 为新的纵轴输出，把每一个样本在经过非线性变换后，在新的坐标平面的位置画出来。同 时也把图 3-9中区分空间区域的网格线画出来，如图 3-10所示。

| V    '      |          |      |
| ----------- | -------- | ---- |
| 0    o      |          |      |
| ，0         | O        | o    |
|             |          |      |
| \    o O    |          | ''   |
|             |          |      |
| ，、、    o |          |      |
| 么    \     | 3 o °    | /    |
|             | O O    z | △厶  |
|             |          |      |
| A A         |          |      |
| A    O      | o        |      |
| A           | :施      | A    |
| 厶          | 厶       |      |
| 厶          | •。么，.. | A    |
| A A         |          |      |

o.o驗縫識通鋼

0.0    0.2    0.4    0.6    0.8    1.0

图 3-9样本和二分类模型的分类边界



图 3-10输入样本经过隐藏层非线性变换后在新的 二维空间中的坐标

可以看到，在新的坐标平面内原来的平 面被严重扭曲，而扭曲之后的空间里样本是 线性可分的。注意到两个区域的分界线在这 个平面内就是一条直线，这就是隐藏层的重 要作用。事实上，如果连接一个 sgn 作为激 活函数的感知机，这个分类问题就解决了。

但是就如在第 2 章提到的，在分类问题中，

直接度量类别标签不如转换成一个分类概 率。所以这里采用的是两个单独的输出接 Softmax作为整层的“激活函数”，把输出 层经过 Softmax 变换后的两个输出值也按照

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-135.jpg)

上面办法画出二维坐标，如图 3-11所示。图 3—n Softmax层输出的结果表示在二维平面中

所以 Softmax 层的作用也一目了然。先通过一次变换让样本对应的值都大于 0，方便 归一化。并且因为^的特性，让样本的区分度尽量高，这一点和 SVM 异曲同工。然后归 一化让两个输出的概率和为 1，最后输出的标签就是按照概率比大小了。

现在回过头来再稍微深入分析一下，为什么隐藏层的非线性变换能够让一个弯曲边界 的样本分开并且成为线性可分的呢？来做这么一件事：以隐藏层的每个感知机的输出值作 为 z 轴，以原始的;^2平面作为另外两个轴，在三维空间中画出样本的分部，可以得到如 图 3-12所示的两个感知机输出样本在 xi~x2-z的三维空间中的分布。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-136.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-137.jpg)

图 3-12将隐藏层的输出作为 z 轴画出的样本在三维空间中的分布

其中图 3-12的左图对应 hl 的输出，右图对应 h2 的输出。两个三维图分别代表把两个 感知机的输出值作为 z 轴，在三维空间中的样本坐标分布。可以看到，第一个隐藏单元 hl 的值相当于把平面上位于左上角半边的样本“抬升”了，而第二个隐藏单元 h2 的值 “抬升” 了位于右上半边部分的样本。到这里其实就很清楚了，接下来只需要一个简单的 线性相加就可以让中间部分标签为 0 的样本“抬升”到最高的区域。这样一个简单的截面 就可以将两种样本划分开了。这也是为什么在图 3-9中可以看到分界线像是两条直线拼起 来的一个折线段，因为边界就是两个感

知机的分类直线组合起来的。在图 3-9 中，虚线所在的部分分别对应了两个 隐藏单元分类的线性边界。用更加简 单的示意图表示就是类似图 3-13所示 的情况。



图 3-13感知机线性组合后形成非线性边界的示意图

单个感知机的非线性变换只能对+/_样本划分出一条线性边界。可是线性组合后就可以 给++和非的样本划分出一个折线的边界。这种对区域的划分和表示其实是神经网络中的

一个重要性质，叫做分布式表征(distributed representation)，第 4 章会详细讲解。总之， 空间被进一步划分了，当然最后具体的边界则由数学优化来确定。事实上第三层输出层做 的就是类似的事情，我们把 Softmax 输出的值也用类似的方式分别作为 z 轴画在图 3-14所 示的三维空间中。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-139.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-140.jpg)

图 3-14将 Softmax 输出作为 z 轴，样本在二维空间中的分布

一目了然，从视觉上确实是区分度更高了，并且每个点在 z 轴所在位置相当于属于当 前类别的概率。这个小例子分别基于 MXNet 和 Caffe 的具体实现将在第 7 章中讲解。

3.2.3隐层神经元数量的作用

前面的例子帮助我们理解了非线性变换的作用。那么神经元数量又起什么作用呢？下 面再来看一个比三角函数分界稍微复杂一点的二分类例子，如图 3-15所示。

显然对于这种圆形的分界线，两个线性分界线的组合就力所不及了。我们尝试增加一 个隐藏层的单元，然后训练，结果如图 3-16所示。

1.0

| 0 8处 | 0^   | U°o °o | O        |
| ----- | ---- | ------ | -------- |
| OOP   | 厶   |        |          |
| 3     | △    | △      | -------J |
|       | A    |        | O        |
| 0O    | 厶   | O      | °oo      |
| r\,   | ° oo | O1 0 . | O        |

0.0    0.2    0.4    06    0.8    1.0



0.0    0.2    0.4    0.6    0.8    1.0

图 3-15两个隐层单元网络不可分的样本在二维平面的分布    图 3-16用三个隐层单元后得到的分类结果

通过标出的代表每个隐藏单元的线性分类边界的虚线，勉强还是能够看出是三条直线 的分界线拼成的。和前面例子一样，把隐藏层的分布也画出来，如图 3-17所示。能发现在 隐藏层变换后的三维空间里样本已经是线性可分的了，和上一个例子的区别只是维度发生 了变化。

接下来再看看图 3-18中 Softmax 层中的一个输出在输入样本的二维平面上的响应。



图 3-17图 3-16的样本在隐层对应的 三维空间中的分布



图 3-18将 Softmax 输出作为 z 轴，样本 在三维空间中的分介

同样是通过非线性变换把一部分样本“抬升”，和其他样本形成了区分。所以总结一 下，从分类边界的角度来看，神经元数目对应着用于“拼接”出分类边界的线段数量，神 经元越多，就能“拼接”出越复杂的边界。而从维度的角度来看，神经元数目对应着非线 性变换后空间的维度，一般来说维度越高，变换后的样本越容易被线性分开。

3.2.4更加复杂的样本和更复杂的神经网络

接着 3.2.3节所讲，更进一步，如果样本分布是如图 3-19所示。

或者再复杂一些，如图 3-20所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-144.jpg)

| O    | oA      |
| ---- | ------- |
| △    | O A ° O |
| O    | A °A    |
| △    | o △ o △ |
| O    | △       |

图 3-20二维平面上更复杂的样本分布

图 3-19二维平面上一个类似螺旋状的样本分布

甚至是高维空间里的复杂分布呢？道理也一样，增加隐藏层单元数或是层数把样本投 射到更容易线性分类的更高维度空间。或者从另一个角度可以理解为对空间进行足够精细 的非线性扭曲，然后进行线性分类。

所以定性来看 4 随着感知机单元数量的增加，神经网络的分类和拟合能力是越来越强 的。而这种能力本质上来源于两个方面：第一是非线性变换，作用是让空间扭曲，样本可 以被线性划分或者更容易在变换后的空间里线性组合得到目标值；第二是维度，维度的增 加极大地增强了样本的线性可分的程度，这一点其实和 SVM 是很像。另一方面维度的增 加也可以让简单的非线性变换组合出复杂的非线性变换，来适应样本的分布，比如 3.2.3 节中的例子。

第 1 章提到过，一个简简单单的两层神经网络（线性输入-Sigmoid隐层-线性输出）， 只要给够隐藏层神经元数目，是可以以任意精度逼近任意有界连续函数的。证明这里就不 讲了。不过通过前面的分析和第 2 章关于非线性变换的内容定性来理解，其实就是通过更 多的单元，一点一点拼出一个更加细腻的抬升/下降的边界，然后通过改变参数控制非线性 的程度，也就是抬升/下降的程度。只要神经元的数量足够多，对于任意的有界连续函数， 通过神经元的组合，都可以一点一点地拼凑出想要的形状。

^>TIPS1: TensorFlow Playground, TensorFlow 的游乐场

本节举的例子是最简单的非线性例子。读者自己也可以试试各种分布的样本在 神经网络中的分类边界是如何构成的。TensorFlow已经提供了这样的网址，供 大家自己动手搭建和训练一个神经网络用于分类一些二维空间中的样本，同时 还能查看训练过程中每层空间的变化。网址为 http://playground.tensorflow.org/。

^TIPS2:神经网络和神经

还是要提一句的是，虽然神经网络的名字里有“神经”两个字，然而通过 3.2 节的讨论可以知道，如果感知机对神经元的模仿还有那么点意思的话，人工神 经网络和真正的神经网络就没有什么关系了，所以有不少研究者更愿意称神经 网络为多层感知机(Multi-Layer Perceptron，MLP )。

3.3后向传播算法

第 1 章曾经提到过，后向传播算法是 David Rumelhart于 1986 年提出后得到了广泛 应用。不过事实上 1974 年的时候，Harvard的博士生 Paul Werbos在他的博士毕业论文中 已经提出过后向传播算法，可惜并没有造成影响。本节一起了解一下这个随着神经网络的 崛起而名声大噪的方法。

3.3.1求神经网络参数的梯度

通过 3.1和 3.2两节的讲解，对于一个典型的全连接神经网络，其实就是一层层的仿射 变换+非线性变换的疊加，所以可以将其看作是一个复合函数。比如一个《层的网络，可 以将其表示成如下的复合函数：

###### z z .    (公式 3-6)

=Sn (/。(心-批-”…^久乂^如’/卜))))…)))

其中表示仿射变换，g表示激活函数，下标表示层数。注意如果这里 x 表示的是 个向量的话，w是个矩阵，*是个向量。第 2 章已经讲过，一般的机器学习问题，都可以 通过损失函数 Z 转换成一个优化问题。将损失函数作为要最小化的目标，数据作为给定的 输入，参数作为可变量进行优化，也就是要优化的变量是公式 3-6中的 IV，Zi和 0。优化问 题的常用方法第 2 章已经讲过了，所以现在的问题是要求神经网络中每个参数的梯度。

第 2 章末尾讲过，复合函数求导可以用链式法则。而通过公式 3-6，神经网络恰好就 是一个参数量庞大的复合函数。算上损失函数 A 也不过是又多“复合” 了一些，所以求导 的链式法则当然也适用。其实这就是后向传播算法的最基本思想：通过链式法则求出所有 参数对损失函数的梯度。

3.3.2 计算图(Computational Graph)

计算图是自动求导方法中的一个基础概念，用在深度学习中主要是用来表达输入输出 以及中间变量(也就是隐藏神经元)之间的计算关系。如图 3-21所示，左边的图是一个简 单的计算图的例子。

这么一个简单的图中，每个节点代表一个变量。每条边代表变量间的关系，具体就是

图 3-21中的公式，先计算然后计算所以计算图很自然地将变量之间的依赖 关系包含了进来。另外需要注意的是，计算图中是不规定计算关系粒度的，所以我们可以 自己定义计算关系(operation)，比如直接定义这样左边的 4 个节点的计算图 就化成了右边 3 个节点的形式。例子中这么做虽然没什么意义，但这种灵活的表达方式是 有很多意义的，比如 Theano 的文档中的一个简单的例子，如图 3-22所示。



图 3-21计算图的例子

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-146.jpg)

先看左边的图，这个例子中先计算了    然后计算了 (假设 6 类 0)，最后 d

再作为输入进一步计算根据小学知识，先乘以一个数然后除以同一个数，结果当 然还是该数本身。所以 d=ab/b=a，左边的图可以化简成右边的图。

简单了解了计算图的基本概念，接下来进入正题，利用计算图通过后向传播来计算梯 度。这里还是用图 3-21中所示的简单例子，计算过程如图 3-23所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-147.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-148.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-149.jpg)

=2 c    =2 c

=2(a+b)    =2{a+b)

图 3-23计算图上的后向传播算法例子

如图 3-23所示，第一步：在有些教材或是习惯中，从输出节点出发，这一步不用算也 知道初/初=1。第二步：计算下一层，也就是 6/的父节点的偏微分初= 第三步: 再下一层，首先来计算 3c/&z = l。至此将和 dc/如相乘，就得到了输出节点 J 对输 入节点 6Z 的偏微分为= = + 也就是我们要求的梯度。注意在计算这一步的 时候，36//如=2c是在第二步计算中已经保存起来的值，所以在这一步中就避免了重复计 算，这是后向传播算法的精髓思想之一。算完了节点 6Z，节点也类似可以算出 dd/da = 2c = 2(a + b)，并且在计算时同样利用前面已经保存的= ，避免了重复计 算。注意在这个例子中，我们发现计算图的每一条边，也表示两个量之间的偏微分的值。

3.3.3利用后向传播算法计算一个神经网络参数的梯度

3.2.2节中讲到了计算图中，计算关系可以自己定义粒度，其实变量也是一样的。所以 考虑 3.3.1节中讲到的情况，变量可以是向量、矩阵或者高维张量（tensor）。在变量和计 算关系都可以灵活定义的前提下，神经网络在计算图中的表示就容易多了，下面来看图 3-24 中所示的例子。



图 3-24计算图节点的合并，感知机中的后向传播

图 3-24中左边的是一个感知机的例子，输入是一个二维向量化七），要优化的参数是 和仏计算被分解为最基本的乘法和加法操作，最后一步是一个 ReLU 非线性变 换。根据图 3-24中的关系，可以对每条边都计算出梯度的表达式，梯度传递的路径和每次 要计算的梯度用虚线表示在图 3-24中。做后向传播的时候只需要和图 3-23中一样，沿着 y 出发把所有虚线连接的路径上的梯度乘起来，直到叶节点就可以了。通过这个例子注意到一 件事，那就是后向传播算法的计算量是和一个计算图中，需要后向传播的边的总数成正比的。

接下来尝试把计算关系、节点形式和维度都变化一下，如图 3-24中的右图所示。对位 相乘实际上就是点积，根据第 2 章讲到的线性代数知识，扩展到多维就是向量和矩阵的乘 法，在图 3-24中用 dot 表示。我们来考虑将 3-24中左边的图和计算关系拓展成一个典型的 神经网络中的一层：W是个矩阵，x、A、p、＜和^都是向量，其中假设 j 的维度是 w，x 的维度是〃。根据第 2 章末尾讲到的链式求导的知识，只要从开始向后传导相关的雅可 比矩阵就可以实现后向传播了。所以和图 3-24的左图是一回事，只不过更加简洁地表达了 则根据计算关系可以知

计算和多维度的情况。另外需要注意的是，在这样一个汁算图中, 道，J/0、人（6）和人⑻都是的对角矩阵，如下：

〉0?1:0 0 0    t2>0?l:0

0

0

t >0?1:0

m

0 0

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-151.jpg)

db'

0

dt2

db2

人（P）

| 0_3z,0 | 0 ••0 •如 2 | dtmdbm J• 0• 0 |      | 0_10 | 0 ••0 •1 •• | • 1-o'• 0 |
| ------ | ---------- | -------------- | ---- | ---- | ----------- | --------- |
| ：     | ••         |                |      | 0    | 0 •         | • 1       |
|        |            | dt             |      |      |             |           |
| 0      | 0 •        | m              |      |      |             |           |
|        |            |                |      |      |             |           |

对于如果计算的时候把 FF 按行展开成一个向量，则是一个 m^mn的矩 阵，其中除了对角线上的每个 lx〃的子矩阵，其他元素都是 0，广义上相当于一个分块对 角矩阵。

|      | dp'nw    nw. | 0        |
| ---- | ------------ | -------- |
|      | 0            | dp2dw2,i |
|      | 0            | 0        |
|      | x。0    x,   | 0 …...…  |
|      | 0            | 0 …      |

如 2

0

0

dpm …dp。

dwm，n

xi

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-152.jpg)

所以总的来说，对于一个典型全连接层，其实只是在输出对应的各个维度上分别求了梯度。 最后加深一下印象，下面给出一个对实数值^进行回归的神经网络例子，如图 3-25所示。



图 3-25后向传播计算一个神经网络参数的梯度

如图 3-25所示为一个双层的全连接神经网络，输出是一个一维变量 y，实测数据以/

丨 2

表示，采用 5^-/）作为损失函数。要计算梯度的参数是 FFp Zh，恥和如，也就是仿射

变换的参数。整个梯度的计算过程是：首先进行前向计算，由输入*开始向前传播，将所 有节点的值计算出来：然后执行后向传播算法，由损失函数£开始，计算梯度并向后（x 所在方向）传播，其中 3£/办=    其他边对应的梯度计算和图 3-24中的例子相似，唯

一的区别是最后一层的输出是单输出（一维）。当然这个例子只是用来表示神经网络计算 图的一种，特别是为了方便说明后向传播的概念。就像前面说的，计算图的粒度和计算关 系都是可以自己定义的，只要方便计算或者实现即可。

^TIPS1:更多关于后向传播算法

如果有读过一些比较早的机器学习和神经网络教材的读者可能会发现，这里说的 后向传播和老教材中不太一样。这是因为许多老教材中，比如 Tom Mitchell的《机 器学习》，一讲到后向传播，往往是和权值更新打包一起讲，而且在那个时候梯度 下降的方法也比较单纯、原始，还没有用到像现在这么多一阶梯度下降的变种。 而随着深度学习的发展，如今更新权值的思路其实变得更简单粗暴了。概括一 下就是，把原来打包式的做法拆开成了两步：即求梯度和梯度下降。所以现在 再提到后向传播，一般只是指第一步，即求梯度。

^TIPS2:深度学习中的张量

本节已经提到了张量，在实际的深度学习应用中，比如卷积层，样本的 mini-batch 等（后面会讲到），很多地方都会接触到张量。张量甚至成了一些框架中的基 本元素，比如 Theano，甚至 TensorFlow 的命名就是张量。

不过要说明的是，深度学习中当提到张量时，和物理或是几何中的张量关系不 大，可以简单认为就是在不同维度上存数据的一种形式。无论对张量的梯度计 算是如何实现的，都可以等效于将张量展开成一个向量，计算完梯度之后，再 按照原来张量中元素对应的顺序“折叠”回一个张量。

3.3.4梯度消失

后向传播算法虽然很有效率地解决了梯度计算的问题，但是并没有完美解决神经网络, 尤其是多层神经网络训练的问题。在第 1 章的神经网络兴衰史里也提到了，当时面临的难 题是梯度消失和梯度爆炸问题，也正是本节要讲的主题。

梯度消失是传统神经网络训练中非常致命的一个问题，其本质是由于链式法则的乘法 特性导致的。比如来考虑深度学习之前在神经网络中最流行激活函数之一 Sigmoid，其表 达式和导数如下：

(7( X

(J [X

l + e_x

e'x

(公式 3-7)

(1

把这个函数的导数画出来，如图 3-26 所示。



图 3-26 Sigmoid函数的导数

对于 Sigmoid，导数的最大值在输入 为 0 处，值为 0.25。考虑一个激活函数都 是 Sigmoid 的多层神经网络，则梯度向后 传导时，每经过一个 Sigmoid 层就需要乘 以一个小于 0.25的梯度。而每乘一个梯度，

则梯度的值又变得更小一些。况且在优化 的过程中，每个激活层输入都在 0 附近的 概率非常非常低。也就是说随着层数的加 深，梯度的衰减会非常大，迅速接近 0，

这就是梯度消失问题。

梯度消失问题是传统多层神经网络难以训练的最大难题。根据上一段对问题的描述可 以知道，这个问题在训练深层网络中的体现是：离输出层越近的参数，梯度越大，成了主 要在进行学习的参数；而远离输出层的参数则只能在接近 0 的梯度则以一个非常小的速率 进行学习。这种情况相当于一个恶性循环，因为靠近输出层的节点的值都是由前面那些学 习速率很慢的层执行前向运算得来。而前面层因为学习速率慢，所以参数未必学到了什么 特征，所以这些后面层的输入随机性会比较强，这样相当于在一个有随机性的数据上进行 学习，即使学习速率再快，最后也未必能真的学到有用的特征。这一过程又会让前面层的 参数更难学到有效的值。所以将 Sigmoid 这一类函数作为激活函数的传统神经网络，随着 层数的加深，效果反而会下降。

因为导致梯 $ 消失的根本原因是小于 0 的梯度连续做乘法。现在看来解决的思路主要 有两种：第一种是第 1 章里提到过的逐层无监督预训练+后向传播微调。这个办法其实严 格来说并不是针对梯度消失问题，只是预先找到一个很好的初始化位置，降低上一段说的 靠近输入层一直在“瞎学”的风险。加上这个方法训练起来也比较麻烦，如今几乎没人用 了，更为主流的解决办法是，第二种方法，ReLU等可以避免梯度衰减的激活函数。不过 需要注意的是，即使有了 ReLU，在非常深的网络中仍然会存在梯度传播困难的问题，因 为小于 1 相乘的衰减并不是梯度减弱的唯一因素，后面内容中还会继续探讨。

3.3.5修正线性单元(ReLU)

ReLU (Rectified Linear Unit)在 3.1.3节中也提过了，这里再展开讲讲，如图 3-27a

所示。

ReLU中，小于 0 的部分直接置 0，大于 0 的部分即为输入。这样即实现了非线性变换, 同时大于 0 的部分梯度为 1。这样对于需要从输入一直传递的信息，激活函数的梯度总是 1，即使连续相乘也不会变小，解决了梯度消失的问题。第 1 章也提到过，2012年 Alex Krizhevsky提出的 Alexnet 里，ReLU是帮助训练快速收敛的一个有力手段。

根据 ReLU 的定义，信息只能在 ReLU 的输入大于 0 的区域进行传播(前向和后向)， 这带来了另外一个优点就是稀疏性。稀疏性不仅对网络的性能提高有帮助，而且从神经科 学的角度，神经元的激活率是非常低的，这也是一种仿生的模拟。不过这种特性也带来一

个问题，就是在输入小于 0 的区域，即使有个很大的梯度传播过来也会戛然而止，也就是 “挂了”，所以这个问题被称做 Dying ReLU。针对这个问题 Andrew Ng组提出了 Leaky

ReLU，如图 3-27b所示，将 ReLU 小于 0 的部分改成一个斜率小于 0 大于-1的线性函数 y=axf对于 Leaky ReLU, a是一个非常小的值，比如论文中给出的是 a=0.01。这个名字非 常形象，感觉就像是二极管中的漏电流(leak current)，不知是不是作者对电子的课程印 象深刻。总之，Leaky ReLU就是用一个非常小的系数让信息和梯度的传播不至于中断。同 时系数很小还一定程度上继承了 ReLU带来的稀疏性优点。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-155.jpg)



c)

图 3-27 ReLU、(Randomized) Leaky ReLU、Parametric ReLU 和 Softplus 示意图

在 Leaky ReLU 的基础上，有人在 Kaggle 的 NDSB (National Data/Science Bowl)竞赛 中，把求平均提高泛化能力的思想加入进来，发明了随机版本的 Leaky ReLU。这个方法具 体来说就是在训练的过程中《按照高斯分布随机取值，测试/验证的时候取均值。直观理解 也参照图 3-27b所示。

既然小于 0 区域的斜率可以取值，那么为什么不把这个值作为一个参数呢？于是 MSRA的 He Kaiming提出了参数化 ReLU (Parametric ReLU)，让 a 称为一个可优化的参 数，如图 3-27b所示。

不管是 ReLU 还是 PReLU，都是有棱有角的，实际应用中还有一个平滑版本的 ReLU, 叫做 Softplus，公式如下：

/(x) = ln(l + ex)    (公式 3-8)

Softplus的曲线如图 3-27c所示，其中虚线是用来对照的 ReLU。Softplus其实就是 Sigmoid的原函数，用来做激活函数是 Bengio 的组于 2000 年 NIPS 的文章里提出的。除了 图 3-27中画出来的这些，ReLU 一族或是类似 ReLU 的激活函数还有 Noisy ReLU、Shifted ReLU、Exponential LU和 Concatenated ReLU等，在此就不展开说了，有兴趣的读者可以

###### 自行学习。

本质上来说，ReLU解决梯度消失的原则是靠梯度等于或接近 1，从而避免连续相乘的 结果衰减，而 ReLU—族或类似的办法也都是基于这一原则的变种。

3.3.6梯度爆炸

梯度的不断衰减是因为连续乘法导致的，那么如果在梯度的连续乘法过程中总是遇上 很大的(绝对)值呢？比如考虑一个全连接网络，随着算法的进行，参数进入了某一个区

域，导致一条路径上的多个节点计算出的值都很 大。在每次计算仿射变换对应的梯度时，都会得 到很大的值，所以在后向传播算法中梯度可能 会因为连续乘法出现非常大的值。一旦这个值 使得权重的更新步长过大，就像第 2 章讲学习率 时举的学习率过大的例子(如图 2-51a)…样，

很可能会让算法不收敛，这个问题被称为梯度爆 炸问题。下面举个比较形象的例子，来看图 3-28。

如图 3-28所示为一个二维的例子。可以看到 函数的曲面有一个类似“断崖”的区域，当优化 的参数值在某一步恰好到了 “断崖”时，会获得



图“断崖”和梯度操性

一个极大的梯度，而如果还是将这个梯度乘以学习率作为迭代步长的话，下一次迭代的步 长会非常大。如图 3-28中的实线箭头所示，从而“飞”出了合理的区域，这就是梯度爆炸。 根据描述和图 3-28中的例子，解决梯度爆炸的方法也很直观，只要把沿梯度方向前进的步 长限制在某个值内就可以避免过大的步长了。根据这个思路，一个常用的办法是如果计算 出来梯度的范数(norm)大于某个阈值，则对梯度以这个范数为基准做归一化，让新的梯 度的范数等于这个值。举个例子，还是以二维情况来讲，如果算出来在“断崖”处的梯度 为(60, 80)，而梯度的范数的阈值限制在 5，则有：

60

x5,

80

x5

(0.6x5,0.8x5) = (3,4)

a/602 +802    7602 +802

这种将梯度的#大值限制住的办法，通常被称为梯度裁剪(gradient clipping)

3.3.7 梯度检查(gradient check)

因为梯度的计算常常不稳定，而且后向传播算法本身依赖于梯度表达式，所以并不是 一个容易实现和调试的算法。因此人们想到了用数值计算的梯度和计算图计算的梯度进行 对比，来检查梯度的计算是否出现了错误，俗称梯度检查(gradient check)。计算的方法 也很直接，对于一个维度而言：

• 102 •



—-—I= lim— ------ ~ — ------ (公式 3-9)

Ad    2e    2 A

也就是说用一个非常小的 A 通过数值的方法实现对梯度的近似。A的取值应该尽量小， 但是又不能太小，否则有可能因为浮点数精度引入计算误差，通常这个值在 le-4〜le-6 之间。

有了数值计算的梯度值，就需要和表达式算出的梯度值进行对比，对比的思路也比较 简单，就是比较梯度差异相对梯度的值，一般用如下公式：

relative error =

I八-八 I

臟(八，八)

(公式 3-10)

其中是解析(analytical)的梯度值，是数值计算(numerical)的梯度值，这个

相对误差作为判断梯度是否出现错误的一个参考。按照斯坦福大学的课程《用于视觉识别 的卷积神经网络》(CS231n)的建议，单精度情况下，这个值大于 le-2则说明梯度很可 能错了，小于 le-7则说明没什么问题，在这两个值之间则视损失函数的平滑度决定。

3.3.8从信息传播的角度看后向传播算法

通过前面的讨论可以知道，无论是梯度消失还是梯度爆炸，其根源都是链式求导法则 的连续相乘的特性。从这个角度来看，梯度消失和梯度爆炸其实都是一个问题，就是梯度 累积相乘带来的不稳定性。那么是否解决了梯度消失和梯度爆炸问题，深层网络的训练就 解决了呢？其实梯度的消失和爆炸只是梯度传播过程中不稳定性体现的两个方面，其他一 些因素也影响了深层网络的训练。

从信息论的角度来看，数据经过处理的步骤越多，则丢失的信息可能也会越多，这叫 做数据处理不等式(Data Processing Inequality, DPI)，表达如下:

X^Y^Z

（公式 3-11）

其中 x—y—z是一个马尔科夫链。/是互信息，表示一个随机变量包含另一个随机变 量信息量的度量，这里可以简单认为是信息相关性的一种度量。第 2y 章里讲到过的 KLL 散 度就是可以描述互信息的一种表达。当然这些都是定义，在这里可以认为就是 hr—Z信 息传递的路径，而代表着信息传播的过程中，只有可能丢失，而不可能增加。 而这种信息的丢失，或者说信息在传播过程中的误差，体现在神经网络中就是后向传播算 法随着传播带来的信息丢失。具体来说要计算梯度都需要输入数据，而数据的分布所携带 的信息在计算后向传播的过程中会一层层丢失，这种信息丢失的多少也是影响算法优化性 能的一个重要原因。

除了 DPI，梯度计算本身的性质也会导致信息随层数增加而丢失，具体来说就是梯度 所代表的线性近似。先来看图 3-29。

从函数展开的角度，梯度下降的本质就是对函数做了 一阶线性近似，然后再进行迭代。如图 3-29所示，画出了 一个非线性曲线的一段，及其某一点梯度所代表的线性近 似。可以看到，离求梯度的点越近，这种近似越好，相反 则误差越大。在每次迭代时，线性近似的误差会按照后向 传播的方向从输出一层层传递到输入层，而且随着迭代的 次数增加，误差越来越大。对于 ReLU 等单元，这种误差 同样存在，虽然大部分区域都是可以被线性近似完美拟合，

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-158.jpg)

但是输入为 0 处，也就是梯度变化的过渡区域是无法被线 性近似的。只要迭代时经过了过渡区域，误差就会产生。

而层数越深，则在后向传播计算后迭代时经过过渡区域的可能性就越高。

针对信息传播的误差问题，也有很多办法，比如增加辅助损失函数(auxiliary loss), 让梯度在传播的路径变得短一些，这在 2014 年 ILSVRC 的冠军网络结构 GoogLeNet 中就 有所体现，使用了两个路径更短的辅助损失函数来促进收敛。直观来看这个方法是帮助靠

近输入的低层的特征有更强的区分性，而从信息传播角度则是减少了传播路径的长度，减 小信息传播中的丢失。

在 2016 年的 ECCV 上，北京大学的林宙辰教授的小组提出了接力后向传播算法，其 基本思想是每次后向传播只传播一个较短的长度，然后利用辅助损失函数像接力一样，把 没有传播到的层再继续利用后向传播算法接着计算。该方法在应用到 2015 年 ILSVRC 的 场景分类竞赛(Scene Classification Challenge)中取得了第一名的好成绩。

3.4随机梯度下降和批量梯度下降

有了数据，有了神经网络模型，又有了后向传播算法计算梯度，第 2 章也讲过了各种 一阶优化算法，感觉万事俱备了。是否直接把所有数据往模型里一放进行训练就可以了呢？ 实际情况并没有这么简单，本节来讨论下训练神经网络，尤其是深度网络中的一些数据层 面的具体做法。

3.4.1全量数据(full-batch)梯度下降

全量数据梯度下降是最直接的做法，每次计算梯度的时候把所有训练数据都考虑进来。 具体来说就是，假设一共有个 7V 个样本的话，计算损失函数的时候，对所有的 7V 个样本都 求一遍损失函数的值，然后求平均：

1 N

.    (公式 3-12)

/-    Jy i=\

这个方法虽然简单、直接，并且能最好地将数据分布的信息代入到梯度计算中，同时 还方便并行计算的实现。但是在大数据时代，数据量少则几万，上不封顶，使用全量数据 的梯度下降就显得很不现实。

3.4.2随机梯度下降(SGD)和小批量数据(mini-batch)

和全量数据梯度下降相对的办法是随机梯度下降(Stochastic Gradient Descent, SGD)。 具体的做法是每次从训练样本中随机抽取一个样本用来计算损失函数，将相应计算出的梯 度作为当前一步梯度下降的依据。因为每次梯度下降只抽出一个样木进行计算，所以 SGD 的第一个优点就是快。比如一个有 10000 个样本的数据，如果用全量数据计算梯度，10000 个样本都算一遍，也就下降一步。而用 SGD 的话，10000个样本都过一遍，就下降了 10000 步！这在计算效率上的差异是显而易见的。

虽然 SGD 在计算效率上有巨大优势，不过上面说的是理想情况，根据描述 SGD 还有 另一个特点是随机性。因为数据是从全量中进行的采样，所以仍然是包含数据分布的信息, 可以知道 SGD 在梯度下降过程中的随机性非常大。随机性像噪声一样带来不确定，但是在 深度学习的应用中是一种优点，因为深度学习中面临的基本都是非凸优化问题。第 2 章讲 过克服非凸问题的一些方法，比如加入冲量。SGD则相当于另一种思路，通过随机性的引

入，实现了梯度下降时的启发式搜索。下面来考虑一个形象的例子，如图 3-30所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-159.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-160.jpg)

a)    b)



d)

图 3-30全量梯度下降和 SGD 对应的损失函数曲面

图 3-30a是在考虑到大量数据时，损失函数曲面的图，可以看到数据量大的时候，随 机性非常小可以忽略，曲面有一个非常稳定的极小值。而图 3-30b、c、d图是考虑 3 个随 机样本对应的损失函数曲面。因为数据都是从同一个分布中而来，所以曲面的大体形状和 图 3-30a相似。而随机性则让曲面在局部和 3-30a产生了很多差异，这种差异的好处是在 大量数据的曲面中，极小值的位置在单个随机样本对应的曲面中未必是极小值。这样一来， 在 SGD 中，梯度有一定概率是朝着“逃出”极小值的方向，从而避免陷入极值，同时梯度 下降的大方向仍然保持正确。总之综合两点考虑，SGD比起全量数据梯度下降还是有很大 优势，但是在实际应用中，尤其是类别很多的分类问题中，SGD的随机性也有可能导致算 法不收敛，所以更为常用的是一个折中的办法，就是小批量数据(mini-batch)。

小批量数据的思路其实还是一样，全量数据太多，纯 SGD 每次用一个样本不太稳定， 所以每次计算梯度随机选取一定数量的部分样本。这样计算量仍然远小于全量数据，同时 一定量的采样既能保持一定的随机性，又能控制随机的程度不会过大导致算法不收敛。此 外 SGD 每次只有一个样本，无法在数据层面并行计算，而 mini-batch则可以并行。既然是 批量的数据，每个批次的样本量的选择也成了重要的超参数。尽管有研究(LeonBotton，

《Large-Scale Machine Learning with Stochastic Gradient Descent》)表明 SGD，也就是样本

量为 1 是最快的，但实际使用中的问题让样本数量的选择成了一个经验活。比如 Github 上 有个热度挺高的库叫做 caffenet-benchmark，里面通过实验评估了各种超参数对 imagenet 训练结果的影响。在这个库的评估里，训练 imagenet-2012数据最优的样本数量是 64。最 后要说的是，mini-batch严格来说和 SGD 不是一回事，不过随着时间的推移，现在在深度 学习中一提到 SGD，其实多数指的都是 mini-batch。

在各种深度学习框架中，经常看到的两个概念一个是迭代次数(iterations)还有一个 是训练了多少代(epoches)。迭代次数就是指更新了多少次参数的值，而代数就是值遍历 了多少次所有的训练样本。

皂 TIPS:深度学习中的极小值

第 2 章已经提到过，高维度中，极小值并不是最大的问题，鞍点反而会更加令人 头疼。事实上，在神经网络尤其是深度学习中，即使存在极小值并且算法最后收 敛到了极小值，也未必是个大问题。根据 Bengio 教授的一些研究，在高维度和 深层网络下，局部最小值往往和全局最小值非常接近，所以效果上也未必有什么 大差别。而 LeCun (《The Loss Surfaces of Multilayer Networks》)、斯坦福大学 以及 Google 的一些最新研究(《Qualitatively Characterizing Neural Network Optimization Problems》，《Identifying and attacking the saddle point problem in high-dimensional non-convex optimization》)则发现，尽管深度学习中损失 A 数 的曲面严格来说是非凸的，但是大致形状也是凸的。总之极小值在深度学习中越 来越不是问题了。

3.4.3数据均衡和数据增加(data augmentation)

现在考虑一个二分类问题，比如区分一张图片里的是一只猫还是一条狗。因为某种原 因，数据搜集的时候只找到了 2000张猫的图片和 50000 张狗的图片作为训练数据。如果直 接把这 52000 张图片放在一起，然后用 mini-batch的方法进行训练，则会出现一个问题， 就是数据的不均衡。比如每次迭代的样本数量为 100 个，那么平均下来每次样本中猫的数 量通常只有 3、4个。这样的结果是计算损失函数的时候，狗的图片比重太大，所以相当于 是针对狗的图片进行了充分的训练。猫的图片带来的损失的影响很低，所以猫的特征就并 不容易被学习到，这会影响分类器最终的性能。

很自然地，我们希望训练数据总体上是均衡的，所以对数量不平均的数据，在训练前 一个必要的步骤就是做数据均衡。最直接的想法就是把猫的图片复制 24 遍，这样猫的样本 也是 50000 张和狗的样本打平了。这样做是有效果的，不过在实际应用中更常用的手段是 做数据增加。

在机器学习中，数据通常都是来源于一个分布。比如猫的图片，无论图片中的猫是什 么颜色，什么姿势，眼睛是瞪着还是眯着，都能看出是一只猫。而所有的猫的图片，都可 以看作是来自于一个分布。所以在做数据平衡的时候可以考虑模拟这个分布来产生一些和 已有数据不完全一样的数据。具体到猫的图片的例子，就是可以考虑把图像旋转一个角度, 左右镜像对称，明暗度做一些改变，或者是裁剪等，如图 3-31所示。



明暗



长宽比



旋转    裁剪

图 3-31猫图片数据增加的例子

做了这些改变之后，仍能一眼看出这是只猫，可以认为图片仍是来自“猫”这个分布。 数据增加不仅可以用来解决样本数量不平衡的问题，还可以带来其他的好处。比如想象一 下 2000 张猫的训练数据全是向左看的猫，那么训练出的模型遇到一只向右看的猫就未必能 判断得很准。所以数据增加的时候可以随机加入水平翻转，让向右看的猫也出现在训练样 本之中，增强模型对从未见过的数据的判断能力，这也是数据增加带来的好处。所以在实 际应用中，即使是数量多的-一类的图片也可以用数据增加的手段来获取更多的训练样本，

来增强模型对于未见过的数据的分类能力。比如猫和狗的例子中，可以通过数据增加把猫 和狗的图片都增加到 10 万张再进行训练。甚至可以在每次训练迭代之前实时在原始图片基 础上随机加上一些变化，相当于拥有了无限的数据增加的样本。现在很多深度学习框架中 都已经出现了这样的预处理结构。

0>TIPS:类均衡采样(Class-aware Sampling)

尽管数据做了均衡后可以保证总体数量上的均衡，但是单纯随机采样的 mini-batch中，样本的分布却还是随机浮动的，尤其是当分类类别大于批量采样 的数目时，有时候一个类出现一个以上的样本，而有的类却一个都不出现，这 些情况的出现对于收敛并不是一个好的因素。所以一个思路是，除了总体上样 本数量的均衡，在梯度下降的每个迭代批次进行采样的时候，要尽量让样本在 类别和数量上都更加均匀。类均衡采样(Li Shen, 《Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks》)就是这个思路下的 一个方法。

这个方法首先将所有的类别列出来，并随机打乱顺序。每次采样的时候按照采 样数量在打乱顺序后的类列表上依次采样，如果采样到列表末尾，则重新打乱 顺序继续采样。这样保证了每个批次数据中，类别的均匀性。然后在每个类别 内再用同样的策略进行随机的样本采样，这样不需要做数据增加就实现了均匀 采样，而且均匀度比平衡样本数量后直接随机采样更高，训练时的整体收敛性 也更好。

3.5数据、训练策略和规范化

从本节开始的内容其实不限于神经网络，而是机器学习中普遍的问题，当然，神经网 络是机器学习算法中的一种。当我们衡量一个机器学习算法是否好的时候，最直接的方法 是衡量算法预测和测量数据的误差。比如对于训练数据，损失函数就是衡量这种误差的方 法。训练一个神经网络时，损失函数降到一个非常小的值，我们说网络很好地收敛了。然 而当网络训练好以后，是否能对训练数据以外的数据也做出很好的预测呢？其实这才是 更看重的指标，这种对训练数据以外数据做出准确预测的能力称为泛化(generalization) 能力。

3.5.1欠拟合和过拟合

欠拟合和过拟合是机器学习中常见也非常简单的两个概念，下面来看图 3-32所示的例子。



图 3-32 —维拟合的例子

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-166.jpg)

图 3-32中所示的 3 个图中都画出了同样的数据点，这些点都是由一个“倒 U”形状的 曲线加上叠加一个高斯分布的噪声产生的。虽然看上去不是很整齐，但还是能一眼看出来 是一个“倒 U”的形状。其中红色的圆点是用来训练数据，而蓝色的三角形点则是训练中 未曾出现的数据，不过就像前面提到的，红色和蓝色的点都是由“倒 U”曲线加上噪声产 生的样本，所以它们是来自同一个分布，在机器学习中，这种分布通常称为数据产生分布 (data-generating distribution)，事实上在前面内容里曾提过这个概念。训练数据和测试数 据来源于同一分布，是机器学习中的一个基本假设。这是一个很直观很显然的事情，比如 我们训练了一个机器学习算法，根据体温、心率和血压用来判断一个人是否很平静，这个 模型无论判断人有多准确，都难以直接用到一只猫身上，因为猫的各项指标来自于完全不 同的分布。或者再白话些就是我们从一件事情中学到的经验未必能放到另一件事情上。总

之，在数据同分布的假设下，从训练数据中学习到的特性，才能有效预测没有见过但是和 训练数据来源于同一分布的数据，这个过程就是泛化。

在这个假设下，图 3-32a中尝试用一个线性的模型去拟合红色的圆点数据，结果可想 而知，不仅红色的训练数据难以拟合，蓝色的测试数据也根本对不上。其根本原因是因为 线性拟合的模型过于简单，表达能力不足以学习到数据点的特性，这种情况称为欠拟合。

图 3-32b中使用了一个复杂得多的模型，比如可以是一个包含很多高次项的多项式，或 者是一个隐藏层单元很多的双层神经网络。这样的模型有了非常强的表达/拟合能力，所有的 红色圆点都被很好地拟合。不过从图 3-32中来看，拟合得有些过分好了，因为数据点是有明 显的中间高的趋势，同时也有噪声附加在这个趋势上。而图 3-32b中的曲线连噪声一起学习到 了，虽然训练数据误差接近于 0，但是对于没见过的蓝色三角形数据点，就完全不靠谱了。这 种情况相当于“记住”每个训练数据的位置，而丧失了对新数据的泛化能力，称为过拟合。

图 3-32中用一个比线性模型复杂，又不会表达能力过分的模型，可以是一个二次函数， 也可以是一个比 3-32b模型简单得多的神经网络。总之是一个恰到好处的模型，成功学习 到了数据的趋势。这种才是我们想要的情况，也就是能够成功泛化的模型。

模型的拟合能力通常被称为 capacity, 一般情况下，如果欠拟合解决办法是通过增加 模型复杂度来增强拟合能力，比如对于神经网络，增加层数或者神经元数量。而过拟合则 不是那么轻易解决的事情，需要在模型复杂度和训练精度之间寻找一个平衡。如果欠拟合 解决办法是增加模型的复杂度以达到更强的学习能力，比如对于神经^网络，增加层数或者 神经元数量。而过拟合则不是能那么轻易解决的事情，因为一般来说减小误差的趋势往往 是通过更复杂的模型实现的，这个趋势容易导致过拟合，需要在模型复杂度和训练精度之 间寻找一个平衡。

3.5.2训练误差和测试误差

前面也提到了数据通常分为两部分，一部分是训练数据，用来让模型做后向传播学习 参数，模型在训练数据上预测值和数据值的误差衡量被称为训练误差(training error)或者 经验误差(empirical error)；另一部分是测试数据，相应的误差衡量被称为测试误差(test error)或者泛化误差(generalization error)。一般来说训练误差是否变小代表着学习的过

程是否收敛，而测试误差是否足够小则和模型对于未见过的样本预测能力的好坏直接相关, 通常来说，测试误差总是大于训练误差，不过这两者随着模型能力的变化趋势并不相同。 想象一下如果可以看到两种误差随着能

力的变化趋势的话，当模型的能力不够 时，训练误差和测试误差都会随着模型能 力的增加而降低，这对应着欠拟合。当模 型的能力恰到好处的时候，测试误差达到 最小。随着模型能力的继续增加，模型开 始在能够学习训练数据分布的基础上，进 而开始学习到每一个训练数据，这就是过 拟合，所以测试误差会越变越大。这个趋 势如图 3-33所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-167.jpg)

需要说明的是，这个趋势曲线中的测试误差和训练误差的差异(generalization gap)随 着模型能力的增长是不断变大的。当模型能力超过了最佳的点时，这个差异的增长速度就 会超过训练误差的下降速度，这就是为什么测试误差会越来越大。

通常来说我们会更希望测试误差和训练误差的差异更小，因为这代表着更好的泛化能 力。由于数据都是来源于某种分布，如果从抽样的观点来看待训练数据，根据最基本的统 计知识，数据量越大，抽样误差越小。也就是说随着数据的增加模型对真实分布描述的误 差就越小，泛化性能就会越好。不过就算数据量可以无限增加，也不能完全解决误差的问 题，因为在数据中还隐含了一个假设，就是我们讨论的都是带有随机性的数据。这些随机 性有可能是误差，有可能是一些难以观测到的变量。为了更好理解数据量和误差的关系， 下面来看个例子，如图 3-34所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-168.jpg)



图 3-34完美模型的误差随数据量的变化

图 3-34a是当数据不多的时候，我们得到了一个如实线所示的最优模型，中间的黑色 虚线代表真实数据下能得到的最优模型。按照前面的论述，数据量少的时候，对真实分布 的描述可能会有较大误差，所以中间的实线和虚线完全没有重合。

自然而然地，我们希望有更多的数据，比如图 3-34b所示的情况，最优模型的泛化能 力就会好一些，但＞训练数据的增加对模型的能力提出了更高的要求。•般来说，这个时 候的最优模型会比数据少的时候复杂一些，但是在训练数据上的误差会上升，而对真实数 据的误差会下降，并且下降程度会超过训练误差的上升程度，也就是说前面提到的训练误 差和测试误差的差异缩小了。可以看到中间的实线和虚线的差异变小了。

再来考虑最极端的情况，假设我们能够获取任意多的数据，或者说多到对真实分布的 描述误差可以忽略，如图 3-34c所示。这种情况下，可以认为得到了完美的模型，训练误 差和测试误差的差异就可以忽略了，它们都会趋向同一个值，称做贝叶斯误差(Bayes error) o

^TIPS:非参数模型

因为本书主题是深度学习，所以截止到这里只讲了参数模型，也就是神经网络。 参数模型可以理解为参数数量给定的模型。比如神经网络，定好了结构的话，参 数数量是确定不变的，学习过程只要根据训练数据和损失函数优化参数的值就可 以了。因为参数和形式的确定，所以给定一个参数模型，模型的能力也是确定的。 非参模型则是和训练数据更紧密相关，参数数量会随着数据的不同而变化，最简 单的例子比如各种插值法，还有 SVM 和高斯过程等。非参数模型相对于模型能 力的控制更容易一些，比如 SVM，核函数半径如果很短，则能够获得很强的分 类能力，训练误差可以很低，甚至每一个样本都成为支持向量，从而过拟合；而 长一些的核函数半径则能降低过拟合的风险。不过非参数模型大都是基于前面提

到的局部泛化的思想，而神经网络则是希望能从数据中学习到特征 (representation)。

3.5.3奥卡姆剃刀没有免费午餐

奥卡姆剃刀(Occam's razor)和没有免费午餐(No Free Lunch Theorems, NFL)是两 个不同的理论，放在一节是因为这两个理论都有些哲学和逻辑的意思，并且名字都有些怪。

先来说说奥卡姆剃刀。14世纪的时候，有个哲学家 William of Ockham，他在自己写的 -本讲道理的书里阐述了一■个观点：Non sunt multiplicanda entia sine necessitate。翻译过来 就是“如无必要，勿增实体。”这句听上去很有道理的观点在 400 年后被苏格兰的玄学和 哲学家 William Hamilton爵士正式命名为 Occam’s razor。无论从原话还是名字来看，都透 着一种简单至上的思想。具体到机器学习中，这个哲学观点代表的意思是：在所有能够很 好解释训练数据的模型中，选择最简单的那个。那么怎样才能算是很好解释训练数据呢？ 理想情况就是图 3-33中虚线右边的部分，最简单的那个就是虚线对应的那一点，通常就是 一个最优(optimal capacity)的模型。在很多传统机器学习算法中，奥卡姆剃刀并不是一 句大道理，而是有统计学习理论的支撑。不过在神经网络，尤其是具体到深度学习中的理 论支撑并不明确，本书也并不涉及这部分内容，有兴趣的读者可以自背研究学习。

接下来再谈谈什么是没有免费的午餐。前面两节已经反复说过，&器学习根本来说就 是从已有数据中学习，然后泛化到未见过的数据。特别地，对于分类问题，就是通过学习 已经带有标签的数据，来预测未见过的数据的类别。前面还提到数据的同分布假设非常重 要，因为同分布才是泛化的前提，那么是否可以有算法从各种所有可能出现的分布中都进 行学习，并且有效泛化呢？答案是否定的，美国数学和计算机科学家 David Wolpert和 William Macready在 20 世紀末指出并证明：如果考虑所有可能的数据分布，不管什么算法， 分类误差的期望值都是一样的。也就是说天上不会掉下一个像免费午餐一样的算法，比其 他算法都好，考虑所有可能的分布的话，什么算法水平都-•样，甚至是一个随机的分类算 法。定性来理解如图 3-35所示。



图 3-35没有对任何分布都有效的分类算法(NFL)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-171.jpg)

图 3-35中黑色实线是分类边界，a、b和 c 图对应 3 种不同分类器和各自分类效果最好 时的样本例子，图 3-35a里分类效果好的线性分类边界，到了 b和 c 图中则不灵。同样道 理看似过拟合的图 3-35c中的分类器，其实对图 3-35c的分布有很好的效果，但在 a 和 b 图中则并不好。所以全局来说并不存在某种算法比另一种算法更好，只有针对特定问题， 特定的数据分布时，找到一个好的算法才有意义。

其实上面说了这么多就两件事：泛化很重要；数据很重要。

3.5.4数据集划分和提前停止

通过前面 3 节的讨论，我们知道数据主要分为训练数据和测试数据。在实际的训练中， 还会使用到一个数据集，叫做验证集。验证集通常被用在训练过程中，但并不作为用于更 新网络权重的训练数据的一部分。主要作用是在训练过程中评估模型的准确度和损失函数 值，作为一个指标选择模型的超参数，比如网络层数、隐藏单元数量，或是激活函数的类 型等。同时验证集也用来观察训练是否已经过拟合，因为验证集并不作为更新网络权重的 数据，所以能体现出网络的泛化能力。在神经网络中，随着训练的进行，在训练集和验证 集上的损失函数值常呈现如图 3-36所示的趋势。

图 3-36和图 3-33的趋势非常相似，只不过 ，



3-36训练过程中损失函数在验证集和 训练集上的趋势

横坐标换成了迭代次数。定性理解这个趋势，在

训练的开始阶段，模型什么都尚未学到，所以无

论是训练集还是验证集误差都很高。随着训练的|

不断迭代，在训练集上的误差会越来越小，这是 f 数

学习到数据特征的阶段，模型仍然欠拟合，所以 i 在训练过程中，训练集和验证集上的误差都下降 了。然后就是过拟合阶段，训练集的误差不断下 降，但是泛化能力则变差，于是验证集上的误差 p 变高了。所以当验证集上值最小时，就是参数最 优的时候。 7

一个非常简单 fcj 策略是每当验证集上的损失函数值达到一个比之前更小的值，就把这 个值记录下来作为最小值，然后继续训练，直到在发现某 i 个最小值之后一个给定的迭代 次数内，如果没有发现新的更小的值，则认为已经过拟合了。于是之前验证集上最小值对 应的模型就是最优的模型，这个简单粗暴的策略叫做提前停止(early-stopping)。提前停 止虽然简单，但是有个潜在的风险就是用来判断是否已经过拟合的次数并不容易确定。这 个次数无论是个常数还是一个比例，如果太长，会浪费计算资源，如果太短，则有可能在 验证集误差还有可能继续下降(比如鞍点或者缓慢离开极小值时)的时候就提前结束。

虽然验证集只是在训练中使用，但是和测试集有个共性就是都不参加到参数的梯度下 降训练中。如表 3-1总结了这 3 种数据集的特点。

表 3-1训练集、验证集和测试集

| 作    用       | 训练集                | 验证集         | 测试集 |
| -------------- | --------------------- | -------------- | ------ |
| 训练模型       | 调节超参数 避免过拟合 | 评估模型性能   |        |
| 使用阶段       | 训练                  | 训练，一定间隔 | 测试   |
| 梯度下降中使用 | 是                    | 否             | 否     |
| 模型性能参考   | 否                    | 是             | 是     |

当然，如果数据非常匮乏的情况下，有个策略是将验证集和测试集合二为一，则比没 有强。

3.5.5病态问题和约束

通过改变模型的拟合能力来避免过拟合并不是一件容易的事情，更常用的办法是使用 规范化对模型的参数进行一定的约束。下面来考虑一个非常简单的例子，求下面方程的解:

2x-y+2=0

这是一个二元一次方程，有无数个解，都在如图 3-37a所示的这条直线上。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-173.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-174.jpg)

图 3-37病态方程求解和规范化    (

这是一个典型的病态(ill-posed)方程，有无数个解可以满足方程，可以通过 x 和 y 带入等式左边求出 0，但是通过等式来推导;v和 y 的值却是不可行的。病态方程除了有不 可逆的性质，在数值计算上也不受欢迎。比如取 x=109, y=2xl09+2，那么如果％和^的系 数发生了很小的变化，则这个变化会被放大很多到等式的右边，在数值计算中，这常常是 不稳定的。

针对病态方程，一个常见的办法是加入一个约束项，缩小*和 y 的取值范围，比如令 x2+/=0.8，如图 3-37b所示，则相当于约束 x 和^在半径为 2/人圆上，于是相切点(-0.8,0.4) 成了一个稳定的解。此外还可以令约束为|x|+[y|=l，则交点也只有一个，就是(-1,0)，也是 一个稳定的唯一解。

3.5.6 L2 规范化(L2 Regularization)

虽然上面讨论的是一个非常简单的解方程的例子，但是和机器学习的问题有许多相似 性。基于参数的机器学习模型某种程度上就是一个不可逆的问题，对于同一个损失函数值， 可以对应很多种不同的参数。甚至在高维度下，极小值和最小值都很接近，所以即使是很 好优化过的模型，也会对应许多不同的参数组合，而这些组合未必是数值稳定的。而且因 为参数的范围更自由，可以得到很小的训练误差，往往都不具有很好的泛化性能。这时候 也可考虑加入一个约束项，这种方法叫做规范化(regularization)。具体来说就是在损失 函数里加上一项，最常用的一种是 L2 规范化：

£(<9) = £(6>) + ^^2    (公式 3-13)

i

其实就是 L2 范数，也就是欧氏距离的平方乘上一个系数。在神经网络中，L2规范化 通常只应用于仿射变换中的线性变换部分，也就是 IVX+6的 IV。根据公式形式，这样一项 加上之后，权重的绝对值大小就会整体倾向于减小，尤其是不会出现特别大的值。所以 L2 规范化还有个名字叫做权重衰减(weight decay)，也有一种理解这种衰减是对权值的一种 惩罚，所以有时候会看到文章或者书里管这一项叫做惩罚(penalty)项。

下面通过一个简单的例子来形象理解一下 L2 规范化的作用。考虑一个只有两个参数 ^和^的模型，其损失函数曲面如图 3-38所示。



图 3-38 L2规范化对目标函数曲面的影响

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-176.jpg)

a)    b)

图 3-38a是一个目标函数，可以看到，最小值所在是一条线，整个曲面像一条山岭倒 过来一样。这样的曲面对应无数个参数组合，单纯用梯度下降法是难以得到确定解的，可 以看作是一个典型的病态问题。但是加上一项 0.1x(Wl2+w22)，贝 lj 曲面变成了如图 3-38b所 示的样子。最小值所在从倒过来的“岭”变成了一个“谷”。需要注意的是“谷”所在的 位置并不是规范@的中心(0,0)，而是根据规范化系数的大小和原来损失函数曲面共同决定。 当规范化系数 a—-时，原来的损失函数可以忽略，则“谷”的位置趋近于(0,0)；当 a-0 时，“谷”的位置趋近于原损失函数曲面中“岭”所在的位置。总之加入这一项之后，梯 度下降法就能够解决了。并且通过这个例子可以看出，L2规范项还起到了帮助收敛的作用。 统计学里这个方法常用来处理多重共线性下的最小二乘法问题，并且有个形象的名字叫做 岭回归(ridge regression) o

3.5.7 L1 规范化(L1 Regularization)

除了 L2规范化，L1规范化也是最常见的规范化方法之一，形式如下：

L(0) = L(0) + a^\0i\    (公式 3-14)

i

其实在图 3-37c所示的例子中已经见过，和 L2 的区别主要是 L2 项的等高线不同，二 维情况的等高线画在了图 3-37c中，是个旋转 45°的正方形。这个性质让 L1 规范化后的 参数更趋向于某些维度为 0，也就是稀疏性。关于这个性质的形象理解还是来看个经典的 二维情况下的例子，如图 3-39所示。

图 3-39中虚线代表的是原损失函数的等高线，实现代表的是规范化项的等高线，左边 a图是 L2 的情况，右边 b 图是 L1 的情况。当整体函数达到最小值的时候，如 a 图中红点 所示的位置。所以能够很清楚地看出，L2项让整体参数都有变小的趋势。而 L1 则会让

参数的方向朝着某个轴靠近，比如图 3-39b中，因为原始损失函数等高线的形状，无论 L1 项的系数怎么变，最终最小值一定是在横轴上。这样的约束可以让有效特征的数量变少， 从而获得稀疏性。因为这个性质，L1规范化经常被用在降噪和图像重建中。在统计学里

L1 规范化也有另外一个名字叫 LASSO，即 Least Absolute Shrinkage and Selection Operator,

是对 LI 规范化的一个简短概括。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-177.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-178.jpg)

图 3-39 L2规范化和 L1 规范化的区别

3.5.8 集成(Ensemble)和随机失活(Dropout)

3.5.7节介绍的是最常见的两种规范化方法，其实广义来说，任何尝试减小算法泛化误 差的改动方法都可以叫规范化，本节介绍的两个方法就是两种比较有代表性的。

首先通过一个例子来了解集成(ensemble)的基本概念，考虑一个平面上的二分类问 题，如图 3-40所示。



图 3-40集成(ensemble)的示意例子

图 3-40中的数据均是测试数据，a、b和 c 图分别是 3 个不同模型在测试数据上的分类 结果。需要注意的是，一般在做集成的时候，相同模型结构的 3 组不同参数是比较常见的 情况。但是并不限定于此，也就是说集成的模型可以是不同的模型结构甚至完全不同的算 法。每个模型错误分类的样本都用圆圈标了出来，可以看到每个模型大体都是准确的，但 是 a、b和 c 图中都会有分类错误的情况发生。定性来看因为每个模型是不同的，所以错误 的情况通常也不一样，所以大部分的情况还是正确分类的。基于这个思路，可以考虑综合 考虑 3 个不同模型的结果，因为错误分类毕竟是少数，所以最终结果的可靠性就会好很多。 所以把这 3 个模型的结果综合考虑，如图 3-40d所示，图 3-40d的左图中画出了 3个模型 的分类边界，对结果我们采取多数投票(majority vote)，也就是 3 个模型分类结果中多数 的结果作为最终结果，如图 3-40d右图所示，结果就很好了。除了多数投票，在实际应用 中常见的办法还有取均值或者中值，总之基本思想都是一样。俗话说得好“三个臭皮匠， 赛过诸葛亮”。

集成法是通过多个模型的结果综合考虑给出最终的结果，虽然准确率和稳定性都会提 高，但是训练多个模型的计算成本也是非常高的，如果训练 10 个左右的模型，则计算成本 高了一个量级。所以集成法通常在 ILSVRC 和 Kaggle 这样的竞赛中更常见，实际应用中使 用更多的则是一个和集成法类似但又不尽相同的方法，在第 1 章也提到过，Alex Krizhevsky 在 ILSVRC 2012中就用到过这种方法：随机失活法(dropout)。

Dropout的底层思想其实也是把多个模型的结果放到一起，只不过实现方式是在一个 模型的内部，而不是真正去训练多个模型。Dropout的示意如图 3-41所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-180.jpg)



图 3-41随机失活(dropout)示意

图 3-41中左边展示的是一个非常简单的神经网络结构。我们选取隐藏层，在每次的梯 度迭代中，让隐藏层的每个单元以一定的概率不被激活。比如图 3-41右边的图，是 0.5 为失活概率情况下的一个例子，&和//3不被激活时，实际上相当于一个包含两个隐层单元 的新网络。在该次梯度迭代结束后，下次迭代中再继续这个过程。通常在应用中，如果 w 个隐藏单元的网络对于问题是最优的，则实际训练用的网络中单元数〃应该是的取 整。注意到在这个过程中存在一个问题，对包含《个单元的一层以失活概率进行 dropout 的时候，有/的概率全部单元都没有激活，这样一来输入输出之间的通路就断了。比如 图 3-41的例子中，有 1/16的概率会没有通路。不过好在实际应用中，A7通常都很大，所以 这个概率会非常非常小，不会形成实际困难。

定性来理解一下，dropout实现的相当于每次迭代都在一个随机挑选特征的子空间进 行。举个不严谨的例子，比如想判断一个动物是不是猫，第一次迭代的时候通过重量、毛 色和体型；第二次迭代通过重量、眼睛形状和奔跑速度；第三次迭代通过体型、尾巴长度、 叫声和食物类型……当然区别在于神经网络中的特征都是学出来的。这样做的好处不仅实 现了集成的思想，还把每个特征之间的关联性降低，增强了泛化能力。

在实现 dropout 的时候，因为训练时候对单元按照一定概率随机置零，所以训练结束

后到了预测阶段需要对参数的值乘上 1 卞，才能保证结果正确。这个步骤也可以在训练时 对输入除以 1-P来等效实现。也有人在预测阶段使用 dropout，方法很直接，就是打开 dropout，运行 n 次网络，得到〃个不同的结果，然后像集成法一样，选取一个最终结果。 Dropout虽然避免了训练多个模型，可并不意味着训练时和单独训练一个模型一样快，通 常还是要慢一些。另外什么样的失活概率 p 是最好的，通常也是个经验值。

3.6监督学习、非监督学习、半监督学习和强化学习

机器学习中通常根据数据是否有标签可以分为监督学习(supervised learning)、非监 督学习(unsupervised learning)和半监督学习(semi-supervised learning)。如果需要算法 与环境交互获得数据则是强化学习(reinforcement learning)。本书中针对的计算机视觉应 用将以监督学习为主，不过本节还是来简单全面地了解一下这 3 种类型的机器学习。

3.6.1 监督学习、非监督学习和半监督学习

截至目前本书中讲过的大部分内容都是监督学习。监督学习的意就是用来训练网络 的数据，我们已经知道其对应的输出，这个输出可以是一个类别标签/也可以是一个或者 多个值。模型经过训练后，遇到新来的数据，可以预测对应的标签或者值。监督学习是最 常见的应用，已知标签的分类和回归问题都属于监督学习。非监督学习则是并不知道数据 的标签，而是根据数据本身的特性，从数据中根据某种度量学习出一些特性。比如想象一 个人从来没见过猫和狗，如果给他看了大量的猫和狗，虽然他还是没有猫和狗的概念，但 是他是能够观察出每个物种的共性和两个物种间的区别的，并对这两种动物予以区分。举 个简单的例子理解一下，如图 3-42所示。

|      |      |      |
| ---- | ---- | ---- |
|      |      |      |

a)    b)

图 3-42监督学习和无监督学习示意

图 3-42a是监督学习的样本，可以看到样本根据类别不同而表示成不同的形状，算法 学习的时候根据标签对空间区域进行划分。图 3-42b是没有标签的样本，虽然没有标签但 是也能很明显看出有 3 个集中的“簇”，每个“簇”中的样本互相靠得更近一些。这种情 况下对样本的划分通常被称为聚类(clustering)，常见的方法有 k-means，混合高斯模型

(GMM, Gaussian Mixture Model)等。广义来说，只要是无须人工标注就能从数据中提 取出特征，都算是无监督学习。从这个角度，第 2 章提到过的 PCA 也可以算在无监督学习 的范畴里。无监督学习通常被认为能够更好地从数据本身分布中挖掘出特征，并且对于数 量不是很大的数据集还能防止过拟合。第 1 章提到过的 2006 年 Hinton 的深度学习开山级 文章《A Fast Learning Algorithm for Deep Belief Nets》里，就是用无监督学习对每一层进行 预训练，然后再用监督学习进行训练。

监督学习和非监督学习都是比较极致的情况，要么数据都有标签，要么一个标签都没 有。在实际应用中，还有比较常见的情况是部分数据有标签，部分没有，把这两种数据都 利用起来称为半监督学习(semi-supervised learning)。

在大数据的趋势下，还有一个越来越流行的概念叫做弱监督学习(weakly supervised learning)，是指用弱一些的标注来帮助训练一个更强条件下的算法。比如图片分类，有标 注的数据虽然好，但是耗费人力去标注，获取成本高。但是没有标注的数据，或是一些不 严格标注的数据，比如用户传图片时贴的标签，相对获取成本就低很多。后者就是一种弱 监督的数据，可能包含噪声，多重标注，或是信息缺失等问题。但使用得当的话，结合前 者能带来更大的数据量和更好的泛化。

3.6.2 强化学习(reinforcement learning)

(深度)强化学习在机器学习中是一个比较另类的分支，随着 AlphaGo 战胜李世石， 强化学习开始跃入大众视野并一下子吸引了很多人的兴趣。强化学习的思想借鉴了很多动 物和环境交互学习的行为。强化学习中算法本身有一个状态(state)，算法借助一个代理

(agent)和环境^environment)交互，交互的结果以奖惩(reward)的形式返回并作用于 算法本身，如图 343 所示。



图 3-43强化学习示意图

代理通过当前的状态产生一个行动，这个行为和环境交互后会让代理处于一个新的状 态，并且同时反馈给代理一个奖惩的分数。这个分数相当于对行为的一种评价，和我们为 算法设置的目的有关。如果定义好的行为得到正分数，不好的行为得到负分，则反馈作用 于算法改进后，在通过代理产生下一个可能让奖惩分数提高的行为。这个过程一直持续， 算法就会在这个不断试探的过程中越变越好。举个例子，比如用强化学习训练一辆小车不 会撞墙或者障碍物，小车就是代理，小车所在的有障碍物的房间的地面就是环境。小车的 状态就是当前的位置，以及当前位置能获得的信息，比如传感器得到的距离信息，或者摄 像头看到的画面。根据当前状态和算法策略，每次小车做出任何一个方向前进的行为后， 如果没有撞墙，则得到一个小的奖励分数，如果撞到了墙，则得到一个较大的惩罚负分。 这样就实现了学习的过程。

因为强化学习的行为都会对应一个奖惩，所以常常有人拿强化学习和监督学习进行比 较。的确强化学习的这种特性在某种程度上相当于从环境中获得了对数据的标注，但这两 种类型的算法还是有很大不同的。首先强化学习的目标和监督学习不一样，强化学习看重 的是行为序列下的长期收益，而监督学习往往关注的是和标签或已知输出的误差。强化学 习的奖惩概念是没有正确或错误之分的，而监督学习标签就是正确的。强化学习是一个学 习+决策的过程，并且有和环境交互的能力，这都是监督学习不具备的。

目前强化学习主要用于机器人、游戏等和环境交互比较多的领域。除了第 1 章提到的 DeepMind，现在专注强化学习的技术重镇还有号称“钢铁侠原型”的 Elon Musk创立的 OpenAI，加州大学伯克利分校等。





# 相关

- 《深度学习与计算机视觉》
