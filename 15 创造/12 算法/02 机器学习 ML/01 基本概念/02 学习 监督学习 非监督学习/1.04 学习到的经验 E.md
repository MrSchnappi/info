---
title: 1.04 学习到的经验 E
toc: true
date: 2019-05-19
---
# 学习到的经验 E


根据学习过程中的不同经验，机器学习算法可以大致分类为非监督(unsupervised)算法和监督(supervised)算法。



## 监督学习与非监督学习


- 非监督学习算法(unsupervised learning algorithm)训练含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。在深度学习中，我们通常要学习生成数据集的整个概率分布，显式地，比如密度估计，或是隐式地，比如合成或去噪。还有一些其他类型的非监督学习任务，例如聚类，将数据集分成相似样本的集合。
- 监督学习算法(supervised learning algorithm)训练含有很多特征的数据集，不过数据集中的样本都有一个标签 (label) 或目标(target)。监督学习算法通过研究数据集，学习如何根据测量结果将样本划分为不同标签。

大致说来：

- 非监督学习涉及观察随机向量 $\mathbf{x}$ 的好几个样本，试图显式或隐式地学习出概率分布 $p(\mathbf{x})$ ，或者是该分布一些有意思的性质
- 而监督学习包含观察随机向量 $\mathbf{x}$ 及其相关联的值或向量 $\mathbf{y}$ ，然后从 $\mathbf{x}$ 预测 $\mathbf{y}$，通常是估计 $p(\mathbf{y} | \mathbf{x})$。<span style="color:red;">嗯，是的。</span>


术语监督学习(supervised learning)源自这样一个视角，教员或者老师提供目标 $\mathbf{y}$ 给机器学习系统，指导其应该做什么。在非监督学习中，没有教员或者老师，算法必须学会在没有指导的情况下理解数据。

## 监督学习与非监督学习的界线通常是模糊的

非监督学习和监督学习不是严格定义的术语。它们之间界线通常是模糊的。

很多机器学习技术可以用于这两个任务。<span style="color:red;">界限是模糊的吗？有很多机器学习技术可以用于这两个任务吗？要补充总结下。之前对于这个分界线没有很注意。</span>例如，概率的链式法则表明对于随机向量 $\mathbf{x} \in \mathbb{R}^{n}$ ，联合分布可以分解成：

$$
p(\mathbf{x})=\prod_{i=1}^{n} p\left(\mathrm{x}_{i} | \mathrm{x}_{1}, \ldots, \mathrm{x}_{i-1}\right)\tag{5.1}
$$

该分解意味着我们可以将其拆分成 $n$ 个监督学习问题，来解决表面上的非监督学习 $p(\boldsymbol{x})$ 。<span style="color:red;">哈哈，哎呦，真的哎！</span>

另外，我们求解监督学习问题 $p(y | \mathbf{x})$ 时，也可以使用传统的非监督学习策略学习联合分布 $p(\mathbf{x}, y)$ ，然后推断：

$$
p(y | \mathbf{x})=\frac{p(\mathbf{x}, y)}{\sum_{y^{\prime}} p\left(\mathbf{x}, y^{\prime}\right)}
$$

<span style="color:red;">上面这个式子没理解。</span>


尽管非监督学习和监督学习并非完全没有交集的正式概念，它们确实有助于粗略分类我们研究机器学习算法时遇到的问题。

传统上，人们将回归、分类或者结构化输出问题称为监督学习，将支持其他任务的密度估计称为非监督学习。<span style="color:red;">嗯。</span>

## 学习范式的其他变种

学习范式的其他变种也是有可能的。例如：

- 半监督学习中，一些样本有监督目标，但其他样本没有。
- 多实例学习中，样本的整个集合被标记为含有或者不含有该类的样本，但是集合中单独的样本是没有标记的。<span style="color:red;">集合整体是标记了的，但是里面单个样本没有标记，那么这个是用来做什么的？补充下。</span>

有些机器学习算法并不是训练于一个固定的数据集上。例如：

- 强化学习 (reinforcement learning) 算法会和环境进行交互，所以学习系统和它的训练过程会有反馈回路。


大部分机器学习算法简单地训练于一个数据集上。数据集可以用很多不同方式来表示。在所有的情况下，数据集都是样本的集合，而样本是特征的集合。

表示数据集的常用方法是设计矩阵(design matrix)。设计矩阵的每一行包含一个不同的样本。每一列对应不同的特征。

当然，每一个样本都能表示成向量，并且这些向量的维度相同，才能将一个数据集表示成设计矩阵。这一点并非永远可能。例如：

- 你有不同宽度和高度的照片的集合，那么不同的照片将会包含不同数量的像素。因此不是所有的照片都可以表示成相同长度的向量。在上述这类情况下，我们不会将数据集表示成 $m$ 行的矩阵，而是表示成 $m$ 个元素的结合：$\left\{\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(m)}\right\}$ 。这种表示方式意味着样本向量 $\boldsymbol{x}^{(i)}$ 和 $\boldsymbol{x}^{(j)}$ 可以有不同的大小。<span style="color:red;">嗯。</span>

在监督学习中，样本包含一个标签或目标和一组特征。例如，我们希望使用学习算法从照片中识别对象。我们需要明确哪些对象会出现在每张照片中。我们或许会用数字编码表示，如 0 表示人、1表示车、2表示猫等。通常在处理包含观测特征的设计矩阵 $X$ 的数据集时，我们也会提供一个标签向量 $\boldsymbol{y}$ ，其中 $y_{i}$ 表示样本 $i$ 的标签。


当然，有时标签可能不止一个数。例如：

- 如果我们想要训练语音模型转录整个句子，那么每个句子样本的标签是一个单词序列。<span style="color:red;">嗯，是的。</span>

正如监督学习和非监督学习没有正式的定义，数据集或者经验也没有严格的区分。这里介绍的结构涵盖了大多数情况，但始终有可能为新的应用设计出新的结构。<span style="color:red;">嗯，遇到的时候也补充到这里来下。</span>


# 相关

- 《深度学习》花书
