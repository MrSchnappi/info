---
title: 31 为什么在训练样本上学习了一个假设就能保证这个假设在训练样本之外的数据上有效
toc: true
date: 2019-08-27
---
# 为什么在训练样本上学习了一个假设就能保证这个假设在训练样本之外的数据上有效

这个就要涉及到 PAC 学习理论了。

PAC 学习相关理论有一个总结：

> 同等条件下，模型越复杂泛化误差越大。同一模型在样本满足一定条件的情况下，样本数量越大，模型泛化误差越小，因此还可以说模型越复杂越吃样本。







# 相关

- 《深度学习》花书
