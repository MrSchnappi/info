---
title: 31 泛化
toc: true
date: 2019-08-27
---

# 泛化

机器学习的目标是使学得的模型能很好地适用于 “新样本”，更确切地说，是 “未见示例” (unseen instance)， 而不是仅仅在训练样本上工作得很好。

即便对聚类这样的无监督学习任务，我们也希望学得的簇划分能适用于没在训练集中出现的样本。

学得模型适用于新样本的能力，称为 “泛化” (generalization) 能力。具有强泛化能力的模型能很好地适用于整个样本空间。

也就是说，尽管训练集通常只是样本空间的一个很小的采样，我们仍希望它能很好地反映出样本空间的特性，否则就很难期望在训练集上学得的模型能在整个样本空间上都工作得很好。<span style="color:red;">嗯。</span>




通常情况下，训练机器学习模型时，我们可以使用某个训练集，在训练集上计算一些被称为训练误差(training error)的度量误差，目标是降低训练误差。到目前为止，我们讨论的是一个简单的优化问题。机器学习和优化不同的地方在于，我们也希望泛化误差(generalization error)(也被称为测试误差(test error))很低。泛化误差被定义为新输入的误差期望。这里，期望的计算基于不同的可能输入，这些输入采自系统在现实中遇到的分布。

通常，我们度量模型在训练集中分出来的测试集(test set)样本上的性能，来评估机器学习模型的泛化误差。<span style="color:red;">嗯。</span>

在我们的线性回归示例中，通过最小化训练误差来训练模型：

$$
\frac{1}{m^{(\text { train })}}\left\|\boldsymbol{X}^{(\text { train })} \boldsymbol{w}-\boldsymbol{y}^{(\text { train })}\right\|_{2}^{2}\tag{5.14}
$$

但是我们真正关注的是测试误差：

$$
\frac{1}{m^{(\mathrm{test})}}\left\|\boldsymbol{X}^{(\text { test }} \boldsymbol{w}-\boldsymbol{y}^{(\text { test })}\right\|_{2}^{2}
$$

# 相关

- 《机器学习》周志华
