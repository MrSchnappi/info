---
title: 33 类别型特征
toc: true
date: 2019-03-23
---
# 类别型特征


类别型特征（Categorical Feature）主要是指性别（男、女）、血型（A、B、AB、O）等只在有限选项内取值的特征。


类别型特征原始输入通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。<span style="color:red;">嗯，是的，决策树可以直接处理字符串形式的输入</span>



类别型特征的字段数据分为两种：一种是有序的，一种是无序的。

有序数据指的是数据可以按照程度来分层（如疼痛程度）：

- 序号编码（Ordinal Encoding）

无序的：

- 独热编码（One-hot Encoding）
- 二进制编码（Binary Encoding）

## **序号编码**

序号编码通常用于处理类别间具有大小关系的数据。例如成绩，可以分为低、中、高三档，并且存在“高>中>低”的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值 ID，例如高表示为 3、中表示为 2、低表示为 1，转换后依然保留了大小关系。


## **独热编码**

独热编码通常用于处理类别间不具有大小关系的特征。例如血型，一共有 4 个取值（A型血、B型血、AB型血、O型血），独热编码会把血型变成一个 4 维稀疏向量，A型血表示为（1，0，0，0），B型血表示为（0，1，0，0），AB型表示为（0，0，1，0），O型血表示为（0，0，0，1）。对于类别取值较多的情况下使用独热编码需要注意以下问题。

（1）使用稀疏向量来节省空间。在独热编码下，特征向量只有某一维取值为 1，其他位置取值均为 0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。<span style="color:red;">向量的稀疏表示当在 tensorflow 中使用的时候要怎么写？明确下。</span>

（2）配合特征选择来降低维度。高维度特征会带来几方面的问题。一是在 K 近邻算法中，高维空间下两点之间的距离很难得到有效的衡量；二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题；三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。<span style="color:red;">配合特征？不过这三个问题的确是问题。</span>

## **二进制编码**

二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别 ID，然后将类别 ID 对应的二进制编码作为结果。以 A、B、AB、O血型为例，表 1.1是二进制编码的过程。A型血的 ID 为 1，二进制表示为 001；B型血的 ID 为 2，二进制表示为 010；以此类推可以得到 AB 型血和 O 型血的二进制表示。可以看出，二进制编码本质上是利用二进制对 ID 进行哈希映射，最终得到 0/1特征向量，且维数少于独热编码，节省了存储空间。<span style="color:red;">那么是不是独热编码就肯定不如二进制编码呢？</span>

<center>

![mark](http://images.iterate.site/blog/image/20190826/l6aScM6BkIlV.png?imageslim){ width=55% }


</center>

还可以进一步了解其他的编码方式，比如 Helmert Contrast、Sum Contrast、Polynomial Contrast、Backward Difference Contrast等。<span style="color:red;">这些是什么？补充下。</span>


# 相关

- 《百面机器学习》
