---
title: 14 特征子集的搜索与评价
toc: true
date: 2019-08-27
---

# 特征子集的搜索与评价

欲从初始的特征集合中选取一个包含了所有重要信息的特征子集，若没有任何领域知识作为先验假设，那就只好遍历所有可能的子集了。

然而这在计算上却是不可行的，因为这样做会遭遇组合爆炸，特征个数稍多就无法进行。

可行的做法是产生一个“候选子集”，评价出它的好坏，基于评价结果产生下一个候选子集，再对其进行评价，..。这个过程持续进行下去，直至无法找到更好的候选子集为止。

显然，这里涉及两个关键环节：

- 如何根据评价结果获取下一个候选特征子集？
- 如何评价候选特征子集的好坏？


## 子集搜索

第一个环节是“子集搜索” (subset search)问题。给定特征集合 $\{a_1,a_2,\cdots ,a_d\}$ ，我们可将每个特征看作一个候选子集，对这 d 个候选单特征子集进行评价，假定 $\{a_2\}$ 最优，于是将 $\{a_2\}$ 作为第一轮的选定集；然后，在上一轮的选定集中加入一个特征，构成包含两个特征的候选子集，假定在这 d-1 个候选两特征子集中  $\{a_2,a_4\}$ 最优，且优于 $\{a_2\}$ 于是将  $\{a_2,a_4\}$ 作为本轮的选定集；……假定在第 $k+1$ 轮时，最优的候选 $(k+1)$ 特征子集不如上一轮的选定集，则停止生成候选子集，并将上一轮选定的特征集合作为特征选择结果。这样逐渐増加相关特征的策略称为“前向”(fdrward)搜索。类似的，若我们从完整的特征集合开始，每次尝试去掉一个无关特征，这样逐渐减少特征的策略 称为“后向”(backward)搜索。还可将前向与后向搜索结合起来，每一轮逐渐增加选定相关特征(这些特征在后续轮中将确定不会被去除)、同时减少无关特征，这样的策略称为“双向”(bidirectional)搜索。

显然，上述策略都是贪心的，因为它们仅考虑了使本轮选定集最优，例如在 第三轮假定选择 $a_5$ 优于 $a_6$ ，于是选定集为  $\{a_2,a_4,a_5\}$ ，然而在第四轮却可能是  $\{a_2,a_4,a_6,a_8\}$ 比所有的 $\{a_2,a_4,a_5,a_i\}$ 都更优。遗憾的是，若不进行穷举搜索，则这样的问题无法避兔。

## 子集评价

第二个环节是“子集评价”(subset evaluation)问题。给定数据集 $D$ ，假定 $D$ 中第 $i$ 类样本所占的比例为 $p_{i}(i=1,2, \ldots,|\mathcal{Y}|)$ 。为便于讨论，假定样本属性均为离散型。对属性子集 $A$ ，假定根据其取值将 $D$ 分成了 $V$ 个子集 $\left\{D^{1}, D^{2}, \ldots, D^{V}\right\}$ ，每个子集中的样本在 $A$ 上取值相同，于是我们可计算属性子集 $A$ 的信息增益

$$
\operatorname{Gain}(A)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)
$$

其中信息熵定义为

$$
\operatorname{Ent}(D)=-\sum_{i=1}^{ | \mathcal{Y |}} p_{k} \log _{2} p_{k}
$$

信息增益 $\operatorname{Gain}(A)$ 越大，意味着特征子集 $A$ 包含的有助于分类的信息越多。于 是，对每个候选特征子集，我们可基于训练数据集 $D$ 来计算其信息増益，以此作为评价准则。


更一般的，特征子集 $A$ 实际上确定了对数据集 D 的一个划分，每个划分区域对应着 $A$ 上的一个取值，而样本标记信息 $Y$ 则对应着对 $D$ 的真实划分，通过估算这两个划分的差异，就能对 $A$ 进行评价。与 $Y$ 对应的划分的差异越小，则说明 $A$ 越好。信息熵仅是判断这个差异的一种途径，其他能判断两个划分差异的机制都能用于特征子集评价。

将特征子集搜索机制与子集评价机制相结合，即可得到特征选择方法。例如将前向搜索与信息熵相结合，这显然与决策树算法非常相似。事实上，决策树可用于特征选择，树结点的划分属性所组成的集合就是选择出的特征子集。其他的特征选择方法未必像决策树特征选择这么明显，但它们在本质上都是显式或隐式地结合了某种(或多种)子集搜索机制和子集评价机制。








# 相关

- 《机器学习》周志华
