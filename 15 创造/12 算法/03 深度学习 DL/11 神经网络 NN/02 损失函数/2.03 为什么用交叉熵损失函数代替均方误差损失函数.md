---
title: 2.03 为什么用交叉熵损失函数代替均方误差损失函数
toc: true
date: 2019-09-03
---

# 为什么用交叉熵代替二次代价函数

（1）**为什么不用二次方代价函数**

由上一节可知，权值 $w$ 和偏置 $b$ 的偏导数为 $\frac{\partial J}{\partial w}=(a-y)\sigma'(z)x$，$\frac{\partial J}{\partial b}=(a-y)\sigma'(z)$， 偏导数受激活函数的导数影响，sigmoid函数导数在输出接近 0 和 1 时非常小，会导致一些实例在刚开始训练时学习得非常慢。<span style="color:red;">嗯。</span>

（2）**为什么要用交叉熵**

交叉熵函数权值 $w$ 和偏置 $b$ 的梯度推导为：

$$
\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)\;，
\frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)
$$

<span style="color:red;">上面这个式子是怎么推导的？</span>

由以上公式可知，权重学习的速度受到 $\sigma{(z)}-y$ 影响，更大的误差，就有更快的学习速度，避免了二次代价函数方程中因 $\sigma'{(z)}$ 导致的学习缓慢的情况。<span style="color:red;">这个以前没有注意到。</span>







# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
