---
title: 4.01 为什么需要 Normalization
toc: true
date: 2019-09-03
---


# 为什么需要 Normalization

## 独立同分布与白化



机器学习界的炼丹师们最喜欢的数据有什么特点？窃以为，莫过于“**独立同分布**”了，即*independent and identically distributed*，简称为 *i.i.d.*

独立同分布并非所有机器学习模型的必然要求（比如 Naive Bayes 模型就建立在特征彼此独立的基础之上，而 Logistic Regression 和 神经网络 则在非独立的特征数据上依然可以训练出很好的模型），但独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。



因此，在把数据喂给机器学习模型之前，“**白化（whitening）**”是一个重要的数据预处理步骤。白化一般包含两个目的：

1. *去除特征之间的相关性* —> 独立；
2. *使得所有特征具有相同的均值和方差* —> 同分布。

白化最典型的方法就是 PCA，可以参考阅读 [PCAWhitening](https://link.zhihu.com/?target=http%3A//ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/)。



## 深度学习中的 Internal Covariate Shift


深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。



Google 将这一现象总结为 Internal Covariate Shift，简称 ICS. 什么是 ICS 呢？

魏秀参在[一个回答](https://www.zhihu.com/question/38102762/answer/85238569)中做出了一个很好的解释：



> 大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有 $x \in \mathcal{X}$,
>
> $$
> P_{s}(Y | X=x)=P_{t}(Y | X=x)
> $$
>
> 但是
>
> $$
> P_{s}(X) \neq P_{t}(X)
> $$
>
> 大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能 “指示” 的样本标记（label）仍然是不变的，这便符合了 covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。


## ICS 会导致什么问题？

简而言之，每个神经元的输入数据不再是“独立同分布”。

1. 上层参数需要不断适应新的输入数据分布，降低学习速度。
2. 下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。
3. 每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。







# 相关

- [详解深度学习中的 Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
