---
title: 3.07 卷积核是否越大越好
toc: true
date: 2019-09-03
---

# 卷积核是否一定越大越好？

在早期的卷积神经网络中（如 LeNet-5、AlexNet），用到了一些较大的卷积核（$11\times11$ 和 $5\times 5$），受限于当时的计算能力和模型结构的设计，无法将网络叠加得很深，因此卷积网络中的卷积层需要设置较大的卷积核以获取更大的感受域。但是这种大卷积核反而会导致计算量大幅增加，不利于训练更深层的模型，相应的计算性能也会降低。

后来的卷积神经网络（VGG、GoogLeNet等），发现通过堆叠 2 个 $3\times 3$ 卷积核可以获得与 $5\times 5$ 卷积核相同的感受视野，同时参数量会更少（$3×3×2+1$ < $ 5×5×1+1$），$3\times 3$ 卷积核被广泛应用在许多卷积神经网络中。因此可以认为，在大多数情况下通过堆叠较小的卷积核比直接采用单个更大的卷积核会更加有效。<span style="color:red;">嗯。</span>

但是，这并不是表示更大的卷积核就没有作用，在某些领域应用卷积神经网络时仍然可以采用较大的卷积核。譬如在自然语言处理领域，由于文本内容不像图像数据可以对特征进行很深层的抽象，往往在该领域的特征提取只需要较浅层的神经网络即可。<span style="color:red;">为什么文本内容不像图像数据可以对特征进行很深的抽象？</span>在将卷积神经网络应用在自然语言处理领域时，通常都是较为浅层的卷积层组成，但是文本特征有时又需要有较广的感受域让模型能够组合更多的特征（如词组和字符），此时直接采用较大的卷积核将是更好的选择。<span style="color:red;">嗯嗯，没想到，原来大的卷积核也是有用的。最好补充例子，不然有点不是很透彻。</span>

综上所述，卷积核的大小并没有绝对的优劣，需要视具体的应用场景而定，但是极大和极小的卷积核都是不合适的，单独的 $1\times 1$ 极小卷积核只能用作分离卷积而不能对输入的原始特征进行有效的组合，极大的卷积核通常会组合过多的无意义特征从而浪费了大量的计算资源。<span style="color:red;">$1\times 1$ 的卷积核是用作分离卷积吗？</span>






### 10.4.4卷积核不是越大越好。

AlexNet中用到了一些非常大的卷积核，比如 11×11、5×5卷积核，之前人们的观念是，卷积核越大，感受野越大，看到的图片信息越多，因此获得的特征越好。但是大的卷积核会导致计算量的暴增，不利于模型深度的增加，计算性能也会降低。于是在 VGG、Inception网络中，利用 2 个 3×3卷积核的组合比 1 个 5×5卷积核的效果更佳，同时参数量（3×3×2+1=19<26=5×5×1+1）被降低，因此后来 3×3卷积核被广泛应用在各种模型中。


# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions)
