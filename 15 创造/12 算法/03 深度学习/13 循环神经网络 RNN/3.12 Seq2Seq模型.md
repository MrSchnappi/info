---
title: 3.12 Seq2Seq模型
toc: true
date: 2019-04-19
---
# Seq2Seq 模型

Seq2Seq，全称 Sequence to Sequence 模型，目前还没有一个很好的中文翻译，我们暂且称之为序列到序列模型。

大致意思是将一个序列信号，通过编码和解码生成一个新的序列信号，通常用于机器翻译、语音识别、自动对话等任务。<span style="color:red;">嗯，应用的还真的很广泛的。</span>

在 Seq2Seq 模型提出之前，深度神经网络在图像分类等问题上取得了非常好的效果。在深度学习擅长的问题中，输入和输出通常都可以表示为固定长度的向量，如果长度稍有变化，会使用补零等操作。

然而像前面提到的几个问题，其序列长度事先并不知道。因此如何突破先前深度神经网络的局限，使其适应于更多的场景，成了 2013 年以来的研究热点，Seq2Seq模型也应运而生。


Seq2Seq，机器翻译，集束搜索（Beam Search）<span style="color:red;">嗯，集束搜索感觉这名字有点霸气，像集束导弹一样。。不过还是比较容易理解的。</span>

## 什么是 Seq2Seq 模型？Seq2Seq 模型有哪些优点？


Seq2Seq 模型的核心思想是，通过深度神经网络将一个作为输入的序列映射为一个作为输出的序列，这一过程由编码输入与解码输出两个环节构成。

在经典的实现中，编码器和解码器各由一个循环神经网络构成，既可以选择传统循环神经网络结构，也可以使用长短期记忆模型、门控循环单元等。

在 Seq2Seq 模型中，两个循环神经网络是共同训练的。<span style="color:red;">嗯，一直想知道是怎么共同训练的。</span>


<center>

![](http://images.iterate.site/blog/image/20190419/omSBSEWTpybe.png?imageslim){ width=55% }

</center>

假想一个复习和考试的场景，如图 10.3所示。

- 我们将学到的历史信息经过了一系列加工整理，形成了所谓的知识体系，这便是编码过程。
- 然后在考试的时候，将高度抽象的知识应用到系列问题中进行求解，这便是解码过程。

譬如对于学霸，他们的网络很强大，可以对很长的信息进行抽象理解，加工内化成编码向量，再在考试的时候从容应答一系列问题。而对于大多数普通人，很难记忆长距离、长时间的信息。在考前只好临时抱佛脚，编码很短期的序列信号，考试时也是听天由命，能答多少写多少，解码出很短时效的信息。


<center>

![](http://images.iterate.site/blog/image/20190419/lswt7M3UUDLW.png?imageslim){ width=55% }

</center>

对应于机器翻译过程，如图 10.4所示：

- 输入的序列是一个源语言的句子，有三个单词 A、B、C，编码器依次读入 A、B、C 和结尾符 `<EOS>`。
- 在解码的第一步，解码器读入编码器的最终状态，生成第一个目标语言的词 W；<span style="color:red;">没想明白为什么这个是合理的？</span>
- 第二步读入第一步的输出 W，生成第二个词 X；
- 如此循环，直至输出结尾符 `<EOS>`。

输出的序列 W、X、Y、Z 就是翻译后目标语言的句子。

在不同的场景中：

- 在文本摘要任务中，输入的序列是长句子或段落，输出的序列是摘要短句。
- 在图像描述文本生成任务中，输入是图像经过视觉网络的特征，输出的序列是图像的描述短句。
- 进行语音识别时，输入的序列是音频信号，输出的序列是识别出的文本。

不同场景中，编码器和解码器有不同的设计，但对应 Seq2Seq 的底层结构却如出一辙。

## Seq2Seq 模型在解码时，有哪些常用的办法？

<span style="color:red;">感觉有好多方法呀，但是 Seq2Seq 没有实际使用过，感触不是特别深。</span>


Seq2Seq模型最核心的部分是其解码部分，大量的改进也是在解码环节衍生的。

Seq2Seq模型最基础的解码方法是贪心法，即选取一种度量标准后，每次都在当前状态下选择最佳的一个结果，直到结束。

贪心法的计算代价低，适合作为基准结果与其他方法相比较。

很显然，贪心法获得的是一个局部最优解，由于实际问题的复杂性，该方法往往并不能取得最好的效果。<span style="color:red;">是的。</span>

集束搜索是常见的改进算法，它是一种启发式算法。该方法会保存 beam size（后面简写为 b）个当前的较佳选择，然后解码时每一步根据保存的选择进行下一步扩展和排序，接着选择前 b 个进行保存，循环迭代，直到结束时选择最佳的一个作为解码的结果。<span style="color:red;">这样搜索量不会不会很大。哦，应该也不会，因为每次都会进行剪枝。。可能很快就只剩一个了。</span>

图 10.5 是 b 为 2 时的集束搜索示例。

<center>

![](http://images.iterate.site/blog/image/20190419/m4cxCP2010SB.png?imageslim){ width=55% }

</center>

由图可见，当前已经有解码得到的第一个词的两个候选：`I` 和 `My`。然后，将`I`和 `My` 输入到解码器，得到一系列候选的序列，诸如 `I decided`、`My decision`、`I thought` 等。最后，从后续序列中选择最优的两个，作为前两个词的两个候选序列。

很显然，如果 b 取 1，那么会退化为前述的贪心法。随着 b 的增大，其搜索的空间增大，最终效果会有所提升，但需要的计算量也相应增大。<span style="color:red;">是的。</span>

在实际的应用（如机器翻译、文本摘要）中，b 往往会选择一个适中的范围，以 8~12 为佳。

解码时使用堆叠的 RNN、增加 Dropout 机制、与编码器之间建立残差连接等，均是常见的改进措施。

在实际研究工作中，可以依据不同使用场景，有针对地进行选择和实践。<span style="color:red;">对应不同场景有哪些不同的选择和实践？</span>

另外，解码环节中一个重要的改进是注意力机制，我们会在下一节深入介绍。注意力机制的引入，使得在解码时每一步可以有针对性地关注与当前有关的编码结果，从而减小编码器输出表示的学习难度，也更容易学到长期的依赖关系。<span style="color:red;">哇塞，想知道注意力机制。</span>

此外，解码时还可以采用记忆网络[28]等，从外界获取知识。<span style="color:red;">怎么使用记忆网络从外界获取知识？</span>


# 相关

- 《百面机器学习》
