---
title: 2.04 Self Attention模型
toc: true
date: 2019-09-29
---

# Self Attention 模型


通过上述对 Attention 本质思想的梳理，我们可以更容易理解本节介绍的 Self Attention模型。


Self Attention 也经常被称为 intra-Attention（内部 Attention），最近一年也获得了比较广泛的使用，比如 Google 最新的机器翻译模型内部大量采用了 Self
Attention模型。

在一般任务的 Encoder-Decoder框架中，输入 Source 和输出 Target 内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在 Target 的元素 Query 和 Source 中的所有元素之间。

而 SelfAttention 顾名思义，指的不是 Target 和 Source 之间的 Attention 机制，而是 Source 内部元素之间或者 Target 内部元素之间发生的 Attention 机制，也可以理解为 Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。

如果是常规的 Target 不等于 Source 情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是 Self Attention机制，一个很自然的问题是：通过 Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入 Self Attention有什么增益或者好处呢？

我们仍然以机器翻译中的 Self Attention来说明，下面两幅图是可视化地表示 Self Attention在同一个英语句子内单词间产生的联系。



<center>

![mark](http://images.iterate.site/blog/image/20190927/BMJBg5sFlLyn.png?imageslim)

</center>

<center>

![mark](http://images.iterate.site/blog/image/20190927/6yqB4BQKfWrG.png?imageslim)

</center>

> 可视化 Self Attention实例

从两张图可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如第一幅图展示的有一定距离的短语结构）或者语义特征（比如第二幅图展示的 its 的指代对象 Law）。

很明显，引入 Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是 RNN 或者 LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。

但是 Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何 Self Attention逐渐被广泛使用的主要原因。<span style="color:red;">厉害呀。</span>


# 相关

- [深度学习中的注意力模型（2017版）](https://zhuanlan.zhihu.com/p/37601161)
