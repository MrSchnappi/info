---
title: 05 实例 线性最小二乘
toc: true
date: 2019-05-18
---
# 实例 线性最小二乘

## 没有约束的时候

假设我们希望找到最小化下式的 $\boldsymbol{x}$ 值

$$
f(\boldsymbol{x})=\frac{1}{2}\|\boldsymbol{A} \boldsymbol{x}-\boldsymbol{b}\|_{2}^{2}\tag{4.21}
$$

存在专门的线性代数算法能够高效地解决这个问题，但是我们也可以探索如何使用基于梯度的优化来解决这个问题，这可以作为这些技术是如何工作的一个简单例子。<span style="color:red;">嗯。</span>

首先，我们计算梯度：

$$
\nabla_{x} f(\boldsymbol{x})=\boldsymbol{A}^{\top}(\boldsymbol{A} \boldsymbol{x}-\boldsymbol{b})=\boldsymbol{A}^{\top} \boldsymbol{A} \boldsymbol{x}-\boldsymbol{A}^{\top} \boldsymbol{b}\tag{4.22}
$$

然后，我们可以采用小的步长，并按照这个梯度下降，见算法 4.1 中的详细信息。


算法 4.1 从任意点 x 开始，使用梯度下降关于 $\boldsymbol{x}$ 最小化 $f(\boldsymbol{x})=\frac{1}{2}\|\boldsymbol{A} \boldsymbol{x}-\boldsymbol{b}\|_{2}^{2}$ 的算法：

<center>

![](http://images.iterate.site/blog/image/20190719/nGgKtH7q3f53.png?imageslim){ width=55% }

</center>


我们也可以使用牛顿法解决这个问题。因为在这个情况下，真实函数是二次的，牛顿法所用的二次近似是精确的，该算法会在一步后收敛到全局最小点。<span style="color:red;">嗯嗯，是的。</span>


## 现在假设我们希望最小化同样的函数，但受的约束

现在假设我们希望最小化同样的函数，但受 $\boldsymbol{x}^{\top} \boldsymbol{x} \leq 1$ 的约束。要做到这一点，我们引入 Lagrangian

$$
L(\boldsymbol{x}, \lambda)=f(\boldsymbol{x})+\lambda\left(\boldsymbol{x}^{\top} \boldsymbol{x}-1\right)\tag{4.23}
$$

现在，我们解决以下问题：

$$
\min _{\boldsymbol{x}} \max _{\lambda, \lambda \geq 0} L(\boldsymbol{x}, \boldsymbol{\lambda})\tag{4.24}
$$

<span style="color:red;">嗯，之前忘记问了，一直想知道这个式子要怎么读？</span>


我们可以用 Moore-Penrose 伪逆：$\boldsymbol{x}=\boldsymbol{A}^{+} \boldsymbol{b}$ 找到无约束最小二乘问题的最小范数解。如果这一点是可行的，那么这也是约束问题的解。否则，我们必须找到约束是活跃的解。关于 $\boldsymbol{x}$ 对 Lagrangian 微分，我们得到方程：

$$
\boldsymbol{A}^{\top} \boldsymbol{A} \boldsymbol{x}-\boldsymbol{A}^{\top} \boldsymbol{b}+2 \lambda \boldsymbol{x}=0\tag{4.25}
$$

这就告诉我们，该解的形式将会是：

$$
\boldsymbol{x}=\left(\boldsymbol{A}^{\top} \boldsymbol{A}+2 \lambda \boldsymbol{I}\right)^{-1} \boldsymbol{A}^{\top} \boldsymbol{b}\tag{4.26}
$$

$\lambda$ 的选择必须使结果服从约束。我们可以关于 $\lambda$ 进行梯度上升找到这个值。为了做到这一点，观察

$$
\frac{\partial}{\partial \lambda} L(\boldsymbol{x}, \lambda)=\boldsymbol{x}^{\top} \boldsymbol{x}-1\tag{4.27}
$$

当 $\boldsymbol{x}$ 的范数超过 $1$ 时，该导数是正的，所以为了跟随导数上坡并相对 $\lambda$ 增加 Lagrangian，我们需要增加 $\lambda$ 。因为 $\boldsymbol{x}^{\top} \boldsymbol{x}$ 的惩罚系数增加了，求解关于 $\boldsymbol{x}$ 的线性方程现在将得到具有较小范数的解。求解线性方程和调整 $\lambda$ 的过程将一直持续到  $\boldsymbol{x}$  具有正确的范数，并且关于 $\lambda$ 的导数是 $0$。





# 相关

- 《深度学习》花书
