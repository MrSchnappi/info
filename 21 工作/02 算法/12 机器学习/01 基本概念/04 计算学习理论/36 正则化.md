---
title: 36 正则化
toc: true
date: 2019-05-22
---
# 可以补充进来的


# 正则化

没有免费午餐定理暗示我们必须在特定任务上设计性能良好的机器学习算法。我们建立一组学习算法的偏好来达到这个要求。当这些偏好和我们希望算法解决的学习问题相吻合时，性能会更好。<span style="color:red;">嗯嗯，是的。</span>


至此，我们具体讨论修改学习算法的方法，只有通过增加或减少学习算法可选假设空间的函数来增加或减少模型的容量。所列举的一个具体示例是线性回归增加或减少多项式的次数。到目前为止讨论的观点都是过度简化的。

算法的效果不仅很大程度上受影响于假设空间的函数数量，也取决于这些函数的具体形式。我们已经讨论的学习算法(线性回归)具有包含其输入的线性函数集的假设空间。对于输入和输出确实接近线性相关的问题，这些线性函数是很有用的。对于完全非线性的问题它们不太有效。例如，我们用线性回归，从 $x$ 预测 $\sin (x)$，效果不会好。

因此我们可以通过两种方式控制算法的性能：

- 一是允许使用的函数种类
- 二是这些函数的数量。

在假设空间中，相比于某一个学习算法，我们可能更偏好另一个学习算法。这意味着两个函数都是符合条件的，但是我们更偏好其中一个。只有非偏好函数比偏好函数在训练数据集上效果明显好很多时，我们才会考虑非偏好函数。

例如，可以加入权重衰减(weight decay)来修改线性回归的训练标准。带权重衰减的线性回归最小化训练集上的均方误差和正则项的和 $J(\boldsymbol{w})$ ，其偏好于平方 $L^{2}$ 范数较小的权重。具体如下：

$$
J(\boldsymbol{w})=\mathrm{MSE}_{\mathrm{train}}+\lambda \boldsymbol{w}^{\top} \boldsymbol{w}\tag{5.18}
$$


其中 $\lambda$ 是提前挑选的值，控制我们偏好小范数权重的程度。当 $\lambda=0$ 时，我们没有任何偏好。越大的 $\lambda$ 偏好范数越小的权重。最小化 $J(\boldsymbol{w})$ 可以看作拟合训练数据和偏好小权重范数之间的权衡。这会使得解决方案的斜率较小，或是将权重放在较少的特征上。我们可以训练具有不同 $\lambda$ 值的高次多项式回归模型，来举例说明如何通过权重衰减控制模型欠拟合或过拟合的趋势，如图 5.5所示。<span style="color:red;">嗯嗯，是的，早就想看这个了。</span>

<center>

![](http://images.iterate.site/blog/image/20190522/MUmzIPYp3DmC.png?imageslim){ width=55% }

</center>

<span style="color:red;">好吧，又拿之前的例子。。</span>

> 图 5.5　我们使用高阶多项式回归模型来拟合图 5.2中的训练样本。真实函数是二次的，但是在这里只使用 9 阶多项式。我们通过改变权重衰减的量来避免高阶模型的过拟合问题。
>
> - (左)当 $\lambda$ 非常大时，我们可以强迫模型学习到一个没有斜率的函数。由于它只能表示一个常数函数，所以会导致欠拟合。
> - (中)取一个适当的 $\lambda$ 时，学习算法能够用一个正常的形状来恢复曲率。即使模型能够用更复杂的形状来表示函数，权重衰减也鼓励用一个带有更小参数的更简单的模型来描述它。
> - (右)当权重衰减趋近于 $0$ (即使用 Moore-Penrose伪逆来解这个带有最小正则化的欠定问题)时，这个 9 阶多项式会导致严重的过拟合，这和我们在图 5.2中看到的一样。

更一般地，正则化一个学习函数 $f(x ; \boldsymbol{\theta})$ 的模型，我们可以给代价函数添加被称为正则化项(regularizer)的惩罚。在权重衰减的例子中，正则化项是 $\Omega(\boldsymbol{w})=\boldsymbol{w}^{\top} \boldsymbol{w}$ 。在第 7 章，我们将看到很多其他可能的正则化项。<span style="color:red;">嗯嗯，想看下所有的正则化的情况以及写法以及对应的效果。</span>

表示对函数的偏好是比增减假设空间的成员函数更一般地控制模型容量的方法。我们可以将去掉假设空间中的某个函数看作对不赞成这个函数的无限偏好。<span style="color:red;">嗯。</span>

在权重衰减的示例中，通过在最小化的目标中额外增加一项，我们明确地表示了偏好权重较小的线性函数。有很多其他方法隐式或显式地表示对不同解的偏好。<span style="color:red;">嗯嗯，想知道所有这些方法，看看他们是怎么对不同的解进行偏好的。嗯，也要学会自己写一些方法来提取想要的偏好。</span>总而言之，这些不同的方法都被称为正则化(regularization)。<span style="color:red;">哦，原来这些方法全部被称为正则化，之前不是特别清楚这一点。</span>

正则化是指修改学习算法，使其降低泛化误差而非训练误差。<span style="color:red;">是的是的，是为了降低泛化误差。训练误差不是通过这个来降低的。</span>

正则化是机器学习领域的中心问题之一，只有优化能够与其重要性相提并论。<span style="color:red;">嗯嗯，没想到。</span>



没有免费午餐定理已经清楚地阐述了没有最优的学习算法，特别是没有最优的正则化形式。反之，我们必须挑选一个非常适合于我们所要解决的任务的正则形式。深度学习中普遍的(特别是本书中的)理念是大量任务(例如所有人能做的智能任务)也许都可以使用非常通用的正则化形式来有效解决。<span style="color:red;">嗯嗯，想知道这些非常通用的正则化形式，以及在真实场景那种的使用效果。</span>







# 相关

- 《深度学习》花书
