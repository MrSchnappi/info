---
title: 43 有限假设空间
toc: true
date: 2018-07-30 11:00:41
---


# 可以补充进来的




# 有限假设空间


## 可分情形

可分情形意味着目标概念 $c$ 属于假设空间 $\mathcal{H}$，$c \in \mathcal{H}$ 给定包含 $m$ 个样例的训练集 $D$，如何找出满足误差参数的假设呢？

容易想到一种简单的学习策略：既然 $D$ 中样例标记都是由目标概念 $c$ 赋予的，并且 $c$ 存在于假设空间 $\mathcal{H}$ 中，那么，任何在训练集 $D$ 上出现标记错误的假设肯定不是目标概念 $c$。于是，我们只需保留与 $D$ 一致的假设，剔除与 $D$ 不一致的假设即可。若训练集 $D$ 足够大，则可不断借助 $D$ 中的样例剔除不一致的假设，直到 $\mathcal{H}$ 中仅剩下一个假设为止，这个假设就是目标概念 $c$ 。通常情形下，由于训练集规模有限，假设空间 $\mathcal{H}$ 中可能存在不止一个与 $D$  一致的 “等效” 假设，对这些等效假设，无法根据 $D$ 来对它们的优劣做进一步区分。

到底需多少样例才能学得目标概念 $c$ 的有效近似呢？

对 PAC 学习来说，只要训练集 $D$ 的规模能使学习算法 $\mathfrak{L}$ 以概率 $1-\delta$ 找到目标假设的 $\epsilon$ 近似即可。

我们先估计泛化误差大于 $\epsilon$ 但在训练集上仍表现完美的假设出现的概率。 假定 $h$ 的泛化误差大于 $\epsilon$，对分布 $\mathcal{D}$ 上随机采样而得的任何样例 $(\boldsymbol{x}, y)$，有

$$
\begin{aligned} P(h(\boldsymbol{x})=y) &=1-P(h(\boldsymbol{x}) \neq y) \\ &=1-E(h) \\ &<1-\epsilon \end{aligned}
$$

由于 $D$ 包含 $m$ 个从 $\mathcal{D}$ 独立同分布采样而得的样例，因此，$h$ 与 $D$ 表现一致的概率为

$$
\begin{aligned} P\left(\left(h\left(\boldsymbol{x}_{1}\right)=y_{1}\right) \wedge \ldots \wedge\left(h\left(\boldsymbol{x}_{m}\right)=y_{m}\right)\right) &=(1-P(h(\boldsymbol{x}) \neq y))^{m} \\ &<(1-\epsilon)^{m} \end{aligned}
$$

我们事先并不知道学习算法 $\mathfrak{L}$ 会输出 $\mathcal{H}$ 中的哪个假设，但仅需保证泛化 误差大于 $\epsilon$ ，且在训练集上表现完美的所有假设出现概率之和不大于 $\delta$ 即可：

$$
\begin{aligned} P(h \in \mathcal{H} : E(h)>\epsilon \wedge \widehat{E}(h)=0) &<|\mathcal{H}|(1-\epsilon)^{m} \\ &<|\mathcal{H}| e^{-m \epsilon} \end{aligned}
$$

令上式不大于 $\delta$ ，即

$$
|\mathcal{H}| e^{-m \epsilon} \leqslant \delta
$$

可得

$$
m \geqslant \frac{1}{\epsilon}\left(\ln |\mathcal{H}|+\ln \frac{1}{\delta}\right)
$$

由此可知，有限假设空间 $\mathcal{H}$ 都是 PAC 可学习的，所需的样例数目如上式所示，输出假设 $h$ 的泛化误差随样例数目的増多而收敛到 $0$，收敛速率为 $O\left(\frac{1}{m}\right)$。

## 不可分情形

对较为困难的学习问题，目标概念 $c$ 往往不存在于假设空间 $\mathcal{H}$ 中。假定对 于任何 $h \in \mathcal{H}, \widehat{E}(h) \neq 0$，也就是说，$\mathcal{H}$ 中的任意一个假设都会在训练集上出现 或多或少的错误。由 Hoeffding 不等式易知：

**引理 12.1** 若训练集 $D$ 包含 $m$ 个从分布 $\mathcal{D}$ 上独立同分布采样而得的样例，$0<\epsilon<1$，则对任意 $h \in \mathcal{H}$，有

$$
P(\widehat{E}(h)-E(h) \geqslant \epsilon) \leqslant \exp \left(-2 m \epsilon^{2}\right)\tag{12.15}
$$
$$
P(E(h)-\widehat{E}(h) \geqslant \epsilon) \leqslant \exp \left(-2 m \epsilon^{2}\right)\tag{12.16}
$$
$$
P(|E(h)-\widehat{E}(h)| \geqslant \epsilon) \leqslant 2 \exp \left(-2 m \epsilon^{2}\right)\tag{12.17}
$$


> $$
> P(|\hat{E}(h)-E(h)| \geq \varepsilon) \leq 2 e^{-2 m \varepsilon^{2}}
> $$
>
> [推导]：已知 Hoeffding 不等式：若 $x_{1}, x_{2} \ldots . . . x_{m}$ 为 $m$ 个独立变量，且满足 $0 \leq x_{i} \leq 1$ ，则对任意 $\varepsilon>0$，有：
>
> $$
> P\left(\left|\frac{1}{m} \sum_{i}^{m} x_{i}-\frac{1}{m} \sum_{i}^{m} E\left(x_{i}\right)\right| \geq \varepsilon\right) \leq 2 e^{-2 m \varepsilon^{2}}
> $$
>
> 将 $x_{i}$ 替换成损失函数 $l\left(h\left(x_{i}\right) \neq y_{i}\right)$，显然 $0 \leq l\left(h\left(x_{i}\right) \neq y_{i}\right) \leq 1$，且独立，带入 Hoeffiding 不等式可得：
>
> $$
> P\left( | \frac{1}{m} \sum_{i}^{m} l\left(h\left(x_{i}\right) \neq y_{i}\right)-\frac{1}{m} \sum_{i}^{m} E\left(l\left(x_{i}\right) \neq y_{i}\right)\right) | \geq \varepsilon ) \leq 2 e^{-2 m \varepsilon^{2}}
> $$
>
> 其中：
>
> $$
> \hat{E}(h)=\frac{1}{m} \sum_{i}^{m} l\left(h\left(x_{i}\right) \neq y_{i}\right)
> $$
>
> $$
> E(h)=P_{x \in \mathbb{D}} l(h(x) \neq y)=E(l(h(x) \neq y))=\frac{1}{m} \sum_{i}^{m} E\left(l\left(h\left(x_{i}\right) \neq y_{i}\right)\right)
> $$
>
> 所以有：
> $$
> P(|\hat{E}(h)-E(h)| \geq \varepsilon) \leq 2 e^{-2 m \varepsilon^{2}}
> $$

**推论 12.1** 若训练集 $D$ 包含 $m$ 个从分布 $\mathcal{D}$ 上独立同分布采样而得的样例，$0<\epsilon<1$，则对任意 $h \in \mathcal{H}$ ，式(12.18)以至少 $1-\delta$ 的概率成立：

$$
\widehat{E}(h)-\sqrt{\frac{\ln (2 / \delta)}{2 m}} \leqslant E(h) \leqslant \widehat{E}(h)+\sqrt{\frac{\ln (2 / \delta)}{2 m}}\tag{12.18}
$$

> $$
> \hat{E}(h)-\sqrt{\frac{\ln (2 / \delta)}{2 m}} \leq E(h) \leq \hat{E}(h)+\sqrt{\frac{\ln (2 / \delta)}{2 m}}
> $$
> [推导]：由（12.17）可知：
> $$
> P(|\hat{E}(h)-E(h)| \geq \varepsilon) \leq 2 e^{-2 m \varepsilon^{2}}
> $$
> 成立
>
> 即:
> $$
> P(|\hat{E}(h)-E(h)| \leq \varepsilon) \geq 1-2 e^{-2 m \varepsilon^{2}}
> $$
> 取 $\delta=2 e^{-2 m \varepsilon^{2}}$，则 $\varepsilon=\sqrt{\frac{\ln (2 / \delta)}{2 m}}$
>
> 所以 $|\hat{E}(h)-E(h)| \leq \sqrt{\frac{\ln (2 / \delta)}{2 m}}$ 的概率不小于 $1-\delta$
>
> 整理得：
> $$
> \hat{E}(h)-\sqrt{\frac{\ln (2 / \delta)}{2 m}} \leq E(h) \leq \hat{E}(h)+\sqrt{\frac{\ln (2 / \delta)}{2 m}}
> $$
> 以至少 $1-\delta$ 的概率成立

推论 12.1表明，样例数目 $m$ 较大时，$h$ 的经验误差是其泛化误差很好的近似。对于有限假设空间 $\mathcal{H}$，我们有

定理 12.1 若 $\mathcal{H}$ 为有限假设空间，$0<\delta<1$，则对任意 $h \in \mathcal{H}$ ，有

$$
P\left(|E(h)-\widehat{E}(h)| \leqslant \sqrt{\frac{\ln |\mathcal{H}|+\ln (2 / \delta)}{2 m}}\right) \geqslant 1-\delta\tag{12.19}
$$

证明，令 $h_{1}, h_{2}, \ldots, h_{|\mathcal{H}|}$ 表示假设空间 $\mathcal{H}$ 中的假设，有

$$
\begin{aligned} & P(\exists h \in \mathcal{H} :|E(h)-\widehat{E}(h)|>\epsilon) \\=& P\left(\left(\left|E_{h_{1}}-\widehat{E}_{h_{1}}\right|>\epsilon\right) \vee \ldots \vee\left(\left|E_{h_{|\mathcal{H}|}}-\widehat{E}_{h_{|\mathcal{H}|}}\right|>\epsilon\right) \right)\\ \leqslant & \sum_{h \in \mathcal{H}} P(|E(h)-\widehat{E}(h)|>\epsilon) \end{aligned}
$$

由式(12.17)可得

$$
\sum_{h \in \mathcal{H}} P(|E(h)-\widehat{E}(h)|>\epsilon) \leqslant 2|\mathcal{H}| \exp \left(-2 m \epsilon^{2}\right)
$$

于是，令 $\delta=2|\mathcal{H}| \exp \left(-2 m \epsilon^{2}\right)$ 即可得式(12.19)。

即在宄的所有假设中找 出最好的一个。


显然，当 $c \notin \mathcal{H}$ 时，学习算法 $\mathfrak{L}$ 无法学得目标概念 $c$ 的 $\epsilon$ 近似。但是，当 假设空间 $\mathcal{H}$ 给定时，其中必存在一个泛化误差最小的假设，找出此假设的 $\epsilon$ 近似也不失为一个较好的目标。 $\mathcal{H}$ 中泛化误差最小的假设是 $\arg \min _{h \in \mathcal{H}} E(h)$ ， 于是，以此为目标可将 PAC 学习推广到 $c \notin \mathcal{H}$ 的情况，这称为“不可知学习”(agnostic learning)。相应的，我们有：

**定义 12.5** 不可知 PAC 可学习(agnostic PAC learnable):令 $m$ 表示从分布 $\mathcal{D}$ 中独立同分布采样得到的样例数目，$0<\epsilon$, $\delta<1$，对所有分布 $\mathcal{D}$，若存在学习算法 $\mathfrak{L}$ 和多项式函数 $\operatorname{poly}(\cdot, \cdot, \cdot, \cdot)$ ，使得对于任何 $m \geqslant \operatorname{poly}(1 / \epsilon, 1 / \delta, \operatorname{size}(\boldsymbol{x}), \operatorname{size}(c))$, $\mathfrak{L}$ 能从假设空间 $\mathcal{H}$ 中输出满足式(12.20)的假设 $h$：

$$
P\left(E(h)-\min _{h^{\prime} \in \mathcal{H}} E\left(h^{\prime}\right) \leqslant \epsilon\right) \geqslant 1-\delta\tag{12.20}
$$

则称假设空间 $\mathcal{H}$ 是不可知 PAC 可学习的。

与 PAC 可学习类似，若学习算法 $\mathfrak{L}$ 的运行时间也是多项式函数 $\operatorname{poly}(1 / \epsilon, 1 / \delta, \operatorname{size}(\boldsymbol{x}), \operatorname{size}(c))$，则称假设空间 $\mathcal{H}$ 是高效不可知 PAC 可学习的，学习算法 $\mathfrak{L}$ 则称为假设空间 $\mathcal{H}$ 的不可知 PAC 学习算法，满足上述要求的最小 $m$ 称为学习算法 $\mathfrak{L}$ 的样本复杂度。


# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
