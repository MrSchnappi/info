# 特征选择

（整理的有点问题）

需参考 [机器学习中，有哪些特征选择的工程方法？](https://www.zhihu.com/question/28641663)

- 介绍
- 特征重要性评估
- 特征子集的搜索与评价
- 常用特征选择方法

（不是很清楚为什么需要特征重要性评估和特征子集的搜索和评价？为什么还有常用特征选择方法？到底使用哪个？）

# 特征选择

概念：

- 从给定的特征集合中选择出相关特征子集的过程。注意，并不一定是线性相关的。

原因：

- 减轻维数灾难。与降维是处理高维数据的 两大主流技术。
- 去除不相关的特征往往使得更容易学习。只留下关键因素，则真相往往更易看清。


去掉那些特征：

- 无关特征。指与当前学习任务无关。
- 冗余特征。它们所包含的信息能从其他特征中推演出来。
  - 例如，立方体对象，若已有“底面长” “底面宽”，则“底面积”是冗余特征

注意：

- 但有时冗余特征会降低学习任务的难度。若某个冗余特征恰好对应了完成学习任务所需的“中间概念”，则该冗余特征是有益的。
  - 例如，若估算立方体的体积，则 “底面积”这个冗余特征的存在将使得体积的估算更容易


## 特征重要度评估

- 回归模型系数判断法
- 信息熵判断法

**除此之外，还可以训练模型来筛选特征，比如带L1或L2惩罚项的Linear Model、Random Forest、GDBT等，它们都可以输出特征的重要度。可以将不同方法的平均重要度作为最终参考指标，筛选掉得分低的特征。**<span style="color:red;">没有很清楚。</span>

### 回归模型系数判断法


对于一些线性模型来说，它们训练出的结果是一个数学公式，类似：

$$y=a_1x_1+a_2x_2+a_3x_3+\cdots$$

其中，$a_1$、$a_2$、$a_3$ 就是我们模型最终得到的系数。

比如，我们可以对处理后的样本进行逻辑回归，得到一个模型，而这个模型里面对应每个特征都有一个系数，那么我们就可以根据这个系数来判断这个特征对于这个目标列的重要程度。

<span style="color:red;">**？** 这个只能知道是不是线性相关吧？</span>

相关度系数：

- 主要是衡量两个变量之间的线性关系

说明：

- 数值在[-1.0, 1.0]区间中。
- 数值越是接近0，两个变量越是线性不相关。但是数值为0，并不能说明两个变量不相关，只是线性不相关而已。

举例：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200223/pqL7xs0RccME.png?imageslim">
</p>


相关系数矩阵是一个对称矩阵，所以只需要关注矩阵的左下角或者右上角。我们可以拆成两点来看：

1. Feature和Label的相关度可以看作是该Feature的重要度，越接近1或-1就越好。
2. Feature和Feature之间的相关度要低，如果两个Feature的相关度很高，就有可能存在冗余。



### 信息熵判断法


比如下面这个数据集：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180728/C4gFF5C6FG.png?imageslim">
</p>


特征列共 3 种颜色，有 5 个样本，特征列信息熵可以计算如下：

$$H(U)=-[\frac{2}{5}\times log(\frac{2}{5})+\frac{2}{5}\times log(\frac{2}{5})+\frac{1}{5}\times log(\frac{1}{5})]$$

我们指导，一个事物的不确定性越大，那么这个事物未来发展的可能性也就越多，那么它的信息量也就越大，信息熵也越大。**这句话对不对？**

那么我们如何来理解特征的重要性和信息熵的关联呢？

这里引出一个名词：信息增益（Information Gain）。在信息增益中，衡量标准是看特征能够为分类系统带来多少信息，带来的信息越多，则该特征越重要。对于一个特征而言，系统有他和没它的时候的信息量将发生变化，而前后信息量的差就是这个特征给系统的信息量。

听着的确是有些道理的。那么具体怎么做 i 呢？

比如，对于上面这个数据集而言，目标列减去特征列的信息增益代表这个特征对结果的影响，公式如下：

$$G(喜好颜色)=H(性别)-H(喜好颜色)$$

在实际的使用中，我们通过遍历每个特征的信息增益来将特征排序，这个顺序既可以用来评判特征的重要性，也可以来在决策树算法中排列树的顺序（参见随机森林算法）。

上面两种方法的评估角度和计算方法不同，实际中需要根据具体的使用场景来决定。




## 特征子集的搜索与评价

寻找好的特征子集有两个问题：

- 如何根据评价结果获取下一个候选特征子集？
- 如何评价候选特征子集的好坏？


**第一个环节，子集搜索**

三种方式：

- 逐渐増加相关特征的策略称为“前向”(forward)搜索。
- 从完整的特征集合开始，每次尝试去掉一个无关特征，这样逐渐减少特征的策略 称为“后向”(backward)搜索。
- 还可将前向与后向搜索结合起来，每一轮逐渐增加选定相关特征(这些特征在后续轮中将确定不会被去除)、同时减少无关特征，这样的策略称为“双向”(bidirectional)搜索。

举例，前向搜索：

给定特征集合 $\{a_1,a_2,\cdots ,a_d\}$ 。

- 我们可将每个特征看作一个候选子集，对这 d 个候选单特征子集进行评价，假定 $\{a_2\}$ 最优，于是将 $\{a_2\}$ 作为第一轮的选定集；
- 然后，在上一轮的选定集中加入一个特征，构成包含两个特征的候选子集，假定在这 d-1 个候选两特征子集中  $\{a_2,a_4\}$ 最优，且优于 $\{a_2\}$ 于是将  $\{a_2,a_4\}$ 作为本轮的选定集；
- ……假定在第 $k+1$ 轮时，最优的候选 $(k+1)$ 特征子集不如上一轮的选定集，则停止生成候选子集，并将上一轮选定的特征集合作为特征选择结果。



存在的问题：

- 显然，上述策略都是贪心的，因为它们仅考虑了使本轮选定集最优。遗憾的是，若不进行穷举搜索，则这样的问题无法避兔。
  - 例如在 第三轮假定选择 $a_5$ 优于 $a_6$ ，于是选定集为  $\{a_2,a_4,a_5\}$ ，然而在第四轮却可能是  $\{a_2,a_4,a_6,a_8\}$ 比所有的 $\{a_2,a_4,a_5,a_i\}$ 都更优。

**第二个环节，子集评价**

（没有很明白）

给定数据集 $D$ ，假定 $D$ 中第 $i$ 类样本所占的比例为 $p_{i}(i=1,2, \ldots,|\mathcal{Y}|)$ 。为便于讨论，假定样本属性均为离散型。对属性子集 $A$ ，假定根据其取值将 $D$ 分成了 $V$ 个子集 $\left\{D^{1}, D^{2}, \ldots, D^{V}\right\}$ ，每个子集中的样本在 $A$ 上取值相同 **？**，于是我们可计算属性子集 $A$ 的信息增益

$$
\operatorname{Gain}(A)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)
$$

其中信息熵定义为

$$
\operatorname{Ent}(D)=-\sum_{i=1}^{ | \mathcal{Y |}} p_{k} \log _{2} p_{k}
$$

信息增益 $\operatorname{Gain}(A)$ 越大，意味着特征子集 $A$ 包含的有助于分类的信息越多。于 是，对每个候选特征子集，我们可基于训练数据集 $D$ 来计算其信息増益，以此作为评价准则。


更一般的，特征子集 $A$ 实际上确定了对数据集 $D$ 的一个划分，每个划分区域对应着 $A$ 上的一个取值，而样本标记信息 $Y$ 则对应着对 $D$ 的真实划分，通过估算这两个划分的差异，就能对 $A$ 进行评价。与 $Y$ 对应的划分的差异越小，则说明 $A$ 越好。信息熵仅是判断这个差异的一种途径，其他能判断两个划分差异的机制都能用于特征子集评价。

将特征子集搜索机制与子集评价机制相结合，即可得到特征选择方法。例如将前向搜索与信息熵相结合，这显然与决策树算法非常相似。事实上，决策树可用于特征选择，树结点的划分属性所组成的集合就是选择出的特征子集。其他的特征选择方法未必像决策树特征选择这么明显，但它们在本质上都是显式或隐式地结合了某种(或多种)子集搜索机制和子集评价机制。





# 常用的特征选择方法

常见的特征选择方法大致可分为三类：

- 过滤式(filter)
- 包裹式(wrapper)
- 嵌入式(embedding)


## 大概介绍如下

- 过滤式选择：先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。典型算法为 Relief 算法。
  - 方差选择法
  - 相关系数法
  - 卡方检验
  - 互信息法
- 包裹式选择：选择直接把最终将要使用的学习器的性能作为特征子集的评价标准。典型算法为 LVM（Las Vegas Wrapper）。
  - 递归特征消除法
- 嵌入式选择：将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。典型算法为岭回归（ridge regression）、LASSO 回归（Least Absolute Shrinkage and Selection Operator）等。
  - 基于惩罚项的特征选择法
  - 基于树模型的特征选择法


## 良好特征的特点包括

- 避免很少使用的离散特征值：良好的特征值应该在数据集中出现大约 5 次以上
- 最好具有清晰明确的含义
- 良好的浮点特征不包含超出范围的异常断点或 “神奇” 的值
- 特征的定义不应随时间发生变化


## 过滤式

概念:

- 过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。


相当于：

- 先用特征选择过程对初始特征进行“过滤”，再用过滤后的特征来训练模型。

### Relief

Relief (Relevant Features) 是一种著名的过滤式特征选择方法，该方法设计了一个“相关统计量”来度量特征的重要性。该统计量是一个向量，其每个分量分别对应于一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定。于是，最终只需指定一个阈值 $\tau$ ，然后选择比 $\tau$  大的相关统计量分量所对应的特征即可；也可指定欲选取的特征个数 $k$ ，然后选择相关统计量分量最大的 $k$ 个特征。

显然，Relief的关键是如何确定相关统计量。给定训练集 $\left\{\left(\boldsymbol{x}_{1}, y_{1}\right)\right.\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right) \}$ 对每个示例 $\boldsymbol{x}_{i}$ ，Relief 先在 $\boldsymbol{x}_{i}$ 的同类样本中寻找 其最近邻 $\boldsymbol{x}_{i, \mathrm{nh}}$，称为“猜中近邻” (near-hit)，再从 $\boldsymbol{x}_{i}$ 的异类样本中寻找其最近邻 $\boldsymbol{x}_{i, \mathrm{nm}}$ ，称为“猜错近邻” (near-miss)，然后，相关统计量对应于属性 $j$ 的分量为

$$
\delta^{j}=\sum_{i}-\operatorname{diff}\left(x_{i}^{j}, x_{i, \mathrm{nh}}^{j}\right)^{2}+\operatorname{diff}\left(x_{i}^{j}, x_{i, \mathrm{nm}}^{j}\right)^{2}
$$

其中 $x_a^j$ 表示样本 $\boldsymbol{x}_{a}$ 在属性 $j$ 上的取值，$\operatorname{diff}\left(x_{a}^{j}, x_{b}^{j}\right)$  取决于属性 $j$ 的类型：若属性 $j$ 为离散型，则 $x_a^j=x_b^j$ 时 $\operatorname{diff}\left(x_{a}^{j}, x_{b}^{j}\right)=0$ ，否则为 1；若属性 $j$ 为连续型， 则 $\operatorname{diff}\left(x_{a}^{j}, x_{b}^{j}\right)=\left|x_{a}^{j}-x_{b}^{j}\right|$ ，注意 $x_a^j$,$x_b^j$ 已规范化到[0,1]区间。

从式(11.3)可看出，若 $\boldsymbol{x}_{i}$ 与其猜中近邻 $\boldsymbol{x}_{i, \mathrm{nh}}$ 在属性 $j$ 上的距离小于  $\boldsymbol{x}_{i}$ 与其猜错近邻 $\boldsymbol{x}_{i, \mathrm{nm}}$ 的距离，则说明属性 $j$ 对区分同类与异类样本是有益的，于是增大属性 $j$ 所对应的统计量分量;

反之，若 $\boldsymbol{x}_{i}$ 与其猜中近邻 $\boldsymbol{x}_{i, \mathrm{nh}}$ 在属性 $j$ 上的距离大于 $\boldsymbol{x}_{i}$ 与其猜错近邻 $\boldsymbol{x}_{i, \mathrm{nm}}$ 的距离，则说明属性 $j$ 起负面作用，于是减小属性 j 所对应的统计量分量。

最后，对基于不同样本得到的估计结果进行平均， 就得到各属性的相关统计量分量，分量值越大，则对应属性的分类能力就越强。

式(11.3)中的 $i$ 出了指出了用于平均的样本下标。实际上 Relief 只需在数据集的 采样上而不必在整个数据集上估计相关统计量。显然，Relief 的时间开销随采样次数以及原始特征数线性増长，因此是一个运行效率很高的过滤式特征选择算法。

### Relief-F

Relief 是为二分类问题设计的，其扩展变体 Relief-F 能处理多分类问题。假定数据集 D 中的样本来自 $|\mathcal{Y}|$ 个类别。对示例 $\boldsymbol{x}_{i}$ ，若它属于第 $k$ 类 $(k\in \{1,2,\cdots ,|\mathcal{Y}|\}$ ，则 Relief-F 先在第 $k$ 类的样本中寻找 $\boldsymbol{x}_{i}$ 的最近邻示例 $\boldsymbol{x}_{i, \mathrm{nh}}$ 并将其作为猜中近邻，然后在第 $k$ 类之外的每个类中找到一个 $\boldsymbol{x}_{i}$ 的最近邻示例作为猜错近邻，记为 $\boldsymbol{x}_{i, l, \operatorname{nm}}(l=1,2, \ldots,|\mathcal{Y}| ; l \neq k)$ 。于是，相关统计量对应于属性 $j$ 的分量为

$$
\delta^{j}=\sum_{i}-\operatorname{diff}\left(x_{i}^{j}, x_{i, \mathrm{nh}}^{j}\right)^{2}+\sum_{l \neq k}\left(p_{l} \times \operatorname{diff}\left(x_{i}^{j}, x_{i, l, \mathrm{nm}}^{j}\right)^{2}\right)
$$

其中 $p_l$ 为第 $l$ 类样本在数据集 $D$ 中所占的比例。


## 包裹式选择

与过滤式特征选择不考虑后续学习器不同，包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价准则。

换言之，包裹式特征选择的目的就是为给定学习器选择最有利于其性能、“量身定做”的特征子集。

一般而言，由于包裹式特征选择方法直接针对给定学习器进行优化，因此：

- 从最终学习器性能来看，包裹式特征选择比过滤式特征选择更好。
- 但另一方面，由于在特征选择过程中需多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择大得多。


### LVW

LVW (Las Vegas Wrapper) 是一个典型的包裹式特征选择方法。它在拉斯维加斯方法(Las Vegas method)框架下使用随机策略来进行子集搜索，并以最终分类器的误差为特征子集评价准则。算法描述如下图所示。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180629/9K4mHIe7lE.png?imageslim">
</p>


算法第 8 行是通过在数据集 $D$ 上，使用交叉验证法来估计学习器 $\mathfrak{L}$ 的误差，注意这个误差是在仅考虑特征子集 $A'$ 时得到的，即特征子集 $A'$ 上的误差，若它比当前特征子集 $A$ 上的误差更小，或误差相当但 $A'$ 中包含的特征 数更少，则将 $A'$ 保留下来。

需注意的是，由于 LVW 算法中特征子集搜索采用了随机策略，而每次特征子集评价都需训练学习器，计算开销很大，因此算法设置了停止条件控制参数 T。然而，整个 LVW 算法是基于拉斯维加斯方法框架，若初始特征数很多(即 $|A|$ 很大)、 $T$ 设置较大，则算法可能运行很长时间都达不到停止条件。换言之，若有运行时间限制，则有可能给不出解。


# 嵌入式选择与 $\mathrm{L}_{1}$ 正则化

在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。

与此不同，嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。

关于正则化相关内容见正则化文件夹。

## 范数 $\mathrm{L}_{1}$ 得到稀疏解的意义

注意到 $\boldsymbol{w}$ 取得稀疏解意味着初始的 $d$ 个特征中仅有对应着 $\boldsymbol{w}$ 的非零分量的特征才会出现在最终模型中，于是，求解 $\mathrm{L}_{1}$ 范数正则化的结果是得到了仅采用一部分初始特征的模型；

换言之，基于 $\mathrm{L}_{1}$ 正则化的学习方法就是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，同时完成。<span style="color:red;">厉害</span>
