
# 偏差 误差 方差


- 概念介绍
- 偏差-方差分解
- 偏差-方差窘境
- 根据偏差和方差对调整训练
- 实时诊断偏差和方差：学习曲线


# 概念介绍

Error = Bias + Variance + Noise


- 误差 Error
- 偏差 Bias
- 方差 Variance

说明：


OK，我们回顾一下上面几个偏差、方差、噪声的公式：

- 误差。
  - 一般把学习器的实际预测值与样本的真实标签之间的差异称为“误差”。反映的是整个模型的准确度。
  - Error = Bias + Variance + Noise
- 偏差
  - 衡量模型拟合训练数据的能力（注：训练数据不一定是整个训练集，而是指用于训练它的那一部分数据，例如：mini-batch）
  - 偏差反映的是模型在样本上的输出与真实值之间的误差，即模型本身的拟合能力。
  - 偏差越小，拟合能力越高，越可能产生过拟合。
- 方差
  - 方差描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。公式：$S_{N}^{2}=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}$
  - 方差越大，数据的分布越分散，模型的稳定程度越差。
  - 方差越小，模型的泛化的能力越高；反之，模型的泛化的能力越低。
  - 如果模型在训练集上拟合效果比较优秀，但是在测试集上拟合效果比较差劣，则方差较大，说明模型的稳定程度较差，出现这种现象可能是由于模型对训练集过拟合造成的。 如下图右列所示。
- 噪声。
  - 描述了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。


<p align="center">
    <img width="99%" height="70%" src="http://images.iterate.site/blog/image/20190826/46JXI1pijVxo.png?imageslim">
</p>




# 偏差-方差分解

“偏差-方差分解” 是解释学习算法泛化性能的一种重要工具。

以回归任务为例：

- 假设：
  - 测试样本 $\boldsymbol{x}$
  - $y$ 为 $\boldsymbol{x}$ 的真实的标记，
  - $y_{d}$ 为 $\boldsymbol{x}$ 在数据集中的标记。（说明：有可能出现噪声使得 $y_{d} \neq y$）
  - $f$ 为训练集 $d$ 上学得模型 $f$ 在 $\boldsymbol{x}$ 上的预测输出。

- 以回归任务为例，学习算法的期望预测为：

$$
\overline{f}=E_{d}[f]
$$

- 使用样本数相同的不同训练集产生的方差为：

$$
\operatorname{var}(\boldsymbol{x})=E_{d}\left[(f-\overline{f})^{2}\right]
$$

- 噪声为：

$$
\varepsilon^{2}=E_{d}\left[\left(y_{d}-y\right)^{2}\right]
$$

- 期望输出与真实标记的差别称为偏差 (bias) ，即

$$
\operatorname{bias}^{2}(\boldsymbol{x})=(\overline{f}-y)^{2}
$$

- 为便于讨论，假定噪声期望为零，即 $E_d[y_d-y]=0$ 。
- 通过简单的多项式展开合并，可对算法的期望泛化误差进行分解：

$$
\begin{aligned} E(f ; d) =&E_{d}\left[\left(f-y_{d}\right)^{2}\right] \\=& E_{d}\left[\left(f-\overline{f}+\overline{f}-y_{d}\right)^{2}\right] \\=& E_{d}\left[(f-\overline{f})^{2}\right]+E_{d}\left[\left(\overline{f}-y_{d}\right)^{2}\right] \\ &+E_{d}\left[2(f-\overline{f})\left(\overline{f}-y_{d}\right)\right] \\ =&E_{d}\left[(f-\overline{f})^{2}\right]+E_{d}\left[\left(\overline{f}-y_{d}\right)^{2}\right] \\=& E_{d}\left[(f-\overline{f})^{2}\right]+E_{d}\left[\left(\overline{f}-y+y-y_{d}\right)^{2}\right] \\=& E_{d}\left[(f-\overline{f})^{2}\right]+E_{d}\left[(\overline{f}-y)^{2}\right]+E_{d}\left[\left(y-y_{d}\right)^{2}\right] \\ &+2 E_{d}\left[(\overline{f}-y)\left(y-y_{d}\right)\right]\\=&E_{d}\left[(f-\overline{f})^{2}\right]+(\overline{f}-y)^{2}+E_{d}\left[\left(y_{d}-y\right)^{2}\right]\end{aligned}
$$

- 说明：
  - 中间的第三步到第四步，由期望预测的公式，最后一项为 0
  - 倒数第二步到倒数第一步，因为噪声期望为 0，因此最后一项为 0.
- 于是：

$$
E(f ; D)=\operatorname{bias}^{2}(\boldsymbol{x})+\operatorname{var}(\boldsymbol{x})+\varepsilon^{2}
$$

- 也就是说，泛化误差可分解为偏差、方差与噪声之和.



偏差-方差分解说明，泛化性能是由下面三个原因共同决定的：

- 学习算法的能力
- 数据的充分性
- 学习任务本身的难度


# 偏差-方差窘境

一般来说，偏差与方差是有冲突的，这称为偏差-方差窘境：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/20200305/UU8MhBPfTt5i.png?imageslim">
</p>


训练程度不同时：

- 训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导了泛化错误率；
- 训练程度的加深, 学习器的拟合能力逐渐増强，训练数据发生的扰动渐渐能被学习器学到，方差， 逐渐主导了泛化错误率；
- 在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合。



# 根据偏差和方差对调整训练

是不是可以避免的：


- 偏差：由于样本可能存在噪声，因此偏差可能是有最小值的，无法减小到0。此时这个“可避免的偏差”反映了算法在训练集上的表现比“最优分类器”差多少。
- 方差：如果拥有足够大数据集，所有的方差都是可以“避免的”，所以不存在所谓的“不可避免的方差”。

- 何时需要获取更多的训练数据？
  - 尽管有更多的数据是无害的，但是，它并不总是如我们期望的那样有帮助。

举例：希望构建一个 5%错误的猫识别器

- 情况1：若训练集错误率为 15%，开发集错误率为 16%。此时：
  - 偏差约为 15%
  - 方差约为 1%
- 情况2：若训练集错误率 = 1%，开发集错误率 = 11%。此时：
  - 偏差约为 1%
  - 方差约为 10%（=11%-1%）
- 情况3：若训练集错误率 = 15%，开发集错误率 = 30%。此时：
  - 偏差约为 15%
  - 方差约为 15%
- 情况4：若训练集错误率为 15%，开发集错误率为 5%。此时：
  - 偏差约为 15%
  - 方差约为 -10%
- 情况5：若训练集错误率 = 0.5%，开发集错误率 = 1%。此时：
  - 偏差约为 0.5%
  - 方差约为 0.5%

此时：

- 情况1：欠拟合
- 情况2：过拟合
- 情况3：模型无法拟合数据
  - 可能数据噪声过多，比如：最优错误率（“不可避免的偏差”）已经是 14% 了。
  - 可能模型有问题，无法拟合
  - 但是，起码说明模型没有很好的泛化到开发集上，因此在由于方差而导致的错误上还有很大的提升空间。
- 情况4：训练集过拟合，并且算法已经过度记忆（over-memorized）训练集
  - 此时应该专注于方差减少的方法，而不是进一步减少偏差的方法。
- 情况5：良好


处理偏差和方差：

- 如果具有较高的可避免偏差，那么可以：
  - 对训练集的数据进行分析，比如，将很差的几个样本拿出来看一下，是不是样本本身有问题，或者噪音太大。
    - 如果样本的问题，那么可以消除样本，
    - 如果这种样本实际中一定要用，那么可以专注于使得算法能更好的适应有背景噪声训练样本的方法。
    - 如果可以，是不是在采集样本的时候进行硬件或者软件对样本进行处理。
  - 增加输入特征。以帮助算法消除特定类别的错误。
  - 通过添加层/神经元来增加神经网络的大小。
  - 减少或消除正则化
  - 尝试新架构
- 如果具有较高的方差，那么可以：
  - 增加训练数据集。这是处理方差问题最简单也是最可靠的方法。
  - 增加正则化方法。如 L2正则化，L1正则化，dropout。该方法减少了方差，但增加了偏差。
  - 添加提前停止（early stopping）：基于开发集错误提前停止梯度下降），该方法减少方差但增加了偏差。提前停止的行为很像正则化方法，一些作者称它为正则化方法。
  - 选择特征以减少输入特征的数目/类型：注意，当你训练集比较小时，特征选择可能非常有用。最好不要减少特征。
  - 基于错误分析的洞察修改输入特征：假设错误分析启发你去创建额外的特征，以帮助算法消除特定类别的错误。这些新特征可能有助于减少偏差和方差。理论上来说，增加更多的特征可能会增加方差，但如果你发现这种情况，那么就使用正则化方法，它通常能够消除方差的增加。
  - 修改模型架构



# 实时诊断偏差和方差：学习曲线

开发集（和测试集）错误应该随着训练集大小的增长而减少。
但随着训练集大小的增加，训练集错误通常会增加。

## 开发错误曲线

绘制：

- 可以使用不同大小的训练集去运行算法。例如，如果你有 1000 个样本，你可以在 100,200,300,…,1000个样本上单独训练算法副本。
- 然后你就能画出开发集错误如何随着训练集大小而变化的曲线了。

图像：


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180812/69Dj9lIGF6.png?imageslim">
</p>


说明：

- 随着训练集大小的增加，开发集错误应该减少。
- 期望错误率
  - 如果我们希望达到人类水平的表现，那么人类错误率可能就是“期望错误率”。
  - 如果我们的学习算法为某些产品提供服务（如提供猫图），我们可能会直观的了解需什么样的水平才能给用户提供出色的体验。
  - 如果你长期从事于一个重要应用，那么你可能会有直觉认为在下一个季度/年内能合理取得多大进展。
- 错误曲线下降时，通过添加更多的数据来达到期望的水平是较为合理的。错误曲线趋于稳定时，此时添加更多数据效果不再明显。

优点：

- 查看学习曲线可能会帮助你避免花费数月时间来收集两倍多的训练数据，只有意识到这并不管用。

缺点：

- 如果你只关注开发错误曲线，如果有更多的数据，你很难推断和准确预测红色的曲线的走向。


这里有一个附加的曲线能够帮助你去评估添加更多数据的影响：训练错误。


## 训练错误曲线

图像：

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180812/dI95EDl4cc.png?imageslim">
</p>

可见：

- 蓝色的“训练错误”曲线随着训练集大小的增长而增长。
- 而且，算法通常在训练集上表现比在开发集上要好。因此，红色的开发错误曲线通常严格地在蓝色训练错误曲线上方。


## 曲线分析


- 偏差大，方差小


<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180812/2L9kKiGjmC.png?imageslim">
</p>


- 偏差较小，方差较大。需要添加更多训练数据可能有助于缩小开发错误和训练错误之间的差距。才能看出来到底是什么情况。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180812/gDACDhIlA9.png?imageslim">
</p>

- 偏差较大，方差也较大。需要在算法中去寻找同时减少偏差和方差的方法。

<p align="center">
    <img width="70%" height="70%" src="http://images.iterate.site/blog/image/180812/jL7lg6hFab.png?imageslim">
</p>


## 绘制学习曲线

训练集：

- 100 个样本

简单方法：

- 随机选择 10 个样本的子集来训练你的算法，然后是 20 个样本，然后 30，直到 100，以 10 个为间隔增加样本数。然后使用这 10 个数据点绘制学习曲线。

问题：

- 当只在 10 个随机选择的样本上训练时，可能不幸选到了特别“bad”的训练集。小的训练集意味着开发和训练错误可能会随机波动。
- 如果你的机器学习应用严重偏倚一个类别（如负样本远比正样本多的猫分类任务），或者类别数比较大（如识别 100 种不同动物类别），那么选择尤其是“不具代表性”或坏的训练集的几率更大。例如，如果 80%的样本是负样本（y=0），只有 20%是正样本（y=1），那么有可能 10 个样本的训练集只包含负样本，因此很难让算法学习到有意义的东西。

此时，训练曲线里的噪声使得很难看见真实的趋势。



方法：

- 可以不是仅对 10 个样本的一个模型进行训练，而是通过从原始 100 个样本的数据集中通过替换的抽样方法【1】选择几个（如 3-10）不同的随机选择的 10 个样本的训练集。在这些数据集上训练不同的模型，并对每个结果模型计算训练集和开发集错误。计算并绘制平均训练错误和平均开发集错误。
- 如果你的训练集比较倾向一种类别，或有很多类别，从 100 个训练样本中选择一个“平衡的”子集而不是随机选择的 10 个训练样本。例如，你可以确保 2/10的样本是正样本，8/10为负样本。更为一般的说，你可以确保每个类别的样本部分尽可能的接近原始训练集的整体部分。

如果你的训练集较大（比如说超过 1000 个样本），并且你的类别分布不是很偏，你可能不需要这些技巧。


绘制成本：

- 绘制学习曲线可能花费很高的计算成本：例如，你可能需要训练 10 个模型，其中分别有 1000 个样本，然后是 2000 个，直到 10000 个。使用小的数据集训练模型比使用大数据集来训练模型要快的多。
- 你可以在 1000、2000、4000、6000和 10000 个样本上训练模型。这样应该仍然可以让你清晰的了解学习曲线的趋势。