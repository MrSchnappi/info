---
title: 12 局部线性嵌入
toc: true
date: 2018-08-12
---
# 流形学习


## 局部线性嵌入

与 Isomap 试图保持近邻样本之间的距离不同，局部线性嵌入(Locally Linear Embedding，简称 LLE) 试图保持邻域内样本之间的线性关系。如图 10.9所示，假定样本点 $\boldsymbol{x}_{i}$ 的坐标能通过它的邻域样本 $\boldsymbol{x}_{j}$ ，$\boldsymbol{x}_{k}$， $\boldsymbol{x}_{l}$ 的坐标通过线性组合而重构出来，即


$$
\boldsymbol{x}_{i}=w_{i j} \boldsymbol{x}_{j}+w_{i k} \boldsymbol{x}_{k}+w_{i l} \boldsymbol{x}_{l}\tag{10.26}
$$

LLE希望式(10.26)的关系在低维空间中得以保持。

LLE先为每个样本  $\boldsymbol{x}_{i}$ 找到其近邻下标集合 $Q_i$ ，然后计算出基于 $Q_i$ 中的样本点对 $\boldsymbol{x}_{i}$ 进行线性重构的系数 $\boldsymbol{w}_{i}$ :


$$
\begin{array}{c}{\min _{\boldsymbol{w}_{1}, \boldsymbol{w}_{2}, \ldots, \boldsymbol{w}_{m}} \sum_{i=1}^{m}\left\|\boldsymbol{x}_{i}-\sum_{j \in Q_{i}} w_{i j} \boldsymbol{x}_{j}\right\|_{2}^{2}} \\ {\text { s.t. } \sum_{j \in Q_{i}} w_{i j}=1}\end{array}\tag{10.27}
$$


其中 $\boldsymbol{x}_{i}$ 和 $\boldsymbol{x}_{j}$ 均为已知，令 $C_{j k}=\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right)^{\mathrm{T}}\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\right)$， $w_{ij}$ 有闭式解：


$$
w_{i j}=\frac{\sum_{k \in Q_{i}} C_{j k}^{-1}}{\sum_{l, s \in Q_{i}} C_{l s}^{-1}}\tag{10.28}
$$

> $$w_{ij}=\cfrac{\sum\limits_{k\in Q_i}C_{jk}^{-1}}{\sum\limits_{l,s\in Q_i}C_{ls}^{-1}}$$
> [推导]：已知
> $$\begin{aligned}
> \min\limits_{\boldsymbol W}&\sum^m_{i=1}\| \boldsymbol x_i-\sum_{j \in Q_i}w_{ij}\boldsymbol x_j \|^2_2\\
> s.t.&\sum_{j \in Q_i}w_{ij}=1
> \end{aligned}$$
> 转换为
> $$\begin{aligned}
> \sum^m_{i=1}\| \boldsymbol x_i-\sum_{j \in Q_i}w_{ij}\boldsymbol x_j \|^2_2 &=\sum^m_{i=1}\| \sum_{j \in Q_i}w_{ij}\boldsymbol x_i- \sum_{j \in Q_i}w_{ij}\boldsymbol x_j \|^2_2 \\
> &=\sum^m_{i=1}\| \sum_{j \in Q_i}w_{ij}(\boldsymbol x_i- \boldsymbol x_j) \|^2_2\\
> &=\sum^m_{i=1}\boldsymbol W^T_i(\boldsymbol x_i-\boldsymbol x_j)(\boldsymbol x_i-\boldsymbol x_j)^T\boldsymbol W_i\\
> &=\sum^m_{i=1}\boldsymbol W^T_i\boldsymbol C_i\boldsymbol W_i
> \end{aligned}$$
> 其中，$\boldsymbol W_i=(w_{i1},w_{i2},\cdot\cdot\cdot,w_{ik})^T$，$k$ 是 $Q_i$ 集合的长度，$\boldsymbol C_i=(\boldsymbol x_i-\boldsymbol x_j)(\boldsymbol x_i-\boldsymbol x_j)^T$，$j \in Q_i$。
> $$
> \sum_{j\in Q_i}w_{ij}=\boldsymbol W_i^T\boldsymbol 1_k=1
> $$
> 其中，$\boldsymbol 1_k$ 为 k 维全 1 向量。
> 运用拉格朗日乘子法可得，
> $$
> J(\boldsymbol W)==\sum^m_{i=1}\boldsymbol W^T_i\boldsymbol C_i\boldsymbol W_i+\lambda(\boldsymbol W_i^T\boldsymbol 1_k-1)
> $$
> $$\begin{aligned}
> \cfrac{\partial J(\boldsymbol W)}{\partial \boldsymbol W_i} &=2\boldsymbol C_i\boldsymbol W_i+\lambda'\boldsymbol 1_k
> \end{aligned}$$
> 令 $\cfrac{\partial J(\boldsymbol W)}{\partial \boldsymbol W_i}=0$，故
> $$\begin{aligned}
> \boldsymbol W_i&=-\cfrac{1}{2}\lambda\boldsymbol C_i^{-1}\boldsymbol 1_k\\
> \boldsymbol W_i&=\lambda\boldsymbol C_i^{-1}\boldsymbol 1_k\\
> \end{aligned}$$
> 其中，$\lambda$ 为一个常数。利用 $\boldsymbol W^T_i\boldsymbol 1_k=1$，对 $\boldsymbol W_i$ 归一化，可得
> $$
> \boldsymbol W_i=\cfrac{\boldsymbol C^{-1}_i\boldsymbol 1_k}{\boldsymbol 1_k\boldsymbol C^{-1}_i\boldsymbol 1_k}
> $$

LLE在低维空间中保持 $\boldsymbol{w}_{i}$ 不变，于是 $\boldsymbol{x}_{i}$ 对应的低维空间坐标 $\boldsymbol{z}_{i}$ 可通过下式求解：

$$
\min _{z_{1}, z_{2}, \ldots, z_{m}} \sum_{i=1}^{m}\left\|z_{i}-\sum_{j \in Q_{i}} w_{i j} z_{j}\right\|_{2}^{2}\tag{10.29}
$$


式(10.27)与(10.29)的优化目标同形，唯一的区别是式(10.27)中需确定的是 $\boldsymbol{w}_{i}$ ，而式(10.29)中需确定的是 $\boldsymbol{x}_{i}$ 对应的低维空间坐标 $\boldsymbol{z}_{i}$ 。


令 $\mathbf{Z}=\left(z_{1}, \boldsymbol{z}_{2}, \ldots, \boldsymbol{z}_{m}\right) \in \mathbb{R}^{d^{\prime} \times m}$ ，$(\mathbf{W})_{i j}=w_{i j}$ ,

$$
\mathbf{M}=(\mathbf{I}-\mathbf{W})^{\mathrm{T}}(\mathbf{I}-\mathbf{W})\tag{10.30}
$$



则式(10.29)可重写为

$$
\begin{array}{c}{\min _{\mathbf{Z}} \operatorname{tr}\left(\mathbf{Z} \mathbf{M} \mathbf{Z}^{\mathrm{T}}\right)} \\ {\text { s.t. } \mathbf{Z Z}^{\mathrm{T}}=\mathbf{I}}\end{array}\tag{10.31}
$$


> $$\begin{aligned}
> &\min\limits_{\boldsymbol Z}tr(\boldsymbol Z \boldsymbol M \boldsymbol Z^T)\\
> &s.t. \boldsymbol Z^T\boldsymbol Z=\boldsymbol I.
> \end{aligned}$$
> [推导]：
> $$\begin{aligned}
> \min\limits_{\boldsymbol Z}\sum^m_{i=1}\| \boldsymbol z_i-\sum_{j \in Q_i}w_{ij}\boldsymbol z_j \|^2_2&=\sum^m_{i=1}\|\boldsymbol Z\boldsymbol I_i-\boldsymbol Z\boldsymbol W_i\|^2_2\\
> &=\sum^m_{i=1}\|\boldsymbol Z(\boldsymbol I_i-\boldsymbol W_i)\|^2_2\\
> &=\sum^m_{i=1}(\boldsymbol Z(\boldsymbol I_i-\boldsymbol W_i))^T\boldsymbol Z(\boldsymbol I_i-\boldsymbol W_i)\\
> &=\sum^m_{i=1}(\boldsymbol I_i-\boldsymbol W_i)^T\boldsymbol Z^T\boldsymbol Z(\boldsymbol I_i-\boldsymbol W_i)\\
> &=tr((\boldsymbol I-\boldsymbol W)^T\boldsymbol Z^T\boldsymbol Z(\boldsymbol I-\boldsymbol W))\\
> &=tr(\boldsymbol Z(\boldsymbol I-\boldsymbol W)(\boldsymbol I-\boldsymbol W)^T\boldsymbol Z^T)\\
> &=tr(\boldsymbol Z\boldsymbol M\boldsymbol Z^T)
> \end{aligned}$$
> 其中，$\boldsymbol M=(\boldsymbol I-\boldsymbol W)(\boldsymbol I-\boldsymbol W)^T$。
> [解析]：约束条件 $\boldsymbol Z^T\boldsymbol Z=\boldsymbol I$ 是为了得到标准化（标准正交空间）的低维数据。


式(10.31)可通过特征值分解求解：$\mathbf{M}$ 最小的 $d'$ 个特征值对应的特征向量组成的矩阵即为 $\mathbf{Z}^{\mathrm{T}}$ 。

LLE的算法描述如图 10.10 所示。算法第 4 行显示出：对于不在样本 $\boldsymbol{x}_{i}$ 邻域区域的样本 $\boldsymbol{x}_{j}$ ，无论其如何变化都对 $\boldsymbol{x}_{i}$ 和 $\boldsymbol{z}_{i}$ 没有任何影响；这种将变动限制在局部的思想在许多地方都有用.

<center>

![](http://images.iterate.site/blog/image/180629/fJ8EcAGcF9.png?imageslim){ width=55% }

</center>








# 相关

- 《机器学习》周志华
- [pumpkin-book](https://github.com/datawhalechina/pumpkin-book)
