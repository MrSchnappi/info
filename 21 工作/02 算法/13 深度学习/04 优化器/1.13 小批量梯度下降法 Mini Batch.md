---
title: 1.13 小批量梯度下降法 Mini Batch
toc: true
date: 2019-09-03
---

# 小批量梯度下降法 Mini Batch



结合了两种方法优点。


Mini-Batch GD是目前最常用的优化算法，严格意义上 Mini-Batch GD也叫做 stochastic GD，所以很多深度学习框架上都叫做 SGD。

为了降低随机梯度的方差，从而使得迭代算法更加稳定，也为了充分利用高度优化的矩阵运算操作，在实际应用中我们会同时处理若干训练数据，该方法被称为小批量梯度下降法（Mini-Batch Gradient Descent）。<span style="color:red;">是的，单个的随机梯度下降震荡有些厉害。</span>

假设需要同时处理 $m$ 个训练数据 $\left\{\left(x_{i_{1}}, y_{i_{1}}\right), \cdots,\left(x_{i_{m}}, y_{i_{m}}\right)\right\}$，则目标函数及其梯度为：


$$
L(\theta)=\frac{1}{m} \sum_{j=1}^{m} L\left(f\left(x_{i_{j}}, \theta\right), y_{i_{j}}\right)\tag{7.44}
$$

$$
\nabla L(\theta)=\frac{1}{m} \sum_{j=1}^{m} \nabla L\left(f\left(x_{i_{j}}, \theta\right), y_{i_{j}}\right)\tag{7.45}
$$

对于小批量梯度下降法的使用，有以下三点需要注意的地方：


1. 如何选取参数 $m$ ？在不同的应用中，最优的 $m$ 通常会不一样，需要通过调参选取。一般 $m$ 取 $2$ 的幂次时能充分利用矩阵运算操作，所以可以在 $2$ 的幂次中挑选最优的取值，例如 32、64、128、256 等。<span style="color:red;">嗯，想知道 $2$ 的幂与不取 $2$ 的幂，计算速度的差别能有多大？对最终结果有影响吗？</span>
2. 如何挑选 $m$ 个训练数据？为了避免数据的特定顺序给算法收敛带来的影响，一般会在每次遍历训练数据之前，先对所有的数据进行随机排序，然后在每次迭代时按顺序挑选 $m$ 个训练数据直至遍历完所有的数据。<span style="color:red;">嗯，好的。</span>
3. 如何选取学习速率 $\alpha$ ？为了加快收敛速率，同时提高求解精度，通常会采用衰减学习速率的方案：一开始算法采用较大的学习速率，当误差曲线进入平台期后，减小学习速率做更精细的调整。最优的学习速率方案也通常需要调参才能得到。<span style="color:red;">也就是说，把 $\alpha$ 跟误差曲线挂钩吗？</span>


综上，通常采用小批量梯度下降法解决训练数据量过大的问题。每次更新模型参数时，只需要处理 $m$ 个训练数据即可，其中 $m$ 是一个远小于总数据量 $M$ 的常数，这样能够大大加快训练过程。<span style="color:red;">嗯。好的。</span>

对比：

![mark](http://images.iterate.site/blog/image/20190818/W4itPgm6zLdr.png?imageslim)

常规梯度下降法走一步要处理到所有二十个例子，但随机算法此时已经走了二十步（每处理一个例子就更新）

# 相关

- 《百面机器学习》
