---
title: 1.07 稀疏表示
toc: true
date: 2019-06-05
---

# 稀疏表示

前文所述的权重衰减直接惩罚模型参数。另一种策略是惩罚神经网络中的激活单元，稀疏化激活单元。这种策略间接地对模型参数施加了复杂惩罚。

我们在 L1 参数正则化中知道 $L^1$ 惩罚如何诱导稀疏的参数，即许多参数为零（或接近于零）。另一方面，表示的稀疏描述了许多元素是零（或接近零）的表示。

我们可以线性回归的情况下简单说明这种区别：


$$\begin{aligned}
\underset{\boldsymbol y ~\in~ \mathbb R^m}{
 \begin{bmatrix}
  18 \\  5 \\ 15 \\ -9 \\ -3
 \end{bmatrix}} =
 \underset{\boldsymbol A ~\in~ \mathbb R^{m \times n}}{
 \begin{bmatrix}
  4 & 0 & 0 & -2 & 0 & 0 \\
  0 & 0 & -1 & 0 & 3 & 0 \\
  0 & 5 & 0 & 0 & 0 & 0 \\
  1 & 0 & 0 & -1 & 0 & -4 \\
  1 & 0 & 0 & 0 & -5 & 0
 \end{bmatrix}}
  \underset{\boldsymbol x ~\in~ \mathbb R^n}{
  \begin{bmatrix}
 2 \\ 3\\ -2\\ -5 \\ 1 \\ 4
 \end{bmatrix} }\\
 \underset{\boldsymbol y ~\in~ \mathbb R^m}{
 \begin{bmatrix}
  -14 \\  1 \\ 19 \\  2 \\ 23
 \end{bmatrix}} =
 \underset{\boldsymbol B ~\in~ \mathbb R^{m \times n}}{
 \begin{bmatrix}
  3 & -1 & 2 & -5 & 4 & 1 \\
  4 & 2 & -3 & -1 & 1 & 3 \\
  -1 & 5 & 4 & 2 & -3 & -2 \\
  3 & 1 & 2 & -3 & 0 & -3 \\
  -5 & 4 & -2 & 2 & -5 & -1
 \end{bmatrix}}
  \underset{\boldsymbol h ~\in~ \mathbb R^n}{
  \begin{bmatrix}
 0 \\ 2 \\ 0 \\ 0 \\ -3 \\ 0
 \end{bmatrix} }
\end{aligned}$$



第一个表达式是参数稀疏的线性回归模型的例子。第二个表达式是数据 $\boldsymbol x$ 具有稀疏表示 $\boldsymbol h$ 的线性回归。也就是说，$\boldsymbol h$ 是 $\boldsymbol x$ 的一个函数，在某种意义上表示存在于 $\boldsymbol x$ 中的信息，但只是用一个稀疏向量表示。

表示的正则化可以使用参数正则化中同种类型的机制实现。

表示的范数惩罚正则化是通过向损失函数 $J$ 添加对表示的范数惩罚来实现的。我们将这个惩罚记作 $\Omega(\boldsymbol h)$。和以前一样，我们将正则化后的损失函数记作 $\tilde J$：


$$\begin{aligned}
 \tilde J(\boldsymbol \theta; \boldsymbol X, \boldsymbol y) =  J(\boldsymbol \theta; \boldsymbol X, \boldsymbol y)  + \alpha \Omega(\boldsymbol h),
\end{aligned}$$


其中 $\alpha \in [0, \infty]$ 权衡范数惩罚项的相对贡献，越大的 $\alpha$ 对应越多的正则化。

正如对参数的 $L^1$ 惩罚诱导参数稀疏性，对表示元素的 $L^1$ 惩罚诱导稀疏的表示：$\Omega(\boldsymbol h) = \|{\boldsymbol h}\|_1 = \sum_i |h_i|$。当然 $L^1$ 惩罚是使表示稀疏的方法之一。其他方法还包括从表示上的 Student-$t$ 先验导出的惩罚和 KL 散度惩罚，这些方法对于将表示中的元素约束于单位区间上特别有用。{HonglakL2008-small}和{Goodfellow2009}都提供了正则化几个样本平均激活的例子，即令 $\frac{1}{m}\sum_i \boldsymbol h^{(i)}$ 接近某些目标值（如每项都是 $.01$ 的向量）。

还有一些其他方法通过激活值的硬性约束来获得表示稀疏。例如，**正交匹配追踪**(orthogonal matching pursuit)通过解决以下约束优化问题将输入值 $\boldsymbol x$ 编码成表示 $\boldsymbol h$


$$\begin{aligned}
 \underset{\boldsymbol h, \|{\boldsymbol h}\|_0 < k}{\arg \min } \|{\boldsymbol x - \boldsymbol W \boldsymbol h}\|^2,
\end{aligned}$$


其中 $\|{\boldsymbol h}\|_0$ 是 $\boldsymbol h$ 中非零项的个数。当 $\boldsymbol W$ 被约束为正交时，我们可以高效地解决这个问题。这种方法通常被称为 OMP-$k$，通过 $k$ 指定允许的非零特征数量。{Coates2011b}证明 OMP-$1$ 可以成为深度架构中非常有效的特征提取器。



含有隐藏单元的模型在本质上都能变得稀疏。在本书中，我们将看到在各种情况下使用稀疏正则化的例子。

# 相关

- 《深度学习》花书
