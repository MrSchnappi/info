---
title: 3.03 批量归一化的基本动机与原理是什么
toc: true
date: 2019-08-31
---

神经网络训练过程的本质是学习数据分布，如果训练数据与测试数据的分布不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有输入数据进行归一化处理。<span style="color:red;">是的。</span>

然而随着网络训练的进行，**每个隐层的参数变化使得后一层的输入发生变化，从而每一批训练数据的分布也随之改变，致使网络在每次迭代中都需要拟合不同的数据分布**，增大训练的复杂度以及过拟合的风险。<span style="color:red;">我擦，真的是这样。每个隐层的参数变化使得后一层的输入发生变化，从而每一批训练数据的分布也随之改变，致使网络在每次迭代中都需要拟合不同的数据分布。嗯，到位。</span>

批量归一化方法是**针对每一批数据**，在网络的**每一层输入之前增加归一化处理**（均值为 0，标准差为 1），将所有批数据强制在统一的数据分布下，**即对该层的任意一个神经元（假设为第 k 维）$\hat{x}^{(k)}$ 采用如下公式**：

$$
\hat{x}^{(k)}=\frac{x^{(k)}-E\left[x^{(k)}\right]}{\sqrt{\operatorname{Var}\left[x^{(k)}\right]}}\tag{9.36}
$$

<span style="color:red;">敢想出来，并且敢做这个归一化也是有些厉害，反向传播的时候要重新计算下。</span>

其中 $x^{(k)}$ 为该层第 $k$ 个神经元的原始输入数据， $E\left[X^{(k)}\right]$ 为这一批输入数据在第 $k$ 个神经元的均值，$\sqrt{\operatorname{Var}\left[x^{(k)}\right]}$ 为这一批数据在第 $k$ 个神经元的标准差。<span style="color:blue;">嗯，这个地方跟我之前理解有些出入。嗯，是对每个神经元来说，这一批的数据进行的归一化，而不是对于某个数据在这一层上进行归一化。嗯。之前的理解有问题。</span>


批量归一化可以看作在每一层输入和上一层输出之间加入了一个新的计算层，对数据的分布进行额外的约束，从而增强模型的泛化能力。

但是批量归一化同时也降低了模型的拟合能力，归一化之后的输入分布被强制为 $0$ 均值和 $1$ 标准差。以 Sigmoid 激活函数为例，批量归一化之后数据整体处于函数的非饱和区域，只包含线性变换，破坏了之前学习到的特征分布。<span style="color:red;">哇塞，是呀！的确，降低了模型的拟合能力，因为处于非饱和区域，无法很大程度上实现非线性拟合。也的确破坏了之前学习到的特征分布。嗯？是的确破坏了吗？</span>

为了恢复原始数据分布，具体实现中引入了变换重构以及可学习参数 $\gamma$ 和 $\beta$：

$$
y^{(k)}=\gamma^{(k)} \hat{x}^{(k)}+\beta^{(k)}\tag{9.37}
$$

其中 $\gamma^{(k)}$ 和 $\beta^{(k)}$ 分别为输入数据分布的方差和偏差。<span style="color:red;">什么是输入数据分布的方差和偏差？是对应这个神经元说的？还是对应这层神经元说的？偏差是个什么？ </span>

- 对于一般的网络，不采用批量归一化操作时，这两个参数高度依赖前面网络学习到的连接权重（对应复杂的非线性）。
- 而在批量归一化操作中，$\gamma$ 和 $\beta$ 变成了该层的学习参数，仅用两个参数就可以恢复最优的输入数据分布，与之前网络层的参数解耦，从而更加有利于优化的过程，提高模型的泛化能力。<span style="color:red;">等等，这个地方还是有点没明白。再理解下。怎么就能与之前的网络层的参数解耦了？为什么 $\gamma$ 和 $\beta$ 变成了该层的学习参数？是通过学习得到的吗？要怎么学习更新？</span>

完整的批量归一化网络层的前向传导过程公式如下：

$$
\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i}\tag{9.38}
$$


$$
\sigma_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2}\tag{9.39}
$$

$$
\hat{x}_{i} \leftarrow \frac{x_{i}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\epsilon}}\tag{9.40}
$$

$$
y_{i} \leftarrow \gamma \hat{x}_{i}+\beta \equiv B N_{\gamma, \beta}\left(x_{i}\right)\tag{9.41}
$$

<span style="color:red;">上面的有几个地方想知道：$\epsilon$ 要怎么得到？$\gamma$ 和 $\beta$ 还是没明白怎么算出来的？</span>


# 相关

- 《百面机器学习》
