---
title: 1.01 一般的分类方法是如何处理非线性的分类问题的
toc: true
date: 2019-08-31
---

# 一般的分类方法是如何处理非线性的分类问题的？



![](http://images.iterate.site/blog/image/180728/baE87lljf1.png?imageslim){ width=55% }

对于普通的线性分类问题来说，LR、linear SVM，都可以作线性分割。他们要做的就是得到一条决策边界。然后把不同的类别分开而已。**LR 和 SVM的核函数再看下？**

那么，对于非线性可分的问题，比如说上面的第二个图，这个时候已经找不到一个超平面来分割两个类别了，那么怎么处理呢？

实际上我们还是有几种方法的：


- 如果这个时候我们用 LR 或者 SVM，我们就可以引入一些非线性的 feature，比如构造一些平方项或者立方项作为特征：\(x_1x_2\)，\(x_1^2\)，\(x_2^2\) ，\(x_1^2x_2\) 等。
- 如果我们用 SVM 的话可以加一个 kernal ，把特征做一个映射，然后再做分割。
- 我们也可以做一个分类器的组合，比如用多个 weak learner 去组成一个 GBDT ，这样就可以组合成一个决策增强树这样一个分类器。

上面这些方法当然是可以的，但是呢，左边这个只是一个简单的情况，在实际的应用中，比如说一个电商的推荐系统，它的特征很多，导致维度很高，在这样的情况下你就没办法像左边一样可视化，也就无法知道你的样本到底是什么样的分布。

所以这时候第一个方法就会有两个问题：


- 当你看不到样本的分布的时候，你就不知道到底是 \(x_1^2\) 有用 还是 \(x_2^2\) 有用，还是两个的乘积有用。
- 而且假如你有 100 万个特征，你去做特征组合，那么最后的维度是非常高的，而且你不知道那个是有用的。


而第二方法这个时候效果也不是很好。**为什么不好？**

在工业界，这个时候大部分会使用第三个方法，即用多个线性分类器组合起来的非线性分类器做分类，但是这也是非常麻烦的。



然而神经网络理论上来说，这些都不是问题，它可以完成任何的一种分布的划分。
