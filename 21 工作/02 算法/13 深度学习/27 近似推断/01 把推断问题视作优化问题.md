---
title: 01 把推断问题视作优化问题
toc: true
date: 2019-06-05
---

# 把推断视作优化问题



精确推断问题可以描述为一个优化问题，有许多方法正是由此解决了推断的困难。通过近似这样一个潜在的优化问题，我们往往可以推导出近似推断算法。



为了构造这样一个优化问题，假设我们有一个包含可见变量 $\boldsymbol v$ 和潜变量 $\boldsymbol h$ 的概率模型。我们希望计算观察数据的对数概率 $\log p(\boldsymbol v;\boldsymbol \theta)$。有时候如果边缘化消去 $\boldsymbol h$ 的操作很费时，我们会难以计算 $\log p(\boldsymbol v;\boldsymbol \theta)$。作为替代，我们可以计算一个 $\log p(\boldsymbol v;\boldsymbol \theta)$ 的下界 $\mathcal L(\boldsymbol v,{\boldsymbol \theta},q)$。这个下界被称为证据下界。这个下界的另一个常用名称是负变分自由能。具体地，这个证据下界是这样定义的：



$$\begin{aligned}
\mathcal L(\boldsymbol v,{\boldsymbol \theta},q) = \log p(\boldsymbol v;{\boldsymbol \theta}) - D_{\text{KL}}(q(\boldsymbol h\mid\boldsymbol v) \boldsymbol ert p(\boldsymbol h\mid\boldsymbol v;{\boldsymbol \theta})),
\end{aligned}$$


其中 $q$ 是关于 $\boldsymbol h$ 的一个任意概率分布。



因为 $\log p(\boldsymbol v)$ 和 $\mathcal L(\boldsymbol v,{\boldsymbol \theta},q)$ 之间的距离是由~KL散度来衡量的，且~KL散度总是非负的，我们可以发现 $\mathcal L$ 总是小于等于所求的对数概率。当且仅当分布 $q$ 完全相等于 $p(\boldsymbol h\mid\boldsymbol v)$ 时取到等号。



令人吃惊的是，对于某些分布 $q$，计算 $\mathcal L$ 可以变得相当简单。通过简单的代数运算我们可以把 $\mathcal L$ 重写成一个更加简单的形式：



$$\begin{aligned}
\mathcal L(\boldsymbol v,{\boldsymbol \theta},q) = & \log p(\boldsymbol v;{\boldsymbol \theta})- D_{\text{KL}}(q(\boldsymbol h\mid\boldsymbol v)\boldsymbol ert p(\boldsymbol h\mid\boldsymbol v;{\boldsymbol \theta})) \\
= & \log p(\boldsymbol v;{\boldsymbol \theta}) - \mathbb E_{\mathbf h\sim q}\log \frac{q(\boldsymbol h\mid\boldsymbol v)}{p(\boldsymbol h\mid\boldsymbol v)} \\
= & \log p(\boldsymbol v;{\boldsymbol \theta}) -  \mathbb E_{\mathbf h\sim q} \log \frac{q(\boldsymbol h\mid\boldsymbol v) }{ \frac{p(\boldsymbol h,\boldsymbol v;{\boldsymbol \theta})}{p(\boldsymbol v; {\boldsymbol \theta})} } \\
= & \log p(\boldsymbol v; {\boldsymbol \theta}) -  \mathbb E_{\mathbf h\sim q} [\log q(\boldsymbol h\mid\boldsymbol v) - \log {p(\boldsymbol h,\boldsymbol v; {\boldsymbol \theta})} + \log {p(\boldsymbol v; {\boldsymbol \theta})} ]\\
= & - \mathbb E_{\mathbf h\sim q}[\log q(\boldsymbol h\mid\boldsymbol v) - \log {p(\boldsymbol h,\boldsymbol v;{\boldsymbol \theta})}].
\end{aligned}$$




这也给出了证据下界的标准定义：


$$\begin{aligned}
\mathcal L(\boldsymbol v,{\boldsymbol \theta},q) = \mathbb E_{\mathbf h\sim q}[\log p(\boldsymbol h , \boldsymbol v)] + H(q).
\end{aligned}$$


对于一个选择的合适分布 $q$ 来说，$\mathcal L$ 是容易计算的。对任意分布 $q$ 的选择来说，$\mathcal L$ 提供了似然函数的一个下界。越好地近似 $p(\boldsymbol h\mid\boldsymbol v)$ 的分布 $q(\boldsymbol h\mid\boldsymbol v)$，得到的下界就越紧，换言之，就是与 $\log p(\boldsymbol v)$ 更加接近。当 $q(\boldsymbol h\mid\boldsymbol v) = p(\boldsymbol h\mid\boldsymbol v)$ 时，这个近似是完美的，也意味着 $\mathcal L(\boldsymbol v,{\boldsymbol \theta},q) = \log {p(\boldsymbol v;{\boldsymbol \theta})} $。



因此我们可以将推断问题看作是找一个分布 $q$ 使得 $\mathcal L$ 最大的过程。精确推断能够在包含分布 $p(\boldsymbol h\mid\boldsymbol v)$ 的函数族中搜索一个函数，完美地最大化 $\mathcal L$。在本章中，我们将会讲到如何通过近似优化寻找分布 $q$ 的方法来推导出不同形式的近似推断。我们可以通过限定分布 $q$ 的形式或者使用并不彻底的优化方法来使得优化的过程更加高效（却更粗略），但是优化的结果是不完美的，不求彻底地最大化 $\mathcal L$，而只要显著地提升 $\mathcal L$。
<!-- %因为只能显著地提升 $\mathcal L$ 而无法彻底地最大化 $\mathcal L$。 -->



无论我们选择什么样的分布 $q$，$\mathcal L$ 始终是一个下界。我们可以通过选择一个更简单或更复杂的计算过程来得到对应的更松或更紧的下界。通过一个不彻底的优化过程或者将分布 $q$ 做很强的限定（并且使用一个彻底的优化过程）我们可以获得一个很差的分布 $q$，但是降低了计算开销。




# 相关

- 《深度学习》花书
