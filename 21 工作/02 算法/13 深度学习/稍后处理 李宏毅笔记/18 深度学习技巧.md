---
title: 18 深度学习技巧
toc: true
date: 2019-08-18
---

![mark](http://images.iterate.site/blog/image/20190818/ykp8kjGYUiyY.png?imageslim)

![mark](http://images.iterate.site/blog/image/20190818/sW7lDa85UYJH.png?imageslim)

当你的模型表现不好，应该怎么处理？

如上图建立 deep learning的三个步骤

•	define a set function

•	goodness of function

•	pick the best function

做完这些事情后，你会得到一个 neural network。在得到 neural network后。
##  神经网络的表现
（1）首先你要检查的是，这个 neural network在你的 training set有没有得到好的结果（是否陷入局部最优），没有的话，回头看，是哪个步骤出了什么问题，你可以做什么样的修改，在 training set得到好的结果。

（2）假如说你在 training set得到了一个好的结果了，然后再把 neural network放在你的 testing data，testing set的 performance 才是我们关心的结果。

如果在 testing data performance不好，才是 overfitting（注：training set上结果就不好，不能说是 overfitting 的问题）。

**小结：如果 training set上的结果变现不好，那么就要去 neural network在一些调整，如果在 testing set表现的很好，就意味成功了。**

（tips：很多人容易忽视查看在 training set上结果，是因为在机器学习中例如是用 SVM 等模型，很容易使得 training set得到一个很好的结果，但是在深度学习中并不是这样的。所以一定要记得查看 training set 上的结果。不要看到所有不好的 performance 都是 overfitting。）

![mark](http://images.iterate.site/blog/image/20190818/RIGniuICpSde.png?imageslim)


例如：在 testing data上看到一个 56-layer和 20-layer，显然 20-layer的 error 较小，那么你就说是 overfitting，那么这是错误的。首先你要检查你在 training data上的结果。

在 training data上 56-layer的 performance 本来就比 20-layer变现的要差很多，在做 neural network时，有很多的问题使你的 train 不好，比如 local mininmize等等，56-layer可能卡在一个 local minimize上，得到一个不好的结果，这样看来，56-layer并不是 overfitting，只是没有 train 的好。
![mark](http://images.iterate.site/blog/image/20190818/ibqHq6rbmdV8.png?imageslim)
 在 deep learning文件上，当你看到一个方式的时候，你首先要想一下说，它是要解什么样的问题，是解决在 deep learning 中一个 training data的 performance 不好，还是解决 testing data performance不好。

当一个方法要被 approaches 时，往往都是针对这两个其中一个做处理，比如，你可能会听到这个方法(dropout),dropout是在 training data表现好，testing data上表现不好的时候才会去使用，当 training data 结果就不好的时候用 dropout 往往会越训练越差。
##  如何改进神经网络？
###  新的激活函数
![mark](http://images.iterate.site/blog/image/20190818/cejCHcNTf28V.png?imageslim)
 现在你的 training data performance不好的时候，是不是你在做 neural 的架构时设计的不好，举例来说，你可能用的 activation function不够好。
![mark](http://images.iterate.site/blog/image/20190818/ulJ6HwFvN9QN.png?imageslim)

在 2006 年以前，如果将网络叠很多层，往往会得到上图的结果。上图，是手写数字识别的训练准确度的实验，使用的是 sigmoid function。可以发现当层数越多，训练结果越差，特别是当网络层数到达 9、10层时，训练集上准确度就下降很多。但是这个不是当层数多了以后就 overfitting，因为这个是在 training set上的结果。

（在之前可能常用的 activation function是 sigmoid function，今天我们如果用 sigmoid function，那么 deeper usually does not imply better，这个不是 overfitting）
###  梯度消失
![mark](http://images.iterate.site/blog/image/20190818/0DTtRLUpjaAE.png?imageslim)
当网络比较深的时候会出现 vanishing Gradient problem

比较靠近 input 的几层 Gradient 值十分小，靠近 output 的几层 Gradient 会很大，当你设定相同的 learning rate时，靠近 input layer 的参数 updata 会很慢，靠近 output layer的参数 updata 会很快。当前几层都还没有更动参数的时候（还是随机的时候），随后几层的参数就已经收敛了。

![mark](http://images.iterate.site/blog/image/20190818/lvwk345nX6ky.png?imageslim)

为什么靠近前几层的参数会特别小呢？

怎么样来算一个参数 w 对 total loss做偏微分，实际上就是对参数做一个小小的变化，对 loss 的影响，就可以说，这个参数 gradient 的值有多大。

给第一个 layer 的某个参数加上△w时，对 output 与 target 之间的 loss 有什么样的变化。现在我们的△w很大，通过 sigmoid function时这个 output 会很小(一个 large input，通过 sigmoid function，得到 small output)，每通过一次 sogmoid function就会衰减一次（因为 sogmoid function会将值压缩到 0 到 1 之间，将参数变化衰减），hidden layer很多的情况下，最后对 loss 的影响非常小(对 input 修改一个参数其实对 output 是影响是非常小)。

理论上我们可以设计 dynamic 的 learning rate来解决这个问题，确实这样可以有机会解决这个问题，但是直接改 activation function会更好，直接从根本上解决这个问题。
###  怎么样去解决梯度消失？
![mark](http://images.iterate.site/blog/image/20190818/hcUDL1XUbE4j.png?imageslim)

 修改 activation function，ReLU input 大于 0 时，input 等于 output，input小于 0 时，output等于 0

选择这样的 activation function有以下的好处：

•	比 sigmoid function比较起来是比较快的

•	生物上的原因

•	无穷多的 sigmoid function叠加在一起的结果(不同的 bias)

•	可以处理 vanishing gradient problem
![mark](http://images.iterate.site/blog/image/20190818/7nq6hazzcixR.png?imageslim)


ReLU activation function 作用于两个不同的 range，一个 range 是当 activation input大于 0 时，input等于 output，另外一个是当 activation function小于 0 是,output等于 0。

那么对那些 output 等于 0 的 neural 来说，对我们的 network 一点的影响都没。加入有个 output 等于 0 的话，你就可以把它从整个 network 拿掉。(下图所示)  剩下的 input 等于 output 是 linear 时，你整个 network 就是 a thinner linear network。

![mark](http://images.iterate.site/blog/image/20190818/6dbiN10Esyrq.png?imageslim)

我们之前说，GD递减，是通过 sigmoid function，sigmoid function会把较大的 input 变为小的 output，如果是 linear 的话，input等于 output，你就不会出现递减的问题。

我们需要的不是 linear network（就像我们之所以不使用逻辑回归，就是因为逻辑回归是线性的），所以我们才用 deep learning ，就是不希望我们的 function 不是 linear，我们需要它不是 linear function，而是一个很复杂的 function。对于 ReLU activation function的神经网络，只是在小范围内是线性的，在总体上还是非线性的。

如果你只对 input 做小小的改变，不改变 neural 的 activation range，它是一个 linear function，但是你要对 input 做比较大的改变，改变 neural 的 activation range，它就不是 linear function。

![mark](http://images.iterate.site/blog/image/20190818/4g3SKy7GBPle.png?imageslim)

1、改进 1 leaky ReLU
ReLU在 input 小于 0 时，output为 0，这时微分为 0，你就没有办法 updata 你的参数，所有我们就希望在 input 小于 0 时，output有一点的值(input小于 0 时，output等于 0.01乘以 input)，这被叫做 leaky ReLU。

2、改进 2 Parametric ReLU

Parametric ReLU在 input 小于 0 时，output等于\alpha zαz\alphaα为 neural 的一个参数，可以通过 training data学习出来，甚至每个 neural 都可以有不同的\alphaα值。

那么除了 ReLU 就没有别的 activation function了吗，所以我们用 Maxout 来根据 training data自动生成 activation function。

3、改进 3Exponential linear Unit (ELU)
![mark](http://images.iterate.site/blog/image/20190818/xIWX0RCp7ALE.png?imageslim)

让 network 自动去学它的 activation function，因为 activation function是自动学出来的，所有 ReLU 就是一种特殊的 Maxout case。

input是 x1,x2，x1,x2乘以 weight 得到 5,7,-1,1。这些值本来是通过 ReLU 或者 sigmoid function等得到其他的一些 value。现在在 Maxout 里面，在这些 value group起来(哪些 value 被 group 起来是事先决定的，如上图所示)，在组里选出一个最大的值当做 output(选出 7 和 1，这是一个 vector 而不是一个 value)，7和 1 再乘以不同的 weight 得到不同的 value，然后 group，再选出 max value。

![mark](http://images.iterate.site/blog/image/20190818/26QfksTrq3QA.png?imageslim)
Maxout network 是怎么样产生不同的 activation function，Maxout有办法做到跟 ReLU 一样的事情。

对比 ReLu 和 Maxout

ReLu：input乘以 w,b，再经过 ReLU 得 a。

Maxout：input中 x 和 1 乘以 w 和 b 得到 z1，z2，x和 1 乘以 w 和 b 得到 z2，z2(现在假设第二组的 w 和 b 等于 0，那么 z2,z2等于 0)，在两个中选出 max 得到 a(如上图所示)
现在只要第一组的 w 和 b 等于第二组的 w 和 b，那么 Maxout 做的事就是和 ReLU 是一样的。

当然在 Maxout 选择不同的 w 和 b 做的事也是不一样的(如上图所示)，每一个 Neural 根据它不同的 wight 和 bias，就可以有不同的 activation function。这些参数都是 Maxout network自己学习出来的，根据数据的不同 Maxout network可以自己学习出不同的 activation function。

上图是由于 Maxout network中有两个 pieces，如果 Maxout network中有三个 pieces，Maxout network会学习到不同的 activation function如下图。
![mark](http://images.iterate.site/blog/image/20190818/WNdvV0Elfr8F.png?imageslim)

面对另外一个问题，怎么样去 training，因为 max 函数无法微分。但是其实只要可以算出参数的变化，对 loss 的影响就可以用梯度下降来 train 网络。
![mark](http://images.iterate.site/blog/image/20190818/xw4wQ8JIOtjm.png?imageslim)

 max operation用方框圈起来，当你知道在一组值里面哪一个比较大的时候，max operation其实在这边就是一个 linear operation，只不过是在选取前一个 group 的 element。把 group 中不是 max value拿掉。
![mark](http://images.iterate.site/blog/image/20190818/7N5n3YUdmixV.png?imageslim)
 没有被 training 到的 element，那么它连接的 w 就不会被 training 到了，在做 BP 时，只会 training 在图上颜色深的实线，不会 training 不是 max value的 weight。这表面上看是一个问题，但实际上不是一个问题。

当你给到不同的 input 时，得到的 z 的值是不同的，max value是不一样的，因为我们有很多 training data，而 neural structure不断的变化，实际上每一个 weight 都会被 training。
###  Adaptive Learning Rate

![mark](http://images.iterate.site/blog/image/20190818/PWBSERjh5VLv.png?imageslim)
 每一个 parameter 都要有不同的 learning rate，这个 Adagrd learning rate 就是用固定的 learnin rate除以这个参数过去所有 GD 值的平方和开根号，得到新的 parameter。

我们在做 deep learnning时，这个 loss function可以是任何形状。
![mark](http://images.iterate.site/blog/image/20190818/GCrV9oxrxJ1z.png?imageslim)
考虑同一个参数假设为 w1，参数在绿色箭头处，可能会需要 learning rate小一些，参数在红色箭头处，可能会需要 learning rate大一些。

你的 error surface是这个形状的时候，learning rate是要能够快速的变动.

在 deep learning 的问题上，Adagrad可能是不够的，这时就需要 RMSProp（该方法是 Hinton 在上课的时候提出来的，找不到对应文献出处）。
![mark](http://images.iterate.site/blog/image/20190818/wQMgD7kauKdb.png?imageslim)
 一个固定的 learning rate除以一个\sigmaσ(在第一个时间点，\sigmaσ就是第一个算出来 GD 的值)，在第二个时间点，你算出来一个 g^1g1，\sigma^1σ1(你可以去手动调一个\alphaα值，把\alphaα值调整的小一点，说明你倾向于相信新的 gradient 告诉你的这个 error surface的平滑或者陡峭的程度。
![mark](http://images.iterate.site/blog/image/20190818/AxqDte5um1jX.png?imageslim)
除了 learning rate的问题以外，我们在做 deep learning的时候，有可能会卡在 local minimize，也有可能会卡在 saddle point，甚至会卡在 plateau 的地方。

其实在 error surface上没有太多的 local minimize，所以不用太担心。因为，你要是一个 local minimize，你在一个 dimension 必须要是一个山谷的谷底，假设山谷的谷底出现的几率是 P，因为我们的 neural 有非常多的参数(假设有 1000 个参数，每一个参数的 dimension 出现山谷的谷底就是各个 P 相乘)，你的 Neural 越大，参数越大，出现的几率越低。所以 local minimize在一个很大的 neural 其实没有你想象的那么多。
![mark](http://images.iterate.site/blog/image/20190818/jdLI3s4ADs0M.png?imageslim)
有一个方法可以处理下上述所说的问题

在真实的世界中，在如图所示的山坡中，把一个小球从左上角丢下，滚到 plateau 的地方，不会去停下来(因为有惯性)，就到了山坡处，只要不是很陡，会因为惯性的作用去翻过这个山坡，就会走到比 local minimize还要好的地方，所以我们要做的事情就是要把这个惯性加到 GD 里面(Mometum)。
现在复习下一般的 GD
![mark](http://images.iterate.site/blog/image/20190818/pXaYjEDqFSMJ.png?imageslim)
 选择一个初始的值，计算它的 gradient，G负梯度方向乘以 learning rate，得到θ1，然后继续前面的操作，一直到 gradinet 等于 0 时或者趋近于 0 时。

当我们加上 Momentu 时
![mark](http://images.iterate.site/blog/image/20190818/jba8AOu0Afu5.png?imageslim)

 我们每次移动的方向，不再只有考虑 gradient，而是现在的 gradient 加上前一个时间点移动的方向

（1）步骤

选择一个初始值\theta……0θ……0然后用 v^0v0去记录在前一个时间点移动的方向(因为是初始值，所以第一次的前一个时间点是 0)接下来去计算在\theta^0θ0上的 gradient，移动的方向为 v^1v1。在第二个时间点，计算 gradient\theta^1θ1，gradient告诉我们要走红色虚线的方向(梯度的反方向)，由于惯性是绿色的方向(这个\lambdaλ和 learning rare一样是要调节的参数，``$\lambda$`会告诉你惯性的影响是多大)，现在走了一个合成的方向。以此类推...

（2）运作
![mark](http://images.iterate.site/blog/image/20190818/Ky7XCcxjjCGe.png?imageslim)

 加上 Momentum 之后，每一次移动的方向是 negative gardient加上 Momentum 的方向(现在这个 Momentum 就是上一个时间点的 Moveing)。

现在假设我们的参数是在这个位置(左上角)，gradient建议我们往右走，现在移动到第二个黑色小球的位置，gradient建议往红色箭头的方向走，而 Monentum 也是会建议我们往右走(绿的箭头)，所以真正的 Movement 是蓝色的箭头(两个方向合起来)。现在走到 local minimize的地方，gradient等于 0(gradient告诉你就停在这个地方)，而 Momentum 告诉你是往右边的方向走，所以你的 updata 的参数会继续向右。如果 local minimize不深的话，可以借 Momentum 跳出这个 local minimize

Adam：RMSProp+Momentum
![mark](http://images.iterate.site/blog/image/20190818/5cp53602mqJq.png?imageslim)


**如果你在 training data已经得到了很好的结果了，但是你在 testing data上得不到很好的结果，那么接下来会有三个方法帮助解决。**

###  Early Stopping
![mark](http://images.iterate.site/blog/image/20190818/JDfBCA9AbCNC.png?imageslim)
![mark](http://images.iterate.site/blog/image/20190818/gDd9eEigfNPy.png?imageslim)

随着你的 training，你的 total loss会越来越小(learning rate没有设置好，total loss 变大也是有可能的)，training data和 testing data的 distribute 是不一样的，在 training data上 loss 逐渐减小，而在 testing data上 loss 逐渐增大。理想上，假如你知道 testing set 上的 loss 变化，你应该停在不是 training set最小的地方，而是 testing set最小的地方(如图所示)，可能 training 到这个地方就停下来。但是你不知道你的 testing set(有 label 的 testing set)上的 error 是什么。所以我们会用 validation 会 解决

会 validation set模拟 testing set，什么时候 validation set最小，你的 training 会停下来。

###  Regularization
类似与大脑的神经，刚刚从婴儿到 6 岁时，神经连接变多，但是到 14 岁一些没有用的连接消失，神经连接变少。
![mark](http://images.iterate.site/blog/image/20190818/1FaKq0Wg1QVw.png?imageslim)
重新去定义要去 minimize 的那个 loss function。

在原来的 loss function(minimize square error, cross entropy)的基础上加一个 regularization term(L2-Norm)，在做 regularization 时是不会加 bias 这一项的，加 regularization 的目的是为了让线更加的平滑(bias跟平滑这件事情是没有任何关系的)。

![mark](http://images.iterate.site/blog/image/20190818/vV6SW2YWswT1.png?imageslim)

在 update 参数的时候，其实是在 update 之前就已近把参数乘以一个小于 1 的值(\eta \lambdaηλ都是很小的值)，这样每次都会让 weight 小一点。最后会慢慢变小趋近于 0，但是会与后一项梯度的值达到平衡，使得最后的值不等于 0。L2的 Regularization 又叫做 Weight Decay，就像人脑将没有用的神经元去除。

regularization term当然不只是平方，也可以用 L1-Norm

![mark](http://images.iterate.site/blog/image/20190818/Rk1JXuhT593D.png?imageslim)

w是正的微分出来就是+1，w是负的微分出来就是-1，可以写为 sgn(w)。

每一次更新时参数时，我们一定要去减一个\eta \lambda sgn(w^t)ηλsgn(wt)值(w是正的，就是减去一个值；若 w 是负的，就是加上一个值，让参数变大)。

L2、L1都可以让参数变小，但是有所不同的，若 w 是一个很大的值，L2下降的很快，很快就会变得很小，在接近 0 时，下降的很慢，会保留一些接近 01 的值；L1的话，减去一个固定的值(比较小的值)，所以下降的很慢。所以，通过 L1-Norm training 出来的 model，参数会有很大的值。
###  Dropout

在 traning 的时候，每一次 update 参数之前，对 network 里面的每个 neural(包括 input)，做 sampling。 每个 neural 会有 p%会被丢掉，跟着的 weight 也会被丢掉。

![mark](http://images.iterate.site/blog/image/20190818/pGRPbP8dblD2.png?imageslim)

做出这个 sample，network structure就等于变瘦了(thinner)，然后，你在去 training 这个细长的 network(每一次 updata 之前都要去做一次) 。所以每次 update 参数时，你拿来 training network structure是不一样的。

![mark](http://images.iterate.site/blog/image/20190818/NemscgsB3CQH.png?imageslim)

  你在 training 时，performance会变的有一点差(某些 neural 不见了)，加上 dropout，你会看到在 testing set会变得有点差，但是 dropout 真正做的事就是让你 testing 越做越好
![mark](http://images.iterate.site/blog/image/20190818/MsYi3MGSKROy.png?imageslim)
 在 testing 上注意两件事情，第一件事情就是在 testing 上不做 dropout。另外一个是在 dropout 时，假设 dropout rate在 training 是 p%， all weight都要乘以(1-p%)(假设 dropout rate是 p%，若在 training 上算出 w=1，那么在 testing 时，把 w 设为 0.5)

**为什么 Dropout 会有用。**
![mark](http://images.iterate.site/blog/image/20190818/VWlvWUnOgOtG.png?imageslim)

为什么在训练的时候要 dropout，但是测试的时候不 dropout。

training的时候，会丢掉一些 neural，假如你在练习轻功的时候，你在脚上绑了一些重物(training)，实际上在战斗把重物拿下来(testing)，那么你就会变得很强。

![mark](http://images.iterate.site/blog/image/20190818/RM8b2NBHU1Kx.png?imageslim)

  每个 neural 就是一个学生，在一个团队中，总是会有被 dropout。你的 partner 会做的差的，你就想着我要好好做。

为什么在 testing 时，dropout要乘以 0.5(1p%) 。

![mark](http://images.iterate.site/blog/image/20190818/6UPjoHH8nDAU.png?imageslim)

假设 dropout rate是 50%，training的时候，你总是会期望丢掉一半的 neural。假设选定一组 weight(w1,w2,w3,w4)。testing时是没有 dropout 的，所以对同一组的 weihgt 来说 testing 的时候和 training 时候的 z 有一个很明显的差距，他的期望约等于 training 的两倍。现在怎么办，把所有的 weight 都乘以 0.5，现在变为将差距变回来。

![mark](http://images.iterate.site/blog/image/20190818/JN5Xgsa3n8YR.png?imageslim)

ensemble方法 我们有一个很大的 training set，每次从 training set里只 sample 一部分的 data，然后 training 很多的 model(每个 model 可能 structure 不一样)，每个 model 可能 variance 很大，但是他们都是很复杂的 model 的话，平均起来，bias就会很小。

![mark](http://images.iterate.site/blog/image/20190818/1TaXn4HWzMr8.png?imageslim)

Dropout的类似于 ensemble 的终极版本，ensemble时，放入 training data，通过不同 network，得到一些结果在把这些结果平均起来，当做你最后的结果。

![mark](http://images.iterate.site/blog/image/20190818/BcGIOmVPocOP.png?imageslim)

当你做 dropout 是其实就是 training 了很多的 network structure，就类似于刚刚的 ensemble 中将数据放进不同的模型中。但是会不会存在 minibatch 对结果的影响呢？其实时不会的，因为在网络中参数时共用的，所以在训练参数的时候是一大堆参数合起来去训练网络。

![mark](http://images.iterate.site/blog/image/20190818/WixsHaWnNT4D.png?imageslim)

 在 testing 的时候，按照 ensemble 方法，把之前的 network 拿出来，然后把你的 100 笔 data 丢到 network 里面去，每一个 network 都会给你一个结果，这些结果的平均值就是最终的结果。但是实际上这些 network 太多了，没办法去给索引 network 丢一个 input。

所以，dropout最神奇的地方是，它告诉你，当一个完整的 network 不做 dropout，而是把它的 weight 乘以(1-p%)，把你的 training data丢进去，得到的 output 就是 average 的值。

![mark](http://images.iterate.site/blog/image/20190818/BNuWnJgSrUEB.png?imageslim)

在这个最简单的 case 里面，ensemble这件事情跟我们把 weight 乘以 1/2得到一样的结果。但是这个结果只有是 linear network才会有这样的结果。





# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)
