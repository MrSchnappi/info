---
title: 27 无监督学习-深度自编码器
toc: true
date: 2019-08-18
---
![mark](http://images.iterate.site/blog/image/20190818/3jujH1LBLaw0.png?imageslim)

![mark](http://images.iterate.site/blog/image/20190818/mdcawJMkQ8eL.png?imageslim)

## 自动编码器
自动编码器的想法是这样子的：我们先去找一个 encoder，这个 encoder input一个东西(假如说，我们来做 NMIST 的话，就是 input 一张 digit，它是 784 维的 vector)，这个 encoder 可能就是一个 neural network，它的 output 就是 code(这个 code 远比 784 维要小的，类似压缩的效果)，这个 coder 代表了原来 input 一张 image compact representation。

但是现在问题是：我们现在做的是 Unsupervised learning，你可以找到一大堆的 image 当做这个 NN encoder的 input，但是我们不知道任何的 output。你要 learn 一个 network，只有一个 input，你没有办法 learn 它。那没有关系，我们要做另外一件事情：想要 learn 一个 decoder，decoder做的事情就是：input一个 vector，它就通过这个 NN decoder，它的 output 就是一张 image。但是你也没有办法 train 一个 NN decoder，因为你只要 output，没有 input。

这两个 network，encoder decoder单独你是没有办法去 train 它。但是我们可以把它接起来，然后一起 train。也就是说：建一个 neural network ，input一张 image，中间变成 code，再把 code 变成原来的 image。这样你就可以把 encoder 跟 decoder 一起学，那你就可以同时学出来了。



那我们刚才在 PCA 里面看过非常类似的概念，从 PCA 开始讲起。我们刚才讲过说：PCA其实在做的事情是：input一张 image x(在刚才的例子里面，我们会让 x-$\bar{x}$ 当做 input，这边我们把减掉 $\bar{x}$ 省略掉，省略掉并不会太奇怪，因为通常在做 NN 的时候，你拿到的 data 其实会 normlize，其实你的 data mean是为 0，所以就不用再去减掉 mean)，把 x 乘以一个 weight，通通 NN 一个 layer 得到 c，c乘以 matrix w的 tranpose 得到 $hat{x}$。$\hat{x}$ 是根据这些 component 的 reconstruction 的结果

### Recap:PCA
在 PCA 里面，我们就是 minimize input跟 reconstruction 的结果。我们要让 x 跟 $\hat{x}$ 的 Eculidean distance越接近越好，这是 PCA 做的是事情。如果把它当成 neural network来看的话，input x就是 input layer，output $\hat{x}$ 就是 output layer，中间的 component weight就是 hidden layer(在 PCA 里面是，它是 linear)。那这个 hidden layer我们通常叫它 Bottleneck later。

![mark](http://images.iterate.site/blog/image/20190818/cf0pI42cQNwm.png?imageslim)

为什么叫 Bottleneck later呢？所以你的 component 的数目通常会比你的 input dimension还要小的多，如果你要当做 layer 看的话，是一个特别窄的 layer，所以我们叫它 Bottleneck later(因为你在 dimension reduction)

前面那个部分是在做 encode，把 input 变为一组 code。后面这个部分在做 decode，如果把 component weight相成 code 的话，后面这件事就是把 code 变成原来的 image。hidden layer的 output 就是我们要找的那些 code。你可以用 gradient descent来解 PCA，PCA只有一个 hidden layer，那我们想把变成更多的 hidden layer。



可以变成更多的 hidden layer，你就搞一个很深的 neural network，它有很多很多层。然后在这个很多很多层的 neural network里面，你 input 一个 x，最后得到的 output 是 $\hat{x}$，你会希望 x 跟 $\hat{x}$ 越接近越好。


中间你会有一个特别窄的 layer，这个特别窄的 layer，有着特别少的 neural ，这些 layer 的 output 就代表了一组 code。从 input layer到 bottle layer，就是 encode。从 bottle layer到最后的 $\hat{x}$ 就是 decode，这个 deep 的自动编码器最早出现 2006 年。

### Deep 自动编码器

那如果按照刚才在 PCA 里面看到的，从 input 到 hidden layer的 $W_1$ 好像要跟最后一个 layer 的 output 的 weight 互为 transpose($W_1^T$)。你在 training 的时候，你可以做到这件事情，可以把左边的 weight 跟右边的 weight 乘起来，在他们在做 training 的时候，永远保持值是一样。做这件事情的好处就是，你现在的 Auto-encode的参数就少一半，比较不会有 overfitting 的情形。但是这件事情不是必要的，没有什么理由说，input到 hidden layer的 $W_1$ 好像要跟最后一个 layer 的 output 的 weight 互为 transpose($W_1^T$)。所以现在常见的做法是：兜一个 neural network，一直 train 下去，不管它 weight 是什么，就是你的结果。

![mark](http://images.iterate.site/blog/image/20190818/R9EY0dnlvB62.png?imageslim)
original image你做 PCA，从 784 维降到 30 维，然后从 30 维 reconstruction 回 784 维，得到的 image 差不多，可以看出说，它是比较模糊的。如果今天是用 deep encoder的话，784维先扩为 1000 维，再不断的下降，下降到 30 维(你很难说：为什么它会设计成这样子)，然后再把它解回来。你会发现说，如果你今天用的是 deep Auto-encoder的话，它的结果看起来非常的好。


![mark](http://images.iterate.site/blog/image/20190818/p73PpOVR4WuP.png?imageslim)
那如果你今天不是把它将到 30 维，而是把它降到二维的话，所以的 digit 被混在一起(不同的颜色代表了不同的数字)。如果你是用 depp Auto-encoder的话，你会发现说：这些数字是分开的。




![mark](http://images.iterate.site/blog/image/20190818/Qri27yy93wru.png?imageslim)

### 自动编码器 - 文本检索
这个 Auto-encoder也可以用在文字处理上，比如说：我们把一篇文章压成一个 code。举例来说：现在要做文字的搜寻，在文字搜寻里面有一招叫做：vector space model。vector space model是说：我们现在把一篇文章都表示成一个空间中的 vector(图中蓝色的圈圈就是一篇文章，经过降维后)。假设使用者查询一个词汇，我们把查询的词汇也变成一个空间中的点。接下里你就是计算这个查询词汇跟每一篇 document 之间的 inner product，选择较为接近，相似程度最高的。这个模型要 work，现在把一个 document 变成一个 vector 表现的是好还是不好。咋样把一个 document 表示成一个 vector，一个方法叫做“Bag-of-word”。这个“Bag-of-wor”的想法是说：我们现在开一个 vector，这个 vector 的 sentence 就是 let think sentence(假设世界上有 10w 词汇，这个 vector 的 sentence 就是 10w 维)

假设现在有一篇 document，有一个句子是："this is an apple"，那这个 document 如果把它表示成一个 vector 的话就是在 this 那一维是 1，is那一维是 1，an那一维是 1，apple那一维是 1，其它为 0。有时候，你想要做的更好，你想要乘上 weight，代表那一个词汇的重要性。这个重要性。但是用这个模型，它没有考虑 Semantics 相关的东西，它不知道台湾大学就是指的台大，它不知道 apple，orange是水果。对它来说：is,an,apple之间是没有任何关系的。


![mark](http://images.iterate.site/blog/image/20190818/cwFPgBM5PXha.png?imageslim)



那我们可以用 Auto-encoder被考虑进来 ，举例来说，你 learn 一个 Auto-encoder，它的 input 就是一个 document 或者是一个 query，通过 encoder 把它压成二维。每一个点代表一个 document，不同颜色的类代表 document 属于哪一类。今天要做搜寻的时候，今天输入一个词汇，那你就把那个 query 也通过这个 encoder 把它变为一个二维的 vector。假设 query 落在这一类，你就可以知道说：这个 query 是跟 Energy markets有关的 document retrieve出来，这个看起来结果是相当惊人的。如果你用 LSA 的话，你得不到类似的结果



![mark](http://images.iterate.site/blog/image/20190818/f2F6YKQgaE0L.png?imageslim)

### 自动编码器 - 类似图像搜索
Auto-encoder也用在 image search上面，假设这张图是你要找的对象(image query)，计算这个 image query跟其他 database 之间 pixel 的相似程度，然后你再看出最像的几张是要的结果。如果你只是这么做的话，其实得不到太好的结果。


你拿这张 image(Wack Jackson)去跟 database 里面的 image 算相似度的话，你找出最像的应该是这几张(如图)，所以你算 pixel 的话是得不到好的结果


![mark](http://images.iterate.site/blog/image/20190818/NmBPceIw0iOh.png?imageslim)

你可以用 Auto-encoder把每一张 image 变成一个 code，然后再 code 上面，再去做搜寻。因为做这件事情是 unsupervised ，所以 learn 一个 Auto-encoder是 unsupervised，所以你要多少 data 都行(supervised是很缺 data 的，unsupervised是不缺 data 的)


input一张 32*32的 image，每一个 pixel 是 RGB 来表示(32 * 32 *3)，变成 8192 维，然后 dimension reduction变成 4096 维，最后一直变为 256 维，你用 256 维的 vector 来描述这个 image。然后你把这个 code 再通过另外一个 decoder(形状反过来，变成原来的 image)，它的 reconstruction 是右上角如图。




![mark](http://images.iterate.site/blog/image/20190818/xtwfWIgBEfuR.png?imageslim)

如果你不是在 pixel 上算相似度，是在 code 上算相似度的话，你就会得到比较好的结果。举例来说：你是用 Wack Jackson当做 image 的话，你找到的都是人脸，可能这个 image 在 pixel label上面看起来是不像的，但是你通过很多的 hidden layer把它转成 code 的时候，在那个 256 维的空间上看起来是像的


Auto-encoder可以用在 Pre-training上面，我们都知道你在 train 一个 neural network的时候，你有时候很烦恼说：你要怎么做参数的 initialization。有没有一些方法让你找到一组好的 initialization，这种找比较好的 initialization 的方法就叫做“Pre-training”。你可以用 Auto-encoder来做 Ptr-training



肿么做呢，假如说：我要 INIST initialization，你可以做一个 neural network input784维，第一个 hidden layer是 1000，第二个 hidden layer是 1000，第三个 hidden layer是 500，然后到 10 维。那我做 Pre-taining的时候，我先 train 一个 Auto-encoder，这个 Auto-encoder input784维，中间有 1000 维的 vector，然后把它变回 784 维，我期望 input 跟 output 越接近越好。你在做这件事的时候，你要稍微小心一点，我们一般做 Auto-encoder的时候，你会希望你的 coder 要比 dimension 还要小。比 dimension 还要大的话，你会遇到的问题是：它突然就不 learn 了。所以你今天发现你的 hidden layer比你的 input 还要大的时候，你要加一个很强的 regularization 在 1000 维上，你可以对这 1000 维的 output 做 L1 的 regularization，你会希望说：这 1000 维的 output 里面，某几维是可以有值的，其他维要必须为 0。这样你就可以避免 Auto-encoder直接把 input 并起来再输出的问题。总之你今天的 code 比你 input 还要大，你要注意这种问题。

现在我们先 learn 了一个 Auto-encoder，从 784 维到 1000 维的 $w^1$ 把它保留下来(把它 fix 住)。接下来把所有的 database 里面的 digit 通通变成 1000 维的 vector，接下来你在 learn 另外一个 Auto-encoder，它把 1000 维的 vector 变成 1000 维的 code，再把 1000 维的 code 变成 1000 维的 vector。你再 learn 这样的 Auto-encoder，它是会让 input 跟 output 越接近越好。然后你再把 $w^2$ 把它保存下来

![mark](http://images.iterate.site/blog/image/20190818/D28yD9aWLJEo.png?imageslim)


然后你再把 $w^2$ 把它保存下来，fix住 $w^2$ 的值。learn第三个 Auto-encoder，input100维，code500维，output1000维，然后得到 weight$w^3$





然后你再把 $w^3$ 把它保存下来，这个 $w^1,w^2,w^3$ 就等于是你再 learn 你的 neural network时的 initialization，然后你再 random initialization500维到 10 维，然后再用 backpropagation 再去调一遍，称这个步骤为 find-tune($w^1,w^2,w^3$ 都是很好的 weight，所以你只是微调它)


这一招(pre-training)在过去 learn 一个 deep 的 neural network还是很需要的，不过现在 neural network不需要 pre-training往往都能 train 的起来。如果你今天有很多的 unlabel data，少量的 label data，你可以用大量的 unlabel data先去把 $w^1,w^2,w^3$learn 好，你最后的 label data只需要稍微调整你的 weight 就好了。所以 pre-training在大量的 unlabel data时还是有用的。



有一个方法可以让 Auto-encoder做的更好，这个叫做 De-noising Auto-encoder(可以参考 reference)。你把原来的 input x加上一些 noise 变成 $x^{'}$，然后你把 $x^{'}$encode以后变成 c，再把 c decode回来变成 y。但是要注意一下，现在在 De-noising Auto-encoder， 你是要 output 跟原来的 input(加 noise 之前的 x)越接近越好。你 learn 出来的结果有 biased，因为 encode 不止 learn 到了这件事，它还 learn 到了把杂序滤掉这件事。

![mark](http://images.iterate.site/blog/image/20190818/qu6f6lqBISYr.png?imageslim)

还有另外一招叫做 contractive auto-encoder，做的事情是：它会希望说，我们在 learn 这个 code 的时候加上一个 contract，这个 contract 是：当 input 有变化的时候，对 code 的影响是被 minimize 的。其实这件事也很像 De-noise auto-encoder，只是从不同的角度来看,De-noise auto-encoder是说：加上 noise 以后，你还要 reconstruct 没有 noise 的结果。Contractive auto-encoder是说：当 input 变化时，也就是加了 noise 以后，对 code 的影响是要小的，所以它们做的事还是蛮类似的。


![mark](http://images.iterate.site/blog/image/20190818/hTCedqa1xOIm.png?imageslim)
## CNN自动编码器

接下来将 CNN auto-encoder，那如果我们今天要处理的对象是 image 的话，我们都知道要用 CNN。那在 CNN 里面处理 image 的时候，会有一些 convolution layer，有 pooling layer，用 convolution 和 pooling 交替，让 image 越来越小。那今天是做 auto-encoder的话，你不止要一个 encoder，还要一 decoder。如果 encoder 是做 convolution pooling convolution pooling，那 decoder 就是在做 deconvolution unpooling deconvolution unpooling(相反的事情)，让 input 跟 output 越接近越好。

![mark](http://images.iterate.site/blog/image/20190818/P0s1ub07E5Jz.png?imageslim)

### 无池化 CNN

在做 pooling 的时候，现在有 4*4的 matrix，接下里你把 matrix 里面这个 pixel 分组(4个一组)，接下来从每一组挑一个最大的，那 image 就变成原来的 1/4。如果你是在做 unpooling 的话，你会做另外一件事，你会先记得说：我刚才在做 pooling 的时候是从哪里取值的(从哪取值，哪里就是白的)。你要做 unpooling 的话，你要把原来小的 matrix 扩大(pooling的时候，是把大的 matrix 变为原来的 1/4，现在是把比较小的 matrix 变成原来的 4 倍)。那肿么做呢，这时候你之前记录的位置就可以派上用场，你之前记的说：我在 pooling 的时候是左上角，所以现在做 unpooling 时，就把这个值放到左上角，其他补 0，此次类推。这个就是 unpooling 的一种方式，做完 unpooling 以后，比较小的 image 会变得比较大，比如说：原来是 14 * 14的 image 会变成 28 *28的 image。你会发现说：它就是把原来的 14 *14的 image 做一下扩散，有些补 0

![mark](http://images.iterate.site/blog/image/20190818/yB54ku2RSgYV.png?imageslim)

这不是 unpooling 唯一的做法，在 keras 里面它的做法是一样的，它是 reapeat 哪些 value。




接下来比较难理解的是 Deconvolution，事实上 Deconvolution 就是 convolution。举例例子(一维的 convolution)，input有五个 dimension，然后我们的 filter size是 3，我们把 input 这个三个 value 分别乘上红色，蓝色，绿色的 weight 得到一个 output。再把这个 filter 移动，再把这个 value 乘上红色，蓝色，绿色的 weight 得到一个 output，这个是 convolution。


你的想象可能是：deconvolution是 convolution 的相反，本来是三个值变成一个值，现在做 deconvolution 是 1 个值变成三个值(如图)，它已经贡献了一些值，第二个也贡献了一些值，那么就把这加起来(重叠的地方就加起来)。这件事就等同于做 convolution，等同于 input 是三个 value，我们会在旁边补 0，接下里我们一样做 convolution。做 convolution 的时候，三个 input 乘上红色、蓝色、绿色的 weight 等于一个值，以此类推。这两个框框做的事情是一样的(我们检查中间这个值，它是三个 value 加起来，这三个 value 是：第一个 output 乘以绿色，第二个 output 乘以蓝色，第三个 output 乘以红色，然后再加起来)


### 反卷积 CNN
你将最后框(deconvolution)跟 convolution 相比较的话，不同点是：它们的 weight 是相反的，但是它们都做的是 convolution 这件事



![mark](http://images.iterate.site/blog/image/20190818/W8d8n36ijKBA.png?imageslim)

等到 RNN 的时候再讲 sequence-to-sequence auto-encoder，我们刚才看到 auto-encoder的 input 是 vector，但很多东西你不该把它表示成 vector。比如说语音(一段声音讯号有长有短，它不是 vector)，文章。你可能用 bag-of-word变成一个 vector，但是你会失去词汇和词汇之间的前后关系，所以是不好的。




我们刚才都是用 encoder 来把原来的 image 变成少的 dimension，我们也用 decoder，这个 decoder 来产生新的 image。把这个 learn 好的 decoder 拿出来，然后你给它一个 random input number，它的 output 希望就是一张图。
## 自动编码器 - 预训练 DNN
我把每一张 784 维的 image，通过 hidden layer，把它 project 到二维，二维再通过 hidden layer解回原来的 image。在 decoder 的部分，那个二维的 vector 画出来如图。

![mark](http://images.iterate.site/blog/image/20190818/Szl3ycDk2EQq.png?imageslim)

我在红色的框框里面，等间隔的去 sample vector出来，把 vector 丢进 NN Decoder里面，然
后让它 output 一张 image 出来。这些二维的 vector，它不见得是某个 image comprise以后的结果，
它不见得原来的 image 就是对应的 vector，丢到 decoder 里面，看看它产生什么。我们发现：在这个红色的框框内，
等距离的做 sample


可能有人说：你肿么知道要 sample 都这个位置，因为我必须要先观察一下二维 vector 的分布，才能知道说：哪边是有值的，才有可能说，从哪个地方 sample 出来，可能是个 image。可是这样子你要先分析一下二维的 code 有点麻烦，那咋样确保在我们希望的 range 里面都是 image 呢？有一个很简单的做法，就是在你的 code 上面加 regularization(L2 regularization)，让 code 比较接近 0。你在 sample 的时候就是在 0 附近，这样你就可以有可能 sample 出的 vector 都能对应到数字。



train出来，你的 code 都会集中在接近 0 的地方。它就以 0 为核心，然后分在在接近 0 的地方。然后我就以 0 为中心，然后在这个红框等距的 sample image ，sample出来就是如图。

![mark](http://images.iterate.site/blog/image/20190818/Rh6L5qhwLYKb.png?imageslim)


这个是很有趣的，这两个 dimension 是有意义的。横轴(从左到右)代表的是有没有一个圈圈(本来是一个很圆的圈圈，慢慢的变为 1)，纵轴代表是：本来是直的，然后慢慢就倒斜。

![mark](http://images.iterate.site/blog/image/20190818/ADgVQA0Sq7JL.png?imageslim)





# 相关

- [leeml-notes](https://github.com/datawhalechina/leeml-notes)
