---
title: 06 反向传播
toc: true
date: 2019-12-06
---
# 可以补充进来的

- 如果可以，把反向传播求出来的梯度值的计算过程补充一下。或者用程序来实现，而不是使用 pytorch 来实现。


# 反向传播

举例：

```py
import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 5x5 square convolution kernel
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
# print(net)


input = torch.randn(1, 1, 32, 32)
output = net(input)
# print(output)

target = torch.randn(10)  # 随机值作为样例
target = target.view(1, -1)  # 使 target 和 output 的 shape 相同 嗯。
# print(target)

criterion = nn.MSELoss()
loss = criterion(output, target)
# print(loss)

# print('\n')

net.zero_grad()     # 清除梯度
print('grad before backward')
print(net.conv1.weight)
print(net.conv1.weight.grad)
print(net.conv1.bias)
print(net.conv1.bias.grad)

print('\n')

loss.backward()
print('grad after backward')
print(net.conv1.weight)
print(net.conv1.weight.grad)
print(net.conv1.bias)
print(net.conv1.bias.grad)
```

输出：

```
grad before backward
Parameter containing:
tensor([[[[ 0.1298, -0.0269, -0.0571, -0.1991,  0.0174],
          [-0.0311,  0.1179, -0.0853, -0.0649,  0.1800],
          [ 0.1139,  0.1483, -0.0822,  0.0760, -0.1595],
          [ 0.1486, -0.1417, -0.0697,  0.1556, -0.0127],
          [ 0.0931, -0.0203, -0.0900,  0.1440, -0.1040]]],


        [[[-0.1542,  0.1524,  0.0943,  0.1885, -0.1426],
          [-0.0284, -0.0381, -0.1052,  0.0314, -0.0420],
          [-0.1234,  0.1083,  0.1240, -0.0822,  0.1884],
          [ 0.0269,  0.1867,  0.1184,  0.1015,  0.1419],
          [-0.0967, -0.1744,  0.0648, -0.0581,  0.1356]]],


        [[[ 0.0333,  0.0294, -0.1055,  0.0672, -0.1357],
          [ 0.0885,  0.0770,  0.1702, -0.1982, -0.0798],
          [-0.0763,  0.0202,  0.1463,  0.0155, -0.1158],
          [ 0.1799,  0.1948, -0.0839,  0.1435,  0.0336],
          [ 0.1210, -0.1819,  0.1450,  0.1640,  0.0552]]],


        [[[-0.0744,  0.1858,  0.0084,  0.1156, -0.0487],
          [ 0.1836, -0.0342, -0.0174,  0.1447,  0.1716],
          [ 0.0093,  0.0927, -0.0909,  0.0426,  0.0151],
          [-0.0589,  0.0171,  0.1599,  0.1592,  0.1399],
          [-0.1545,  0.1365, -0.0749,  0.1585, -0.1617]]],


        [[[ 0.1816, -0.1643,  0.1972,  0.1031, -0.0744],
          [-0.1447,  0.1614, -0.1934, -0.1333, -0.1463],
          [ 0.1919,  0.0881,  0.0386,  0.1280,  0.1159],
          [-0.1927, -0.0373,  0.1588,  0.1065, -0.0174],
          [-0.1774,  0.0754,  0.1274,  0.0696,  0.0592]]],


        [[[-0.1207,  0.1549,  0.0083,  0.1670,  0.1603],
          [-0.1440, -0.0854, -0.0566,  0.1231, -0.0752],
          [ 0.0564,  0.1204,  0.1313,  0.1521, -0.1969],
          [ 0.1467,  0.0056, -0.0652, -0.0890, -0.0107],
          [ 0.0335, -0.1809, -0.1535,  0.0749, -0.1562]]]], requires_grad=True)
None
Parameter containing:
tensor([-0.0079, -0.0966,  0.1879,  0.1863,  0.0354, -0.0181],
       requires_grad=True)
None


grad after backward
Parameter containing:
tensor([[[[ 0.1298, -0.0269, -0.0571, -0.1991,  0.0174],
          [-0.0311,  0.1179, -0.0853, -0.0649,  0.1800],
          [ 0.1139,  0.1483, -0.0822,  0.0760, -0.1595],
          [ 0.1486, -0.1417, -0.0697,  0.1556, -0.0127],
          [ 0.0931, -0.0203, -0.0900,  0.1440, -0.1040]]],


        [[[-0.1542,  0.1524,  0.0943,  0.1885, -0.1426],
          [-0.0284, -0.0381, -0.1052,  0.0314, -0.0420],
          [-0.1234,  0.1083,  0.1240, -0.0822,  0.1884],
          [ 0.0269,  0.1867,  0.1184,  0.1015,  0.1419],
          [-0.0967, -0.1744,  0.0648, -0.0581,  0.1356]]],


        [[[ 0.0333,  0.0294, -0.1055,  0.0672, -0.1357],
          [ 0.0885,  0.0770,  0.1702, -0.1982, -0.0798],
          [-0.0763,  0.0202,  0.1463,  0.0155, -0.1158],
          [ 0.1799,  0.1948, -0.0839,  0.1435,  0.0336],
          [ 0.1210, -0.1819,  0.1450,  0.1640,  0.0552]]],


        [[[-0.0744,  0.1858,  0.0084,  0.1156, -0.0487],
          [ 0.1836, -0.0342, -0.0174,  0.1447,  0.1716],
          [ 0.0093,  0.0927, -0.0909,  0.0426,  0.0151],
          [-0.0589,  0.0171,  0.1599,  0.1592,  0.1399],
          [-0.1545,  0.1365, -0.0749,  0.1585, -0.1617]]],


        [[[ 0.1816, -0.1643,  0.1972,  0.1031, -0.0744],
          [-0.1447,  0.1614, -0.1934, -0.1333, -0.1463],
          [ 0.1919,  0.0881,  0.0386,  0.1280,  0.1159],
          [-0.1927, -0.0373,  0.1588,  0.1065, -0.0174],
          [-0.1774,  0.0754,  0.1274,  0.0696,  0.0592]]],


        [[[-0.1207,  0.1549,  0.0083,  0.1670,  0.1603],
          [-0.1440, -0.0854, -0.0566,  0.1231, -0.0752],
          [ 0.0564,  0.1204,  0.1313,  0.1521, -0.1969],
          [ 0.1467,  0.0056, -0.0652, -0.0890, -0.0107],
          [ 0.0335, -0.1809, -0.1535,  0.0749, -0.1562]]]], requires_grad=True)
tensor([[[[-7.3353e-03,  3.6751e-03,  4.7128e-03, -1.5284e-04, -9.7388e-03],
          [-1.1087e-02,  2.7938e-02, -9.7990e-03, -1.1622e-02,  1.8315e-02],
          [-1.1424e-02,  3.0173e-02, -4.6260e-03, -1.1772e-02,  1.0124e-02],
          [ 1.7981e-02, -4.4035e-04,  1.5460e-02,  2.5172e-02,  1.3124e-02],
          [ 3.3847e-03,  5.4839e-03, -5.4916e-03, -1.0294e-02, -4.4970e-03]]],


        [[[-3.0840e-03, -3.6557e-03,  4.2825e-03, -4.2791e-03, -9.6432e-03],
          [-1.9974e-03, -7.0324e-04, -2.5037e-03, -1.4970e-02,  2.9525e-03],
          [ 3.7154e-03,  7.4381e-03,  1.7660e-02, -5.6211e-04,  1.1265e-02],
          [ 9.1613e-03,  9.0895e-03,  1.0138e-04,  2.3282e-03,  1.3651e-02],
          [-7.1127e-03, -6.4612e-03, -2.3573e-03, -1.1297e-02, -2.1099e-02]]],


        [[[-1.2322e-02, -9.0830e-03,  8.1530e-03, -1.7122e-02, -1.9905e-04],
          [-2.6160e-03, -1.3558e-02, -2.7499e-03,  6.0567e-03,  2.1281e-03],
          [ 1.2720e-02,  5.9244e-03, -2.8008e-03, -1.0448e-02, -6.8551e-03],
          [ 1.5402e-03, -1.0055e-02,  2.6569e-02, -3.8730e-03, -3.0700e-03],
          [-1.3432e-02,  2.0433e-03,  2.2267e-02, -3.3958e-03, -1.0025e-02]]],


        [[[-5.9978e-03,  2.5514e-02,  1.4442e-02, -1.4300e-02,  4.0292e-03],
          [ 1.7653e-03, -1.0038e-02,  6.2324e-03,  2.6461e-02, -1.1496e-02],
          [ 5.9450e-03, -1.2618e-02,  2.9144e-03,  1.0038e-02, -6.2911e-03],
          [ 9.4947e-03, -1.0829e-02,  4.9286e-03, -5.1179e-05, -2.4110e-03],
          [-1.9742e-02,  1.1651e-02, -3.2173e-02, -1.5575e-02, -8.8124e-03]]],


        [[[-7.3263e-03, -3.6331e-02,  2.0143e-02,  2.6179e-03, -2.4272e-02],
          [-1.2566e-03,  1.7463e-02,  5.5991e-03, -1.0305e-02, -8.4787e-03],
          [ 1.6725e-02,  2.4901e-03, -5.9375e-03, -3.8535e-03, -9.0544e-03],
          [ 2.7398e-03, -4.6110e-03, -8.2480e-03,  1.0556e-02,  3.9395e-03],
          [ 6.8139e-03,  9.5260e-03, -4.6335e-03,  9.9482e-03, -8.8695e-03]]],


        [[[ 1.8355e-02,  1.7187e-03, -6.6530e-03, -2.0524e-02, -1.8267e-02],
          [ 1.5976e-02,  2.2146e-03, -2.2569e-03,  1.2740e-03, -1.0327e-02],
          [-1.9494e-02,  1.2708e-02,  1.8021e-03, -1.9825e-02,  2.0559e-02],
          [-2.5696e-03,  6.4415e-03, -1.1098e-02,  8.6071e-03, -1.1884e-02],
          [-4.8307e-03,  1.9736e-02,  1.7294e-02, -8.7074e-05,  1.5000e-02]]]])
Parameter containing:
tensor([-0.0079, -0.0966,  0.1879,  0.1863,  0.0354, -0.0181],
       requires_grad=True)
tensor([ 0.0172,  0.0031, -0.0200,  0.0254,  0.0153, -0.0369])
```

可见：

- 在 backward 之前卷积层的权重和偏置的梯度都是 None。


注意：

- 在调用 `loss.backward()` 获得反向传播的误差之前，需要调用 `net.zero_grad()` 将网络内所有参数的梯度缓存清零。**否则梯度将被累加到已存在的梯度。**





# 相关

- [pytorch-handbook](https://github.com/zergtant/pytorch-handbook)
