---
title: 05 操作
toc: true
date: 2019-07-02
---
.5 操作
和转换不同，操作执行数据集上的计划任务。一旦完成数据转换，则可以执行相应转换。这可能不包含转换（例如，.take（n）会仅仅返回 n 条来自 RDD 的记录，即使你们没有对 RDD 做任何转换）或者直接执行一系列转换。
2.5.1 .take（……）方法
这可以说是最有用的方法（也是用得最多的方法，如.map（……）方法）。该方法优于.collect（……），因为它只返回单个数据分区的前 n 行，对比之下，.collect（……）返回的是整个 RDD。处理大数据集时，这个区别很重要：
如果你想要些随机的记录，可以使用.takeSample（……），这个方法有三个参数：第一个参数代表采样是否应该被替换；第二个参数指定要返回的记录数量；第三个参数是伪随机数发生器的种子：
2.5.2 .collect（……）方法
该方法将所有 RDD 的元素返回给驱动程序。正如之前提出的警告，在此就不重复了。
2.5.3 .reduce（……）方法
.reduce（……）方法使用指定的方法减少 RDD 中的元素。
你可以利用该方法计算 RDD 总的元素数量：
这行代码输出的总数是 15。
首先通过.map（……）转换，创建一个包含 rdd1 所有值的列表，然后使用.reduce（……）方法对结果进行处理。在每一个分区里，reduce（……）方法运行求和方法（lambda表达式）将该总和返回给最终聚合所在的驱动程序节点。这里要谨慎一点。reducer传递的函数需要是关联的，即元素顺序改变，结果不变；该函数还需要是交换的，即操作符顺序改变，结果不变。
关联规则的例子是（5＋2）＋3＝5＋（2＋3），交换规则的例子是 5＋2＋3＝3＋2＋5。因此，你需要注意你传递给 reducer 的功能是什么。
如果忽略前面的规则，你可能会遇到麻烦（假设你的代码都在运行）。例如假设我们有以下 RDD（只有一个分区！）：
如果以某种方式减少数据，我们希望通过后面的结果划分当前的结果，我们希望值是 10：
但是，如果将数据划分为三个分区，结果是错误的：
产生的结果是 0.004。
.reduceByKey（……）方法和.reduce（……）方法类似，但.reduceByKey（……）是在键–键基础上进行：
这段代码输出的结果是：


2.5.4 .count（……）方法
.count（……）统计出了 RDD 里的元素数量。使用以下的代码：
该代码输出 6，也就是 data_reduce RDD里的确切元素数量。
.count（……）方法产生了和如下方法同样的结果，但是该方法不需要把整个数据集移动到驱动程序：
如果数据集是 key-value形式，你可以使用.countByKey（）方法获取不同键的计数。运行以下代码：
这段代码产生的结果如下：
2.5.5 .saveAsTextFile（……）方法
正如方法的名字所说，对 RDD 执行.saveAsTextFile（……）可以让 RDD 保存为文本文件：每个文件一个分区：
要读取它，需要解析它，因为所有行都被视为字符串：
读取的键列表和我们最初有的匹配：
2.5.6 .foreach（……）方法
这个方法对 RDD 里的每个元素，用迭代的方式应用相同的函数；和.map（……）比较，.foreach（……）方法按照一个接一个的方式，对每一条记录应用一个定义好的函数。当您希望将数据保存到 PySpark 本身不支持的数据库时，该方法很有用。
这里，我们用它来打印（打印到 CLI，不是 Jupyter Notebook）存储在 data_key RDD中的所有记录：
如果现在切换到 CLI，你应该看到所有的记录都被打印出来了。注意，每一次记录打印的顺序很可能都是不同的。
