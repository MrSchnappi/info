---
title: 04 自然语言处理
toc: true
date: 2019-06-05
---
# 可以补充进来的

- 还是有很多没有看明白的。不过这书真挺好的，提纲挈领。


# 自然语言处理

自然语言处理(natural language processing，NLP)是让计算机能够使用人类语言，例如英语或法语。为了让简单的程序能够高效明确地解析，计算机程序通常读取和发出特殊化的语言。而自然语言通常是模糊的，并且可能不遵循形式的描述。自然语言处理中的应用如机器翻译，学习者需要读取一种人类语言的句子，并用另一种人类语言发出等同的句子。许多 NLP 应用程序基于语言模型，语言模型定义了关于自然语言中的字、字符或字节序列的概率分布。

与本章讨论的其他应用一样，非常通用的神经网络技术可以成功地应用于自然语言处理。然而，为了实现卓越的性能并扩展到大型应用程序，一些领域特定的策略也很重要。为了构建自然语言的有效模型，通常必须使用专门处理序列数据的技术。<span style="color:red;">什么处理序列数据的技术？lstm 吗？</span>

在很多情况下，我们将自然语言视为一系列词，而不是单个字符或字节序列。因为可能的词总数非常大，基于词的语言模型必须在极高维度和稀疏的离散空间上操作。为了使这种空间上的模型在计算和统计意义上都高效，研究者已经开发了几种策略。

## 1. n-gram

语言模型(language model)定义了自然语言中标记序列的概率分布。根据模型的设计，标记可以是词、字符甚至是字节。标记总是离散的实体。最早成功的语言模型基于固定长度序列的标记模型，称为 n-gram。一个 n-gram是一个包含 $n$ 个标记的序列。

基于 n-gram 的模型定义一个条件概率 - 给定前 $n-1$ 个标记后的第 $n$ 个标记的条件概率。该模型使用这些条件分布的乘积定义较长序列的概率分布：

$$
P\left(x_{1}, \ldots, x_{\tau}\right)=P\left(x_{1}, \ldots, x_{n-1}\right) \prod_{t=n}^{\tau} P\left(x_{t} | x_{t-n+1}, \ldots, x_{t-1}\right)\tag{12.5}
$$

这个分解可以由概率的链式法则证明。初始序列 $P\left(x_{1}, \dots, x_{n-1}\right)$ 的概率分布可以通过带有较小 $n$ 值的不同模型建模。

训练 $n$-gram 模型是简单的，因为最大似然估计可以通过简单地统计每个可能的 $n$-gram 在训练集中出现的次数来获得。几十年来，基于 $n$-gram 的模型都是统计语言模型的核心模块(Jelinek and Mercer,1980;Katz,1987;Chen and Goodman,1999)。<span style="color:red;">嗯。</span>

对于小的 $n$ 值，模型有特定的名称：$n=1$ 称为一元语法(unigram)，$n=2$ 称为二元语法(bigram)，$n=3$ 称为三元语法(trigram)。这些名称源于相应数字的拉丁前缀和希腊后缀“-gram”，分别表示所写之物。

通常我们同时训练 $n$-gram 模型和 $n-1$-gram 模型。这使得下式可以简单地通过查找两个存储的概率来计算。

$$
P\left(x_{t} | x_{t-n+1}, \ldots, x_{t-1}\right)=\frac{P_{n}\left(x_{t-n+1}, \ldots, x_{t}\right)}{P_{n-1}\left(x_{t-n+1}, \ldots, x_{t-1}\right)}\tag{12.6}
$$

为了在 $P_{n}$ 中精确地再现推断，我们训练 $P_{n-1}$ 时必须省略每个序列最后一个字符。

举个例子，我们演示三元模型如何计算句子“THE DOG RAN AWAY.”的概率。句子的第一个词不能通过上述条件概率的公式计算，因为句子的开头没有上下文。取而代之，在句子的开头我们必须使用词的边缘概率。<span style="color:red;">怎么使用词的边缘概率的？</span>因此我们计算 $P_3$(THE DOG RAN)。最后，可以使用条件分布 $P$(AWAY|DOG RAN)(典型情况)来预测最后一个词。将这与式(12.6)放在一起，我们得到：

<center>

![](http://images.iterate.site/blog/image/20190719/wua7JnkLEO16.png?imageslim){ width=55% }

</center>

<span style="color:red;">嗯呢，没错，n = 4,3,2 带进上面的式子就得到这个式子了。</span>


n-gram 模型最大似然的基本限制是，在许多情况下从训练集计数估计得到的 $P_{n}$ 很可能为零(即使元组($x_{t-n+1}, \dots, x_{t}$)可能出现在测试集中)。<span style="color:red;">是呀。</span>这可能会导致两种不同的灾难性后果。当 $P_{n-1}$ 为零时，该比率是未定义的，因此模型甚至不能产生有意义的输出。当 $P_{n-1}$ 非零而 $P_{n}$ 为零时，测试样本的对数似然为 $-\infty$ 。

怎么解决呢？

- 为避免这种灾难性的后果，大多数 n-gram 模型采用某种形式的平滑(smoothing)。平滑技术将概率质量从观察到的元组转移到类似的未观察到的元组。见 Chen and Goodman(1999)的综述和实验对比。其中一种基本技术基于向所有可能的下一个符号值添加非零概率质量。这个方法可以被证明，计数参数具有均匀或 Dirichlet先验的贝叶斯推断。<span style="color:red;">什么意思？怎么添加非零概率质量？什么是有 Dirichlet 先验的贝叶斯推断？</span>
- 另一个非常流行的想法是包含高阶和低阶 n-gram 模型的混合模型，其中高阶模型提供更多的容量，而低阶模型尽可能地避免零计数。如果上下文 $x_{t-n+k}, \dots, x_{t-1}$ 的频率太小而不能使用高阶模型，回退方法(back-off methods)就查找低阶 n-gram。更正式地说，它们通过上下文 $x_{t-n+k}, \dots, x_{t-1}$ 估计 $x_{t}$ 上的分布，并增加 $k$ 直到找到足够可靠的估计。<span style="color:red;">嗯，这个应该是可以的。</span>

经典的 n-gram 模型特别容易引起维数灾难。因为存在 $|\mathbb{V}|^{n}$ 可能的 n-gram，而且 $|\mathbb{V}|$ 通常很大。即使有大量训练数据和适当的 n，大多数 n-gram也不会出现在训练集中。经典 n-gram模型的一种观点是执行最近邻查询。换句话说，它可以被视为局部非参数预测器，类似于 k-最近邻。这些极端局部预测器面临的统计问题已经在第 5.11.2节中描述过。语言模型的问题甚至比普通模型更严重，因为任何两个不同的词在 one-hot 向量空间中的距离彼此相同。因此，难以大量利用来自任意“邻居”的信息——只有重复相同上下文的训练样本对局部泛化有用。为了克服这些问题，语言模型必须能够在一个词和其他语义相似的词之间共享知识。

为了提高 n-gram模型的统计效率，基于类的语言模型(class-based language model)(Brown et al.,1992; Ney and Kneser,1993; Niesler et al.,1998)引入词类别的概念，然后属于同一类别的词共享词之间的统计强度。这个想法使用了聚类算法，基于它们与其他词同时出现的频率，将该组词分成集群或类。随后，模型可以在条件竖杠的右侧使用词类 ID 而不是单个词 ID。<span style="color:red;">嗯，也挺好。</span>混合(或回退)词模型和类模型的复合模型也是可能的。尽管词类提供了在序列之间泛化的方式，但其中一些词被相同类的另一个替换，导致该表示丢失了很多信息。<span style="color:red;">嗯。</span>

## 2. 神经语言模型

神经语言模型(neural language model,NLM)是一类用来克服维数灾难的语言模型，它使用词的分布式表示对自然语言序列建模(Bengio et al.,2001b)。<span style="color:red;">什么是词的分布式表示？</span>不同于基于类的 n-gram模型，神经语言模型在能够识别两个相似的词，并且不丧失将每个词编码为彼此不同的能力。神经语言模型共享一个词(及其上下文)和其他类似词(和上下文之间)的统计强度。模型为每个词学习的分布式表示，允许模型处理具有类似共同特征的词来实现这种共享。例如，如果词 dog 和词 cat 映射到具有许多属性的表示，则包含词 cat 的句子可以告知模型对包含词 dog 的句子做出预测，反之亦然。因为这样的属性很多，所以存在许多泛化的方式，可以将信息从每个训练语句传递到指数数量的语义相关语句。维数灾难需要模型泛化到指数多的句子(指数相对句子长度而言)。该模型通过将每个训练句子与指数数量的类似句子相关联克服这个问题。<span style="color:red;">这段没有很明白。</span>

我们有时将这些词表示称为词嵌入(word embedding)。在这个解释下，我们将原始符号视为维度等于词表大小的空间中的点。词表示将这些点嵌入到较低维的特征空间中。在原始空间中，每个词由一个 one-hot向量表示，因此每对词彼此之间的欧氏距离都是 $\sqrt{2}$。在嵌入空间中，经常出现在类似上下文(或共享由模型学习的一些“特征”的任何词对)中的词彼此接近。这通常导致具有相似含义的词变得邻近。图 12.3 放大了学到的词嵌入空间的特定区域，我们可以看到语义上相似的词如何映射到彼此接近的表示。

<center>

![](http://images.iterate.site/blog/image/20190719/6Bh5BvBTIINM.png?imageslim){ width=55% }

</center>

> 图 12.3　从神经机器翻译模型获得的词嵌入的二维可视化(Bahdanau et al.,2015)。此图在语义相关词的特定区域放大，它们具有彼此接近的嵌入向量。国家在左图，数字在右图。注意，这些嵌入是为了可视化才表示为二维。在实际应用中，嵌入通常具有更高的维度并且可以同时捕获词之间多种相似性。<span style="color:red;">嗯。</span>

<span style="color:red;">嗯，这个还是理解的。</span>


其他领域的神经网络也可以定义嵌入。例如，卷积网络的隐藏层提供“图像嵌入”。因为自然语言最初不在实值向量空间上，所以 NLP 从业者通常对嵌入的这个想法更感兴趣。隐藏层在表示数据的方式上提供了更质变的戏剧性变化。<span style="color:red;">没明白，为什么卷积神经网络的隐藏层提供了图像嵌入？大概有点理解，但是理解不是很深。</span>

使用分布式表示来改进自然语言处理模型的基本思想不必局限于神经网络。它还可以用于图模型，其中分布式表示是多个潜变量的形式(Mnih and Hinton,2007)。<span style="color:red;">什么意思，怎么用于图模型？什么多个潜变量的形式？</span>

## 3. 高维输出

在许多自然语言应用中，通常希望我们的模型产生词(而不是字符)作为输出的基本单位。对于大词汇表，由于词汇量很大，在词的选择上表示输出分布的计算成本可能非常高。在许多应用中，$\mathbb{V}$ 包含数十万词。表示这种分布的朴素方法是应用一个仿射变换，将隐藏表示转换到输出空间，然后应用 softmax 函数。假设我们的词汇表 $\mathbb{V}$ 大小为 $|\mathbb{V}|$。因为其输出维数为 $|\mathbb{V}|$，描述该仿射变换线性分量的权重矩阵非常大。这造成了表示该矩阵的高存储成本，以及与之相乘的高计算成本。因为 softmax 要在所有 $|\mathbb{V}|$ 输出之间归一化，所以在训练时以及测试时执行全矩阵乘法是必要的——我们不能仅计算与正确输出的权重向量的点积。因此，输出层的高计算成本在训练期间(计算似然性及其梯度)和测试期间(计算所有或所选词的概率)都有出现。对于专门的损失函数，可以有效地计算梯度(Vincent et al.,2015)，但是应用于传统 softmax 输出层的标准交叉熵损失时会出现许多困难。<span style="color:red;">嗯。</span>

假设 $\boldsymbol{h}$ 是用于预测输出概率 $\hat{\boldsymbol{y}}$ 的顶部隐藏层。如果我们使用学到的权重 $\boldsymbol{W}$ 和学到的偏置 $\boldsymbol{b}$ 参数化从 $\boldsymbol{h}$ 到 $\hat{\boldsymbol{y}}$ 的变换，则仿射 softmax 输出层执行以下计算：

$$
a_{i}=b_{i}+\sum_{j} W_{i j} h_{j} \quad \forall i \in\{1, \ldots,|\mathbb{V}|\}\tag{12.8}
$$
$$
\hat{y}_{i}=\frac{e^{a_{i}}}{\sum_{i^{\prime}=1}^{|\mathbb{V}|} e^{a_{i^{\prime}}}}\tag{12.9}
$$

如果 $\boldsymbol{h}$ 包含 $n_{h}$ 个元素，则上述操作复杂度是 $O\left(|\mathbb{V}| n_{h}\right)$。在 $n_{h}$ 为数千和 $|\mathbb{V}|$ 数十万的情况下，这个操作占据了神经语言模型的大多数计算。<span style="color:red;">是的。</span>

### 3.1 使用短列表

第一个神经语言模型(Bengio et al.,2001b,2003)通过将词汇量限制为 10 000 或 20 000 来减轻大词汇表上 softmax 的高成本。Schwenk and Gauvain(2002)和 Schwenk(2007)在这种方法的基础上建立新的方式，将词汇表 $\mathbb{V}$ 分为最常见词汇(由神经网络处理)的短列表(shortlist) $\mathbb{L}$ 和较稀有词汇的尾列表 $\mathbb{T}=\mathbb{V} \backslash \mathbb{L}$ (由 n-gram 模型处理)。为了组合这两个预测，神经网络还必须预测在上下文 $C$ 之后出现的词位于尾列表的概率。我们可以添加额外的 sigmoid 输出单元估计 $P(i \in \mathbb{T} | C)$ 实现这个预测。<span style="color:red;">是的。</span>额外输出则可以用 $\mathbb{V}$ 来估计中所有词的概率分布，如下：

$$
\begin{aligned} P(y=i | C)=& 1_{i \in \mathbb{L}} P(y=i | C, i \in \mathbb{L})(1-P(i \in \mathbb{T} | C)) \\ &+1_{i \in \mathbb{T}} P(y=i | C, i \in \mathbb{T}) P(i \in \mathbb{T} | C) \end{aligned}\tag{12.10}
$$

其中 $P(y=i | C, i \in \mathbb{L})$ 由神经语言模型提供，$P(y=i | C, i \in \mathbb{T})$ 由 n-gram模型提供。<span style="color:red;">嗯嗯，很赞，想法和实现。</span>稍作修改，这种方法也可以在神经语言模型的 softmax 层中使用额外的输出值，而不是单独的 sigmoid 单元。

短列表方法的一个明显缺点是，神经语言模型的潜在泛化优势仅限于最常用的词，这大概是最没用的。<span style="color:red;">是呀。。常用词 n-gram 也可以处理的很好吧。不知道</span>这个缺点引发了处理高维输出替代方法的探索，如下所述。

### 3.2 分层 Softmax

<span style="color:red;">这一节没有怎么看懂。</span>

减少大词汇表 $\mathbb{V}$ 上高维输出层计算负担的经典方法(Goodman,2001)是分层地分解概率。$|\mathbb{V}|$ 因子可以降低到 $\log |\mathbb{V}|$ 一样低，而无须执行与 $|\mathbb{V}|$ 成比例数量(并且也与隐藏单元数量 $n_{h}$ 成比例)的计算。Bengio(2002)和 Morin and Bengio(2005)将这种因子分解方法引入神经语言模型中。<span style="color:red;">厉害</span>

我们可以认为这种层次结构是先建立词的类别，然后是词类别的类别，然后是词类别的类别的类别等。这些嵌套类别构成一棵树，其叶子为词。在平衡树中，树的深度为 $\log |\mathbb{V}|$ 。选择一个词的概率是由路径(从树根到包含该词叶子的路径)上的每个节点通向该词分支概率的乘积给出。图 12.4是一个简单的例子。Mnih and Hinton(2009)也描述了使用多个路径来识别单个词的方法，以便更好地建模具有多个含义的词。计算词的概率则涉及在导向该词所有路径上的求和。<span style="color:red;">有些没有很理解。</span>

<center>

![](http://images.iterate.site/blog/image/20190719/1g9NXnjvNS50.png?imageslim){ width=55% }

</center>

> 图 12.4: 词类别简单层次结构的示意图，其中 8 个词 $w_{0}, \dots, w_{7}$ 组织成三级层次结构。树的叶子表示实际特定的词。内部节点表示词的组别。任何节点都可以通过二值决策序列（0= 左，1=右）索引，从根到达节点。超类 $(0)$ 包含类 $(0,0)$ 和 (0, 1)，其中分别包含词 $\left\{w_{0}, w_{1}\right\}$ 和 $\left\{w_{2}, w_{3}\right\}$ 的集合，类似地超类 (1) 包含类 (1, 0) 和 (1, 1)，分别包含词 $\left\{w_{4}, w_{5}\right\}$ 和 $\left\{w_{6}, w_{7}\right\}$。如果树充分平衡，则最大深度（二值决策的数量）与词数 $|\mathbb{V}|$ 的对数同阶：从 $|\mathbb{V}|$ 个词中选一个词只需执行 $\mathcal{O}(\log |\mathbb{V}|)$ 次操作（从根开始的路径上的每个节点一次操作）。在该示例中，我们乘三次概率就能计算词 $y$ 的概率，这三次概率与从根到节点 $y$ 的路径上每个节点向左或向右的二值决策相关联。令 $b_{i}(y)$ 为遍历树移向 $y$ 时的第 $i$ 个二值决策。对输出 $y$ 进行采样的概率可以通过条件概率的链式法则分解为条件概率的乘积，其中每个节点由这些位的前缀索引。例如，节点 (1, 0) 对应于前缀 $\left(b_{0}\left(w_{4}\right)=1, b_{1}\left(w_{4}\right)=0\right)$，并且 $w_{4}$ 的概率可以如下分解：
>
> $$
> \begin{aligned} P\left(\mathrm{y}=w_{4}\right) &=P\left(\mathrm{b}_{0}=1, \mathrm{b}_{1}=0, \mathrm{b}_{2}=0\right) \\ &=P\left(\mathrm{b}_{0}=1\right) P\left(\mathrm{b}_{1}=0 | \mathrm{b}_{0}=1\right) P\left(\mathrm{b}_{2}=0 | \mathrm{b}_{0}=1, \mathrm{b}_{1}=0\right) \end{aligned}
> $$

为了预测树的每个节点所需的条件概率，我们通常在树的每个节点处使用逻辑回归模型，并且为所有这些模型提供与输入相同的上下文 $C$ 。因为正确的输出编码在训练集中，我们可以使用监督学习训练逻辑回归模型。我们通常使用标准交叉熵损失，对应于最大化正确判断序列的对数似然。

因为可以高效地计算输出对数似然(低至  $\log |\mathbb{V}|$ 而不是 $|\mathbb{V}|$)，所以也可以高效地计算梯度。这不仅包括关于输出参数的梯度，而且还包括关于隐藏层激活的梯度。

优化树结构最小化期望的计算数量是可能的，但通常不切实际。给定词的相对频率，信息理论的工具可以指定如何选择最佳的二进制编码。为此，我们可以构造树，使得与词相关联的位数量近似等于该词频率的对数。然而在实践中，节省计算通常事倍功半，因为输出概率的计算仅是神经语言模型中总计算的一部分。例如，假设有 $l$ 个全连接的宽度为 $n_h$ 的隐藏层。令 $n_{b}$ 是识别一个词所需比特数的加权平均值，其加权由这些词的频率给出。在这个例子中，计算隐藏激活所需的操作数增长为 $O\left(\ln _{h}^{2}\right)$，而输出计算增长为 $O\left(n_{h} n_{b}\right)$。只要 $n_{b} \leq l n_{h}$ ，我们可以通过收缩 $n_h$ 比收缩 $n_{b}$ 减少更多的计算量。事实上，$n_{b}$ 通常很小。因为词汇表的大小很少超过一百万，而 $\log _{2}\left(10^{6}\right) \approx 20$，所以可以将 $n_{b}$ 减小到大约 $20$，但 $n_h$ 通常大得多，大约为 $10^{3}$ 或更大。我们可以定义深度为 $2$ 和分支因子为 $\sqrt{|\mathbb{T}|}$ 的树，而不用仔细优化分支因子为 $2$ 的树。这样的树对应于简单定义一组互斥的词类。基于深度为 $2$ 的树的简单方法可以获得层级策略大部分的计算益处。

一个仍然有点开放的问题是如何最好地定义这些词类，或者如何定义一般的词层次结构。早期工作使用现有的层次结构(Morin and Bengio,2005)，但也可以理想地与神经语言模型联合学习层次结构。学习层次结构很困难。对数似然的精确优化似乎难以解决，因为词层次的选择是离散的，不适于基于梯度的优化。然而，我们可以使用离散优化来近似地最优化词类的分割。

分层 softmax 的一个重要优点是，它在训练期间和测试期间(如果在测试时我们想计算特定词的概率)都带来了计算上的好处。

当然即使使用分层 softmax，计算所有 $|\mathbb{V}|$ 个词概率的成本仍是很高的。另一个重要的操作是在给定上下文中选择最可能的词。不幸的是，树结构不能为这个问题提供高效精确的解决方案。

其缺点是在实践中，分层 softmax 倾向于更差的测试结果(相对基于采样的方法)，我们将在下文描述。这可能是因为词类选择得不好。

### 3.3 重要采样

<span style="color:red;">没看懂</span>


加速神经语言模型训练的一种方式是，避免明确地计算所有未出现在下一位置的词对梯度的贡献。每个不正确的词在此模型下具有低概率。枚举所有这些词的计算成本可能会很高。相反，我们可以仅采样词的子集。使用式(12.8)中引入的符号，梯度可以写成如下形式：

$$
\begin{aligned} \frac{\partial \log P(y | C)}{\partial \theta} &=\frac{\partial \log \operatorname{softmax}_{y}(a)}{\partial \theta} \\ &=\frac{\partial}{\partial \theta} \log \frac{e^{a_{y}}}{\sum_{i} e^{a_{i}}} \\ &=\frac{\partial}{\partial \theta}\left(a_{y}-\log \sum_{i} e^{a_{i}}\right) \\ &=\frac{\partial a_{y}}{\partial \theta}-\sum_{i} P(y=i | C) \frac{\partial a_{i}}{\partial \theta} \end{aligned}
$$

其中 $\boldsymbol{a}$ 是 presoftmax 激活(或得分)向量，每个词对应一个元素。第一项是正相(positive phase)项，推动 $a_{y}$ 向上；而第二项是负相(negative phase)项，对于所有 $i$ 以权重 $P(i | C)$ 推动 $a_{i}$ 向下。由于负相项是期望值，我们可以通过蒙特卡罗采样估计。然而，这将需要从模型本身采样。从模型中采样需要对词汇表中所有的 $i$ 计算 $P(i | C)$，这正是我们试图避免的。

我们可以从另一个分布中采样，而不是从模型中采样，这个分布称为提议分布(proposal distribution)(记为 q)，并通过适当的权重校正从错误分布采样引入的偏差(Bengio and S′en′ecal,2003; Bengio and S′en′ecal,2008)。这是一种称为重要采样(Importance Sampling)的更通用技术的应用，我们将在第 12.4.3.3节中更详细地描述。不幸的是，即使精确重要采样也不一定有效，因为我们需要计算权重 $p_{i} / q_{i}$ ，其中的 $p_{i}=P(i | C)$ 只能在计算所有得分 $a_{i}$ 后才能计算。这个应用采取的解决方案称为有偏重要采样，其中重要性权重被归一化加和为 $1$。当对负词 $n_{i}$ 进行采样时，相关联的梯度被加权为：

$$
w_{i}=\frac{p_{n_{i}} / q_{n_{i}}}{\sum_{j=1}^{N} p_{n_{j}} / q_{n_{j}}}
$$

这些权重用于对来自 $q$ 的 $m$ 个负样本给出适当的重要性，以形成负相估计对梯度的贡献：

$$
\sum_{i=1}^{|\mathbb{V}|} P(i | C) \frac{\partial a_{i}}{\partial \theta} \approx \frac{1}{m} \sum_{i=1}^{m} w_{i} \frac{\partial a_{n_{i}}}{\partial \theta}
$$

一元语法或二元语法分布与提议分布 $q$ 工作得一样好。从数据估计这种分布的参数是很容易的。在估计参数之后，也可以非常高效地从这样的分布采样。

重要采样(Importance Sampling)不仅可以加速具有较大 softmax 输出的模型。更一般地，它可以加速具有大稀疏输出层的训练，其中输出是稀疏向量而不是 $n$ 选 $1$。其中一个例子是词袋(bag of words)。词袋具有稀疏向量 $\boldsymbol{v}$ ，其中 $v_{i}$ 表示词汇表中的词 $i$ 存不存在文档中。或者，$v_{i}$ 可以指示词 $i$ 出现的次数。由于各种原因，训练产生这种稀疏向量的机器学习模型的成本可能很高。在学习的早期，模型可能不会真的使输出真正稀疏。此外，将输出的每个元素与目标的每个元素进行比较，可能是描述训练的损失函数最自然的方式。这意味着稀疏输出并不一定能带来计算上的好处，因为模型可以选择使大多数输出非零，并且所有这些非零值需要与相应的训练目标进行比较(即使训练目标是零)。Dauphin et al.(2011)证明可以使用重要采样加速这种模型。高效算法最小化“正词”(在目标中非零的那些词)和相等数量的“负词”的重构损失。负词是被随机选取的，如使用启发式采样更可能被误解的词。该启发式过采样引入的偏差则可以使用重要性权重校正。

在所有这些情况下，输出层梯度估计的计算复杂度被减少为与负样本数量成比例，而不是与输出向量的大小成比例。

### 3.4 噪声对比估计和排名损失

<span style="color:red;">没仔细看</span>

为减少训练大词汇表的神经语言模型的计算成本，研究者也提出了其他基于采样的方法。早期的例子是 Collobert and Weston(2008a)提出的排名损失，将神经语言模型每个词的输出视为一个得分，并试图使正确词的得分 $a_{y}$ 比其他词 $a_{i}$ 排名更高。提出的排名损失则是

$$
L=\sum_{i} \max \left(0,1-a_{y}+a_{i}\right)
$$

如果观察到词的得分 $a_{y}$ 远超过负词的得分 $a_{i}$ (相差大于 1)，则第 $i$ 项梯度为零。这个准则的一个问题是它不提供估计的条件概率，条件概率在很多应用中是有用的，包括语音识别和文本生成(包括诸如翻译的条件文本生成任务)。

最近用于神经语言模型的训练目标是噪声对比估计，将在第 18.6节中介绍。这种方法已成功应用于神经语言模型(Mnih and Teh,2012;Mnih and Kavukcuoglu,2013)。

## 4. 结合 n-gram 和神经语言模型

n-gram模型相对神经网络的主要优点是 n-gram模型具有更高的模型容量(通过存储非常多的元组的频率)，并且处理样本只需非常少的计算量(通过查找只匹配当前上下文的几个元组)。如果我们使用哈希表或树来访问计数，那么用于 n-gram的计算量几乎与容量无关。相比之下，将神经网络的参数数目加倍通常也大致加倍计算时间。当然，避免每次计算时使用所有参数的模型是一个例外。嵌入层每次只索引单个嵌入，所以我们可以增加词汇量，而不会增加每个样本的计算时间。一些其他模型，例如平铺卷积网络，可以在减少参数共享程度的同时添加参数以保持相同的计算量。<span style="color:red;">什么是平铺卷积网络？</span>然而，基于矩阵乘法的典型神经网络层需要与参数数量成比例的计算量。

因此，增加容量的一种简单方法是将两种方法结合，由神经语言模型和 n-gram 语言模型组成集成(Bengio et al.,2001b,2003)。

对于任何集成，如果集成成员产生独立的错误，这种技术可以减少测试误差。集成学习领域提供了许多方法来组合集成成员的预测，包括统一加权和在验证集上选择权重。Mikolov　et al.(2011a)扩展了集成，不是仅包括两个模型，而是包括大量模型。我们也可以将神经网络与最大熵模型配对并联合训练(Mikolov et al.,2011b)。<span style="color:red;">怎么联合训练的？</span>该方法可以被视为训练具有一组额外输入的神经网络，额外输入直接连接到输出并且不连接到模型的任何其他部分。额外输入是输入上下文中特定 n-gram 是否存在的指示器，因此这些变量是非常高维且非常稀疏的。<span style="color:red;">没有很明白，到底是怎么实现的？</span>

模型容量的增加是巨大的(架构的新部分包含高达 $|s V|^{n}$ 个参数)，但是处理输入所需的额外计算量是很小的(因为额外输入非常稀疏)。

## 5. 神经机器翻译

机器翻译以一种自然语言读取句子并产生等同含义的另一种语言的句子。机器翻译系统通常涉及许多组件。在高层次，一个组件通常会提出许多候选翻译。由于语言之间的差异，这些翻译中的许多翻译是不符合语法的。例如，许多语言在名词后放置形容词，因此直接翻译成英语时，它们会产生诸如“apple red”的短语。提议机制提出建议翻译的许多变体，理想情况下应包括“red apple”。翻译系统的第二个组成部分(语言模型)评估提议的翻译，并可以评估“red apple”比“apple red”更好。<span style="color:red;">嗯，都是这种组件形式吗？</span>

最早的机器翻译神经网络探索中已经纳入了编码器和解码器的想法(Allen 1987; Chris-man 1991; Forcada and -Neco 1997)，而翻译中神经网络的第一个大规模有竞争力的用途是通过神经语言模型升级翻译系统的语言模型(Schwenk et al.,2006;Schwenk,2010)。之前，大多数机器翻译系统在该组件使用 n-gram模型。机器翻译中基于 n-gram的模型不仅包括传统的回退 n-gram模型(Jelinek and Mercer,1980; Katz,1987; Chen and Goodman,1999)，而且包括最大熵语言模型(maximum entropy language models)(Berger et al.,1996)，其中给定上下文中常见的词，affine-softmax层预测下一个词。

传统语言模型仅仅报告自然语言句子的概率。因为机器翻译涉及给定输入句子产生输出句子，所以将自然语言模型扩展为条件的是有意义的。如第 6.2.1.1节所述，可以直接地扩展一个模型，该模型定义某些变量的边缘分布，以便在给定上下文 $C$ ($C$ 可以是单个变量或变量列表)的情况下定义该变量的条件分布。Devlin et al.(2014)在一些统计机器翻译的基准中击败了最先进的技术，他给定源语言中的短语 $\mathrm{s}_{1}, \mathrm{s}_{2}, \ldots, \mathrm{s}_{k}$ 后使用 MLP 对目标语言的短语 $t_{1}, t_{2}, \ldots, t_{k}$ 进行评分。这个 MLP 估计 $P\left(t_{1}, t_{2}, \ldots, t_{k} | s_{1}, s_{2}, \dots, s_{k}\right)$。这个 MLP 的估计替代了条件 n-gram模型提供的估计。

基于 MLP 方法的缺点是需要将序列预处理为固定长度。为了使翻译更加灵活，我们希望模型允许可变的输入长度和输出长度。RNN具备这种能力。第 10.2.4节描述了给定某些输入后，关于序列条件分布 RNN 的几种构造方法，并且第 10.4节描述了当输入是序列时如何实现这种条件分布。<span style="color:red;">嗯嗯，想知道。</span>在所有情况下，一个模型首先读取输入序列并产生概括输入序列的数据结构。我们称这个概括为“上下文” $C$。上下文 $C$ 可以是向量列表，或者向量或张量。读取输入以产生 $C$ 的模型可以是 RNN(Cho et al.,2014b;Sutskever et al.,2014;Jean et al.,2014)或卷积网络(Kalchbrenner and Blunsom,2013)。另一个模型(通常是 RNN)，则读取上下文 $C$ 并且生成目标语言的句子。在图 12.5中展示了这种用于机器翻译的编码器-解码器框架的总体思想。


<center>

![](http://images.iterate.site/blog/image/20190719/KXq4GXE3HiJS.png?imageslim){ width=55% }

</center>


> 图 12.5: 编码器-解码器架构在直观表示（例如词序列或图像）和语义表示之间来回映射。使用来自一种模态数据的编码器输出（例如从法语句子到捕获句子含义的隐藏表示的编码器映射）作为用于另一模态的解码器输入（如解码器将捕获句子含义的隐藏表示映射到英语），我们可以训练将一种模态转换到另一种模态的系统。这个想法已经成功应用于很多领域，不仅仅是机器翻译，还包括为图像生成标题。<span style="color:red;">嗯嗯，有些赞~ 再理解下。</span>

为生成以源句为条件的整句，模型必须具有表示整个源句的方式。早期模型只能表示单个词或短语。从表示学习的观点来看，具有相同含义的句子具有类似表示是有用的，无论它们是以源语言还是以目标语言书写。研究者首先使用卷积和 RNN 的组合探索该策略(Kalchbrenner and Blunsom,2013)。后来的工作介绍了使用 RNN 对所提议的翻译进行打分(Cho et al.,2014b)或生成翻译句子(Sutskever et al.,2014)。Jean et al.(2014)将这些模型扩展到更大的词汇表。

### 5.1 使用注意力机制并对齐数据片段

使用固定大小的表示概括非常长的句子(例如 60 个词)的所有语义细节是非常困难的。这需要使用足够大的 RNN，并且用足够长的时间训练得很好才能实现，如 Cho et al.(2014b)和 Sutskever et al.(2014)所表明的。然而，更高效的方法是先读取整个句子或段落(以获得正在表达的上下文和焦点)，然后一次翻译一个词，每次聚焦于输入句子的不同部分来收集产生下一个输出词所需的语义细节。这正是 Bahdanau et al.(2015)第一次引入的想法。<span style="color:red;">厉害呀，要怎么实现呢？</span>图 12.6中展示了注意力机制，其中每个时间步关注输入序列的特定部分。

<center>

![](http://images.iterate.site/blog/image/20190719/lsWRMm3y6Lj5.png?imageslim){ width=55% }

</center>

> 图 12.6: 由 Bahdanau et al. (2015) 引入的现代注意力机制，本质上是加权平均。注意力机制对具有权重 $\alpha^{(t)}$ 的特征向量 $\boldsymbol{h}^{(t)}$ 进行加权平均形成上下文向量 $\boldsymbol{c}$。在一些应用中，特征向量 $\boldsymbol{h}$ 是神经网络的隐藏单元，但它们也可以是模型的原始输入。权重 $\alpha^{(t)}$ 由模型本身产生。它们通常是区间 `[0, 1]` 中的值，并且旨在仅仅集中在单个 $\boldsymbol{h}^{(t)}$ 周围，使得加权平均精确地读取接近一个特定时间步的特征向量。权重 $\alpha^{(t)}$ 通常由模型另一部分发出的相关性得分应用 softmax 函数后产生。注意力机制在计算上需要比直接索引期望的 $\boldsymbol{h}^{(t)}$ 付出更高的代价，但直接索引不能使用梯度下降训练。基于加权平均的注意力机制是平滑、可微的近似，可以使用现有优化算法训练。


我们可以认为基于注意力机制的系统有三个组件：

- 读取器读取原始数据(例如源语句中的源词)并将其转换为分布式表示，其中一个特征向量与每个词的位置相关联。
- 存储器存储读取器输出的特征向量列表。这可以被理解为包含事实序列的存储器，而之后不必以相同的顺序从中检索，也不必访问全部。
- 最后一个程序利用存储器的内容顺序地执行任务，每个时间步聚焦于某个存储器元素的内容(或几个，具有不同权重)。

第三组件可以生成翻译语句。

当用一种语言书写的句子中的词与另一种语言的翻译语句中的相应词对齐时，可以使对应的词嵌入相关联。早期的工作表明，我们可以学习将一种语言中的词嵌入与另一种语言中的词嵌入相关联的翻译矩阵(etal.,2014)，与传统的基于短语表中频率计数的方法相比，可以产生较低的对齐错误率。更早的工作(Klementievetal.,2012)也对跨语言词向量进行了研究。这种方法的存在很多的扩展。例如，允许在更大数据集上训练的更高效的跨语言对齐(Gouwsetal.,2014)。

## 6. 历史展望

在对反向传播的第一次探索中，Rumelhart et al.(1986a)等人提出了分布式表示符号的思想，其中符号对应于族成员的身份，而神经网络捕获族成员之间的关系，训练样本形成三元组如(Colin、Mother、Victoria)。神经网络的第一层学习每个族成员的表示。例如，Colin的特征可能代表 Colin 所在的族树，他所在树的分支，他来自哪一代等等。我们可以将神经网络认为是将这些属性关联在一起的计算学习规则，可以获得期望预测。模型则可以进行预测，例如推断谁是 Colin 的母亲。

Deerwester et al.(1990)将符号嵌入的想法扩展到对词的嵌入。这些嵌入使用 SVD 学习。之后，嵌入将通过神经网络学习。

自然语言处理的历史是由流行表示(对模型输入不同方式的表示)的变化为标志的。在早期对符号和词建模的工作之后，神经网络在 NLP 上一些最早的应用(Miikkulainen and Dyer,1991;Schmidhuber,1996)将输入表示为字符序列。

Bengio et al.(2001b)将焦点重新引到对词建模并引入神经语言模型，能产生可解释的词嵌入。这些神经模型已经从在一小组符号上的定义表示(20世纪 80 年代)扩展到现代应用中的数百万字(包括专有名词和拼写错误)。这种计算扩展的努力导致了第 12.4.3节中描述的技术发明。

最初，使用词作为语言模型的基本单元可以改进语言建模的性能(Bengio et al.,2001b)。而今，新技术不断推动基于字符(Sutskever et al.,2011)和基于词的模型向前发展，最近的工作(Gillick et al.,2015)甚至建模 Unicode 字符的单个字节。

神经语言模型背后的思想已经扩展到多个自然语言处理应用，如解析(Henderson,2003,2004;Collobert,2011)、词性标注、语义角色标注、分块等，有时使用共享词嵌入的单一多任务学习架构(Collobert and Weston,2008a;Collobert et al.,2011a)。

随着 t-SNE降维算法的发展(vander Maaten and Hinton,2008)以及 Joseph Turian在 2009 年引入的专用于可视化词嵌入的应用，用于分析语言模型嵌入的二维可视化成为一种流行的工具。



# 相关

- 《深度学习》花书
