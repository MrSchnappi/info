---
title: 05 文本表示模型
toc: true
date: 2019-03-23
---
# 文本表示模型


文本是一类非常重要的非结构化数据，如何表示文本数据一直是机器学习领域的一个重要研究方向。



## 有哪些文本表示模型？它们各有什么优缺点？

主要有这些文本表示模型：

- 词袋模型（Bag of Words） TF-IDF（Term Frequency-Inverse Document Frequency）
- 主题模型（Topic Model）
- 词嵌入模型（Word Embedding）

**词袋模型和 N-gram模型**

最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。

常用 TF-IDF来计算权重，公式为：

$$TF-IDF(t,d)=TF(t,d)×IDF(t)$$，


其中 $TF(t,d)$ 为单词 t 在文档 d 中出现的频率，$IDF(t)$ 是逆文档频率，用来衡量单词 t 对表达语义所起的重要性，表示为

$$IDF(t)=log\frac{文章总数}{包含单词 t 的文章总数+1}$$ ．

直观的解释是，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分某篇文章特殊语义的贡献较小，因此对权重做一定惩罚。<span style="color:red;">为什么是一个 log 的惩罚？</span>


将文章进行单词级别的划分有时候并不是一种好的做法，比如英文中的 natural language processing（自然语言处理）一词，如果将 natural，language，processing这 3 个词拆分开来，所表达的含义与三个词连续出现时大相径庭。通常，可以将连续出现的 n 个词（n≤N）组成的词组（N-gram）也作为一个单独的特征放到向量表示中去，构成 N-gram模型。

另外，同一个词可能有多种词性变化，却具有相似的含义。在实际应用中，一般会对单词进行词干抽取（Word Stemming）处理，即将不同词性的单词统一成为同一词干的形式。<span style="color:red;">词干抽取是怎么做的？</span>

**主题模型**

主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。<span style="color:red;">这个地方补充下。</span>

**词嵌入与深度学习模型**

词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（通常 K=50~300维）上的一个稠密向量（Dense Vector）。K维空间的每一维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。<span style="color:red;">这种映射是怎么做的？忘记了。</span>

由于词嵌入将每个词映射成一个 K 维的向量，如果一篇文档有 N 个词，就可以用一个 N×K维的矩阵来表示这篇文档，但是这样的表示过于底层。

在实际应用中，如果仅仅把这个矩阵作为原文本的表示特征输入到机器学习模型中，通常很难得到令人满意的结果。因此，还需要在此基础之上加工出更高层的特征。

在传统的浅层机器学习模型中，一个好的特征工程往往可以带来算法效果的显著提升。而深度学习模型正好为我们提供了一种自动地进行特征工程的方式，模型中的每个隐层都可以认为对应着不同抽象层次的特征。从这个角度来讲，深度学习模型能够打败浅层模型也就顺理成章了。<span style="color:red;">怎么就顺理成章了？</span>

卷积神经网络和循环神经网络的结构在文本表示中取得了很好的效果，主要是由于它们能够更好地对文本进行建模，抽取出一些高层的语义特征。<span style="color:red;">这个地方讲的是啥？？怎么就是能更好的对文本进行建模了？怎么就抽取一些高层的语义特征了？到底是什么高层的语义特征？</span>与全连接的网络结构相比，卷积神经网络和循环神经网络一方面很好地抓住了文本的特性，另一方面又减少了网络中待学习的参数，提高了训练速度，并且降低了过拟合的风险。



# 相关

- 《百面机器学习》
