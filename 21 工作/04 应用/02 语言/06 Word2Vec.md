---
title: 06 Word2Vec
toc: true
date: 2019-03-23
---
# Word2Vec


谷歌 2013 年提出的 Word2Vec 是目前最常用的词嵌入模型之一。<span style="color:red;">现在的 Word2Vec 还是最常用的词嵌入模型吗？</span>

Word2Vec实际是一种浅层的神经网络模型，它有两种网络结构，分别是 CBOW（Continues Bag of Words）和 Skip-gram。

## Word2Vec是如何工作的？


- CBOW 的目标是根据上下文出现的词语来预测当前词的生成概率。如下图 (a) 所示。<span style="color:red;">CBOW 是什么的缩写？</span><span style="color:blue;">哦，上面写了，是 Continues Bag of Words。 </span>
- Skip-gram 是根据当前词来预测上下文中各词的生成概率，如下图 (b) 所示。

![](http://images.iterate.site/blog/image/20190323/B6MFg7oOTHeS.png?imageslim){ width=55% }

<span style="color:red;">以前看过，不过没怎么认真看，现在看到真的有些厉害。</span>

其中 w（t）是当前所关注的词，w（t−2）、w（t−1）、w（t＋1）、w（t＋2）是上下文中出现的词。这里前后滑动窗口大小均设为 2。

CBOW 和 Skip-gram 都可以表示成由输入层（Input）、映射层（Projection）和输出层（Output）组成的神经网络。

输入层中的每个词由独热编码方式表示，即所有词均表示成一个 N 维向量，其中 N 为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为 1，其余维度的值均设为 0。

在映射层（又称隐含层）中，K个隐含单元（Hidden Units）的取值可以由 N 维输入向量以及连接输入和隐含单元之间的 N×K 维权重矩阵计算得到。在 CBOW 中，还需要将各个输入词所计算出的隐含单元求和。<span style="color:red;">各个输入词所计算出的隐含单元求和？嗯，是的。</span>

同理，输出层向量的值可以通过隐含层向量（K维），以及连接隐含层和输出层之间的 K×N维权重矩阵计算得到。输出层也是一个 N 维向量，每维与词汇表中的一个单词相对应。最后，对输出层向量应用 Softmax 激活函数，可以计算出每个单词的生成概率。<span style="color:red;">嗯。</span>

Softmax激活函数的定义为


$$P(y=w_n|x)=\frac{e^{x_n}}{\sum_{k=1}^{N}e^{x_k}}$$


其中 $x$ 代表 $N$ 维的原始输出向量，$x_n$ 为在原始输出向量中，与单词 $w_n$ 所对应维度的取值。


接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化。<span style="color:red;">使得语料库中所有单词的整体生成概率最大化？没有很明白？</span>

从输入层到隐含层需要一个维度为 N×K的权重矩阵，从隐含层到输出层又需要一个维度为 K×N的权重矩阵，学习权重可以用反向传播算法实现，每次迭代时将权重沿梯度更优的方向进行一小步更新。

但是由于 Softmax 激活函数中存在归一化项的缘故，推导出来的迭代公式需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢，由此产生了 Hierarchical Softmax和 Negative Sampling两种改进方法，有兴趣的读者可以参考 Word2Vec 的原论文[3]。训练得到维度为 N×K和 K×N的两个权重矩阵之后，可以选择其中一个作为 N 个词的 K 维向量表示。<span style="color:red;">这两种改进方法还是要理解下的。</span>

## Word2Vec 与 LDA 有什么区别和联系？

谈到 Word2Vec 与 LDA 的区别和联系，首先，LDA是利用文档中单词的共现关系来对单词按主题聚类，也可以理解为对“文档-单词”矩阵进行分解，得到“文档-主题”和“主题-单词”两个概率分布。而 Word2Vec 其实是对“上下文-单词”矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了上下文共现的特征。<span style="color:red;">嗯，是的。</span>也就是说，如果两个单词所对应的 Word2Vec 向量相似度较高，那么它们很可能经常在同样的上下文中出现。

需要说明的是，上述分析的是 LDA 与 Word2Vec 的不同，不应该作为主题模型和词嵌入两类方法的主要差异。

主题模型通过一定的结构调整可以基于“上下文-单词”矩阵进行主题推理。同样地，词嵌入方法也可以根据“文档-单词”矩阵学习出词的隐含向量表示。

主题模型和词嵌入两类方法最大的不同其实在于模型本身，主题模型是一种基于概率图模型的生成式模型，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测的隐含变量（即主题）；而词嵌入模型一般表达为神经网络的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的稠密向量表示。<span style="color:red;">嗯，是的。不过这个地方还有点似懂非懂。要再补充下，对每一种方法都有深刻的理解。</span>




# 相关

- 《百面机器学习》
