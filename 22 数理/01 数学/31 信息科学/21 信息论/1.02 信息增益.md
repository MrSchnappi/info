---
title: 1.02 信息增益
toc: true
date: 2019-09-03
---

# 信息增益的理解

定义：以某特征划分数据集前后的熵的差值。

熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合 $D$ 划分效果的好坏。  ​

假设划分前样本集合 $D$ 的熵为 $H(D)$。使用某个特征 $A$ 划分数据集 $D$，计算划分后的数据子集的熵为 $H(D|A)$。

则信息增益为：

$$
g(D,A)=H(D)-H(D|A)
$$

<span style="color:red;">一直感觉这种分割前减去分割后的熵的形式有点违和。</span>

​注：在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集 $D$。

思想：计算所有特征划分数据集 $D$，得到多个特征划分数据集 $D$ 的信息增益，从这些信息增益中选择最大的，因而当前结点的划分特征便是使信息增益最大的划分所使用的特征。

另外这里提一下信息增益比相关知识：

$$
信息增益比=惩罚参数 \times 信息增益
$$

信息增益比本质：在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。<span style="color:red;">为什么特征个数较多的时候，惩罚参数较小？</span>

惩罚参数：数据集 $D$ 以特征 $A$ 作为随机变量的熵的倒数。<span style="color:red;">这句话通顺吗？惩罚参数是这个吗？</span>





# 相关

- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions) 原文
